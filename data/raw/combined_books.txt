============================================================
Introduction to Classical and Quantum Computing (Wong)
============================================================

Thomas G. Wong
Introduction to Classical and
Quantum Computing
Copyright © 2022 Thomas Wong
All rights reserved. No part of this publication may be reproduced, distributed, or
transmitted in any form or by any means, incuding photocopying, recording, or other
electronic or mechanical methods, without the prior written permission of the pub-
lisher, except in the case of brief quotations embodied in critical reviews and certain
other noncommerical uses permitted by copyright law.
ISBN: 979-8-9855931-0-5 (Paperback)
ISBN: 979-8-9855931-1-2 (Hardcover)
Library of Congress Control Number: 2022900358
Book design by Thomas Wong
www.thomaswong.net
Published by Rooted Grove, Omaha, Nebraska
www.rootedgrove.com
3 4 5 6 7 8 9 10
For my students.
Preface
Dear Reader,
This textbook for newcomers who are interested in quantum computing as a po-
tential career, but who may not be ready for advanced books or courses. The only
prerequisite for this book is trigonometry, also called pre-calculus. You are not ex-
pected to have taken advanced math beyond that, and you are not expected to have
experience with programming. So, if you are an advanced high school student or a
beginning university student, this textbook is for you.
That said, this book is not merely a conceptual overview of quantum computing.
I will teach the math and programming skills that may be missing. Since you are
interested in quantum computing as a potential career, I want to equip you with the
skills you will need for more advanced topics.
If you are more advanced, and especially if you have already studied linear al-
gebra, then you may find this textbook too elementary. For a more mathematically
rigorous introduction to quantum information science, I refer you to Quantum Com-
putation and Quantum Information by Michael Nielsen and Isaac Chuang, affec-
tionately called “Mike and Ike,” like the chewy, fruit candy with the same name. It
is the standard advanced text, and for good reason.
I hope this textbook will help you realize that you can do it, that you can under-
stand quantum computing. I hope it will inspire you to study quantum computing
more deeply, and I hope that some of you might even choose quantum computing
as a career. If so, I look forward to calling you colleagues and learning from your
discoveries.
This textbook stemmed from an introductory special-topics course that I taught
at Creighton University, and I thank each class of students for sharing the journey of
developing and refining the course content. I must also thank those who have taught
me quantum computing in both formal and informal roles. I could not have done it
without you.
Tom Wong
v
Contents
1
Classical Information and Computation . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.1
Coins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.2
Dice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.3
Encoding Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.1.4
Physical Bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.5
Binary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.6
ASCII. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.2
Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.2.1
Single-Bit Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.2.2
Two-Bit Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.2.3
Logic Gates as Physical Circuits. . . . . . . . . . . . . . . . . . . . . . . . 15
1.2.4
Multiple Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.2.5
Universal Gates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.3
Adders and Verilog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.3.1
Adding Binary Numbers by Hand . . . . . . . . . . . . . . . . . . . . . . 27
1.3.2
Half Adder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.3.3
Full Adder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
1.3.4
Ripple-Carry Adder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.3.5
Ripple-Carry with Full Adders . . . . . . . . . . . . . . . . . . . . . . . . . 36
1.3.6
Circuit Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.4
Circuit Simplification and Boolean Algebra. . . . . . . . . . . . . . . . . . . . . 37
1.4.1
Order of Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
1.4.2
Association, Commutivity, and Distribution . . . . . . . . . . . . . . 38
1.4.3
Identities Involving Zero and One . . . . . . . . . . . . . . . . . . . . . . 39
1.4.4
Single-Variable Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.4.5
Two-Variable Identities and De Morgan’s Laws . . . . . . . . . . . 40
1.4.6
Circuit Simplification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
1.5
Reversible Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
1.5.1
Reversible Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
1.5.2
Irreversible Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
vii
viii
Contents
1.5.3
Toffoli Gate: A Reversible AND Gate . . . . . . . . . . . . . . . . . . . 46
1.5.4
Making Irreversible Gates Reversible . . . . . . . . . . . . . . . . . . . 48
1.6
Error Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
1.6.1
Errors in Physical Devices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
1.6.2
Error Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
1.6.3
Error Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
1.7
Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
1.7.1
Asymptotic Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
1.7.2
Complexity Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
1.8
Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
1.8.1
Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
1.8.2
Incrementing Binary Numbers . . . . . . . . . . . . . . . . . . . . . . . . . 64
1.8.3
Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
1.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
2
One Quantum Bit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.1
Qubit Touchdown: A Quantum Computing Board Game . . . . . . . . . . 73
2.2
Superposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.2.1
Zero or One . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.2.2
Superposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
2.2.3
Review of Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 80
2.3
Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
2.3.1
Measurement in the Z-Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
2.3.2
Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
2.3.3
Measurement in Other Bases. . . . . . . . . . . . . . . . . . . . . . . . . . . 86
2.3.4
Consecutive Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
2.4
Bloch Sphere Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
2.4.1
Global and Relative Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
2.4.2
Spherical Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
2.4.3
Cartesian Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
2.5
Physical Qubits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
2.6
Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
2.6.1
Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
2.6.2
Classical Reversible Gates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
2.6.3
Common One-Qubit Quantum Gates . . . . . . . . . . . . . . . . . . . . 102
2.6.4
General One-Qubit Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
2.7
Quantum Circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
2.7.1
Circuit Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
2.7.2
Quirk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
2.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3
Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
3.1
Quantum States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
3.1.1
Column Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
3.1.2
Row Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
Contents
ix
3.2
Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
3.2.1
Inner Products Are Scalars . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
3.2.2
Orthonormality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.2.3
Projection, Measurement, and Change of Basis . . . . . . . . . . . 120
3.3
Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.3.1
Gates as Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.3.2
Common One-Qubit Gates as Matrices . . . . . . . . . . . . . . . . . . 127
3.3.3
Sequential Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.3.4
Circuit Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
3.3.5
Unitarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
3.3.6
Reversibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
3.4
Outer Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.4.1
Outer Products Are Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.4.2
Completeness Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
3.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
4
Multiple Quantum Bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
4.1
Entanglion: A Quantum Computing Board Game . . . . . . . . . . . . . . . . 137
4.1.1
Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
4.1.2
Connection to Quantum Computing . . . . . . . . . . . . . . . . . . . . . 139
4.2
States and Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.2.1
Tensor Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.2.2
Kronecker Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
4.2.3
Measuring Individual Qubits . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.2.4
Sequential Single-Qubit Measurements . . . . . . . . . . . . . . . . . . 146
4.3
Entanglement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
4.3.1
Product States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
4.3.2
Entangled States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
4.4
Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.4.1
One-Qubit Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.4.2
Two-Qubit Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
4.4.3
Toffoli Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
4.4.4
No-Cloning Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
4.5
Quantum Adders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.5.1
Classical Adder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.5.2
Making the Classical Adder a Quantum Gate . . . . . . . . . . . . . 168
4.5.3
Quantum Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
4.5.4
Quantum Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
4.5.5
Quantum Carry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
4.5.6
Quantum Ripple-Carry Adder . . . . . . . . . . . . . . . . . . . . . . . . . . 175
4.5.7
Circuit Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
4.5.8
Adding in Superposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
4.6
Universal Quantum Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
4.6.1
Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
4.6.2
Components of a Universal Gate Set . . . . . . . . . . . . . . . . . . . . 185
x
Contents
4.6.3
Examples of Universal Gate Sets . . . . . . . . . . . . . . . . . . . . . . . 186
4.6.4
Solovay-Kitaev Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
4.6.5
Quantum Computing without Complex Numbers. . . . . . . . . . 187
4.7
Quantum Error Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
4.7.1
Decoherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
4.7.2
Bit-Flip Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.7.3
Phase-Flip Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4.7.4
Shor Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
4.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
5
Quantum Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
5.1
IBM Quantum Experience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
5.1.1
Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
5.1.2
Circuit Composer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
5.1.3
Quantum Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5.1.4
Simulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.2
Quantum Assembly Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
5.2.1
OpenQASM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
5.2.2
Quantum Experience Standard Header . . . . . . . . . . . . . . . . . . 221
5.2.3
OpenQASM in IBM Quantum Experience . . . . . . . . . . . . . . . 222
5.2.4
Quantum Adder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
5.3
Qiskit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
5.3.1
Circuit Composer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
5.3.2
Quantum Lab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
5.3.3
Simulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
5.3.4
Quantum Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
5.4
Other Quantum Programming Languages . . . . . . . . . . . . . . . . . . . . . . 236
5.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6
Entanglement and Quantum Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
6.1
Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
6.1.1
Product States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.1.2
Maximally Entangled States . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.1.3
Partially Entangled States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.2
Bell Inequalities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.2.1
EPR Paradox and Local Hidden Variables . . . . . . . . . . . . . . . . 240
6.2.2
Bell Inequalities and the CHSH Inequality . . . . . . . . . . . . . . . 241
6.2.3
Quantum Processor Experiment . . . . . . . . . . . . . . . . . . . . . . . . 246
6.2.4
Other Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
6.2.5
No-Signaling Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
6.2.6
Other Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
6.3
Monogamy of Entanglement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
6.3.1
Classical Correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
6.3.2
Quantum Entanglement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
6.4
Superdense Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
Contents
xi
6.4.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
6.4.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
6.4.3
Quantum Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
6.5
Quantum Teleportation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
6.5.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
6.5.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
6.5.3
Quantum Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
6.6
Quantum Key Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
6.6.1
Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
6.6.2
Classical Solution: Public Key Cryptography . . . . . . . . . . . . . 263
6.6.3
Quantum Solution: BB84 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
6.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
7
Quantum Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.1
Circuit vs Query Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.1.1
Circuit Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.1.2
Query Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
7.1.3
Quantum Oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
7.1.4
Phase Oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
7.2
Parity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
7.2.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
7.2.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
7.2.3
Quantum Solution: Deutsch’s Algorithm . . . . . . . . . . . . . . . . . 278
7.2.4
Generalization to Additional Bits . . . . . . . . . . . . . . . . . . . . . . . 280
7.3
Constant vs Balanced Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.3.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.3.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
7.3.3
Quantum Solution: Deutsch-Jozsa Algorithm . . . . . . . . . . . . . 283
7.4
Secret Dot Product String . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
7.4.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
7.4.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
7.4.3
Quantum Solution: Bernstein-Vazirani Algorithm . . . . . . . . . 288
7.4.4
Recursive Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
7.5
Secret XOR Mask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
7.5.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
7.5.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
7.5.3
Quantum Solution: Simon’s Algorithm . . . . . . . . . . . . . . . . . . 294
7.5.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
7.6
Brute-Force Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
7.6.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
7.6.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
7.6.3
Quantum Solution: Grover’s Algorithm . . . . . . . . . . . . . . . . . . 299
7.6.4
Reflection About Uniform State . . . . . . . . . . . . . . . . . . . . . . . . 303
7.6.5
Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
7.7
Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
xii
Contents
7.7.1
Application: Analyzing Music . . . . . . . . . . . . . . . . . . . . . . . . . 306
7.7.2
Classical Solution: Fast Fourier Transform . . . . . . . . . . . . . . . 311
7.7.3
Quantum Solution: Quantum Fourier Transform . . . . . . . . . . 314
7.7.4
Inverse Quantum Fourier Transform . . . . . . . . . . . . . . . . . . . . 320
7.8
Phase / Eigenvalue Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
7.8.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
7.8.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
7.8.3
Quantum Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
7.8.4
Multiple Eigenstates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
7.9
Period of Modular Exponentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
7.9.1
The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
7.9.2
Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
7.9.3
Quantum Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
7.10 Factoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
7.10.1 The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
7.10.2 Classical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
7.10.3 Quantum Solution: Shor’s Algorithm. . . . . . . . . . . . . . . . . . . . 341
7.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
8
Next Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
8.1
Careers in Quantum Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
8.2
Technical Next Steps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
8.3
Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
8.4
Parting Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
Answers to Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
Chapter 1
Classical Information and Computation
Computers have come a long way. The first computers were people, not machines,
who performed calculations. Indeed, the term “computer” dates as far back as the
early 1600’s, centuries before the digital age. Human computers persisted into mod-
ern history, with NASA, for example employing people to compute launch trajec-
tories for the space program and other scientific endeavors through the 1960’s. Of
course, mechanical and then electronic computers have since taken over, evolving
from massive machines that filled entire rooms, to personal computers on our desk-
tops, to smaller and smaller devices. Now, nearly everyone has a electronic com-
puter in their pocket—a smartphone—that is more powerful than the computers that
landed people on the moon. Computers have become so polished that we can use
and even program them without understanding how they work at a fundamental
level. That’s not a bad thing. It has allowed computers to become tools for more and
more people. My physics students can perform numerical computations and solve
scientific problems without needing to understand bits and bytes.
One day, quantum computing will get to this point of accessibility, where we can
use and program them without worrying about their details. But we are not quite
there, yet. In their development, quantum computers are where classical computers
were decades ago. Their inner working still matter, and to understand these inner
workings, it is helpful to understand the inner workings of regular, classical com-
puters. So, in this chapter, we will look at the basics of classical computing. If you
have studied the fundamentals of classical computing or electrical engineering, this
may be review for you. Even so, the topics may be worth seeing again because they
have been carefully selected for their quantum analogues in later chapters.
Furthermore, quantum computing is not developing in isolation of classical com-
puting. Many of the design decisions for quantum computers stem out of what is
done with classical computers. Without knowing classical computing, some aspect
of quantum computing may seem arbitrary. A rudimentary understanding classical
computing makes it easier to understand quantum computing.
The following table lists many of the concepts we will be covering in this book.
1
2
1 Classical Information and Computation
Concept
Classical
Quantum
Fundamental Unit
Bit
Qubit
Gates
Logic Gates
Unitary Gates
Gates Reversible
Sometimes
Always
Universal Gate Set (Example)
{NAND}
{H,T,CNOT}
Programming Language (Example)
Verilog
OpenQASM
Algebra
Boolean
Linear
Error Correcting Code (Example)
Repetition Code
Shor Code
Complexity Class
P
BQP
Strong Church-Turing Thesis
Supports
Possibly Violates
In this chapter, we will cover the “Classical” column, beginning with bits. Then,
we will perform computation on these bits using logic gates and include discus-
sions about universal and reversible logic gates. The math of classical computing is
boolean algebra, and we can program classical circuits using hardware description
languages. We will then look at classes of problems that are easy or hard for com-
puters, and we will end this chapter with the prospect that quantum computers may
be significantly faster than classical computers at some tasks. In the rest of the book,
we will cover the quantum computing column of the table.
1.1 Bits
The term bits has become fairly commonplace, where many people will state that
bits are zeros and ones. Whether you already knew that does not matter. The point
is, have we taken a moment to consider what bits really are? Are they just numbers?
Why are they important for computers? Let us begin our journey by exploring bits
deeply.
1.1.1 Coins
Consider a coin, such as the United States one-cent penny from 2016, which features
former President Abraham Lincoln as “heads” and the Union shield as “tails.”
Assuming that coins do not balance on their edges, a single coin lying on a flat
surface either has heads facing up or tails facing up. Let us call these two possible
conditions, or states, heads (H) and tails (T).
1.1 Bits
3
If we have two coins, there are four possible states: Both coins can be heads
(HH), the first can be heads and the second can be tails (HT), the first can be tails
and the second can be heads (TH), or both can be tails (TT). That is, the possible
states are:
HH, HT, TH, TT.
Since the first coin has two possible states (heads or tails), and the second coin has
two possible states (heads or tails), there are 2 × 2 = 22 = 4 possible states for the
two coins.
Adding a third coin, there are now eight possible states. They could all be heads,
some mixture of heads and tails, or all tails. Listing all the permutations, the possible
states are now
HHH, HHT, HTH, HTT, THH, THT, TTH, TTT.
Since each of the three coins has two possible states, there are 2 × 2 × 2 = 23 = 8
possible states for three coins.
Generalizing this, if we have n coins, the possible states range from all heads,
through a mixture of heads and tails, to all tails:
H ...HH, H ...HT, ..., T ...TT.
With n coins, there are 2n possible states.
Exercise 1.1. How many possible states do (a) four coins have? (b) five coins? You do not need to
list the states, just how many there are.
1.1.2 Dice
In contrast, consider a standard six-sided die.
It has six possible outcomes, the numbers 1 through 6. So, the possible states of a
die are
1,2,3,4,5,6.
Next, say we have two dice. Each die can take the values 1 through 6, so both
could be 1, the first could be 1 and the second could be 2, and so forth. If the first
die is 3 and the second is 5, let us write the configuration as (3,5). Then, listing all
of these, the possible states of two dice are
4
1 Classical Information and Computation
(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),
(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),
(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),
(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),
(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),
(6,1),(6,2),(6,3),(6,4),(6,5),(6,6).
Since each die has six possible states, there are 6 ×6 = 62 = 36 possible states for
the two dice.
Following the pattern, with three dice, there are 63 = 216 possible states. Listing
these would take too much space. Generalizing, with n dice, there are 6n possible
states.
Exercise 1.2. How many possible states do (a) four dice have? (b) five dice? You do not need to
list the states, just how many there are. A calculator may be useful.
1.1.3 Encoding Information
Now, how much information can coins and dice carry? Say I am trying to com-
municate the colors of the rainbow, which in the United States are typically listed
as red, orange, yellow, green, blue, indigo, and violet.1 These seven colors can be
represented, or encoded, by the possible configurations of three coins or two dice:
Color Coins Dice
Red
HHH (1,1)
Orange HHT (1,2)
Yellow HTH (1,3)
Green HTT (1,4)
Blue
THH (1,5)
Indigo THT (1,6)
Violet TTH (2,1)
So, if I give you three coins, first heads then tails and then tails again, you can
decode it and determine the color green. Alternatively, I can give you two dice, the
first 1 and the second 4, to indicate the color green. In this example, the rest of the
configurations (TTT for the coin, and (2,2) through (6,6) for the dice) are unused and
do not mean anything, or they can be assigned to the same colors so that multiple
states can encode green, for example.
We see that a coin carries less information than a die; it takes three coins to
distinguish the seven colors of the rainbow, compared to two dice. This is because a
1 In American elementary schools, many students memorize this by taking the first letter of each
color and combining them into the acronym ROY G. BIV.
1.1 Bits
5
coin can only distinguish between two states (heads and tails), while a die is able to
distinguish between six states (1 through 6). Since two states is the fewest number
of states that can be distinguished, a coin carries the smallest amount of information
possible. This leads us the following idea:
Something with two states carries the
smallest amount of information possible.
Exercise 1.3. Some board games use a twenty-sided die. How many twenty-sided die does it take
to encode the seven colors of the rainbow?
Exercise 1.4. How many (a) coins and (b) six-sided dice would it take to represent the 26 letters
of the English alphabet? Ignore upper and lowercase, spaces, punctuation, etc., so there’s only 26
letters total.
1.1.4 Physical Bits
Many physical systems only have two states. We already discussed coins, which
can be heads or tails. Another example is a light switch, which can be “off” or
“on.” As another example, information is stored on optical discs (e.g., CDs, DVDs,
and Blu-ray discs) using a laser that burns holes into the disc, called “pits.” For
example, one of my students took a zoomed-in picture of a DVD using an atomic
force microscope, shown below:
Image credit: Jeffrey Y. Wong, Creighton University, taken August 24, 2018
The picture shows microscopic holes and trenches burned into the disc, i.e., the pits.
Where there is no hole is called a “land.” So, the two states are whether there is a pit
or a land, and to read the disc, a laser shines on the disc and detects whether there is
a pit or a land.
Physical systems with more than two possible states can be treated as only having
two if we simply ignore the rest of the possible states. For example, if I have an
6
1 Classical Information and Computation
electronic circuit, then many different voltage levels are possible.2 In conventional
computers, however, we typically only use two values, 0 volts and 5 volts, so we
effectively only have two states.
All of these examples have, or effectively have, just two states, called by differ-
ent names: heads/tails, off/on, pits/lands, and 0 V / 5 V. Rather than using so many
different names, it is often easier to use generic names for the two states so that we
can describe the state without regard for the underlying physical system. Mathemat-
ically, it is convenient to use the binary digits 0 and 1. This is summarized in the
table below
Physical System
States
Coin
Heads Tails
Switch
Off
On
Disc
Pit
Land
Voltage
0 V
5 V
Binary Digit
0
1
With this convention, regardless of the physical system, we can just refer to the two
possible states as 0 and 1, the binary digits.
Since binary digits are used so much in computing, an abbreviation for binary
digit was invented: the bit. So, a bit is just a binary digit, which can be 0 or 1, which
represent the states of any physical system with two states. Since systems with two
states carry the smallest amount of information possible, a bit carries the smallest
amount of information possible:
A bit is the smallest unit of classical information.
1.1.5 Binary
Previously, when we had three coins, we wrote the eight possible states as
HHH, HHT, HTH, HTT, THH, THT, TTH, TTT.
Now, replacing heads and tails with the bits 0 and 1, the eight possible states are
now written as
000,001,010,011,100,101,110,111.
In other words, we can write the state in terms of numbers, and numbers are useful
because we can use math to describe and manipulate them.
2 Voltage is the amount of electrical pressure pushing charges in the circuit, but you do not need to
know this.
1.1 Bits
7
What kind of numbers are these states? They are binary numbers, or base 2
numbers. These are also called binary strings or bit strings. For example, if we
have five bits, one possible state is
11010
or
110102,
where the subscript of 2 can be included to clarify that it is a binary (base-2) number.
We pronounce this as “one one zero one zero,” and optionally say “base 2” after-
ward. It is not pronounced “eleven-thousand ten” because it is not a regular, decimal
(base-10) number. Actually, what decimal number does it correspond to?
To figure this out, let us first remind ourselves how normal decimal numbers
work. Consider the number “six-thousand one-hundred seventy-four” (6174). It has
a six in the thousands place, literally meaning there are six thousands, a one in the
hundreds place, meaning one hundred, a seven in the tens place, meaning seven tens,
and a four in the ones place, meaning four ones. That is,
6174 = 6·1000+1·100+7·10+4·1
= 6·103 +1·102 +7·101 +4·100.
So, each digit represents how many of each power of 10 we have.
For binary numbers, each digit similarly denotes how many of each power of
2 we have. Back to our question from a couple paragraphs ago, we can find what
decimal number that 110102 corresponds to:
110102 = 1·24 +1·23 +0·22 +1·21 +0·20
= 1·16+1·8+0·4+1·2+0·1
= 26.
So, we have one in the sixteens place, one in the eights place, zero in the fours
place, one in the twos place, and zero in the ones place, and the binary number
11010 corresponds to the decimal number 26 twenty-six.
In the example 110102, the leftmost bit contributes 16 to the number, the most
of any bit. For this reason, the leftmost bit is called the most significant bit. Simi-
larly, the rightmost bit can only contribute 1 to the number, and for this reason, the
rightmost bit is called the least significant bit.
We can also count in binary. To understand how, let us think about how we nor-
mally count from zero to one-hundred. For clarity, we write the leading zeros. To
begin, the rightmost digit increments from 0 to 9:
000,001,002,003,004,005,006,007,008,009,...
The rightmost digit has reached its maximum value, so for the next number, it rolls
over from 9 to 0, and the middle digit increments by 1, yielding 010. Continuing,
010,011,012,013,014,015,016,017,018,019,...
8
1 Classical Information and Computation
Again, the rightmost digit has reached its maximum value, so for the next num-
ber, it rolls over from 9 to 0, and the middle digit increments by 1, yielding 020.
Continuing,
020,021,022,023,...,098,099,...
The rightmost digit has again reached its maximum value, so for the next number,
it rolls over from 9 to 0, and the middle digit needs to increment by 1. But it has
also reached its maximum value, so it also rolls over from 9 to 0, and the left digit
is incremented, yielding 100.
We can apply the same procedure to count in binary. Say we have a bit string
of length 3. It starts with 000, and we increment the rightmost digit, yielding 001.
Incrementing again, the rightmost bit has reached its maximum value (since a bit
can only be 0 or 1), so it rolls over from 1 to 0, and the middle bit is incremented,
yielding 010. Incrementing again, we have 011. Incrementing again, the rightmost
bit rolls over from 1 to 0, the middle bit also rolls over from 1 to 0, and the leftmost
bit increments from 0 to 1, yielding 100. Continuing this procedure, we can count
from 000 to 111, which is counting in decimal from 0 to 7:
Binary
Decimal
(Base 2) (Base 10)
000
0
001
1
010
2
011
3
100
4
101
5
110
6
111
7
Exercise 1.5. Convert the following binary numbers (base 2) to decimal numbers (base 10):
(a) 101112.
(b) 110010102.
Exercise 1.6. Convert the following decimal numbers (base 10) to binary numbers (base 2):
(a) 42.
(b) 495.
Exercise 1.7. Base-16, commonly called hexadecimal, is another frequently used number system
in computing. The sixteen digits are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F. So the letter A is
ten in decimal, B is eleven in decimal, ..., and F is fifteen in decimal. For example, converting the
hexadecimal number F2A to decimal,
F2A = 162 ·F+161 ·2+160 ·A
= 256·15+16·2+1·10
= 3840+32+10
= 3882.
(a) Convert the hexadecimal number 3B7C to a decimal (base 10) number.
1.1 Bits
9
(b) Convert the hexadecimal number FF to a binary (base 2) number. (So two hexadecimal num-
bers can represent eight bits.)
(c) HTML uses hexadecimal to encode colors using the RGB color model. RGB stands for the
(additive) primary colors red, green, and blue, and by adding together different amounts of
their light, the other colors can be produced. [From painting, you may be familiar with the
(subtractive) primary colors, red, yellow, and blue.] The amount of red, green, and blue ranges
from 0 to 255, with 0 being none of the color, and 255 being the full amount of the color. This
range of 0 to 255 corresponds to the hexadecimal numbers 00 through FF. An HTML color
code uses six hexadecimal numbers, like FA10E4, with the left two digits (FA) corresponding
to the amount of red, the middle two digits (10) corresponding to the amount of green, and
the right two digits (E4) corresponding to the amount of blue. This particular mix of colors
results in a bright pink. Convert the hexadecimal numbers FA, 10, and E4 to decimal.
Exercise 1.8. Negative numbers can be encoded in binary using two’s complement, where the most
significant bit is negative, while the remaining bits are positive. For example, in two’s complement,
110102 = 1·(−24)+1·23 +0·22 +1·21 +0·20
= 1·(−16)+1·8+0·4+1·2+0·1
= −6.
Convert each of the following two’s complement numbers to decimal:
Binary
Decimal
(Two’s Complement) (Base 10)
000
?
001
?
010
?
011
?
100
?
101
?
110
?
111
?
1.1.6 ASCII
Computers store information using bits (0’s and 1’s), but in our world, we often
store information and communicate using text (letters, punctuation, etc.). How do
we bridge this divide? How do we encode text using bits?
Historically, computers encoded letters, numbers, symbols, and special com-
mands (like carriage return or newline) using the American Standard Code for Infor-
mation Interchange, commonly abbreviated as ASCII (pronounced ass-key). ASCII
uses 7 bits, so they have 27 = 128 possible states from 0000000 through 1111111. Of
these, ninety-five of the bit strings encode printable characters, and they are shown
in Table 1.1. The remaining thirty-three bit strings encode non-printable characters,
like the “escape” key.
For example, the following binary string encodes the text “Tom.”
10
1 Classical Information and Computation
Table 1.1: Printable ASCII characters (glyphs) and their binary and decimal encod-
ings.
Binary Decimal Glyph
0100000
32
space
0100001
33
!
0100010
34
”
0100011
35
#
0100100
36
$
0100101
37
%
0100110
38
&
0100111
39
’
0101000
40
(
0101001
41
)
0101010
42
*
0101011
43
+
0101100
44
,
0101101
45
-
0101110
46
.
0101111
47
/
0110000
48
0
0110001
49
1
0110010
50
2
0110011
51
3
0110100
52
4
0110101
53
5
0110110
54
6
0110111
55
7
0111000
56
8
0111001
57
9
0111010
58
:
0111011
59
;
0111100
50
<
0111101
61
=
0111110
62
>
0111111
63
?
Binary Decimal Glyph
1000000
64
@
1000001
65
A
1000010
66
B
1000011
67
C
1000100
68
D
1000101
69
E
1000110
70
F
1000111
71
G
1001000
72
H
1001001
73
I
1001010
74
J
1001011
75
K
1001100
76
L
1001101
77
M
1001110
78
N
1001111
79
O
1010000
80
P
1010001
81
Q
1010010
82
R
1010011
83
S
1010100
84
T
1010101
85
U
1010110
86
V
1010111
87
W
1011000
88
X
1011001
89
Y
1011010
90
Z
1011011
91
[
1011100
92
\
1011101
93
]
1011110
94
ˆ
1011111
95
Binary Decimal Glyph
1100000
96
`
1100001
97
a
1100010
98
b
1100011
99
c
1100100
100
d
1100101
101
e
1100110
102
f
1100111
103
g
1101000
104
h
1101001
105
i
1101010
106
j
1101011
107
k
1101100
108
l
1101101
109
m
1101110
110
n
1101111
111
o
1110000
112
p
1110001
113
q
1110010
114
r
1110011
115
s
1110100
116
t
1110101
117
u
1110110
118
v
1110111
119
w
1111000
120
x
1111001
121
y
1111010
122
z
1111011
123
{
1111100
124
|
1111101
125
}
1111110
126
˜
1010100
|
{z
}
T
1101111
|
{z
}
o
1101101
|
{z
}
m
= Tom
So, if I want to send you my name, I can just send you these twenty-one bits. Once
you have the bits, you decode it into the text “Tom.”
In modern times, there are many more characters to encode. There are other
languages with different alphabets or characters, plus emojis and other symbols.
So, more than 7 bits are needed to encode all of them. The most common modern
standard is UTF-8, or the Unicode Transformation Format, and it uses up to 32 bits
(for 232 = 4294967296 states). The first 128 bit strings in UTF-8 are the ASCII
characters.
1.2 Logic Gates
11
Exercise 1.9. Write your first name as an ASCII bit string.
Exercise 1.10. Decode the following ASCII characters:
1010001 1110101 1100001 1101110 1110100 1110101 1101101
1.2 Logic Gates
In the previous section, we introduced bits, the fundamental unit of classical in-
formation. We can encode whatever information we would like using bits, such as
words, characters, and even pictures. In this section, we explore how to manipulate
bits so that we can compute using them.
We manipulate bits using logic gates, which take one or more bits as inputs and,
depending on the input, outputs one or more bits. Let us explore the simplest exam-
ples of logic gates next, which manipulate a single bit. Then, we will learn about
logic gates that act on two bits. Following this, we will discuss what logic gates are
physically and explain how they can perform all computations.
1.2.1 Single-Bit Gates
The simplest logic gates take one bit as input and then outputs one bit, which we
can draw as a circuit diagram:
Gate
Input
Output
This circuit is read left to right. The input bit on the left travels along the line or wire
into the gate, which we have drawn as a generic box. A bit comes out of the gate on
the right, traveling along the line, and it is the output.
Since the input is a single bit, it can only be a zero or a one. We can list these two
possibilities in a table called a truth table:
Input Output
0
?
1
?
Depending on which gate we have, the outputs will be different, so we have used
question marks as placeholders for now. How many possible outputs are there? Well,
there are two outputs, and each output can be 0 or 1, so there are 2×2 = 4 possible
outputs. Hence, there are four possible single-bit gates, which we describe now.
• The identity gate does nothing to the bit: 0 remains 0, and 1 remains 1. The
identity gate is sometimes depicted by a triangle:
12
1 Classical Information and Computation
A
A
A A
0 0
1 1
In our circuit diagram, we have labeled the input bit as A. Since it goes through
the identity gate (the triangle) unchanged, the output on the other side is also A.
Or as a wire, A comes in, nothing happens, and A comes out. Above, we also
filled in the truth table so that the input and output are both simply A. The first
row of the truth table indicates that if the input is 0, the output is 0. The second
row indicates that if the input is 1, the output is 1.
Since the identity gate does nothing, we often omit the triangle and just draw a
longer wire:
A
A
Again, this is read left-to-right, so the input bit A moves through the wire and
comes out the other side as the output, unchanged.
The identity gate is sometimes called the buffer gate.
• The NOT gate flips a bit from 0 to 1, or 1 to 0. Its circuit diagram is a triangle
with a small circle:
A
A
A A
0 1
1 0
The input bit A goes through the NOT gate from the left, and the resulting output
is A, where the overline denotes negation (i.e., the flipped or opposite bit). The
behavior of this circuit is completely described by the above truth table, and it
shows that when 0 is the input, 1 is the output, and vice versa.
In many texts, A is also denoted ¬A, where ¬ means negation. The NOT gate is
sometimes called the inverter gate.
• The always 0 gate always outputs 0, regardless of the input. It does not have a
standard circuit diagram since it is not commonly used, but its truth table is
A 0
0 0
1 0
The first line of the truth table indicates that when the input is 0, the output is 0.
The second line indicates that when the input is 1, the output is 0.
• The always 1 gate always outputs 1, regardless of the input. It does not have a
standard circuit diagram since it is not commonly used, but its truth table is
A 1
0 1
1 1
The first line of the truth table indicates that when the input is 0, the output is 1.
The second line indicates that when the input is 1, the output is 1.
These are all four possible logic gates with a single input and a single output.
1.2 Logic Gates
13
1.2.2 Two-Bit Gates
A two-bit logic gate takes two bits as input, say A and B. Although a two-bit logic
gate can have multiple outputs, the simplest case just has one output. So, its circuit
diagram and truth table would look like
Gate
A
B
Output
A B Output
0 0
?
0 1
?
1 0
?
1 1
?
This truth table has four rows because there are four possible inputs: A and B can
both be 0, A can be 0 and B can be 1, A can be 1 and B can be 0, or both can be
1. Note we listed these in numerical order, since 00, 01, 10, and 11 are the decimal
numbers 0, 1, 2, and 3. Writing them in numerical order is the convention.
Depending on the gate, the outputs will be different, so they have question marks
in the above truth table for now. Since each of the four outputs can be 0 or 1, there
are 24 = 16 possible two-bit gates. Next, we discuss five of the most important ones.
• The AND gate outputs 1 only when both input bits are 1. Its circuit diagram and
truth table are
A
B
AB
A B AB
0 0 0
0 1 0
1 0 0
1 1 1
In this circuit diagram, two bits A and B go through the AND gate, resulting in
AB. Note that standard multiplication works here: 0·0 = 0, 0·1 = 0, 1·0 = 0,
and 1 · 1 = 1. In many texts, AB is also denoted A ∧B. This is called the AND
gate because it outputs 1 when A and B are 1. Or, in the language of logic, if we
take 0 to be false and 1 to be true, then AB is true when A and B are both true.
• The OR gate, which outputs 1 if either input (or both) is 1. Its circuit diagram
and truth table are
A
B
A + B
A B A+B
0 0
0
0 1
1
1 0
1
1 1
1
Following many texts, we denote the OR of A and B as A + B, although this is
not actually addition since 1+1 = 1. In some texts, it is also denoted A∨B.
• The Exclusive OR (XOR) gate, which outputs 1 when only one input is one, but
not both. Its circuit diagram and truth table are
14
1 Classical Information and Computation
A
B
A ⊕B
A B A⊕B
0 0
0
0 1
1
1 0
1
1 1
0
We write the XOR of A and B as A⊕B, i.e., a plus sign with a circle around it.
Mathematically, ⊕is addition modulo 2, meaning we take the remainder after
dividing by 2. You are probably familiar with modulo in other contexts, like a
circle has 360◦, so 370◦is equivalent to 10◦. Mathematically, we would write
this as 370◦= 10◦mod 360◦, meaning when you divide 370◦by 360◦, you get
a remainder of 10◦. With ⊕we take the remainder after dividing by 2, so we
have
0 = 0 mod 2,
1 = 1 mod 2,
2 = 0 mod 2,
3 = 1 mod 2,
4 = 0 mod 2,
5 = 1 mod 2,
...
Hence, the last line of the truth table for A⊕B is 1⊕1 = 2 mod 2 = 0 mod 2.
• The NAND gate, which stands for NOT of AND, and which outputs the NOT
of the AND of the bits. Its circuit diagram and truth table are
A
B
AB
A B AB
0 0 1
0 1 1
1 0 1
1 1 0
Note the circuit diagram is a regular AND gate, with a small circle to indicate a
NOT. We denote NAND by negating an AND, so it is written as AB.
• The NOR gate, which stands for NOT of OR, and which outputs the NOT of the
OR of the bits. Its circuit diagram and truth table are
A
B
A + B
A B A+B
0 0
1
0 1
0
1 0
0
1 1
0
Note the circuit diagram is a regular OR gate, with a small circle to indicate a
NOT. We denote NOR by negating an OR, so it is A+B.
1.2 Logic Gates
15
Exercise 1.11. Consider the following gate that inverts the inputs before passing them into an OR
gate, sometimes called a negative-OR gate:
A
B
A + B
(a) Write the truth table for this circuit.
(b) What logic gate is this equivalent to?
Exercise 1.12. Consider the following gate that inverts the inputs before passing them into an AND
gate, sometimes called a negative-AND gate:
A
B
AB
(a) Write the truth table for this circuit.
(b) What logic gate is this equivalent to?
1.2.3 Logic Gates as Physical Circuits
So far, we have discussed what logic gates do, but we have not discussed what logic
gates are. Let us address that here. While there are many different ways to make
logic gates, but the most common way is using electric circuits. I do not assume that
you know circuits, so we will start slowly.
To begin, here is a drawing of a circuit that consists of a battery, two switches, a
light bulb, and some wires to connect them:
In the above drawing, the two switches correspond to inputs A and B. They are both
in the “on” position, so both inputs are 1. In this case, electricity is able to flow
through the circuit, coming out from the left side of the battery (the side of the
16
1 Classical Information and Computation
battery with the bump, or the positive side), down through the switches, right across
the bottom wire, up through the light bulb, and back into the right side of the battery
(the flat side of the battery, or the negative side). Electricity flows from the positive
side of the battery to the negative side, and this counter-clockwise flow is indicated
by arrows on the wires. Since electricity is flowing through the light bulb, it turns
on. The light bulb is the output, and so the output is 1 when both inputs A and B are
1.
Next, let us turn off only the first switch while keeping the second switch on, so
A = 0 and B = 1:
In the above drawing, the electricity is unable to flow in the circuit because the first
switch is turned off. This disconnects the connections, so the electricity does not
have a path to flow. It does not matter that switch B is still on. So, the light bulb is
off, and the output is 0.
Let us try the opposite. We turn on switch A and turn off switch B, so A = 1 and
B = 0:
1.2 Logic Gates
17
In the above drawing, electricity again is unable to flow. It does not matter that
electricity can get through switch A, it cannot get through switch B. So, the light
bulb is off, and the output is 0.
Finally, let us turn off both switches, so A = 0 and B = 0:
Reordering these results so that the inputs are in numerical order, the truth table
for this circuit is
A B Output
0 0
0
0 1
0
1 0
0
1 1
1
This is the truth table for the AND gate, so the circuit we have been examining is
an AND gate. The light bulb only lights up with switch A and switch B are both on.
This is an example of how to create a logic gate using an electrical circuit.
Let us look at another circuit, this time consisting of one switch (input) and one
light bulb (output):
In the above picture, current flows out from the top of the battery (the positive side,
which has a bump). It cannot pass through the switch because the switch is off, so
it travels through the light bulb and returns into the bottom of the battery. So, when
A = 0, the output is 1.
18
1 Classical Information and Computation
Now, let us flip on the switch:
Current again flows out from the top of the battery. Since the switch is on, the
current can flow through either the switch or the light bulb. It turns out that given
this option, electricity will simply flow through the switch only and not the light
bulb. This is because it takes practically no effort to flow through the switch, but
it takes considerable effort to flow through the light bulb. Since no electricity is
flowing through the light bulb, it remains off. Thus, when A = 1, the output is 0.
Summarizing these results in a truth table,
A Output
0
1
1
0
This is the truth table for a NOT gate, so the circuit in this example is a NOT gate.
Similarly, we can create simple electrical circuits that implement each of the
other logic gates.
Exercise 1.13. Consider the following electrical circuit:
Answer the following questions.
(a) Say A = 0 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(b) Say A = 0 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(c) Say A = 1 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
1.2 Logic Gates
19
(d) Say A = 1 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(e) What logic gate does the circuit correspond to?
Exercise 1.14. Consider the following electrical circuit:
Answer the following questions.
(a) Say A = 0 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(b) Say A = 0 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(c) Say A = 1 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(d) Say A = 1 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(e) What logic gate does the circuit correspond to?
Exercise 1.15. Consider the following electrical circuit:
Answer the following questions.
(a) Say A = 0 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(b) Say A = 0 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(c) Say A = 1 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(d) Say A = 1 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(e) What logic gate does the circuit correspond to?
Exercise 1.16. In some homes in the United States, special switches are used so that two switches
control a single light. Often, these switches are located at opposite ends of a stairway or a hallway,
and either switch can be used to turn the light on or off. A traditional switch enables or disables
the flow of electricity through a single wire. In contrast, these special switches, called three pole
switches, choose between two different wires. The following electrical circuit gives an example:
20
1 Classical Information and Computation
Each switch has three poles, labeled C, 0, and 1. Switch A is current flipped up, which connects C
and 0. Switch B is currently flipped down, which connects C and 1. In this configuration, there is
a complete path for the electricity to flow. It comes out of the positive end of the battery, through
Switch A along A = 0, then down to B = 1, then through Switch B, then down through the light
bulb, left through the bottom wire, and up to the negative end of the battery. So, the light bulb is on
when A = 0 and B = 1.
(a) Say A = 0 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(b) Say A = 1 and B = 0. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(c) Say A = 1 and B = 1. Sketch the circuit and draw arrows indicating the path through which
the electricity is flowing. Is the light bulb off or on?
(d) What logic gate does the circuit correspond to?
We have seen that electrical circuits can be used to create logic gates by con-
necting switches in various ways. The switches themselves have changed over time,
however. Early electrical computers used relays or vacuum tubes as the switches.
A relay is a switch that uses an electrically controlled magnet (i.e., an “electro-
magnet”) to turn on and off the switch. In a vacuum tube (also called a vacuum
valve), one can turn on and off the flow of electricity between two pieces of metal,
one called an anode, and other called a cathode, that is heated. For example, in
the 1940’s, during World War II, the British built a now-famous computer called
Colossus to help break German codes. The first generation Colossus computer had
roughly 1,600 vacuum tubes, and the second generation had about 2,400. Vacuum
tubes often failed, however, and needed replacement.
Nowadays, computers use transistors as the switches. They are typically made of
silicon, a common semiconductor, with some other elements to control how easily
electricity flows through certain areas. Transistors have several benefits. They have
no moving parts (i.e., they are “solid-state” devices), so they are more reliable. They
are smaller, which allows computers to be smaller, and they are also faster. Cur-
rently, a single computer processor can have tens of billions of transistors, which is
a huge improvement over Colossus’s 2,400 vacuum tubes.
Exercise 1.17. Visit the website https://en.wikipedia.org/wiki/Transistor_cou
nt.
1.2 Logic Gates
21
(a) Pick an older computer processor. Which one did you pick, what year was it introduced, and
how many transistors did it have?
(b) Pick a newer computer processor. Which one did you pick, what year was it introduced, and
how many transistors did it have?
1.2.4 Multiple Gates
We can combine logic gates to create more interesting operations. For example, say
we nest two AND gates together:
A
B
AB
C
ABC
A B C AB ABC
0 0 0 0
0
0 0 1 0
0
0 1 0 0
0
0 1 1 0
0
1 0 0 0
0
1 0 1 0
0
1 1 0 1
0
1 1 1 1
1
To find the above truth table, we can first calculate AB, which is the output of the
first AND gate. Then, to get the final output, we can take the AND of AB with C,
and we see that this is precisely ABC, the AND of all three bits, because the output
is 1 only when all three inputs are 1. Then, for simplicity, we often draw this as a
single AND gate, but with three inputs:
A
B
C
ABC
With more inputs, additional AND gates can be nested, or more input lines can be
drawn on a single AND gate.
As a more complicated example, consider the following circuit, which contains
an XOR gate, NOT gate, and AND gate:
A
B
A ⊕B
A ⊕B
C
A ⊕BC
The truth table for this, including its intermediate steps, is shown below:
22
1 Classical Information and Computation
A B C A⊕B A⊕B A⊕BC
0 0 0
0
1
0
0 0 1
0
1
1
0 1 0
1
0
0
0 1 1
1
0
0
1 0 0
1
0
0
1 0 1
1
0
0
1 1 0
0
1
0
1 1 1
0
1
1
This resulting behavior does not have a standard, nice name, unlike the first example
of the three-bit AND gate. To write it as a single gate, we can just give it whatever
name we would like and draw it as a generic box:
Name
A
B
C
A ⊕BC
Exercise 1.18. Consider the XOR of three bits A, B, and C, which can either be two two-bit XOR
gates strung together, or a single three-bit XOR gate:
A
B
A ⊕B
C
A ⊕B ⊕C
A
B
C
A ⊕B ⊕C
(a) What is the truth table for this circuit?
(b) When there is an even number of 1’s in the input (we call this even parity), what is the output?
(c) When there is an odd number of 1’s in the input (we call this odd parity), what is the output?
Exercise 1.19. What is the truth table for the following circuit diagram?
A
B
C
Output
Note the solid dot simply means that the wires are connected there. Then, C is both inputs of the
NAND gate, so the gate takes the NAND of C with itself.
Exercise 1.20. Answer the following questions:
(a) How many possible one-bit logic gates are there?
(b) How many possible two-bit logic gates are there?
(c) How many possible three-bit logic gates are there?
(d) How many possible four-bit logic gates are there?
(e) How many possible n-bit logic gates are there?
1.2 Logic Gates
23
1.2.5 Universal Gates
Previously, we described five of the sixteen possible two-bit gates (AND, OR, XOR,
NAND, and NOR). But what about the other nine? To make matters worse, for
a three-bit gate, the truth table has eight entries, and each output can be 0 or 1,
meaning there are 28 = 256 possible three-bit gates. Do we need to list all of them,
too?
Thankfully, the answer is no. We do not need separate gates for all of these pos-
sibilities because we can reproduce all of them using just a few type of gates. We
call a set of gates that can perform all possible logic operations a universal gate set,
and here are some examples:
• {NOT,AND,OR} is a universal gate set. Given any truth table, we can im-
plement it using only NOT, AND, and OR gates. For example, consider the
following truth table for a circuit with three inputs, A, B, and C:
A B C Output
0 0 0
1
0 0 1
1
0 1 0
0
0 1 1
0
1 0 0
1
1 0 1
0
1 1 0
1
1 1 1
0
We can create a circuit composed of NOTs, ANDs, and ORs with this truth
table by looking at every case where the output is 1, then combining them all
together using ORs. Beginning with the first line of the truth table, the output is
1 when A = 0, B = 0, and C = 0. This is ABC. Next, the output is 1 when A = 0,
B = 0, and C = 1. This is ABC. Jumping to the fifth line of the truth table, the
output is 1 when A = 1, B = 0, and C = 0, which is ABC. Finally, from the
seventh line of the truth table, the output is 1 when A = 1, B = 1, and C = 0,
which is ABC. The output is 1 whenever any of these are true, so the circuit is
ABC +ABC +ABC +ABC. As a circuit diagram, it is:
24
1 Classical Information and Computation
A
B
C
This is a rather complicated circuit, and later in this chapter, we will learn an
algebraic way to simplify such circuits (it simplifies to A+B + AC). But for
now, the important point is that we can always use NOT, AND, and OR gates in
this manner to implement any truth table.
Exercise 1.21. Draw a circuit diagram using only NOT, AND, and OR gates that implements the
XOR gate.
Exercise 1.22. Draw a circuit diagram using only NOT, AND, and OR gates that corresponds to
the following truth table with input bits A, B, and C:
A B C Output
0 0 0
0
0 0 1
1
0 1 0
1
0 1 1
0
1 0 0
0
1 0 1
1
1 1 0
0
1 1 1
1
• {NOT,AND} is a universal gate set. To prove this, note the following circuit of
NOTs and ANDs calculates the OR of bits A and B:
A
B
AB
A B A B AB AB
0 0 1 1 1
0
0 1 1 0 0
1
1 0 0 1 0
1
1 1 0 0 0
1
1.2 Logic Gates
25
First, the inputs A and B go through NOT gates, resulting in A and B. Then,
those become the inputs into an AND gate, resulting in AB. Finally, this is in-
verted, yielding AB. This is the exact same truth table as A +B, so this circuit,
which only contains NOTs and an AND, implements the OR gate. Then, since
{NOT,AND,OR} is a universal gate set, just {NOT,AND} is also a universal
gate set.
Exercise 1.23. Draw a circuit diagram using only NOT and AND gates that corresponds to the
following truth table with input bits A, B, and C:
A B C Output
0 0 0
0
0 0 1
0
0 1 0
1
0 1 1
0
1 0 0
0
1 0 1
1
1 1 0
0
1 1 1
0
• {NAND} is a universal gate set, so we say that NAND is a universal gate. So,
with only NAND gates, one can construct all logic gates. To prove this, let us
show that NAND gates can produce the NOT and AND gates, since we already
know that {NOT,AND} is universal.
First, we can get a NOT gate from NAND by connecting a bit to both inputs:
A
AA
A AA AA
0 0
1
1 1
0
Next, to get AND, note that NAND followed by NOT is simply AND. That is,
if we apply NOT twice, they cancel out, leaving AND:
A
B
AB
AB
A B AB AB
0 0 1
0
0 1 1
0
1 0 1
0
1 1 0
1
Since {NOT,AND} is a universal set of gates, and NAND can reproduce both
of them, NAND is a universal gate.
Exercise 1.24. Draw a circuit diagram using only NAND gates that implements the OR gate.
Exercise 1.25. Draw a circuit diagram using only NAND gates that implements the following truth
table:
A B Output
0 0
1
0 1
0
1 0
0
1 1
1
26
1 Classical Information and Computation
Exercise 1.26. Draw a circuit diagram using only NAND gates that corresponds to the following
truth table with input bits A, B, and C:
A B C Output
0 0 0
1
0 0 1
0
0 1 0
1
0 1 1
0
1 0 0
1
1 0 1
0
1 1 0
1
1 1 1
1
Hint: You could create a circuit using {NOT,AND,OR}, then replace the ORs with NOTs and
ANDs so that only {NOT,AND} are used, and then replace the NOTs and ANDs with NANDs.
This would be a lot of NAND gates, however! To save you from this tedium, I will tell you that
this truth table can be created with just two NAND gates.
• {NOT,OR} is a universal gate set. See Exercise 1.27.
Exercise 1.27. In this problem, we will prove that {NOT,OR} is universal. To do this, we already
know that {NOT,AND,OR} is universal, so we simply need to show that AND gates can be
constructed using {NOT,OR}.
Write the truth table for the following circuit, and verify that it corresponds to the AND gate.
A
B
Output
• {NOR} is a universal gate set, so NOR is a universal gate. See Exercise 1.28.
Exercise 1.28. In this problem, we will prove that NOR is a universal gate. To do this, we know
from Exercise 1.27 that {NOT,OR} is universal, so we simply need to show that NOT and OR can
be constructed using NOR gates.
(a) Write the truth table for the following circuit, and verify that it corresponds to the NOT gate.
A
Output
(b) Write the truth table for the following circuit, and verify that it corresponds to the OR gate.
A
B
Output
1.3 Adders and Verilog
27
1.3 Adders and Verilog
As an application of the logic gates we have introduced, let us create a logic circuit
that adds two binary numbers. First, we will give an overview of how to manually
add binary numbers. Then, we will construct different circuits for adding the various
parts of a binary number, and we will code these circuits using Verilog, a hardware
description language. Finally, we will assemble these circuits into a full circuit that
adds binary numbers. In doing so, we will see that logic circuits can be used to
compute things, like adding numbers, and indeed everything that a computer does
is based on logic circuits.
1.3.1 Adding Binary Numbers by Hand
Say we want to add two binary numbers, like 1011 “+” 1110, where “+” denotes
normal addition, not OR. So we want to find
1011
“+” 1110
Let us work through each column of bits, from right to left.
Starting with the rightmost bits, 1 plus 0 is 1. Nothing car-
ries to the next column, so we write a small zero above the
next column.
(carry)
0
1011
“+” 1110
(sum)
1
Now for the second bits (from the right), we have the carry
of zero plus one plus one. In decimal, 0+1+1 = 2, but in
binary, it is 10. So, the sum at the bottom is 0, and 1 carries
to the next column.
(carry)
1 0
1011
“+” 1110
(sum)
01
For the third bits (from the right), we have 1+0+1, which
again is 2 in decimal or 10 in binary, so the sum at the
bottom is 0, and 1 carries out to the next column.
(carry)
1 1 0
1011
“+” 1110
(sum)
001
28
1 Classical Information and Computation
Finally, for the leftmost bits, we have 1+1+1, which is 3
in decimal or 11 in binary. So, the sum at the bottom is 1,
and 1 carries to the next column. This final carry becomes
a fifth digit of the sum.
(carry) 1 1 1 0
1011
“+” 1110
(sum) 11001
Thus, 1011 “+” 1110 = 11001. Converting this from binary to decimal, 11 + 14 =
25, as it should be.
In general, to add two 4-bit numbers A3A2A1A0 and B3B2B1B0, we need to carry
four numbers C4,C3,C2,C1, and we get a five-bit sum S4S3S2S1S0:
(carry) C4 C3 C2 C1
A3A2A1A0
“+”
B3B2B1B0
(sum) S4S3 S2 S1 S0
Since the leftmost carry is the leftmost digit of the sum, S4 = C4.
In the next several sections, we will create a circuit to perform this addition pro-
cess. To add the rightmost column, we need a circuit that adds two bits and outputs
a carry and a bit of the sum. This is called a half adder. For the remaining columns,
we need a circuit that adds three bits (a carry into the sum and the two bits we want
to add) and outputs a carry and a bit of a sum. This is called a full adder. By string-
ing together a half adder and several full adders into a circuit called a ripple-carry
adder, we can add binary numbers.
1.3.2 Half Adder
Let us begin by creating a circuit that adds two bits. In general, there are four possi-
bilities for these two bits:
(carry) 0
0
“+” 0
(sum) 0
(carry) 0
0
“+” 1
(sum) 1
(carry) 0
1
“+” 0
(sum) 1
(carry) 1
1
“+” 1
(sum) 0
Let us call the input bits A and B, the sum S, and the carry bit C. Then the truth table
is
A B S C
0 0 0 0
0 1 1 0
1 0 1 0
1 1 0 1
1.3 Adders and Verilog
29
From this, we see that the sum is the XOR of A and B, and the carry is the AND of
A and B. Thus,
S = A⊕B,
C = AB.
As a circuit diagram, it is
A
B
S
C
This is called a half adder.
Rather than manually wiring this circuit, we can code it using a hardware de-
scription language (HDL), which describes the structure and behavior of computer
hardware, in this case, digital logic circuits. HDLs can be used to program a field-
programmable gate array (FPGA), which is an integrated circuit whose logic gates
can be configured in software.
The two most common HDLs for logic design are Verilog and VHDL, and here
we will focus on Verilog. Although the following Verilog code can be used for actual
hardware, we will use an online Verilog simulator at https://tutorialspoint.c
om/compile_verilog_online.php. Here is the website when it is first loaded:
If you click the Execute button in the top-left corner of the webpage, the following
appears in the Result section of the webpage:
30
1 Classical Information and Computation
In the first and second lines, $ indicates that a command is being given. In the first
line, the command is iverilog, which is the compiler. The option -o main speci-
fies that the output (-o) should be a file called main, so after this command is run, we
should get an executable file called main. Continuing, the first line, *.v denotes all
files with the extension .v, so we are compiling all files with that extension. Notice
from the left pane that our code is in a file called main.v, so this will be compiled.
Now, in the second line, vvp is the simulator, and the executable we are simulating
is called main. Our program outputs Hello, World to the third line.
Now, in the left panel, we can replace the code with the Verilog code for the half
adder:
module main;
reg A,B;
wire S,C;
xor xor1(S,A,B);
and and1(C,A,B);
initial
begin
A=0;
B=1;
#5; // Wait 5 time units.
$display("Sum = ",S);
$display("Carry = ",C);
end
endmodule
In the main module, we define two registers A and B. Each register allows the as-
signment and storage of a bit, which we will later do in the initialization block. We
also define two wires S and C. A wire takes some value depending on the circuit,
and we cannot simply define it to take some value. Next, we create the circuit by
creating an instance of the XOR gate which we call xor1, and an instance of the
AND gate which we call and1. For both of these, the first argument is the output,
and the remaining arguments are the inputs. For example, xor xor1(S,A,B) com-
putes the XOR of A and B, and the result is S. Now for the initial block, we assign
the registers A and B the values 0 and 1, respectively. After this assignment has been
made, we wait 5 of the simulation’s time units so that the circuit has had a chance
to respond to our inputs. Then, we print the values of the carry and sum. Executing
this, we get
1.3 Adders and Verilog
31
As expected, when we add 0 and 1, we get 1 with a carry of 0.
To make the half adder more easily reusable, we can replace it by a single circuit
symbol, which we label HA for half adder:
HA
A
B
S
C
In Verilog, we do this by creating a module (or function) for the half adder:
module halfadd(S,C,A,B);
input A,B;
output S,C;
xor xor1(S,A,B);
and and1(C,A,B);
endmodule
module main;
reg A,B;
wire S,C;
halfadd half1(S,C,A,B);
initial
begin
A=0;
B=1;
#5; // Wait 5 time units.
$display("Sum = ",S);
$display("Carry = ",C);
end
endmodule
Here, the name of the module is halfadd, and it takes four parameters (S,C,A,B),
which we specify as inputs and outputs. The half adder contains an instance of the
AND gate and an instance of the XOR gate. In the main module, we instantiate the
half adder, and the instance is named half1. Executing this, we get
This is the exact same result as before, which is expected because we did not change
anything except move the half adder into a module.
Exercise 1.29. Code the following circuit in Verilog.
32
1 Classical Information and Computation
A
C
B
Using your Verilog code, try all possible inputs for A and B and fill in the outputs in the following
truth table:
A B C
0 0 ?
0 1 ?
1 0 ?
1 1 ?
Do your results make sense?
1.3.3 Full Adder
After adding the first two bits, the remaining bits need to be added along with what-
ever was carried in. Since this includes a carry bit, it is called a full adder. Let us call
the bits we are adding A and B, and let us call the bit we carry in Cin. The outputs
are the sum S and carry out Cout. Here is a truth table for what the addition should
do:
A B Cin S Cout
0 0 0 0
0
0 0 1 1
0
0 1 0 1
0
0 1 1 0
1
1 0 0 1
0
1 0 1 0
1
1 1 0 0
1
1 1 1 1
1
From the truth table, we see that the sum S is 1 whenever one of the inputs A, B, or
Cin are 1, or when all three of them are 1. This is equivalent to the XOR of all three
bits:
S = A⊕B⊕Cin.
From this truth table, we also see that the carry out is 1 when A and B are both 1, or
when Cin = 1 and A⊕B = 1:
Cout = AB+Cin(A⊕B).
Putting these together, the logic circuit is
1.3 Adders and Verilog
33
A
B
Cin
S
Cout
Note that there are two groups containing an AND and an XOR:
A
B
Cin
S
Cout
Each of these is a half adder! So the full adder is equivalent to
HA
HA
Cin
A
B
Cout
S
To make the full adder more easily reusable, we can replace it by a single circuit
symbol, which we label FA for full adder:
FA
Cin
A
B
S
Cout
In Verilog, we can code a module for the full adder:
module halfadd(S,C,A,B);
34
1 Classical Information and Computation
input A,B;
output S,C;
xor xor1(S,A,B);
and and1(C,A,B);
endmodule
module fulladd(S,Cout,Cin,A,B);
input Cin,A,B;
output S,Cout;
wire w1,w2,w3;
halfadd half1(w1,w2,A,B);
halfadd half2(S,w3,Cin,w1);
or or1(Cout,w3,w2);
endmodule
module main;
reg Cin,A,B;
wire S,Cout;
fulladd full1(S,Cout,Cin,A,B);
initial
begin
Cin=1;
A=0;
B=1;
#5; // Wait 5 time units.
$display("Sum = ",S);
$display("Carry = ",Cout);
end
endmodule
Executing this, we get
As expected, 1+0+1 results in 0 with a carry out of 1.
Exercise 1.30. The full adder from class contains two XOR gates, two AND gates, and one OR
gate. Replace the OR gate with an XOR gate. What is the truth table of this new circuit? How does
it compare to the truth table of the full adder from class?
1.3.4 Ripple-Carry Adder
Using these adders, we can assemble them to add binary numbers. Let us denote
the bits of the binary numbers A3A2A1A0 and B3B2B1B0. We can use a half adder to
1.3 Adders and Verilog
35
add bits A0 and B0, then carry the bit into a full adder to add bits A1 and B1, then
continue adding the successive bits using full adders:
HA
FA
FA
FA
A0
B0
A1
B1
A2
B2
A3
B3
S0
S1
S2
S3
C4 = S4
This is called a ripple-carry adder, since the carry from one addition ripples to the
next addition. Let us code it in Verilog:
module halfadd(S,C,A,B);
input A,B;
output S,C;
xor xor1(S,A,B);
and and1(C,A,B);
endmodule
module fulladd(S,Cout,Cin,A,B);
input Cin,A,B;
output S,Cout;
wire w1,w2,w3;
halfadd half1(w1,w2,A,B);
halfadd half2(S,w3,Cin,w1);
or or1(Cout,w3,w2);
endmodule
module rippleadd(S,A,B);
input [3:0] A,B;
output [4:0] S;
output Cout;
wire [3:1] C;
halfadd half1(S[0],C[1],A[0],B[0]);
fulladd full1(S[1],C[2],C[1],A[1],B[1]);
fulladd full2(S[2],C[3],C[2],A[2],B[2]);
fulladd full3(S[3],S[4],C[3],A[3],B[3]);
endmodule
module main;
reg [3:0] A,B;
wire [4:0] S;
wire Cout;
36
1 Classical Information and Computation
rippleadd ripple1(S,A,B);
initial
begin
A=4’b1011;
B=4’b0011;
#5; // Wait 5 time units.
$display(A,"+",B,"=",S);
$display("%b",A,"+%b",B,"=%b",S);
end
endmodule
To explain this code, in the rippleadd module, the line input [3:0] A,B; defines
A and B to each have four inputs indexed from 3 to 0. That is, A = A3A2A1A0, and
the way we write each of these bits in Verilog is A[3], A[2], A[1], and A[0]. The same
holds for B. In the next line, we similarly define S = S4S3S2S1S0. A couple lines
later, we define C[3], C[2], and C[1] as the three carry bits, and we did not define
C[4] because this final carry out is exactly S[4]. In the initial block, A and B now
binary numbers of length 4. We assign their values in binary using 4’b followed by
the binary number. By default, displaying A and B prints them as decimal numbers,
so we use the %b format code to print them in binary instead.
Exercise 1.31. Using the 4-bit adder that we coded in Verilog, what is 1001 plus 0111?
Exercise 1.32. Code an 8-bit adder in Verilog. Use it to add 10101101 and 00111001.
1.3.5 Ripple-Carry with Full Adders
For simplicity, one may prefer to only have full adders in the circuit. To do this, we
can replace the first half adder with a full adder with a carry-in of 0.
1.4 Circuit Simplification and Boolean Algebra
37
FA
FA
FA
FA
C0 = 0
A0
B0
A1
B1
A2
B2
A3
B3
S0
S1
S2
S3
C4 = S4
In other words, we are adding
(carry) C4 C3 C2 C1 C0
A3A2A1A0
“+”
B3B2B1B0
(sum) S4S3 S2 S1 S0
but fixing C0 = 0.
Exercise 1.33. Using Verilog, code the 4-bit ripple-carry adder that only uses full adders, no half
adder. Use it to add 0110 and 1110.
1.3.6 Circuit Complexity
If the ripple-carry adder consists entirely of full adders, then adding two n-bit num-
bers requires n full adders. Each full adder uses five logic gates, for a total of 5n
logic gates.
Of course, since the first full adder has a carry-in of zero, it can be replaced with
a half adder. This reduces the number of gates by three, so the number of logic gates
is 5n−3.
1.4 Circuit Simplification and Boolean Algebra
Previously, we learned that {NOT,AND,OR} is a universal gate set. We demon-
strated this by creating a circuit that implemented a given truth table. The circuit was
ABC+ABC+ABC+ABC, and we claimed that it could be simplified to A+B+AC.
Now, we are going to prove this. One way is to show that their truth tables are the
38
1 Classical Information and Computation
same. But, it would be nice to have an algebraic way to show that their outputs are
equal, i.e.,
ABC +ABC +ABC +ABC = A+B+AC.
In this section, we will learn how to do this using the rules of algebra for bits, called
boolean algebra. These rules will allow us to simplify complicated circuits in a
systematic, algebraic manner.
1.4.1 Order of Operations
First, we need to learn how to read boolean expressions. Consider the following
expression on three inputs A, B, and C, which contains one OR and one AND:
A+BC.
When implementing this, do we do the OR first, i.e., (A + B)C, or the AND first,
i.e., A+(BC)? Does it even matter? To see, let us work out the truth table for each
option:
A B C (A+B)C A+(BC)
0 0 0
0
0
0 0 1
0
0
0 1 0
0
0
0 1 1
1
1
1 0 0
0
1
1 0 1
1
1
1 1 0
0
1
1 1 1
1
1
So the order in which we do the operations matters. Just like in regular math, where
we multiply (and divide) before adding (and subtracting), the convention in boolean
algebra is that AND is done first, then OR. Thus,
A+BC = A+(BC).
1.4.2 Association, Commutivity, and Distribution
AND and OR also follow several familiar properties from elementary algebra. First,
they are associative:
• ABC = (AB)C = A(BC)
• A+B+C = (A+B)+C = A+(B+C)
AND and OR are also commutative:
1.4 Circuit Simplification and Boolean Algebra
39
• AB = BA
• A+B = B+A
Finally, both are distributive, meaning AND distributes into OR, and OR distributes
into AND:
• A(B+C) = AB+AC
• A+(BC) = (A+B)(A+C)
The last point is likely the most foreign. It is not true when adding and multiplying
numbers (e.g., 2+3·4 ̸= (2+3)(2+4), since the left-hand side evaluates to 14, and
the right-hand side evaluates to 30), but it is true of AND/OR with bits!
1.4.3 Identities Involving Zero and One
Our first identities involve 0 and 1. They can be proved by writing out their truth
tables, or by the arguments given below.
• A0 = 0, since A and 0 are never both 1.
• A1 = A, since A and 1 are both 1 when A is 1.
• A+0 = A, since 0 is never 1, so A or 0 is whatever A is.
• A+1 = 1, since A or 1 is always 1.
The first three are consistent with elementary algebra: multiplying by 0 yields 0,
multiplying by 1 does nothing, and adding 0 does nothing. The last point, however,
is different. In boolean algebra, “adding” 1 yields 1 because it is the OR operator,
not addition.
1.4.4 Single-Variable Identities
There are several identities involving logic gates on only a single bit A and its inverse
A. They can be proved by writing out their truth tables, or by the arguments given
below.
• A = A, since inverting twice results in the original bit.
• AA = A, since A and itself is just A.
• AA = 0, since A and A are inverses, they are never both 1.
• A+A = A, since A or itself is just A.
• A+A = 1, since A and A are inverses, one of them is always 1.
These identities now differ substantially from adding and multiplying numbers, e.g.,
3·3 = 32.
40
1 Classical Information and Computation
1.4.5 Two-Variable Identities and De Morgan’s Laws
There are some important identities involving two bits A and B. They can be proved
by writing out their truth tables, or since they are less obvious, we also provide
algebraic proofs for each.
• A + AB = A. Regardless of B, when A = 0, A + AB = 0, and when A = 1, A +
AB = 1, so the result is entirely dependent on A. As an algebraic proof,
A+AB = A1+AB = A(1+B) = A1 = A.
• A+AB = A+B. When A = 1, A+AB is clearly 1. When A = 0, then A = 1, so
AB = B. As an algebraic proof,
A+AB = A+(AB) = (A+A)(A+B) = (1)(A+B) = A+B.
The next two points are called De Morgan’s Laws, which states that the NOT of
an AND is the OR of the NOTs, and the NOT of an OR is the AND of the NOTs.
• AB = A + B. For AB to be 1, AB must be 0. Thus, A and B are not both 1. At
least one of them must be 0, which is A + B. In Exercise 1.11, this statement
that NAND and negative-OR are equivalent was proved using truth tables. The
following algebraic proof is more complicated, so consider it optional.
Proof. We begin by showing that if X and Y satisfy X +Y = 1 and XY = 0, then
they are inverses, i.e., Y = X:
X = X1 = X(X +Y) = XX +XY = 0+XY
= XY +XY = (X +X)Y = 1Y = Y.
In the second equality, we used X +Y = 1, and in the fifth equality, we used
XY = 0. With these two assumptions, we found that X = Y, so the inverse of
X is Y. It must also be true that the inverse of Y is X, which we can prove by
inverting both sides of X = Y, resulting in X = Y, which simplifies to X = Y, so
the inverse of Y is X.
Now, assigning X = AB and Y = A+B, let us show that X +Y = 1 and XY = 0.
First,
X +Y = AB+(A+B)
Substitution
= AB+(A+B)(1)
A1 = A
= AB+(A+B)(A+A)
A+A = 1
= AB+AA+AA+BA+BA
Distributive Property
= AB+0+A+AB+BA
AA = 0,AA = A,Commutative Prop
= A(B+B)+A+BA
Distributive Property
= A(1)+A+BA
B+B = 1
1.4 Circuit Simplification and Boolean Algebra
41
= 1+BA
A1 = A,A+A = 1
= 1.
1+A = 1
Next,
XY = AB(A+B)
Substitution
= ABA+ABB
Distributive Property
= AAB+ABB
Commutative Property
= 0B+A0
AA = 0
= 0+0
A0 = 0
= 0.
Since X +Y = 1 and XY = 0, X = Y, and so AB = A+B, which is De Morgan’s
Law.
⊓⊔
(The square symbol signals the end of a proof.)
• A+B = AB. The left-hand side is 1 when the OR of A and B is 0, which only
occurs when A and B are both 0. The right-hand side is 1 only when A and B
are both 0. So, the two sides are equal. Logically, this says that A or B is false,
since it is false that neither A or B are true, both A and B must be false. In
Exercise 1.12, this statement that NOR and negative-AND are equivalent was
proved using truth tables. As with the previous point, the following algebraic
proof is more complicated, so consider it optional.
Proof. The proof is very similar to the previous one, but we take X = A+B and
Y = AB. First, we show that X +Y = 1:
X +Y = (A+B)+AB = (A+B)(A+A)+AB = AA+AA+BA+BA+AB
= (A+BA)+A(B+B) = A+A = 1.
Next, we show that XY = 0:
XY = (A+B)AB = AAB+BAB = 0B+A0 = 0.
Since X +Y = 1 and XY = 0, X = Y, and so A+B = AB, which is De Morgan’s
Law.
⊓⊔
Symbolically, breaking the NOT overline bar into two can be done at the expense of
changing AND to OR, or vice versa. A useful mnemonic for it is, “Break the line,
change the sign!”
In our previous discussion of universal gates, we showed that the NOT of A could
be implemented by making A both inputs of a NAND gate. We can now prove this
algebraically using De Morgan’s Law. Starting with the NAND of A with itself,
AA = A+A = A.
42
1 Classical Information and Computation
We also showed that the OR gate, A + B, could be implemented using three NOTs
and one AND, AB. We proved this by writing the truth table, but now we can also
prove it algebraically using De Morgan’s theorem:
AB = A+B = A+B.
Similarly, we showed in Exercise 1.27 using a truth table that the AND gate, AB,
could be implemented using three NOTs and one OR, A+B. Using De Morgan’s
theorem,
A+B = AB = AB.
1.4.6 Circuit Simplification
Using these boolean identities, let us go through three examples of simplifying cir-
cuits using boolean algebra, beginning with the example from the beginning of the
section.
1. Back to the example that started this section, let us show that ABC + ABC +
ABC +ABC = A+B+AC using boolean algebra:
ABC +ABC +ABC +ABC = AB(C +C)+A(B+B)C
Distributive Property
= AB1+A1C
A+A = 1
= AB+AC
A1 = A
= A+B+AC.
De Morgan’s Law
Note the second-to-last line uses three NOT gates, two AND gates, and one OR
gate, whereas the last line uses two NOT gates, one AND gate, and two OR
gates, so it uses one fewer gate.
2. As another example, consider the following circuit with seven gates:
ABC + ABC
A
B
C
This circuit implements ABC +ABC. Let us use boolean algebra to simplify it:
1.4 Circuit Simplification and Boolean Algebra
43
ABC +ABC = (A+B)C +ABC
De Morgan’s Law
= (A+B)C +ABC
B = B
= AC +BC +ABC
Distributive Property
= AC +(1+A)BC
Distributive Property
= AC +1BC
1+A = 1
= AC +BC
1B = B
= (A+B)C.
Distributive Property
This complicated circuit is equivalent to (A+B)C, which only has three gates:
(A + B)C
A
B
C
3. For our third example, we have (A+B)(A+B+C)C:
(A + B)(A + B + C)C
C
A
B
As drawn, it has five gates. Two of the gates, however, have three inputs, and if
we were to replace each of them with two two-bit gates, we would have seven
gates total. Simplifying it,
(A+B)(A+B+C)C
= AAC +ABC +ACC +BAC +BBC +BCC
Distributive Property
= 0C +ABC +A0+BAC +BC +B0
AA = 0,BB = B
= ABC +BAC +BC
A0 = 0,A+0 = A
= (A+A+1)BC
Distributive Property
= (1)BC
A + 1 = 1
= BC.
1A = A
44
1 Classical Information and Computation
So, the output of the circuit is only dependent on B and C, not A. It only needs
one NOT gate and one AND gate.
Exercise 1.34. Simplify A(A+B). Hint: Your final result should be a single logic gate.
Exercise 1.35. Simplify (A+B)(A+B+C). Hint: Your final result should have one NOT gate and
two OR gates.
Exercise 1.36. Simplify the following circuit:
A
B
C
Exercise 1.37. Consider the following truth table with input bits A, B, and C:
A B C Output
0 0 0
0
0 0 1
0
0 1 0
1
0 1 1
1
1 0 0
1
1 0 1
0
1 1 0
1
1 1 1
0
(a) Create a circuit consisting of NOT, AND, and OR gates that implements the truth table.
(b) Simplify your previous circuit using boolean algebra.
1.5 Reversible Logic Gates
1.5.1 Reversible Gates
A reversible gate, is a logic gate where, given the output(s) of the gate, we can
always determine what the input(s) was (were). An example is the NOT gate:
1.5 Reversible Logic Gates
45
A
A
A A
0 1
1 0
From its truth table, the outputs are unique, so it is always possible to reverse the
operation. That is, if we know that the output of the NOT gate is 1, we know that the
input must have been 0, and if we know that the output is 0, we know that the input
must have been 1. The gate is reversible because, given the output, we can always
determine the input.
1.5.2 Irreversible Gates
An irreversible gate is the opposite of a reversible gate. Given the output(s) of the
gate, it is not always possible to determine what the input(s) was (were). An example
is the AND gate:
A
B
AB
A B AB
0 0 0
0 1 0
1 0 0
1 1 1
From the truth table, if the output of the AND gate is 1, then we know with certainty
that the inputs were both 1. If the output of the gate is 0, however, then it is impos-
sible to know from this information alone which of the other three inputs (00, 01,
and 10) were used. So in general, we are unable to determine the inputs to the AND
gate from its output. Thus, it is irreversible.
Notice the AND gate has two input bits, which have four possible states (00, 01,
10, and 11), and one output bit, which has two possible states (0 and 1). Since there
are fewer possibilities for the outputs than the inputs, the gate must be irreversible.
Conversely, if there are fewer input bits than output bits, the circuit is still irre-
versible because some of the outputs will be undetermined. For example, consider
the following truth table with one input bit and two output bits:
A B C
0 0 1
1 1 0
The question, “If both outputs are 0, what was the input?” is undefined by the truth
table, so its inverse is not completely specified.
Thus, for a logic gate to be reversible, it must have the same number of input bits
and output bits. The converse is not true, however. Just because a logic gate has the
same number of input bits and output bits does not necessarily mean it is reversible.
For example, for the gates corresponding to the following truth tables, the gate on
the left is reversible, but the gate on the right is irreversible:
46
1 Classical Information and Computation
A B C D
0 0 0 1
0 1 1 0
1 0 1 1
1 1 0 0
Reversible
A B C D
0 0 0 0
0 1 0 1
1 0 0 1
1 1 1 0
Irreversible
The gate on the left is reversible because, given the output, it is always possible to
determine the input. The gate on the right is irreversible because given the output
01, one does not know if the input was 01 or 10.
Another way to contrast reversible and irreversible gates is whether information
is lost. With a reversible gate, no information is lost since we can always recover
the inputs from the outputs. With an irreversible gate, however, information is lost
since, given an output, we generally do not know what the inputs were.
Exercise 1.38. Are the following gates reversible or irreversible? (a) OR. (b) XOR. (c) NAND. (d)
NOR.
Exercise 1.39. Are the following gates reversible or irreversible?
(a) The half-adder from Section 1.3.2.
(b) A gate with two inputs (A and B) and two outputs (C and D), whose truth table is shown
below:
A B C D
0 0 0 0
0 1 1 1
1 0 1 0
1 1 0 1
Exercise 1.40. The Fredkin gate is depicted below:
Fredkin
A
B
C
A′
B′
C′
It takes three inputs A, B, and C, and has three outputs A′, B′, and C′. A is the control bit. If A = 0,
then nothing happens to B and C. If A = 1, however, then B and C are swapped. The control bit is
unchanged, so A′ = A. Thus, the Fredkin gate is a controlled-SWAP gate.
(a) Write the truth table for the Fredkin gate.
(b) Based on the truth table, is the Fredkin gate reversible or irreversible? Why?
1.5.3 Toffoli Gate: A Reversible AND Gate
We have learned that the AND gate is irreversible, meaning we lose information
when we use an AND gate because we generally cannot reconstruct the inputs from
the outputs. It would be nice to have a reversible version of the AND gate, and in
this section we introduce one called the Toffoli gate.
The Toffoli gate has three inputs A, B, and C. To be reversible, it needs to have
three outputs, and they are A, B, and AB⊕C:
1.5 Reversible Logic Gates
47
Toffoli
A
B
C
A
B
AB ⊕C
The Toffoli gate can be constructed using an AND gate and an XOR gate. We take
the AND of A and B, and we take the XOR of AB with C to get AB⊕C.
A
A
B
B
AB
C
AB ⊕C
The truth table for the Toffoli gate is
A B C A B AB⊕C
0 0 0 0 0
0
0 0 1 0 0
1
0 1 0 0 1
0
0 1 1 0 1
1
1 0 0 1 0
0
1 0 1 1 0
1
1 1 0 1 1
1
1 1 1 1 1
0
We see from the truth table that the outputs are unique, so the Toffoli gate is re-
versible. Notice when C = 0, the third output is the AND of A and B. This can also
be seen using boolean algebra, since when C = 0, AB⊕C = AB⊕0 = AB, which is
the AND of A and B. Thus, the Toffoli gate is a reversible version of the AND gate.
From the above truth table, notice there are two rows where A = 0 and B = 0,
and the third outputs of these two rows are opposite each other. Similarly, there are
two rows where A = 0 and B = 1, and the third outputs are again opposite each
other. This is true for every pair of rows with fixed A and B, and it ensures each
output of the truth table is unique, so the circuit is reversible. This observation that
each pair of rows has opposite third outputs can be proven using boolean algebra.
From before, when C = 0, the third output is AB. When C = 1, the third output is
AB ⊕C = AB ⊕1 = AB, so it is opposite AB. Furthermore, since AB is the NAND
of A and B, and since NAND is universal, the Toffoli gate is also universal.
Also from the truth table, notice the third bit is flipped when A and B are both 1.
For this reason, the Toffoli gate is also called the controlled-controlled-NOT gate or
CCNOT gate. Whether the third bit is flipped is controlled by whether the first two
bits are 1. That is, if A = B = 1, then the Toffoli gate flips C, and otherwise it does
nothing.
48
1 Classical Information and Computation
Exercise 1.41. Consider the anti-Toffoli gate, which flips the third bit if the first two bits are both
0:
Anti-
Toffoli
A
B
C
A
B
AB ⊕C
(a) Write the truth table for the anti-Toffoli gate.
(b) In the above picture of the anti-Toffoli gate, AB appears in the third output. Which law from
boolean algebra says that this is equal to A+B?
(c) When C = 0, is the third output the AND, OR, XOR, NAND, or NOR of A and B?
(d) When C = 1, is the third output the AND, OR, XOR, NAND, or NOR of A and B?
(e) Construct an anti-Toffoli gate using one Toffoli gate and four NOT gates.
1.5.4 Making Irreversible Gates Reversible
In the last section, we showed that the AND gate can be made reversible by XORing
its output with a third input C. This procedure works in general, not just for AND
gates. Say a gate has inputs A and B and one output f(A,B), which is a function that
outputs 0 or 1 depending on the inputs A and B:
Gate
A
B
f(A, B)
For example, for the AND gate, the function would map f(0,0) = 0, f(0,1) = 0,
f(1,0) = 0, and f(1,1) = 1. Such a gate is irreversible because it has fewer outputs
than inputs, but we can make it reversible by XORing its output with a third input
C:
A
A
B
B
Gate
f(A, B)
C
f(A, B) ⊕C
A B C A B f(A,B)⊕C
0 0 0 0 0
f(0,0)
0 0 1 0 0
f(0,0)
0 1 0 0 1
f(0,1)
0 1 1 0 1
f(0,1)
1 0 0 1 0
f(1,0)
1 0 1 1 0
f(1,0)
1 1 0 1 1
f(1,1)
1 1 1 1 1
f(1,1)
In the rightmost column of the truth table, we used f(A,B) ⊕0 = f(A,B) and
f(A,B) ⊕1 = f(A,B). So, this implements the original gate when C = 0. But, the
overall circuit is reversible. In the output, every permutation of A and B shows up
twice, once with f(A,B) and once with f(A,B), ensuring that the outputs are unique.
1.5 Reversible Logic Gates
49
We can generalize this technique several ways. First, the gate could be a function
of any number of variables. For example, say we have a gate with three inputs A, B,
C, and one output f(A,B,C):
Gate
A
B
C
f(A, B, C)
This must be irreversible because there are fewer outputs than inputs. To make it
reversible, we add a fourth input D that we XOR with f(A,B,C):
A
A
B
B
C
C
Gate
f(A, B, C)
D
f(A, B, C) ⊕D
Now, when D = 0, the bottom wire outputs f(A,B,C), and when D = 1, the bottom
wire outputs f(A,B,C). The truth table for this is
A B C D A B C D⊕f(A,B,C)
0 0 0 0 0 0 0
f(0,0,0)
0 0 0 1 0 0 0
f(0,0,0)
0 0 1 0 0 0 1
f(0,0,1)
0 0 1 1 0 0 1
f(0,0,1)
0 1 0 0 0 1 0
f(0,1,0)
0 1 0 1 0 1 0
f(0,1,0)
0 1 1 0 0 1 1
f(0,1,1)
0 1 1 1 0 1 1
f(0,1,1)
1 0 0 0 1 0 0
f(1,0,0)
1 0 0 1 1 0 0
f(1,0,0)
1 0 1 0 1 0 1
f(1,0,1)
1 0 1 1 1 0 1
f(1,0,1)
1 1 0 0 1 1 0
f(1,1,0)
1 1 0 1 1 1 0
f(1,1,0)
1 1 1 0 1 1 1
f(1,1,1)
1 1 1 1 1 1 1
f(1,1,1)
50
1 Classical Information and Computation
In the output, every permutation of A, B, and C shows up twice, once with f(A,B,C)
and f(A,B,C), ensuring that the outputs are unique. So, this circuit is reversible.
We can also generalize the technique to gates with multiple outputs. For example,
say we have an irreversible gate with two inputs A and B and two outputs f(A,B)
and g(A,B), which are functions of the inputs:
Gate
A
B
f(A, B)
g(A, B)
To make this reversible, follow the same procedure, but now we add two extra inputs
and two XOR gates, one each for f(A,B) and g(A,B):
A
A
B
B
Gate
f(A, B)
C
f(A, B) ⊕C
g(A, B)
D
g(A, B) ⊕D
As before, for the third output, when C = 0, then f(A,B) ⊕C = f(A,B) ⊕0 =
f(A,B), and when C = 1, then f(A,B) ⊕C = f(A,B) ⊕1 = f(A,B). Similarly, the
fourth output is g(A,B) when D = 0 and g(A,B) when D = 1. So, the truth table of
this circuit is
1.5 Reversible Logic Gates
51
A B C D A B C ⊕f(A,B) D⊕g(A,B)
0 0 0 0 0 0
f(0,0)
g(0,0)
0 0 0 1 0 0
f(0,0)
g(0,0)
0 0 1 0 0 0
f(0,0)
g(0,0)
0 0 1 1 0 0
f(0,0)
g(0,0)
0 1 0 0 0 1
f(0,1)
g(0,1)
0 1 0 1 0 1
f(0,1)
g(0,1)
0 1 1 0 0 1
f(0,1)
g(0,1)
0 1 1 1 0 1
f(0,1)
g(0,1)
1 0 0 0 1 0
f(1,0)
g(1,0)
1 0 0 1 1 0
f(1,0)
g(1,0)
1 0 1 0 1 0
f(1,0)
g(1,0)
1 0 1 1 1 0
f(1,0)
g(1,0)
1 1 0 0 1 1
f(1,1)
g(1,1)
1 1 0 1 1 1
f(1,1)
g(1,1)
1 1 1 0 1 1
f(1,1)
g(1,1)
1 1 1 1 1 1
f(1,1)
g(1,1)
In the output, every permutation of A and B shows up four times, once with f(A,B)
and g(A,B), once with f(A,B) and g(A,B), once with f(A,B) and g(A,B), and once
with f(A,B) and g(A,B), ensuring that the outputs are unique. So, this is reversible.
Exercise 1.42. Consider the following single-bit gates.
(a) The identity gate f(A) = A. Its truth table is
A f(A)
0
0
1
1
Is this reversible or irreversible? If it is irreversible, turn it into a reversible circuit.
(b) The “always-1’ gate f(A) = 1. Its truth table is
A 1
0 1
1 1
Is this reversible or irreversible? If it is irreversible, turn it into a reversible circuit.
Exercise 1.43. Write the truth table for XOR. Is it reversible or irreversible? If it is irreversible,
turn it into a reversible circuit, and write the truth table of the reversible circuit.
Exercise 1.44. Consider the following two-bit gates, each with two outputs.
(a) The gate
Gate
A
B
A ⊕B
B
52
1 Classical Information and Computation
What is the truth table corresponding to this gate? Is it reversible or irreversible? If it is
irreversible, turn it into a reversible circuit.
(b) The gate
Gate
A
B
A ⊕B
AB
What is the truth table corresponding to this gate? Is it reversible or irreversible? If it is
irreversible, turn it into a reversible circuit.
Exercise 1.45. Consider the full adder from Section 1.3.3:
FA
Cin
A
B
S
Cout
Turn this into a reversible circuit.
1.6 Error Correction
1.6.1 Errors in Physical Devices
High energy particles like cosmic rays, neutrons produced in the earth’s atmosphere,
radiation from nuclear testing, and radiation from particle accelerators can strike
computers, causing bits to flip from 0 to 1 and from 1 to 0. This is known as a single
event upset.
Similarly, if we are transmitting data through the internet, then some bits may
become corrupted. For example, photons traveling through a fiber optic cable can
leak out due to imperfections in the cable. The sender may have sent a 1, but the
receiver gets a 0.
Fortunately, as long as the number of errors is limited, there are schemes to detect
and/or correct errors.
Exercise 1.46. Watch “The Universe is Hostile to Computers” by Veritasium on YouTube:
https://www.youtube.com/watch?v=AaZ_RSt0KP8
Answer the following questions and fill in the blanks.
(a) For the election in Belgium, Maria Vindevogel received 4096 extra votes because the thir-
teenth bit had flipped from 0 to 1, which added 212 = 4096 extra votes to her count. How
many extra votes would she have received if the fourteenth bit had flipped instead?
(b) “In 1978, Intel reported some strange errors popping up in their 16 kilobit dynamic ran-
dom access memory, or DRAM. Ones would
to
zeros with no apparent cause. The problem turned out to be the ceramic packaging the
chip was encased in. With the demand for semiconductor packaging skyrocketing in the
1.6 Error Correction
53
1970’s, a new manufacturing plant was constructed on the Green River in Colorado. Unfortu-
nately, this site happened to be just downstream of an old uranium mill.
made their way into the river and then into the ceramic packaging for
Intel’s microchips. Intel scientists investigating the problem found that even trace amounts of
uranium and thorium in the ceramic was were sufficient to cause problems. In their DRAM,
memory was stored as the
or
of electrons in a
semiconductor well. The alpha particles emitted by uranium and thorium were energetic and
ionizing enough to create electron-hole pairs in the silicon. If an alpha particle struck in just
the right place, it could create a large number of free charge carriers, causing electrons to
accumulate in the well, flipping a one to a zero. This is known as a
.”
(c) “The reason this problem was identified in the 1970s was because chip components had been
to the point where a single alpha particle could produce enough charge
to flip a bit.”
(d) “But the next year, he [Hess] conducted seven balloon flights up to an altitude of 5200 m.
And here he discovered something remarkable. While there was an initial drop in radiation
for the first several hundred meters, above one kilometer or so the level
with increasing altitude. At his his maximum height, the level of radiation was several times
than it was on the ground. The radiation appeared to be coming, not
from the earth, but from the
.”
(e) “Victor Hess had discovered
, high energy radia-
tion from space. But what were these rays exactly, and where were they coming from? Well,
today we know they aren’t electromagnetic rays, as many expected, but
.
Around 90% are protons, 9% are helium nuclei, and 1% are heavier nuclei. Some of them are
from the sun, but they have comparatively low energy. High energy cosmic rays moving very
close to the speed of light come from exploding stars, supernovae, in our own galaxy and
in others. And the highest energy particles are thought to come from
, including the super massive black holes at the center of galaxies.”
(f) “But primary cosmic rays like these don’t make it down to earth’s surface. Instead, they
collide with air molecules around 25 kilometers above the ground and create new parti-
cles like pions. These collide and decay into other particles like neutrons, protons, muons,
electrons, positrons, and photons, which in turn collide with other molecules in one long
. So from a single primary cosmic ray comes a shower of particles
streaming toward the earth. It is one of these particles that investigators suspect struck a
in a computer in Belgium, flipping the thirteenth bit from a zero to a
one and giving Maria Vindevogel 4096 extra votes.”
(g) “These days, there are a number of ways computer chips are made resilient in the face of bit
flips, like
(or ECC) mem-
ory.”
(h) “In 1996, IBM estimated that for each 256 megabytes of RAM, one bit flip occurs per
. And the main culprit seems to be neutrons created in the shower of
particles from
.”
(i) For a commercial airplane “At cruising altitude, this increases the chance of a single event
upset by
times.”
(j) “On one five day [space shuttle] mission, STS 48, there were
separate
bit flips.”
(k) “Above the atmosphere, cosmic rays are so energetic sometimes you can even see them.”
“Once in a while you have your eye closed and you’re not asleep yet, and if you wait a
little while, you occasionally will see a
of light. And we think it is
heavy particles or individual bursts of energy coming from radiation that are either going
through the eyeball itself or the optic nerve. And the way that your body registers radiation
going through it is amazingly enough by showing you a little flash in one of your eyes just to
remind you that you are subject to the radiation of not only our sun, but
of the universe that is radiating at you.”
54
1 Classical Information and Computation
1.6.2 Error Detection
The simplest way to detect errors is to repeat each bit multiple times so that multiple
physical bits encode a single logical bit. This is called the repetition code.
For example, say we use two physical bits to encode one logical bit, i.e.,
00 encodes 0,
11 encodes 1.
So, if we want to send the letter “Q” in ASCII, which is represented by the seven-bit
string 1010001, we would actually send
11 00 11 00 00 00 11.
If one of the physical bits is flipped, say due to a single event upset or transmis-
sion error, then instead of receiving 00 or 11, the recipient would receive
01
or
10.
If the recipient gets either of these pairs, they know that an error has occurred. This
is an example of an error-detecting code. The recipient can then request that the
message be resent.
The binary strings 00, 01, 10, and 11 are called codewords.
The parity of a bit string is whether the bit string has an even or odd number of
1’s. The codewords 00 and 11 have even parity, while the codewords 01 an 10 have
odd parity, so parity can be used to distinguish whether an error has occurred. Such
indications of errors are called error syndromes.
Note parity can be computed using XOR, since 0⊕0 = 1⊕1 = 0 indicates even
parity, while 0⊕1 = 1⊕0 = 1 indicates odd parity.
If more than one error occurs, then our encoding is unable to reliably detect
the error. For example, if we transmit 00 and both bits get flipped to 11, then the
recipient has no way of knowing that an error occurred. So, if we use the repetition
code with two physical bits per logical bit, only one-bit errors can be detected.
Exercise 1.47. Another error-detecting scheme is to send a parity bit. For example, say I want to
transmit to you the character “Q” in ASCII, which is encoded by seven bits 1010001. The parity
of this bit string is odd, since there is an odd number of 1’s in the string. (Alternatively, taking the
XOR of all the bits yields 1.) I append this parity of 1 to the transmission, so I send you the eight
bits 10100011.
Say the bits you actually receive are 11100011.
(a) Calculate the parity of the first seven bits you received.
(b) Does the parity you calculated in part (a) match the parity bit (the last bit)?
(c) Did an error occur in the transmission? Why or why not?
(d) Can this scheme reliably detect if more than one error occurred? Why or why not?
1.6 Error Correction
55
1.6.3 Error Correction
Besides being able to detect errors, it would be nice to also fix them. We call this
error correction.
For example, if we use three bits for the repetition code, then
000 encodes 0,
111 encodes 1.
Now if a single bit gets flipped, the possible codewords are 001, 010, 100, 110, 101,
and 011. We can correct the error by taking the majority vote:
001,010,100 →000,
110,101,011 →111.
This is an example of an error-correcting code.
This majority rule can also be implemented using parity checks as the error syn-
dromes. Let us calculate the parity of the left two bits and the parity of the right two
bits. Tabulating these for each possible codeword b2b1b0:
Codeword b2 ⊕b1
b1 ⊕b0
000
0
0
001
0
1
010
1
1
011
1
0
100
1
0
101
1
1
110
0
1
111
0
0
So if both parity bits are 0, the codeword is either 000 or 111, and there is no error.
If the left parity bit is 0 and the right is 1, then the codeword is either 001 or 110,
and the rightmost bit was flipped. If the left parity bit is 1 and the second is 0, then
the codeword is either 011 or 100, and the left bit was flipped. Finally, if both parity
bits are 1, then the codeword is 010 or 101, and the middle bit was flipped.
Note using parity checks allows us to detect and correct errors without needing
to know the codeword. If we know that a bit flip occurred in the middle bit, we do
not need to know if the codeword was 010 or 101. We can simply flip the middle
bit to correct it, and proceed. This will be an important notion in quantum error
correction, where determining the codeword can ruin the computation, so we must
be able to correct errors without knowing the precise codeword.
Since this error-correcting code uses three physical bits to encode a logical bit, it
is more likely that a bit will flip because there are more of them. If a single bit flips,
it can be corrected. If two or all three bits flip, however, we cannot correct the error.
We say an uncorrectable error has occurred. Using a little math, we can determine
56
1 Classical Information and Computation
when error correction decreases the chance of an uncorrectable error because we
can fix single bit flips, or when it increases the chance of an uncorrectable error
because there are more bits to flip.
First, let p denote the probability of a single bit flipping. Without error correction,
if the bit flips an uncorrectable error has occurred, so the probability of an uncor-
rectable occurring is p. Now with three physical bits representing one logical bit,
single-bit errors can be corrected, so an uncorrectable error only occurs when two
or three of the bits get flipped. The probability of two specific bits getting flipped
while the third remains unflipped is p2(1 −p), where p2 comes from the two bits
getting flipped, and 1 −p comes from the unflipped third bit. Since there are three
combinations for two of the three bits to be flipped (the two bits could be the first
two bits, the last two bits, or the first and last bits), the probability of any two bits
getting flipped is 3p2(1 −p). Another way to get the coefficient of 3 is using the
combination “3 choose 2,” where “n choose k” is
nCk =
n!
k!(n−k)!,
where the exclamation point denotes factorial, e.g., 5! = 5 · 4 · 3 · 2 · 1 and 0! = 1.
Combinations are also called binomial coefficients.
Binomial coefficients can be calculated manually, perhaps with the help of a
calculator. They can also be calculated using a computer algebra system, which is
computer software that solves problems using algebraic manipulation and more. In
this textbook, we will provide calculations in both Mathematica and SageMath, and
you can choose whichever you prefer.
• Mathematica. Among physicists, a popular computer algebra system is Math-
ematica. It is proprietary, however, so it must be purchased, although many
universities pay for a Mathematica subscription that allows their students to
use it. The combination nCk can be computing using Mathematica’s Binomial
function. For example, 3C2 is:
Binomial[3,2]
The output of this is 3, as expected.
• SageMath. Another popular computer algebra system is SageMath, often simply
called Sage. It is based on the Python programming language. One of the main
benefits of SageMath is that it is open-source, so anyone can download it from
sagemath.org and install it for free. SageMath’s binomial function can be
used to compute the combination nCk. For example, 3C2 is:
sage: binomial(3,2)
3
We see that the answer is 3, as expected.
An uncorrectable error also occurs when all three bits get flipped, and the proba-
bility of that occurring is p3. The coefficient of this is simply 1 because there is only
1 way to choose 3 bits out of 3. Alternatively, 3C3 = 1. Adding together the prob-
ability of two bits flipping [3p2(1 −p)] with the probability of three bits flipping
1.6 Error Correction
57
(p3), the probability of an uncorrectable error occurring is
3p2(1−p)+ p3.
As long as this is less than p, which is the probability that an error occurs without
error correction, then it is favorable to do error correction. That is, it is favorable to
do error correction when
3p2(1−p)+ p3 < p.
We can solve this inequality using a computer algebra system. As promised, we will
provide both Mathematica and SageMath code:
• Mathematica. Using Mathematica’s Reduce function, the inequality can be
solved using
Reduce[3 pˆ2(1-p) + pˆ3 < p, p]
Executing this, the output is
0 < p < 1
2
||
p > 1.
In this result, the double bar || means “or.” So, there are two situations when
the 3-bit repetition code is better than no error correction, when 0 < p < 1/2 or
when p > 1.
• SageMath. Using SageMath’s solve function, the inequality can be solved us-
ing
sage: p = var(’p’)
sage: solve(3*p**2*(1-p) + p**3 < p, p)
[[p > 0, p < (1/2)], [p > 1]]
So, there are two situations when the 3-bit repetition code is better than no error
correction. The first is when p > 0 and p < 1/2. That is, when 0 < p < 1/2.
The second is when p > 1.
Thus, using either Mathematica or SageMath, the repetition code with three bits is
better when the probability of a single bit flip is
0 < p < 1
2,
p > 1.
Since p is a probability, it cannot be less than 0 or greater than 1. So, the 3-bit
repetition code is better than no error correction when
p < 1
2.
Thus, as long as the error probability is less than 1/2, error correction is beneficial.
For example, if p = 0.1, then 3p2(1−p)+ p3 = 0.028, so there is a smaller chance
that an uncorrectable error occurs. If p = 0.6, then 3p2(1−p)+ p3 = 0.648, so an
uncorrectable error is more likely to occur.
58
1 Classical Information and Computation
Exercise 1.48. Using the repetition code, you have a three-bit string that encodes a logical bit.
(a) If the parity of the left two bits is odd, and the parity of the right two bits is also odd, has a
single-bit error occurred? If yes, which bit was flipped (the left, middle, or right bit)?
(b) If the parity of the left two bits is odd, but the parity of the right two bits is even, has a
single-bit error occurred? If yes, which bit was flipped (the left, middle, or right bit)?
(c) If the probability of a single bit flipping is p = 0.2, what is the probability that an uncorrectable
error occurs with the 3-bit repetition code? In this case, does the 3-bit repetition code increase
or decrease the probability of an uncorrectable error occurring?
(d) If the probability of a single bit flipping is p = 0.7, what is the probability that an uncorrectable
error occurs with the 3-bit repetition code? In this case, does the 3-bit repetition code increase
or decrease the probability of an uncorrectable error occurring?
Exercise 1.49. Using the repetition code, you have a five-bit string that encodes a logical bit. This
allows for the correction of two-bit errors, since three correct bits is the majority vote over the two
incorrect bits. Say you receive a codeword b4b3b2b1b0.
(a) If b4 ⊕b3 = 0, b3 ⊕b2 = 0, b2 ⊕b1 = 1, and b1 ⊕b0 = 1, did an error occur? If an error has
occurred, which bit(s) were flipped?
(b) If b4 ⊕b3 = 0, b3 ⊕b2 = 1, b2 ⊕b1 = 0, and b1 ⊕b0 = 1, did an error occur? If an error has
occurred, which bit(s) were flipped?
(c) If the probability of a single bit flipping is p, what is the probability of an uncorrectable error
occurring with this repetition code of 5 bits? Hint: One and two-bit flips can be corrected,
so uncorrectable errors occur if three, four, or five bits are flipped. The probability of each
occurring also depends on the number of combinations of bits flipping.
(d) For what values of p is your result in part (c) less than p (the error rate without error correc-
tion)?
(e) If p = 0.1, what is the probability that an uncorrectable error occurs with the 5-bit repetition
code? In this case, does the 5-bit repetition code increase or decrease the probability of an
uncorrectable error occurring?
(f) For comparison, if p = 0.1, what is the probability that an uncorrectable error occurs with the
3-bit repetition code? How does it compare to the 5-bit code?
1.7 Computational Complexity
1.7.1 Asymptotic Notation
Say we want to add two 4-bit numbers using the ripple-carry adder from Sec-
tion 1.3.4. This takes one half-adder, which has two logic gates, and three full-
adders, which each have five logic gates, for a total of 2 +3 ·5 = 17 logic gates. If
we generalize this and add two n-bit numbers with a ripple-carry adder, it takes one
half-adder and (n −1) full-adders, for a total of 2 + (n −1)5 = 5n −3 logic gates.
So, as the length n of the binary numbers increases, the number of logic gates 5n−3
scales linearly with n. This scaling can be expressed through asymptotic notation.
Big-O notation is used to give an upper bound on the asymptotic behavior of a
function. We write 5n −3 = O(n2) to mean that 5n −3 scales less than or equal to
n2 for large n. Mathematically, f(n) = O(g(n)) means there exists constants c and
n0 such that f(n) ≤cg(n) for all values of n greater than n0. Some other examples
include 5n −3 = O(nlog(n)) and 5n −3 = O(2n). Since the scaling is “less than
1.7 Computational Complexity
59
or equal to,” we can also write 5n −3 = O(n). Big-O is the most commonly used
asymptotic notation, and it is useful for specifying the worst-case behavior of an
algorithm. In our case, the number of logic gates needed to add two binary numbers
of length n is no worse than linear, i.e., O(n).
If we want the inequality to be strictly “less than,” we use little-o notation. So we
can write 5n−3 = o(n2), but 5n−3 ̸= o(n). Mathematically, f(n) = o(g(n)) means
for all c > 0 there exists some n0 (which may depend on c) such that f(n) < cg(n)
for all values of n greater than n0.
Similarly, a lower bound on the asymptotic behavior of a function is denoted
using big-Omega notation. We can write 5n−3 = Ω(√n) to mean that 5n−3 scales
greater than or equal to √n for large n. Mathematically, f(n) = Ω(g(n)) means
there exists constants c and n0 such that f(n) ≥cg(n) for all values of n greater
than n0. We could also write 5n −3 = Ω(1) to mean that 5n −3 is lower bounded
by a constant. Since the inequality is “greater than or equal to,” we can also write
5n−3 = Ω(n).
As before, if we want the inequality to be strictly “greater than,” we use a low-
ercase symbol, or little-omega notation. So, we can write 5n −3 = ω(√n), but
5n −3 ̸= ω(n). Mathematically, f(n) = ω(g(n)) means for all c > 0 there exists
some n0 (which may depend on c) such that f(n) > cg(n) for all values of n greater
than n0.
Finally, to specify that 5n −3 scales linearly with n, we use big-Theta notation.
We write this as 5n −3 = Θ(n), and it means that 5n −3 is both upper bounded
and lower bounded by n, asymptotically. That is, 5n−3 = O(n) and 5n−3 = Ω(n).
Combining the mathematical definitions of each, f(n) =Θ(g(n)) means there exists
constants c1, c2, and n0 such that c1g(n) ≤f(n) ≤c2g(n) for all values of n greater
than n0.
These asymptotic notations are summarized in Table 1.2.
Table 1.2: Summary of asymptotic notations. The mathematical symbol ∃means
“there exists,” ∋means “such that,” and ∀means “for all.”
Notation
Description
Definition
f(n) = O(g(n)) f scales ≤g
∃c,n0 ∋f(n) ≤cg(n) ∀n > n0
f(n) = o(g(n)) f scales < g
∀c > 0 ∃n0 ∋f(n) < cg(n) ∀n > n0
f(n) = Ω(g(n)) f scales ≥g
∃c,n0 ∋f(n) ≥cg(n) ∀n > n0
f(n) = ω(g(n)) f scales > g
∀c > 0 ∃n0 ∋f(n) > cg(n) ∀n > n0
f(n) = Θ(g(n)) f scales = g ∃c1,c2,n0 ∋c1g(n) ≤f(n) ≤c2g(n) ∀n > n0
Exercise 1.50. Consider the following functions, one quadratic and another cubic:
f(n) = 5247n2,
g(n) = 11n3.
(a) Evaluate both functions when n = 100. Which is bigger, f(100) or g(100)?
60
1 Classical Information and Computation
(b) Evaluate both functions when n = 500. Which is bigger, f(500) or g(500)?
(c) At what value of n does g(n) surpass f(n)?
(d) Label each of the following statements as true or false:
f(n) = O(g(n))
true / false
f(n) = o(g(n))
true / false
f(n) = Θ(g(n))
true / false
f(n) = Ω(g(n))
true / false
f(n) = ω(g(n))
true / false
Exercise 1.51. Match each function to a possible asymptotic notation. Although each asymptotic
notation can match multiple functions, each should only be used once.
(a) 3n2 +7n+4
(i) o(√n)
(b) logn
(ii) Ω(2n)
(c) 5en
(iii) Θ(n2)
(d) π√n
(iv) O(1)
(e) π
(v) ω(1)
1.7.2 Complexity Classes
Since each logic gate takes some time to apply, we say an algorithm that utilizes,
say, O(n) logic gates, takes O(n) time, ignoring the fact that some of the logic gates
can be done in parallel.
In computer science, an algorithm is called efficient if it takes polynomial time
or less. Polynomial time means the number of gates scales as a polynomial in n,
such as the ripple-carry adder’s 5n−3 gates, or algorithms that take n2, n3, or n1000
gates. Any other power function is also efficient, such as n2.5, since this scales less
than the polynomial n3. Another example is √n = n0.5, since this scales less than
the polynomial n. Algorithms that takes constant time O(1) or logarithmic time
O(log(n)) are faster than polynomial time, so they are also efficient. A loglinear
runtime O(nlog(n)) is also efficient, since it grows faster than n but slower than
n2. On the other hand, an algorithm is inefficient if it takes more than polynomial
time, called superpolynomial time. This includes algorithms that take exponential
time, such as 2n or en/1000. It also includes algorithms that take subexponential time,
which are less than exponential time but greater than polynomial time, such as 2n1/3.
Often, we call problems easy if their solutions are efficient, and problems hard
if their solutions are inefficient. For example, adding numbers is easy because there
is a polynomial-time algorithm for it. In practice, however, an easy problem whose
algorithm takes n1000 time may be harder than a hard problem that takes en/1000
time because, for small problem sizes, the polynomial is bigger than the exponen-
tial. Despite this, distinguishing between easy and hard problems, or efficient and
inefficient algorithms, in this manner is still useful.
1.7 Computational Complexity
61
Different problems are easier or harder than others, and we classify them using
complexity classes, or classes of problems of some complexity or difficulty. Prob-
lems that can be solved efficiently (in polynomial time) by a classical computer are
in the complexity class P, which stands for Polynomial-Time. Some problems in P
include:
• Matchmaking in the “stable marriage problem.” Given n men and n women
who have ranked the other group in order of preference, marry the men with the
women so that no two people would rather be with each other than with their
spouses. The Gale-Shapley algorithm solves this in O(n2) time.
• Determining if a number is prime. Recall a prime number is a whole number
greater than 1 whose only factors are 1 and itself, so 2, 3, 5, 7, 11, and 13 are
all prime numbers, but 4 is not prime because it is equal to 2 × 2, and 6 is not
prime because it is equal to 2×3. If the number to be tested has n digits, then the
Agrawal–Kayal–Saxena (AKS) class of algorithms can determine if the number
is prime in O(n6) time, ignoring factors log(n).
• Maximizing a linear function constrained by linear inequalities, also known as
“linear programming.” If there are n variables, algorithms exist that run in O(n3)
and faster.
Another complexity class is the problems for which a solution can be quickly
verified by a computer in polynomial time. This class is called NP (the N stands
for a non-deterministic Turing machine—more on this later, and the P stands for
polynomial), and it includes problems such as:
• Factoring, since it is easy to multiply the factors together to verify it equals the
original number.
• Testing if two networks are equivalent, since it is easy to verify that two net-
works are equivalent if a map between the two is provided. This is known as the
“graph isomorphism” problem.
Many problems of practical importance are in the class NP.
Certain problems within NP have a special property called completeness, and
we call these problems NP-COMPLETE. If we can find an efficient solution to any
NP-COMPLETE problem, then we can use it to find an efficient solution to any NP
problem. That is, the overhead in applying the algorithm to other NP problems is at
most polynomial, so an efficient solution to one NP-COMPLETE problem yields an
efficient solution to all NP problems. Some NP-COMPLETE problems include:
• Solving n×n Sudoku puzzles.
• Finding the shortest possible tour that visits a list of cities exactly once and
returns to its starting point, known as the “traveling salesman problem.”
• Determining whether a tour that visits each location once and returns to its
starting point exists, which is called the “Hamiltonian path problem.”
• Determining whether a set of items can fit into certain boxes, known as the “bin
packing problem.”
A literal million dollar question is whether P and NP are equal. The Clay Math-
ematics Institute will pay one million U.S. dollars to whomever can prove whether
62
1 Classical Information and Computation
the problems whose solutions are efficiently found are the same as the problems
whose solutions are efficiently verified. It is known that all problems in P are con-
tained within NP, since if one can efficiently solve a problem, one can also efficiently
check proposed solutions by comparing them to the answer. But, it is unknown if
NP contains any problems that are not in P. The general conjecture is that P ̸= NP.
Another complexity class is PSPACE, which contains all the problems that can
be solved by a computer using a polynomial amount of memory, without any limits
on time. Generalizations of many games are in PSPACE, such as:
• Determining whether a winning strategy exists for the game of checkers gener-
alised to an n×n board.
• Winning a generalized level of Super Mario Bros.
It is known that NP is contained in PSPACE because one has unlimited of time
to check all possible answers. Although it seems like PSPACE should be a larger
class of problems than NP, there is currently no proof. The general conjecture is
that NP ̸= PSPACE. It is even unknown if PSPACE is larger than P, but again the
conjecture is that P ̸= PSPACE.
To summarize, it is believed that P ̸= NP and NP ̸= PSPACE (and hence PSPACE ̸=
P), but none of these relations have been proved. These suspected relationships are
summarized in Fig. 1.1.
PSPACE
NP
NP-
complete
P
Fig. 1.1: The generally accepted relationships between the complexity classes NP,
NP-COMPLETE, P, and PSPACE.
Exercise 1.52. Consider the following runtimes. Is each efficient or inefficient? (a) log2(n) (b) n3
(c) n5 log4(n) (d) 2
√n (e) 2n/2 (f) n!
1.8 Turing Machines
63
Exercise 1.53. Visit http://www.complexityzoo.com/. Look up the complexity class
BPP.
(a) What does BPP stand for?
(b) BPP often identified as what?
Exercise 1.54. What is a problem in NP that is believed to not be in P?
Exercise 1.55. Visit the Clay Mathematics Institute website at claymath.org and find the Mil-
lenium Problems.
(a) List the seven Millenium Prize Problems. Is each problem solved or unsolved?
(b) Who first proposed the P vs NP problem? In what year?
Exercise 1.56. Visit the Wikipedia list of NP-COMPLETE problems at https://en.wikiped
ia.org/wiki/List_of_NP-complete_problems. Describe three NP-COMPLETE prob-
lems.
1.8 Turing Machines
So far, we have examined circuit-based computers consisting of bits and logic gates.
While circuit-based computers are very useful in practice, they are not always the
best kind of computer to study mathematically. Other types of computers, or models
of computation, exist and may be more convenient. In this section, we examine the
Turing machine, which was introduced in 1936 and is still used today by computer
scientists who study the abilities and limitations of computers.
1.8.1 Components
A Turing machine consists of four parts:
▷
0
0
1
0
. . .
qs
(head)
(state)
(tape)
Program
(qs, ▷, ▷, →, q1)
...
1. A tape divided into cells, each with a symbol from some finite alphabet. The
left end of the tape is denoted by ▷, and the right end of the tape can extend as
far as needed. In this textbook, the remaining cells can contain nothing (blank),
0, or 1.
2. A head that can read or write to the tape, and then move left one cell (←), right
one cell (→), or stay put (•).
64
1 Classical Information and Computation
3. A register that stores the state of the Turing machine. Only a finite number
of states are allowed. Two special states are required, a starting state qs and a
halting state qh indicating that the program has finished.
4. A list of instructions or program. For each step, the Turing machine starts at the
top of the list of instructions and goes down the list until it finds a line matching
the current state of the machine and the current symbol on the tape. Then it
write to the tape, moves according to the instruction, and updates the state of
the machine.
While modern computers are not built like this, Turing machines can compute ev-
erything that a circuit-based computer can compute. But, they are easier to describe
mathematically, so they allow computer scientists to study the power and limitations
of computers. Before we discuss this, let us look at an example of how a Turing ma-
chine can compute something.
Exercise 1.57. Visit the Wikipedia page on Model of Computation at https://en.wikiped
ia.org/wiki/Model_of_computation. Describe three models of computation besides
the Turing model.
1.8.2 Incrementing Binary Numbers
Let us show how a Turing machine can increment (add 1 to) a binary number. First,
consider incrementing 1011 by hand:
(carry) 0 1 1
1011
“+” 0001
(sum) 1100
From this process, we can deduce an algorithm for how to increment binary num-
bers. Starting at the rightmost column, when we add 1 to 1, we get 0, and we carry a
1 to the next column. Again, we add 1 to 1 and get 0, carrying 1 to the next column.
Now we add 1 to 0, which yields 1, and nothing carries. Any remaining digits will
be unchanged. Thus, to increment binary numbers, we can flip all 1’s on the right to
0’s, and when we reach the rightmost 0, we flip it to a 1.
This computation can be done using a Turing machine using the following pro-
gram:
1.8 Turing Machines
65
Current State Current Tape Write to Tape Move Update State
qs
▷
▷
→
q1
q1
0
0
→
q1
q1
1
1
→
q1
q1
←
q2
q2
1
0
←
q2
q2
0
1
•
qh
We will show that this correctly increments 1011 in a moment, but first let us give
an overview of the program. The first line says that when the Turing machine starts,
it should move right and change states to q1. In the next three lines, the state is q1,
and the Turing machine keeps moving right until it reaches a blank cell, at which
it moves left and changes to the state q2. This moves the head all the way to the
rightmost bit. Now in the state q2, the last two lines say to flip any 1’s to 0’s, but the
moment we reach a 0, we should flip it to a 1 and then halt.
Now, let us follow the program to increment 1011. The Turing machine begins
in the state qs with its head at the leftmost cell:
▷
1
0
1
1
. . .
qs
The machine goes through its list of instructions from top to bottom, searching for
the first instruction that matches its current state qs with current tape symbol ▷. It
finds the entry (qs,▷,▷,→,q1). So it writes ▷to the tape (so it is unchanged), moves
one cell right, and updates the state register to q1:
▷
1
0
1
1
. . .
q1
The Turing machine again goes through its list of instructions from top to bottom,
searching for the first instruction that matches its current state q1 with current tape
symbol 1. It finds the entry (q1,1,1,→,q1). So it writes 1 to the tape (so it is un-
changed), moves one cell right, and sets the state register to q1 (so it is unchanged):
▷
1
0
1
1
. . .
q1
Now the head reads 0 on the tape, so it searches its program from top to bottom,
finding the instruction (q1,0,0,→,q1). Following this, it writes 0 to the tape (so it
is unchanged), moves the head right, and sets the state to q1 (so it is unchanged):
▷
1
0
1
1
. . .
q1
66
1 Classical Information and Computation
Now the tape reads 1, so the machine follows the instruction (q1,1,1,→,q1). Fol-
lowing this, the head moves right again (while the tape and state are unchanged):
▷
1
0
1
1
. . .
q1
The machine again follows the instruction (q1,1,1,→,q1), moving the head right:
▷
1
0
1
1
. . .
q1
Thus far, while the Turing machine was in the state q1, the head moved to the right
past all the bits. Now the head reads blank, so the Turing machine follows (q1, , ,←
,q2), writing a blank symbol (so it is unchanged), moving the head left, and updating
the state to q2:
▷
1
0
1
1
. . .
q2
The head is now at the rightmost bit, and the Turing machine follows (q2,1,0,←
,q2), writing a 0 and moving left:
▷
1
0
1
0
. . .
q2
The Turing machine follows (q2,1,0,←,q2) again, writing a 0 and moving left:
▷
1
0
0
0
. . .
q2
Finally, the Turing machine follows (q2,0,1,•,qh), writing a 1 and halting:
▷
1
1
0
0
. . .
qh
So in the state q2, the Turing machine moves to the left, flipping all 1’s to 0’s, until
it sees the first 0, at which it flips it to a 1 and halts. As seen on the final tape, we
have incremented 1011 to 1100, as expected.
Exercise 1.58. A Turing machine follows the program shown below:
1.8 Turing Machines
67
Current State Current Tape Write to Tape Move Update State
qs
▷
▷
→
q1
q1
0
0
→
q2
q1
1
1
→
q1
q2
0
0
→
q2
q2
1
1
→
q2
q1
0
•
qh
q2
1
•
qh
(a) Apply this program to the tape shown to the right until it
halts. What is the resulting tape?
▷
0
0
. . .
qs
(b) Apply this program to the tape shown to the right until it
halts. What is the resulting tape?
▷
0
1
. . .
qs
(c) Apply this program to the tape shown to the right until it
halts. What is the resulting tape?
▷
1
0
. . .
qs
(d) Apply this program to the tape shown to the right until it
halts. What is the resulting tape?
▷
1
1
. . .
qs
(e) From your answers in parts (a) through (d), what does this Turing machine compute?
(f) What does the program do if the tape initially has more than two bits?
Exercise 1.59. Write a program for a Turing machine that calculates the parity of a bit string of
arbitrary length. Assume the tape starts with the symbol ▷and is followed by the bit string, which
can be any length, followed by blanks. Write the parity as 0 or 1 (even or odd) to the blank after
the bit string, and then halt.
1.8.3 Church-Turing Thesis
Turing machines are important for many reasons, but especially because of two
long-held beliefs regarding computation:
• The Church-Turing Thesis says that everything that is computable can be com-
puted with a Turing machine, although it could take a long time (e.g., exponen-
tial time).
This correctly suggests that there are problems that cannot be computed. They
are called undecidable problems, the most famous of which is the halting prob-
lem (see Exercise 1.60). Aside from such uncomputable problems, everything
else can be computed, and it can be computed using a Turing machine.
68
1 Classical Information and Computation
• The Strong Church-Turing Thesis says that any model of computation, be it
the circuit model or something else, can be simulated by a probabilistic Turing
machine with at most polynomial overhead.
A probabilistic Turing machine is a Turing machine where the state of the sys-
tem can be set probabilistically, such as by the flip of a coin. The Strong Church-
Turing Thesis says that a probabilistic Turing Machine can perform the same
computations as any other kind of computer, and it only needs at most polyno-
mially more steps than the other computer. This means an efficient algorithm
on one kind of computer is also an efficient algorithm on a probabilistic Turing
machine, since adding a polynomial overhead to a polynomial time algorithm
is still a polynomial time algorithm. This is why defining efficient algorithms as
those that run in polynomial time or faster is a useful notion.
Quantum computers would not violate the regular Church-Turing Thesis. That
is, what is impossible to compute remains impossible. The hope, however, is that
quantum computers can violate the Strong Church-Turing Thesis, that they will ef-
ficiently solve problems that are inefficient on classical computers.
While there is no proof of this hope, there is strong evidence. Here are three
examples:
• Quantum computers can efficiently factor numbers using Shor’s algorithm,
which will be covered near the end of this book. Factoring is believed to be
hard for classical computers, and the best known classical algorithm for fac-
toring runs in subexponential time. Note factoring is in NP since it is easy to
check if proposed factors are correct by multiplying them, but no algorithm has
been found to put the problem in P. The believed difficulty of factoring is the
basis for RSA, a widely used type of cryptography. RSA will be explained in
Section 6.6.2. This would be a very useful application of quantum computers,
but currently, quantum computers are too small to factor anything larger than
21 = 3×7.
• Determining the results of random quantum programs is called random circuit
sampling. This is easy for quantum computers, but it is believed to be hard
for classical computers. The best known classical algorithm takes exponential
time. In 2019, scientists at Google and the University of California, Santa Bar-
bara (UCSB) used a quantum processor to perform random circuit sampling. It
took their quantum processor about three minutes and twenty seconds. In con-
trast, they argued that a typical classical computer would take approximately
10 000 years. Shortly after this result was published, IBM argued that Summit,
the largest supercomputer in the world at the time, could theoretically perform
the computation in 2.5 days rather than 10 000 years if it also used its mas-
sive amounts of hard drive disk storage. This approach could use roughly 72
petabytes of storage3. In practice, using an entire supercomputer for any amount
of time is prohibitive, let alone for 2.5 days. This illustrates just how hard it is
3 For the advanced reader, the experiment used 53 quantum bits. It takes 253 numbers to write
down the state of these quantum bits, and if each number uses 8 bytes, storing the entire quantum
state would take 253 ·8 = 256 = 72.058×1015 bytes = 72.058 petabytes.
1.8 Turing Machines
69
for classical computers, as we currently understand them, to perform random
circuit sampling; either a ridiculous amount of time or a ridculous amount of
storage is necessary to reproduce the results, as far as we know. Google/UCSB’s
result is the first demonstration of quantum computational supremacy, where a
quantum computer performs a task that is out-of-reach of the known abilities of
classical computers, without regard for whether the task is useful or not.
• Another example is called boson sampling. Bosons are a type of particle, and
examples include photons (particles of light) that mediate the electromagnetic
force, gluons that mediate the strong nuclear force, the W± and Z0 bosons that
mediate the weak force, and the Higgs boson that explains why particles have
mass. By passing photons through half-silvered mirrors and reflecting them off
mirrors, one gets a probability distribution for where the photons end up. This is
a quantum process that is theoretically easy for a quantum device to perform. On
the other hand, it is theoretically hard for a classical computer to compute. The
best known classical algorithm takes exponential time. This boson sampling
problem is in the complexity class #P (pronounced “sharp P”), and it is just as
unlikely to be equal to P as NP is to equal P. A proof that P and #P are not equal,
however, does not exist, just like a proof that P and NP are not equal does not
exist. In 2020 and 2021, a series of experiments from the University of Science
and Technology of China demonstrated boson sampling using photons. This is
another example of quantum computational supremacy.
In each of the above examples, quantum computers are superpolynomially faster
than the best known classical algorithms. This suggests that quantum computers can
efficiently solve problems that are intractable for classical computers or (probabilis-
tic) Turing machines, and so quantum computers may overturn the Strong Church-
Turing Thesis. These are not proofs, however, because the difficulty for classical
computers is not proven.
Despite this, there are other problems for which quantum computers do yield
provable speedups over classical computers, but these speedups are polynomial at
best. For example, quantum computers can search an unordered database in the
square root of the amount of time a classical computer would take. These provable
speedups do not violate the Strong Church-Turing Thesis since the thesis allows for
polynomial overhead.
The complexity class of problems efficiently solved by a quantum computer is
called BQP. It stands for Bounded-Error Quantum Polynomial-Time, and its sus-
pected relationship to P, NP, NP-COMPLETE, and PSPACE is depicted in Fig. 1.2.
As we will see in this book, quantum computers can efficiently simulate classical
computers, so they can efficiently solve everything that a classical computer can
efficiently solve, which is precisely P, so P is definitely contained within in BQP.
The question is how much bigger BQP is than P, if at all. A proof of this may be
distant, however, as whether NP and PSPACE are bigger than P is still unsolved.
Exercise 1.60. The halting problem is to determine whether a computer program halts (finishes
running) or runs forever (like an infinite loop). Turing proved in 1936 that an algorithm to solve
this in general does not exist. For this problem, let us go through a sketch of a modern proof.
70
1 Classical Information and Computation
PSPACE
NP
NP-
complete
P
BQP
Fig. 1.2: The generally accepted relationships between the complexity classes BQP,
NP, NP-COMPLETE, P, and PSPACE.
Say there exists a program H(P) that returns true if the program P halts and false otherwise. We
will be proving that such a program cannot exist. In pseudocode, we can write H(P) as
program H(P):
if P halts
return true
else:
return false
end
Next, if H(P) exists, we can write a program Z(P) that does the opposite. Z(P) takes a program P
and runs/loops forever if H(P) returns true, and it halts if H(P) returns false. In pseudocode,
program Z(P):
if H(P) is true:
run forever
else:
halt
end
(a) Say H says that some program P1 halts, i.e., H(P1) returns true. Does Z(P1) halt or run forever?
(b) Say H says that some program P2 runs forever, i.e., H(P2) returns false. Does Z(P2) halt or
run forever?
Now consider what happens if we run Z(Z), i.e., program Z with its own code as input.
(c) If H(Z) returns true, does Z(Z) halt or run forever? Where is the contradiction?
(d) If H(Z) returns false, does Z(Z) halt or run forever? Where is the contradiction?
These contradictions mean that H(P) cannot exist.
Exercise 1.61. Watch “The Halting Problem - An Impossible Problem to Solve” by Up and Atom
on YouTube:
1.9 Summary
71
https://www.youtube.com/watch?v=t37GQgUPa6k
Answer the following questions and fill in the blanks.
(a) David Hilbert asked three questions...
Is mathematics
?
Is mathematics
?
Is mathematics
?
(b) Some more complicated programs can even have other
as inputs, and
some can even have
as inputs.
(c) What is the Goldbach conjecture?
(d) If Hal says that Randy halts, what does Barrie do?
(e) If Hal says that Randy runs forever, what does Barrie do?
(f) If Hal says that Barrie halts, what does Barrie do? Is this a problem?
(g) If Hal says that Barrie runs forever, what does Barrie do? Is this a problem?
(h) In the same paper Turing also managed to answer Hilbert’s third question, that no, mathemat-
ics is
. There are some problems that we simply
.
Exercise 1.62. Visit the Wikipedia list of undecidable problems at
https://en.wikipedia.org/wiki/List_of_undecidable_problems
Describe three undecidable problems.
Exercise 1.63. Take a look at Google/UCSB’s paper on quantum computational supremacy at
https://doi.org/10.1038/s41586-019-1666-5
From the abstract, fill in the blanks.
(a) “Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit
.”
(b) “This dramatic increase in speed compared to
classical algorithms is an experimental realization of
[computational]
.”
Exercise 1.64. Read the following excerpts from Scott Aaronson’s article “The Limits of Quantum
Computers” in Scientific American (March 2008):
(a) Page 65, box titled “The Good News.” What would be the “killer app” for quantum comput-
ers?
(b) Page 66, box titled “What Classical Computers Can and Cannot Do.” Fill in the blank: “NP-
complete problems are the
of the NP problems.”
(c) Page 67, box titled “Where Quantum Computers Fit In.” Are quantum computers expected to
solve some, most, or all problems in NP?
1.9 Summary
The smallest unit of classical information is the bit, which has two possible states, 0
or 1. Bits can be used to encode information, such as using ASCII. Bits are operated
on by logic gates, including NOT, AND, OR, XOR, NAND, and NOR. Together,
these gates can be used to perform any computation, and subsets of these gates are
also universal, such as {NOT, AND, OR} and {NAND}. The mathematics that de-
scribes logic gates is called boolean algebra. Logic gates can be made reversible, and
72
1 Classical Information and Computation
the Toffoli gate is a reversible version of the AND gate. In physical systems, errors
sometimes occur, but as long as the error rates are sufficiently low, they can be cor-
rected. Classical computers can efficiently solve some problems, while other prob-
lems take a superpolynomial amount of time. It is believed that quantum computers
can efficiently solve some of the problems that are hard for classical computers.
Chapter 2
One Quantum Bit
By drawing parallels to classical computing from the previous chapter, we can in-
troduce quantum information and computation in a natural way. Since we have not
yet defined “quantum,” just take it to mean a different set of rules, which we will
bring up as they become relevant.
2.1 Qubit Touchdown: A Quantum Computing Board Game
Quantum physics has a reputation for being difficult and confusing, exacerbated by
(incorrect) references to quantum mechanics in movies to justify their sci-fi plot ele-
ments. To help reduce this intimidation factor and ease the introduction of quantum
computing, I made a board game called Qubit Touchdown. It is shown in Fig. 2.1,
and it is available print-on-demand from The Game Crafter.1
Qubit Touchdown is a two player game. It consists of a game board, shown in
Fig. 2.2a, an orange football token, a die with only zeros and ones on it, and fifty-
two action cards, shown in Fig. 2.2b. To start the game, one player “kicks off” by
rolling the binary die, and the football token starts at zero or one, depending on
the outcome of the roll. Beginning with the other player, players take turns playing
action cards, which move the football according to the lines and arrows on the game
board. For example, at position 0, H moves the ball to position +;
√
X moves the
ball to −i; X and Y move the ball to to 1; and Z, I, and S keep the ball at position 0.
For the measurement card, if the football is at 0 or 1, nothing happens. Otherwise,
one kicks off again by rolling the binary die. When a player scores a touchdown by
moving the ball into their opponent’s endzone, they roll the die to kick off again,
and play continues with the other player. Whoever scores the most touchdowns, by
the time all the action cards have been used, wins.
All of these game mechanics come from quantum computing.
1 https://www.thegamecrafter.com/games/qubit-touchdown
73
74
2 One Quantum Bit
Fig. 2.1: The complete Qubit Touchdown board game.
It is not necessary to play Qubit Touchdown in order to continue with this text-
book.
Exercise 2.1. In Qubit Touchdown, say the ball is at position 0. Where would it move if you played
(a) X, (b) Y, (c) Z, (d) H, (e) S, (f)
√
X, (g) I, (h) measurement?
Exercise 2.2. In Qubit Touchdown, say the ball is at position i. Where would it move if you played
(a) X, (b) Y, (c) Z, (d) H, (e) S, (f)
√
X, (g) I, (h) measurement?
2.2 Superposition
2.2.1 Zero or One
A quantum bit, or qubit, is both similar to and different from a classical bit in some
important ways. First, like a classical bit, a qubit can take two values, 0 or 1. Using
bra-ket notation or Dirac notation from quantum physics, we write 0 and 1 enclosed
between a vertical bar and an angle bracket called a ket:
|0⟩,
|1⟩.
Although it may seem strange to write a quantum 0 and a quantum 1 like this, it will
be very useful later.
We can visualize these distinct states, |0⟩and |1⟩, as the north and south poles of
a sphere of radius 1 called the Bloch sphere:
2.2 Superposition
75
(a)
(b)
Fig. 2.2: The Qubit Touchdown (a) game board and (b) action cards.
76
2 One Quantum Bit
x
y
z
|0⟩
|1⟩
Following the standard physics convention, the x-axis comes out of the page, the
y-axis points to the side, and z-axis is oriented up. Then, since the Bloch sphere
has radius 1, |0⟩corresponds to the (x,y,z) point (0,0,1), and |1⟩corresponds to
(0,0,−1).
2.2.2 Superposition
If we had a classical bit, |0⟩and |1⟩would be the only two states. But, the laws of
quantum mechanics allow the state of a qubit to be a combination of |0⟩and |1⟩,
called a superposition of |0⟩and |1⟩. For example, here is a state that is equal parts
|0⟩and |1⟩:
1
√
2
(|0⟩+|1⟩).
In this state, the coefficient of |0⟩is 1/
√
2, and the coefficient of |1⟩is also 1/
√
2.
So, it is equal parts |0⟩and |1⟩. Given this, it should be on the equator of the Bloch
sphere, which is halfway between the north and south poles:
x
y
z
This state is at the (x,y,z) point (1,0,0), where the Bloch sphere intersects the x-
axis. Later, we will learn how to calculate these coordinates, but for now, we will
focus on building geometric intuition.
2.2 Superposition
77
There are many other states on the equator of the Bloch sphere, all of which are
equal parts |0⟩and |1⟩. We can reach them by changing the relative phase of |0⟩and
|1⟩. For example, if we instead use a negative sign, we get
x
y
z
1
√
2 (|0⟩−|1⟩)
This state is at the point (−1,0,0), on the −x-axis. To reach the y-axis at (0,1,0),
we instead use a phase of i = √−1:
x
y
z
1
√
2 (|0⟩+ i|1⟩)
We see that imaginary and complex numbers are used in quantum computing. In
case it has been a while since you have used them, in the next section, we will
review complex numbers in detail. Continuing, to reach the −y-axis at (0,−1,0),
we use a phase of −i:
x
y
z
1
√
2 (|0⟩−i|1⟩)
78
2 One Quantum Bit
These states appear frequently enough that they have names: “plus,” “minus,” “i,”
and “minus i”:
|+⟩= 1
√
2
(|0⟩+|1⟩),
|−⟩= 1
√
2
(|0⟩−|1⟩),
|i⟩= 1
√
2
(|0⟩+i|1⟩),
|−i⟩= 1
√
2
(|0⟩−i|1⟩).
(2.1)
Drawing them together with |0⟩and |1⟩on the Bloch sphere, we get the following,
which also appears on the back of every action card in Qubit Touchdown:
x
y
z
|0⟩
|1⟩
|+⟩
|−⟩
|i⟩
|−i⟩
In Qubit Touchdown, these six states correspond to the six positions on the game
board in Fig. 2.2a. The football token corresponds to a qubit, so the ball moving
around the game board corresponds to a qubit changing between these states (more
on this later).
Of course, there are many other points on the equator of the Bloch sphere. We
can reach them using other complex phases, such as
2.2 Superposition
79
x
y
z
1
√
2

|0⟩+ eiπ/6|1⟩

Superpositions are not restricted to the equator, either. They can favor |0⟩or |1⟩by
being in the northern or southern hemisphere, such as
x
y
z
√
3
2 |0⟩+ 1
2|1⟩
x
y
z
2
3|0⟩+ 1 −2i
3
|1⟩
In fact,
A qubit can be any point on the Bloch sphere.
Again, we will later see how to calculate where a quantum state is on the Bloch
sphere. Before we do that, however, we need to review complex numbers and discuss
measuring qubits.
Exercise 2.3. Draw a Bloch sphere and label the following locations:
(a) Where a qubit is exactly |0⟩.
(b) Where a qubit is exactly |1⟩.
(c) Where a qubit is half |0⟩and half |1⟩.
(d) Where a qubit is more |0⟩than |1⟩.
(e) Where a qubit is more |1⟩than |0⟩.
80
2 One Quantum Bit
2.2.3 Review of Complex Numbers
A complex number z is a number with a real part x plus i times an imaginary part y:
z = x+iy.
For example, 1 + i
√
3 is a complex number. The parts are called components. We
denote the real component of z as ℜ(z), and it just equals x:
ℜ(z) = x.
Similarly, we denote the imaginary component as ℑ(z), and it just equals y:
ℑ(z) = y.
For example, ℜ(1 + i
√
3) = 1 and ℑ(1 + i
√
3) =
√
3. Real numbers are complex
numbers; their imaginary parts are just zero. Similarly, imaginary numbers are com-
plex numbers; their real parts are just zero.
The above form x+iy is called the Cartesian form or rectangular form of a com-
plex number. In quantum computing, it is often useful to write a complex number
as its length r times its complex phase eiθ:
z = reiθ.
This is called the polar form of a complex number, and any complex number can be
written this way. To convert from the Cartesian form x + iy to the polar form reiθ,
we use the following equations, which we will prove in a moment:
r =
p
x2 +y2,
(2.2)
θ = tan−1 y
x

.
(2.3)
To convert from the polar form reiθ to the Cartesian form x+iy, we use the following
equations, which we will also prove in a moment:
x = rcosθ
(2.4)
y = rsinθ.
(2.5)
These relationships can be proved a couple different ways:
• Geometrically, complex number is a point on the complex plane, which has the
real component as its x-axis and the imaginary component as its y-axis:
2.2 Superposition
81
Re
Im
x + iy
x
y
r
θ
Its length is r, and the angle it makes counter-clockwise from the real axis is
θ. Since x and y are the legs of a right triangle, and r is the hypotenuse, the
Pythagorean theorem says that x2 + y2 = r2. Taking the square root of each
side, we get r =
p
x2 +y2, which proves Eq. (2.2).
Next, from the drawing, θ is an angle in a right triangle, and y is opposite of
θ, x is adjacent, and r is the hypotenuse. We can relate these quantities using
the trigonometric functions sine, cosine, and tangent.2 First, using sine, we get
sinθ = y/r. Multiplying both sides by r, we get y = rsinθ, which is Eq. (2.5).
Next using cosine, we get cosθ = x/r. Multiplying both sides by r, we get
x = rcosθ, which is Eq. (2.4). Finally, using tangent, we get tanθ = y/x. Taking
the inverse tangent of both sides, we get θ = tan−1(y/x), which is Eq. (2.3).
• Algebraically, we can write a complex phase in Cartesian form using Euler’s
formula, which says says that
eiθ = cosθ +isinθ.
(2.6)
If you have taken more advanced math than is required for this textbook, you
may be familiar with Euler’s formula and its proof. (It is typically seen during
the second semester of calculus.) If not, no worries. You do not need to know
where it comes from. It is simple enough to memorize now, which you should
do, since it will show up again.
Now using Euler’s formula, we can write the polar form of a complex number
as
z = reiθ = r(cosθ +isinθ) = rcosθ
| {z }
x
+irsinθ
| {z }
y
.
Thus, the real part of z is x = rcosθ, which is Eq. (2.4), and the imaginary part
is y = rsinθ, which is Eq. (2.5). If we take the sum of the squares of these
components, we get
x2 +y2 = r2 cos2 θ +r2 sin2 θ = r2 (cos2 θ +sin2 θ)
|
{z
}
1
= r2.
2 Many people remember the trigonometric functions using the mnemonic SOH-CAH-TOA, where
sine is opposite over hypotenuse (SOH), cosine is adjacent over hypotenuse (CAH), and tangent is
opposite over adjacent (TOA).
82
2 One Quantum Bit
In the last equality, we used the Pythagorean identity sin2 θ +cos2 θ = 1 from
trigonometry. Taking the square root of both sides, we get r =
p
x2 +y2, which
proves Eq. (2.2). Next, if we divide y = rsinθ by x = rcosθ, we get
y
x = sinθ
cosθ = tanθ.
Taking the inverse tangent of both sides, we get Eq. (2.3).
For example, say we have a complex number in Cartesian form, z = 1+i
√
3. To con-
vert this to polar form, we calculate its length r =
q
12 +
√
3
2 = √1+3 =
√
4 = 2
and angle θ = tan−1(
√
3/1) = π/3 radians or 60◦. The convention is to use radians,
so putting these together, the polar form is z = 2eiπ/3.
There are a few more aspects of complex numbers that come up frequently in
quantum computing, so let us review them now:
• The complex conjugate (or just conjugate) of a complex number is the complex
number obtained by negating its imaginary part. That is, we replace i with −i.
We denote the complex conjugate of z as z∗, so if z = x+iy = reiθ, then
z∗= x−iy = re−iθ.
From the previous example,
 1+i
√
3
∗= 1−i
√
3 and
 2eiπ/3∗= 2e−iπ/3.
• The norm of a complex number z, which we denote |z|, is simply its length r:
|z| = r.
Then from the previous example, |1+i
√
3| = 2, since its length r is 2.
• The norm-square of a complex number z, which we denote |z|2, is simply the
square of its norm, so it is r2:
|z|2 = r2.
From the previous example, |1 + i
√
3|2 = 4. One way to calculate the norm-
square is multiplying a complex number by its conjugate:
|z|2 = z∗z = zz∗.
(2.7)
We can prove that this works using either the Cartesian or polar form. In Carte-
sian form,
zz∗= (x+iy)(x−iy) = x2 −ixy+ixy−i2y2 = x2 +y2 = r2 = |z|2,
where in the third step, we used i2 = √−12 = −1. In polar form,
zz∗= reiθre−iθ = r2eiθ−iθ = r2e0 = r2 = |z|2.
Finally, Eq. (2.7) gives us another way to find the norm of z, by taking the square
root of z times its conjugate z∗:
2.3 Measurement
83
|z| =
q
|z|2 =
√
z∗z =
√
zz∗.
Exercise 2.4. Consider the complex number z = 1+2i.
(a) Find ℜ(z).
(b) Find ℑ(z).
(c) Plot z as a point in the complex plane.
(d) Write z in polar form reiθ.
(e) Find z∗.
(f) Find |z|.
(g) Find |z|2.
Exercise 2.5. Consider the complex number z = −3−i.
(a) Find ℜ(z).
(b) Find ℑ(z).
(c) Plot z as a point in the complex plane.
(d) Write z in polar form reiθ. Hint: The angle should be between π and 3π/2 (i.e., 180◦and
270◦).
(e) Find z∗.
(f) Find |z|.
(g) Find |z|2.
2.3 Measurement
2.3.1 Measurement in the Z-Basis
In a previous section, we had the following qubit, which was on the equator of the
Bloch sphere:
x
y
z
1
√
2

|0⟩+ eiπ/6|1⟩

Although the laws of quantum mechanics permit this superposition of |0⟩and |1⟩,
it also demands that if we measure the qubit, such as at the end of a computation
in order to read the result, we get a single, definite value. That is, we get |0⟩or
|1⟩, each with some probability, not a superposition of |0⟩and |1⟩. Geometrically,
this particular qubit lies on the equator, halfway between the north and south poles,
so if we measure it, we get |0⟩with probability 1/2 or |1⟩with probability 1/2. To
84
2 One Quantum Bit
calculate these probabilities, we take the norm-square of the coefficient of |0⟩or |1⟩.
That is, the probability of getting |0⟩is

1
√
2

2
= 1
2,
and the probability of getting |1⟩is

eiπ/6
√
2

2
= eiπ/6
√
2
e−iπ/6
√
2
= e0
2 = 1
2.
The coefficients are called amplitudes, so:
The probability is given by the norm-square of the amplitude.
Let us look at another example, which we also saw earlier:
x
y
z
2
3|0⟩+ 1 −2i
3
|1⟩
Geometrically, since the qubit is closer to the south pole, we expect that the prob-
ability of getting |1⟩is greater than the probability of getting |0⟩. To get the exact
probabilities, we calculate the norm-square of each amplitude:

2
3

2
= 4
9,
and

1−2i
3

2
= 1−2i
3
1+2i
3
= 1+2i−2i−4i2
9
= 5
9.
So, if we measure the qubit, the probability of getting |0⟩is 4/9, and the probability
of getting |1⟩is 5/9. As expected, the probability of getting |1⟩is greater than the
probability of getting |0⟩, and also note that the total probability is 4/9 + 5/9 = 1,
as it must.
Exercise 2.6. A qubit is in the state
2.3 Measurement
85
1+i
√
3
3
|0⟩+ 2−i
3
|1⟩.
If you measure the qubit, what is the probability of getting
(a) |0⟩?
(b) |1⟩?
Say a qubit is superposition of |0⟩and |1⟩. Say we measure it, and the outcome
is |0⟩. Now for something new: The qubit is no longer in a superposition of |0⟩and
|1⟩. It is now simply |0⟩, and we know because we measured it. Measuring the qubit
changed it. It forced it to take a stand. We say the state has collapsed to |0⟩. If we
measure the qubit again, we get |0⟩with probability 1. This aspect of measurement
is important enough to box:
Measurement collapses the qubit.
In Qubit Touchdown, playing the measurement action card, or kicking off after a
touchdown, corresponds to measuring the qubit. For the measurement action card,
if the ball is at position 0 or 1, then nothing happens, just like measuring a qubit in
state |0⟩or |1⟩returns the same state with probability 1. If the ball is at position i or
−i, then the binary die is rolled, and the ball is moved to 0 or 1 with 50% probability
each, just like measuring a qubit in the |i⟩or |−i⟩states collapses the state to |0⟩or
|1⟩with 50% probability. Finally, after scoring a touchdown, the ball is either at the
+ or −positions in the endzones. Then, the binary die is rolled, moving the ball
to 0 or 1 with 50% probability each. In the same way, when a qubit is in the |+⟩
or |−⟩state, measuring it yields |0⟩or |1⟩with 50% probability each, and the state
collapses to whichever state was measured.
Exercise 2.7. A qubit is in the state
2
3|0⟩+ 1+2i
3
|1⟩.
Say you measure the qubit and get |0⟩. If you measure the qubit a second time, what is the proba-
bility of getting
(a) |0⟩?
(b) |1⟩?
2.3.2 Normalization
We say a quantum state is normalized if its total probability is 1, as it should be.
Sometimes, we must find an overall normalization constant to make this true. For
example, a qubit is in the state
A
√
2|0⟩+i|1⟩

.
86
2 One Quantum Bit
We normalize this state by finding the normalization constant A that ensures that the
total probability is 1. So,
1 = (A
√
2)(A
√
2)∗+(Ai)(Ai)∗
= 2|A|2 +|A|2
= 3|A|2
|A|2 = 1
3.
As we will prove later, the overall phase does not matter, so we might as well pick
A to be real. Thus,
A = 1
√
3,
and the normalized state is
1
√
3
√
2|0⟩+i|1⟩

.
Exercise 2.8. A qubit is in the state
eiπ/8
√
5
|0⟩+β|1⟩.
What is a possible value of β?
Exercise 2.9. A qubit is in the state
A

2eiπ/6|0⟩−3|1⟩

.
(a) Normalize the state (i.e., find A).
(b) If you measure the qubit, what is the probability that you get |0⟩?
(c) If you measure the qubit, what is the probability that you get |1⟩?
2.3.3 Measurement in Other Bases
Even though we introduced |0⟩and |1⟩as the north and south poles, respectively, of
the Bloch sphere, the Bloch sphere is not a planet, and it is not spinning. Then, any
two opposite points could be taken as the north and south poles. For example, |+⟩
and |−⟩could be the north and south poles, or |i⟩and |−i⟩, or any opposite points.
A set of distinct measurement outcomes is called a basis, and {|0⟩,|1⟩} is called the
Z-basis because they lie on the z-axis of the Bloch sphere. Similarly, {|+⟩,|−⟩} is
called the X-basis because they lie on the x-axis of the Bloch sphere, and {|i⟩,|−i⟩}
is called the Y-basis because they lie on the y-axis of the Bloch sphere. We can
measure with respect to any of these bases, or with respect to any two states on
opposite sides of the Bloch sphere.
For example, consider a qubit in the state
2.3 Measurement
87
√
3
2 |0⟩+ 1
2|1⟩,
which appears on the Bloch sphere at an angle of 30◦above the x-axis, as shown
below:
x
y
z
|0⟩
|1⟩
|+⟩
|−⟩
|i⟩
|−i⟩
•
√
3
2 |0⟩+ 1
2|1⟩
Let us measure this with respect to four different bases: the Z-basis {|0⟩,|1⟩}, the
X-basis {|+⟩,|−⟩}, the Y-basis {|i⟩,|−i⟩}, and a fourth basis (below).
1. If we measure the qubit in the Z-basis {|0⟩,|1⟩}, then we get |0⟩with probability
3/4 or |1⟩with probability 1/4.
2. What if we measure in the X-basis {|+⟩,|−⟩} instead? Geometrically, the prob-
ability of getting |+⟩should be much higher than the probability of getting |−⟩
because the state is so much closer to |+⟩on the Bloch sphere. To calculate the
probabilities precisely, we need to express the state in terms of |+⟩and |−⟩so
that we can identify the amplitudes and then find their norm-squares. From the
definitions of |+⟩and |−⟩in Eq. (2.1), we have
|0⟩= 1
√
2
(|+⟩+|−⟩),
|1⟩= 1
√
2
(|+⟩−|−⟩).
Substituting into the state of our qubit,
√
3
2 |0⟩+ 1
2|1⟩=
√
3
2
1
√
2
(|+⟩+|−⟩)+ 1
2
1
√
2
(|+⟩−|−⟩)
=
√
3+1
2
√
2
|+⟩+
√
3−1
2
√
2
|−⟩.
Now the amplitudes are easy to identify, and we can find the probabilities by
taking their norm-squares. The probability of measuring |+⟩is

√
3+1
2
√
2

2
=
√
3+2
4
≈0.93,
and the probability of measuring |−⟩is
88
2 One Quantum Bit

√
3−1
2
√
2

2
= −
√
3+2
4
≈0.07.
This is consistent with the Bloch sphere, since the state is much closer to |+⟩
than it is to |−⟩.
3. Now what if we measure in the {|i⟩,|−i⟩} basis? Geometrically, the Bloch
sphere reveals that the state is halfway between |i⟩and |−i⟩, so we get one
or the other with probability 1/2 each. We can also calculate this by rewrit-
ing the state in terms of |i⟩and |−i⟩and then finding the norm-square of the
amplitudes. From the definitions of |i⟩and |−i⟩in Eq. (2.1),
|0⟩= 1
√
2
(|i⟩+|−i⟩),
|1⟩= −i
√
2
(|i⟩−|−i⟩).
Substituting,
√
3
2 |0⟩+ 1
2|1⟩=
√
3
2
1
√
2
(|i⟩+|−i⟩)+ −i
√
2
(|i⟩−|−i⟩)
=
√
3−i
2
√
2
|i⟩+
√
3+i
2
√
2
|−i⟩.
So, the probability of getting |i⟩is

√
3−i
2
√
2

2
= 3+1
8
= 1
2,
and the probability of getting |−i⟩is

√
3+i
2
√
2

2
= 3+1
8
= 1
2,
as expected.
4. Consider the following two states, which we will call |a⟩and |b⟩:
|a⟩=
√
3
2 |0⟩+ i
2|1⟩,
|b⟩= i
2|0⟩+
√
3
2 |1⟩.
Here they are on the Bloch sphere:
2.3 Measurement
89
x
y
z
•
√
3
2 |0⟩+ 1
2|1⟩
√
3
2 |0⟩+ i
2|1⟩
i
2|0⟩+
√
3
2 |1⟩
|a⟩is located 30◦above the y-axis, and |b⟩is located 30◦below the −y-axis.
(You will prove this in Exercise 2.16.) Since they are located on opposite points
of the Bloch sphere, they are a basis. Let us measure in this {|a⟩,|b⟩} basis.
Geometrically, the qubit is closer to |a⟩, so we expect a higher probability of
getting |a⟩than |b⟩. To calculate the precise numbers, we first write |0⟩and |1⟩
in terms of |a⟩and |b⟩:
|0⟩=
√
3
2 |a⟩−i
2|b⟩,
|1⟩= −i
2 |a⟩+
√
3
2 |b⟩.
Substituting, the state of our qubit is
√
3
2 |0⟩+ 1
2|1⟩=
√
3
2
 √
3
2 |a⟩−i
2|b⟩
!
+ 1
2
 
−i
2 |a⟩+
√
3
2 |b⟩
!
= 3−i
4 |a⟩+
√
3(1−i)
4
|b⟩.
Taking the norm-square of each amplitude, the probability of getting |a⟩is

3−i
4

2
= 9+1
16
= 5
8,
and the probability of getting |b⟩is

√
3(1−i)
4

2
= 3
8.
Later, when we describe qubits in the mathematics of linear algebra, we will see
another way to convert between bases.
Exercise 2.10. A qubit is in the state
1
2|0⟩−
√
3
2 |1⟩.
90
2 One Quantum Bit
(a) If you measure it in the Z-basis {|0⟩,|1⟩}, what states can you get and with what probabilities?
(b) Write the qubit’s state in terms of |+⟩and |−⟩.
(c) If you measure it in the basis {|+⟩,|−⟩}, what states can you get and with what probabilities?
Exercise 2.11. The following two states are opposite points on the Bloch sphere:
|a⟩=
√
3
2 |0⟩+ i
2|1⟩,
|b⟩= i
2|0⟩+
√
3
2 |1⟩.
So, we can measure relative to them. Now consider a qubit in the state
1
2|0⟩−
√
3
2 |1⟩.
(a) Write the qubit’s state in terms of |a⟩and |b⟩.
(b) If you measure the qubit in the basis {|a⟩,|b⟩}, what states can you get and with what proba-
bilities?
2.3.4 Consecutive Measurements
We have already seen that measuring a qubit collapses the state to whatever was
measured. This can lead to interesting statistics, even more so if we change the
measurement basis. For example, consider the following three measurements:
1. Say we first measure the qubit in the Z-basis {|0⟩,|1⟩}. Then, the qubit collapses
to |0⟩or |1⟩.
2. Next, if we measure in the X-basis {|+⟩,|−⟩}, then since both |0⟩and |1⟩are
halfway between |+⟩and |−⟩, the qubit collapses to |+⟩or |−⟩, each with
probability 1/2.
3. If we then measure in the Z-basis {|0⟩,|1⟩}, then since |+⟩and |−⟩are halfway
between |0⟩and |1⟩, the probability of each is 1/2. We can continue alternating
between these two measurement bases, each time having a 50:50 chance of
getting each outcome.
Exercise 2.12. A qubit is in the state |0⟩. If you measure it in the X-basis {|+⟩,|−⟩} and then
measure it again in the Z-basis {|0⟩,|1⟩}, what is the probability of getting
(a) |0⟩?
(b) |1⟩?
2.4 Bloch Sphere Mapping
We know that a qubit can be visualized as a point on the Bloch sphere. Now, let us
explain how to determine where the point should be.
2.4 Bloch Sphere Mapping
91
2.4.1 Global and Relative Phases
Say the qubit from the last section is multiplied by an overall, global phase:
eiθ
 √
3
2 |0⟩+ 1
2|1⟩
!
,
for some angle θ. If we measure this in the Z-basis, {|0⟩,|1⟩} the probability of
getting |0⟩is
eiθ
√
3
2

2
= 3
4,
and the probability of getting |1⟩is
eiθ 1
2

2
= 1
4,
as they were without the global phase. So, the phase does not change anything.
If we instead measure in the X-basis {|+⟩,|−⟩}, then we can rewrite the state as
eiθ
 √
3+1
2
√
2
|+⟩+
√
3−1
2
√
2
|−⟩
!
.
Then, the probability of getting |+⟩is
eiθ
√
3+1
2
√
2

2
=
√
3+2
4
≈0.93,
and the probability of measuring |−⟩is
eiθ
√
3−1
2
√
2

2
= −
√
3+2
4
≈0.07,
as they were before without the global phase. So again, the phase does not change
anything.
This is true no matter what measurement basis we use, and it leads to the follow-
ing result:
Global phases are physically irrelevant.
As such, global phases can be dropped/ignored. States that differ by a global phase
are actually the same state; they correspond to the same point on the Bloch sphere.
Note that a relative phase is physically significant, such as
92
2 One Quantum Bit
|+⟩= 1
√
2
(|0⟩+|1⟩)
vs
|i⟩= 1
√
2
(|0⟩+i|1⟩) = 1
√
2

|0⟩+eiπ/2|1⟩

.
These correspond to different points on the Bloch sphere, and they can be distin-
guished by measurements in appropriate bases. Although measuring |+⟩and |i⟩in
the Z-basis yields the same statistics, i.e., |0⟩with probability 1/2 or |1⟩with prob-
ability 1/2, measuring in the X-basis {|+⟩,|−⟩}yields different results. Measuring
|+⟩in the X-basis always yields |+⟩, but measuring |i⟩in the X-basis yields |+⟩or
|−⟩with a 50:50 probability.
Exercise 2.13. Is there a measurement that can distinguish the following pairs of states? If yes,
give a measurement. If no, explain your reasoning.
(a) |+⟩= 1
√
2
(|0⟩+|1⟩) and eiπ/8|+⟩= eiπ/8
√
2
(|0⟩+|1⟩).
(b) |+⟩= 1
√
2
(|0⟩+|1⟩) and |−⟩= 1
√
2
(|0⟩−|1⟩).
(c) |0⟩and eiπ/4|0⟩.
2.4.2 Spherical Coordinates
A generic quantum state is typically called ψ (the Greek letter “psi,” which is pro-
nounced “sigh”), and since it is quantum, we write it as a ket |ψ⟩. Now say we have
a generic qubit |ψ⟩with some amplitudes α and β:
|ψ⟩= α|0⟩+β|1⟩,
where |α|2 +|β|2 = 1 for normalization. Since the global phase does not matter, we
can assume that α is real and positive, and β may be complex. To determine the
location of this qubit on the Bloch sphere, we first parameterize, or write in terms
of other parameters, α and β in terms of two angles θ and φ:
α = cos
θ
2

,
β = eiφ sin
θ
2

.
With 0 ≤θ ≤π and 0 ≤φ < 2π, this captures all the properties we need: α is
real and positive, β is complex, and the state is normalized. Substituting, we have
rewritten the qubit’s state as
|ψ⟩= cos
θ
2

|0⟩+eiφ sin
θ
2

|1⟩.
(2.8)
Let us work through an example. Say a qubit is in the state
2.4 Bloch Sphere Mapping
93
3+i
√
3
4
|0⟩−1
2|1⟩.
We see that the amplitude of |0⟩is complex, but in Eq. (2.8), it needs to be real. To
make it real, we first convert it to polar form. Since (3+i
√
3)/4 = (
√
3/2)eiπ/6, the
state is
√
3
2 eiπ/6|0⟩−1
2|1⟩.
Factoring, this becomes
eiπ/6
 √
3
2 |0⟩−e−iπ/6 1
2|1⟩
!
≡
√
3
2 |0⟩−e−iπ/6 1
2|1⟩,
where ≡denotes “equivalent to,” and the states are equivalent because the global
phase does not matter and can be dropped. Comparing this to the Bloch sphere
parameterization in Eq. (2.8), we still need to change the minus sign to a plus sign.
We can do this using eiπ = −1. Then, the state is
√
3
2 |0⟩+eiπe−iπ/6 1
2|1⟩=
√
3
2 |0⟩+ei5π/6 1
2|1⟩.
Now it takes the form of Eq. (2.8), and we identify
cos
θ
2

=
√
3
2 ,
eiφ = ei5π/6,
sin
θ
2

= 1
2.
Solving the first or last equation for θ using the inverse cosine or inverse sine, and
solving the second equation for φ, we get
θ = π
3 ,
φ = 5π
6 .
Thus, plugging into Eq. (2.8), the state of the qubit is equivalent to
cos
π/3
2

|0⟩+ei5π/6 sin
π/3
2

|1⟩.
Next, to map this qubit to a location on the Bloch sphere, we identify θ and φ as
the following angles:
94
2 One Quantum Bit
x
y
z
θ
φ
So, θ measures the angle down from the north pole, called the polar angle, and
φ measures the angle across from the x-axis in the xy plane, called the azimuthal
angle. If you have taken more advanced mathematics than is required for this book,
these are precisely spherical coordinates with a radius of 1. Spherical coordinates
are typically covered in the third semester of calculus, but everything you need for
this textbook will be explained here.
Returning to our example, we had θ = π/3 and φ = 5π/6, so the state is located
on the Bloch sphere at an angle of π/3 or 60◦from the north pole, and 5π/6 or 150◦
rotated to the side:
x
y
z
3 + i
√
3
4
|0⟩−1
2|1⟩
Exercise 2.14. A qubit is in the state
|i⟩= 1
√
2
(|0⟩+i|1⟩).
(a) Where on the Bloch sphere is this state? Give your answer in (θ,φ) coordinates.
(b) Sketch the point on the Bloch sphere.
Exercise 2.15. A qubit is in the state
1−i
2
√
2
|0⟩+
√
3
2 |1⟩.
(a) Where on the Bloch sphere is this state? Give your answer in (θ,φ) coordinates.
2.4 Bloch Sphere Mapping
95
(b) Sketch the point on the Bloch sphere.
Exercise 2.16. Consider the following two states from Exercise 2.11:
|a⟩=
√
3
2 |0⟩+ i
2|1⟩,
|b⟩= i
2|0⟩+
√
3
2 |1⟩.
Prove these are opposite points of the Bloch sphere by finding their points in spherical coordinates
(θa,φa) and (θb,φb). Verify that θb = π −θa and φb = φa + π, which means they lie on opposite
points of the Bloch sphere.
2.4.3 Cartesian Coordinates
We can also determine the (x,y,z) coordinates, called Cartesian coordinates, of a
point on the Bloch sphere. To see how to determine these from the spherical coordi-
nates (θ,φ), let us first zoom in and focus on the polar angle θ:
x
y
z
θ
φ
zoom
−−−→
x
y
z
sin θ
cos θ
1
θ
φ
In this picture, we drew a dashed line from the point to the z-axis, perpendicular to
it. This creates a right triangle, which we shaded gray. Since the radius of the Bloch
sphere is 1, the hypotenuse of the gray triangle is 1, which we labeled in the picture.
Then, since sinθ is opposite over hypotenuse, and the hypotenuse is 1, we get that
sinθ is the opposite side, which we labeled in the picture. Similarly, since cosθ is
adjacent over hypotenuse, and the hypotenuse is 1, we get that cosθ is the adjacent
side, which we also labeled in the picture. This adjacent side gives the z-coordinate
(height) of the point, i.e., z = cosθ.
To get the x and y coordinates, we look at the projection onto the xy-plane:
96
2 One Quantum Bit
x
y
z
1
sin θ
cos θ
sin θ
θ
φ
sin θ cos φ
sin θ sin φ
First, we copied sinθ from the top dashed line to below, since they are opposite sides
of a rectangle. This is the hypotenuse of the bottom gray triangle. Then, since sinφ
is opposite over hypotenuse, we get that the opposite side is equal to the hypotenuse
times sinφ, or sinθ sinφ, which we labeled in the drawing. This corresponds to
the y-coordinate (distance to the right) of the point, i.e., y = sinθ sinφ. Similarly,
since cosφ is adjacent over hypotenuse, we get that the adjacent side is equal to
the hypotenuse times cosφ, or sinθ cosφ. This is labeled as the left side of the
gray triangle in the drawing, and it corresponds to the x-coordinate (distance out of
the page), i.e., x = sinθ cosφ. Combining these results with the z-coordinate, we
can convert from spherical coordinates to Cartesian coordinates using the following
formulas:
x = sinθ cosφ,
y = sinθ sinφ,
z = cosθ.
Continuing the previous example, we had θ = π/3 and φ = 5π/6, so the (x,y,z)
coordinates are of the qubit are
x = sin
π
3

cos
5π
6

=
√
3
2 · −
√
3
2
= −3
4,
y = sin
π
3

sin
5π
6

=
√
3
2 · 1
2 =
√
3
4 ,
z = cos
π
3

= 1
2.
So, the qubit corresponds to point (−3/4,
√
3/4,1/2) on the Bloch sphere.
Exercise 2.17. In Cartesian (x,y,z), coordinates, where on the Bloch sphere is the state from
(a) Exercise 2.14?
(b) Exercise 2.15?
2.5 Physical Qubits
97
2.5 Physical Qubits
Physically, any quantum system with two distinct states can be used as a qubit.
While one is not expected to be familiar with quantum physics, some examples
include:
• Photons, or quantum particles of light, have a property called polarization. A
photon’s polarization can be vertical or horizontal, or a superposition of both,
and we can use this as a qubit.
• Trapped ions. An ion is an atom that has an overall charge (instead of being
neutral) because it has gained or lost one or more electrons. Individual ions can
be trapped in space using electric fields. Two energy levels of an ion can be used
as a qubit.
• Cold atoms. Neutral atoms can be trapped at low temperatures using a magneto-
optical trap, which uses magnetic fields and lasers to cool and trap the atoms.
Another approach is using an optical lattice constructed by laser beams. Once
trapped, two energy levels of an atom can be used as a qubit.
• Nuclear magnetic resonance. The nuclei of atoms and molecules have a quan-
tum property called spin, which can be used as a qubit. Spins can be identified
by their frequency of precession when subject to a strong magnetic field.
• Quantum dots. An electron can be bound to a small semiconductor device, sim-
ilar to an electron bound to the nucleus of an atom. In these “artificial atoms,”
the spin of an electron, which can be “spin up” or “spin down,” can be used as
a qubit.
• Defect qubits. A diamond crystal may have a missing carbon atom, and if we
replace a carbon atom next to this vacancy with a nitrogen atom, we get a
“spin triplet” that can be used for quantum computing. This is called a nitrogen-
vacancy center in diamond.
• Superconductors. In a superconducting circuit, charge flows with zero resis-
tance. The magnetic flux across an inductor and the charge on a capacitor
cause a harmonic potential energy with equally spaced, discrete energy levels.
A Josephson junction is a thin insulating layer that is added, and it changes the
potential energy so that the energy levels become unequally spaced. Then, the
energy levels can be distinguished, and two of them can be used as a qubit.
Informationally, these systems contain the same amount of information: two distinct
quantum states. So we simply use |0⟩and |1⟩for the rest of the discussion.
Exercise 2.18. A review of various ways to build a quantum computer is the article “Quantum
Computers” by Ladd et al. (2010). The published version in Nature 464, 45–53 is available at
https://dx.doi.org/10.1038/nature08812, but it may require a subscription. Search
through the text and fill in the blanks:
(a) Photons: “Realizing a qubit as the
state of a photon is appealing because
photons are relatively free of the
that plagues other quantum systems.”
(b) Trapped ions: “Individual atomic ions can be confined in free space with nanometre precision
using appropriate
from nearby electrodes.”
(c) Cold atoms: “An array of cold neutral atoms may be confined in free space by a pattern of
crossed
, forming an optical lattice.”
98
2 One Quantum Bit
(d) Nuclear magnetic resonance: “In a
, nuclear Larmor frequencies vary
from atom to atom [...] Irradiating the nuclei with resonant
pulses allows
manipulation of nuclei of distinct frequency, giving generic
gates.”
(e) Quantum dots: “These ‘artificial atoms’ occur when a small semiconductor nanostructure, im-
purity or impurity complex binds one or more electrons or holes (empty valence-band states)
into a localized potential with
,
which is analogous to an electron bound to an
.”
(f) Doped solids: “
may then be stored in either the
donor electron, or in the state of the single 31P nuclear spin, accessed via the electron-nuclear
coupling.”
(g) Nitrogen-vacancy centers: “The
state of a nitrogen-vacancy centre may
then be coherently manipulated with resonant
fields, and then detected
in a few milliseconds via spin-dependent flourescence in an
micro-
scope.”
(h) Superconductors: “There are three basic types of superconducting qubits—
,
and
.”
Exercise 2.19. Visit https://en.wikipedia.org/wiki/List_of_companies_invo
lved_in_quantum_computing_or_communication for a list of companies involved in
quantum computing. For each of the following types of qubits, name a company that is using them.
(a) Photons
(b) Trapped ions
(c) Cold atoms
(d) Nuclear magnetic resonance (NMR)
(e) Quantum dots
(f) Defect qubits
(g) Superconductors
Exercise 2.20. Visit https://qubitzoo.com and pick your favorite qubit.
(a) What is the name of your qubit? (e.g., exchange-only qubit, not Steve.)
(b) What type of technology is your qubit?
(c) What is some motivation for building a qubit this way?
Exercise 2.21. Visit https://en.wikipedia.org/wiki/Qubit#Physical_impleme
ntations and pick a qubit.
(a) What is the physical system or technology used for your qubit (physical support)?
(b) What is the name of your type of qubit?
(c) What physical property is used to store information (information support)?
(d) What state is typically used for |0⟩?
(e) What state is typically used for |1⟩?
2.6 Quantum Gates
2.6.1 Linear Maps
Quantum gates act on qubits, like logic gates act on bits. In this section, we will
explore what quantum gates are.
A quantum gate transforms the state of a qubit into other states. As we will
see later, we often use the capital letter U to denote a quantum gate. For example,
consider a quantum gate that performs the following map:
2.6 Quantum Gates
99
U|0⟩=
√
2−i
2
|0⟩−1
2|1⟩,
U|1⟩= 1
2|0⟩+
√
2+i
2
|1⟩.
A quantum gate must be linear, meaning we can distribute it across superpositions:
U(α|0⟩+β|1⟩) = αU|0⟩+βU|1⟩
= α
 √
2−i
2
|0⟩−1
2|1⟩
!
+β
 
1
2|0⟩+
√
2+i
2
|1⟩
!
=
 
α
√
2−i
2
+β 1
2
!
|0⟩+
 
−α 1
2 +β
√
2+i
2
!
|1⟩.
For this to be a valid quantum gate, the total probability must remain 1. Assuming
the original state was normalized, i.e., |α|2 + |β|2 = 1, we can calculate the total
probability by summing the norm-square of each amplitude to see if it is still 1:
α
√
2−i
2
+β 1
2

2
+
−α 1
2 +β
√
2+i
2

2
=
 
α
√
2−i
2
+β 1
2
! 
α∗
√
2+i
2
+β ∗1
2
!
+
 
−α 1
2 +β
√
2+i
2
! 
−α∗1
2 +β ∗
√
2−i
2
!
= |α|2 (
√
2−i)(
√
2+i)
4
+αβ ∗
√
2−i
4
+βα∗
√
2+i
4
+|β|2 1
4
+|α|2 1
4 −αβ ∗
√
2−i
4
−βα∗
√
2+i
4
+|β|2 (
√
2+i)(
√
2−i)
4
= |α|2 3
4 +|β|2 1
4 +|α|2 1
4 +|β|2 3
4
= |α|2 +|β|2
= 1.
So, U is a valid quantum gate. Then,
Quantum gates are linear maps that keep the total probability equal to 1.
In the next chapter, when we introduce linear algebra, we will learn that operators
correspond to tables of numbers called matrices, and a valid quantum gate is a type
of matrix called a unitary matrix. That is why quantum gates are often labeled U,
for unitary.
100
2 One Quantum Bit
Exercise 2.22. Consider a map U that transforms the Z-basis states as follows:
U|0⟩= |0⟩+|1⟩,
U|1⟩= |0⟩−|1⟩.
Say |ψ⟩= α|0⟩+β|1⟩is a normalized quantum state, i.e., |α|2 +|β|2 = 1.
(a) Calculate U|ψ⟩.
(b) From your answer to (a), is U a valid quantum gate? Explain your reasoning.
Exercise 2.23. Consider a map U that transforms the Z-basis states as follows:
U|0⟩=
√
3
2 |0⟩+
√
3+i
4
|1⟩,
U|1⟩=
√
3+i
4
|0⟩−
√
3+3i
4
|1⟩.
Say |ψ⟩= α|0⟩+β|1⟩is a normalized quantum state, i.e., |α|2 +|β|2 = 1.
(a) Calculate U|ψ⟩.
(b) From your answer to (a), is U a valid quantum gate? Explain your reasoning.
2.6.2 Classical Reversible Gates
Recall from Section 1.5 that a classical logic gate is reversible if its outputs are
unique. For example, a gate with input A and output B with the following truth table
is reversible, since it is always possible to determine the input from the output:
A B
0 1
1 0
Thus, the gate does the following to a bit:
0 →1,
1 →0.
How would this gate act on a qubit? It would map the following:
Gate|0⟩= |1⟩,
Gate|1⟩= |0⟩.
Then, acting on a superposition α|0⟩+β|1⟩, where |α|2 +|β|2 = 1, it would do the
following:
Gate(α|0⟩+β|1⟩) = α|1⟩+β|0⟩= β|0⟩+α|1⟩.
We see that α and β simply got swapped. So, the final state is normalized, since
|β|2 +|α|2 = |α|2 +|β|2 = 1.
2.6 Quantum Gates
101
Thus, this classical reversible logic gate is also a valid quantum gate.
This is true in general. Any classical reversible logic gate simply permutes (shuf-
fles) the amplitudes around. This chapter is just on a single qubit, but jumping ahead
to Chapter 4 on multiple qubits, if there were more than two amplitudes, a classical
reversible logic gate would just permute them, so the state stays normalized. Thus,
Classical reversible logic gates are valid quantum gates.
In contrast, irreversible gates are not valid quantum gates. For example, consider
the irreversible gate with the following truth table:
A B
0 0
1 0
This would act on the basis states of a qubit as:
Gate|0⟩= |0⟩,
Gate|1⟩= |0⟩.
Then, acting on a superposition,
Gate(α|0⟩+β|1⟩) = α|0⟩+β|0⟩= (α +β)|0⟩.
Now, the amplitudes are not permuted. Instead, they are combined. The total prob-
ability of this is
|α +β|2 = |α|2 +|β|2 +α∗β +αβ ∗= 1+α∗β +αβ ∗̸= 1,
so this is not a valid quantum gate.
Exercise 2.24. Consider each of the following classical logic gates with input A, output B, and
truth table shown below. Is each gate a valid quantum gate? Why?
(a)
A B
0 0
1 1
(b)
A B
0 1
1 1
Exercise 2.25. Consider each of the following classical logic gates with inputs A and B, outputs C
and D, and truth table shown below. Is each gate a valid quantum gate? Why?
(a)
A B C D
0 0 0 1
0 1 1 1
1 0 0 0
1 1 1 0
(b)
A B C D
0 0 0 0
0 1 0 0
1 0 1 0
1 1 1 1
102
2 One Quantum Bit
2.6.3 Common One-Qubit Quantum Gates
Although any probability-preserving linear map is a valid quantum gate, let us list
some important one-qubit gates that frequently appear in quantum computing:
• The identity gate turns |0⟩into |0⟩and |1⟩into |1⟩, hence doing nothing:
I|0⟩= |0⟩,
I|1⟩= |1⟩.
This is a classical reversible gate (the identity gate), so it keeps states normal-
ized and is a valid quantum gate.
In Qubit Touchdown, this corresponds to the Identity Gate action card, which
does nothing to the football’s position.
• The Pauli X gate, or NOT gate, turns |0⟩into |1⟩, and |1⟩into |0⟩:
X|0⟩= |1⟩,
X|1⟩= |0⟩.
This is a classical reversible gate (the NOT gate), so it keeps states normalized
and is a valid quantum gate.
On the Bloch sphere, it can be shown that X is a rotation about the x-axis by
180◦:
x
y
z
With this rotation in mind, we geometrically see that X causes |0⟩(the north
pole) to rotate to |1⟩(the south pole), and vice versa. We also see that |i⟩and
|−i⟩rotate to each other, whereas |+⟩and |−⟩are unchanged. Note, however,
that mathematically X|−⟩= −|−⟩≡|−⟩since the global phase does not matter.
If we apply the X gate twice, we rotate around the x-axis of the Bloch sphere
by 360◦, which does nothing. Then, X2 = I. We can use this fact to simplify
consecutive applications of X. For example,
X1001 = X1000X =
 X2500 X = I500X = X.
In Qubit Touchdown, the Pauli X Gate action card corresponds to the X gate.
2.6 Quantum Gates
103
• The Pauli Y gate turns |0⟩into i|1⟩, and |1⟩into −i|0⟩:
Y|0⟩= i|1⟩,
Y|1⟩= −i|0⟩.
This is not a classical gate at all because of the i and −i. Let us prove that it is
a valid quantum gate by acting on a general superposition:
Y (α|0⟩+β|1⟩) = αY|0⟩
|{z}
i|1⟩
+β Y|1⟩
|{z}
−i|0⟩
= iα|1⟩−iβ|0⟩= −iβ|0⟩+iα|1⟩.
The total probability of this is
|−iβ|2 +|iα|2 = (−iβ)(iβ ∗)+(iα)(−iα∗) = |β|2 +|α|2 = 1,
so it is a valid quantum gate.
On the Bloch sphere, it can be shown that Y is a rotation about the y-axis by
180◦:
x
y
z
So, if we apply the Y gate twice, we rotate around the y-axis of the Bloch sphere
by 360◦, which does nothing. Then, Y 2 = I. In Qubit Touchdown, the Pauli Y
Gate action card corresponds to the Y gate.
• The Pauli Z gate keeps |0⟩as |0⟩and turns |1⟩into −|1⟩:
Z|0⟩= |0⟩,
Z|1⟩= −|1⟩.
This is not a classical gate at all. In Exercise 2.28, you will show that this is a
valid quantum gate.
On the Bloch sphere, it can be shown that Z is a rotation about the z-axis by
180◦:
104
2 One Quantum Bit
x
y
z
As before, Z2 = I. In Qubit Touchdown, the Pauli Z Gate action card is the Z
gate.
Exercise 2.26. Calculate Z217X101Y 50 (α|0⟩+β|1⟩).
Exercise 2.27. Prove that
(a) XZXZ(α|0⟩+β|1⟩) = −(α|0⟩+β|1⟩).
(b) ZXZX(α|0⟩+β|1⟩) = −(α|0⟩+β|1⟩).
This will be used later in the textbook when we discuss Grover’s algorithm.
• Phase gate, which is the square root of the Z gate (i.e., S2 = Z):
S|0⟩= |0⟩,
S|1⟩= i|1⟩.
In Exercise 2.28, you will show that this is a valid quantum gate.
On the Bloch sphere, it can be shown that S is a rotation about the z-axis by 90◦:
x
y
z
90◦
Now, S2 rotates by 90◦twice, so it is equivalent to rotating by 180◦. Then,
S2 = Z. We would need to apply S four times in order to return to the same point
on the Bloch sphere, so S4 = I. In Qubit Touchdown, the Phase Gate action card
is the S gate.
2.6 Quantum Gates
105
• T gate (also called π/8 gate), which is the square root the S gate (i.e., T 2 = S),
or fourth root of the Z gate:
T|0⟩= |0⟩,
T|1⟩= eiπ/4|1⟩.
In Exercise 2.28, you will show that this is a valid quantum gate.
On the Bloch sphere, T is a rotation about the z-axis by 45◦:
x
y
z
45◦
Then, it is obvious that T 2 = S and T 4 = Z, since rotating by 45◦twice is
equivalent to rotating by 90◦, and rotating by 45◦four times is equivalent to
rotating by 180◦.
Exercise 2.28. Consider the gate Rz(θ), which rotates about the z-axis by angle θ:
Rz(θ)|0⟩= |0⟩,
Rz(θ)|1⟩= eiθ|1⟩.
The Z gate, S gate, and T gate are all specific instances of the Rz gate, with Z = Rz(π), S = Rz(π/2),
and T = Rz(π/4). Say |ψ⟩= α|0⟩+β|1⟩is a normalized quantum state, i.e., |α|2 +|β|2 = 1.
(a) Calculate Rz(θ)|ψ⟩.
(b) Show that the total probability of Rz(θ)|ψ⟩is 1, so Rz(θ) is a valid quantum gate, and hence,
Z, S, and T are all valid quantum gates.
• The Hadamard gate turns |0⟩into |+⟩, and |1⟩into |−⟩:
H|0⟩= 1
√
2
(|0⟩+|1⟩) = |+⟩,
H|1⟩= 1
√
2
(|0⟩−|1⟩) = |−⟩.
In Exercise 2.29, you will show that this is a valid quantum gate.
On the Bloch sphere, it can be shown that H is a rotation about the x+z-axis by
180◦:
106
2 One Quantum Bit
x
y
z
x + z
Then, H maps between |0⟩and |+⟩, between |1⟩and |−⟩, and between |i⟩and
|−i⟩. Let us also prove these algebraically. From the definition of the Hadamard
gate, we already have H|0⟩= |+⟩and H|1⟩= |−⟩. Going in the other direction,
H|+⟩= H 1
√
2
(|0⟩+|1⟩)
= 1
√
2
(H|0⟩+H|1⟩)
= 1
√
2
 1
√
2
(|0⟩+|1⟩)+ 1
√
2
(|0⟩−|1⟩)

= |0⟩,
and similarly (Exercise 2.30),
H|−⟩= |1⟩.
We also have
H|i⟩= H 1
√
2
(|0⟩+i|1⟩)
= 1
√
2
(H|0⟩+iH|1⟩)
= 1
√
2
 1
√
2
(|0⟩+|1⟩)+ i
√
2
(|0⟩−|1⟩)

= 1
√
2
1+i
√
2
|0⟩+ 1−i
√
2
|1⟩

= 1
√
2

eiπ/4|0⟩+e−iπ/4|1⟩

= eiπ/4 1
√
2

|0⟩+e−iπ/2|1⟩

= eiπ/4 1
√
2
(|0⟩−i|1⟩)
2.6 Quantum Gates
107
= eiπ/4|−i⟩
≡|−i⟩.
Similarly (Exercise 2.30),
H|−i⟩= e−iπ/4|i⟩≡|i⟩.
If we apply the H gate twice, we rotates by 360◦, which does nothing. So,
H2 = I.
In Qubit Touchdown, the Hadamard Gate action card is the Hadamard gate,
which is why it moves the football between 0 and +, between 1 and −, and
between i and −i.
Exercise 2.29. Say |ψ⟩= α|0⟩+β|1⟩is a normalized quantum state, i.e., |α|2 +|β|2 = 1.
(a) Calculate H|ψ⟩.
(b) Show that the total probability of H|ψ⟩is 1, so H is a valid quantum gate.
Exercise 2.30. Work out the math to show that
(a) H|−⟩= |1⟩.
(b) H|−i⟩= e−iπ/4|i⟩≡|i⟩.
We can combine these quantum gates to create all sorts of states. For example,
HSTH|0⟩= HST 1
√
2
(|0⟩+|1⟩)
= HS 1
√
2

|0⟩+eiπ/4|1⟩

= H 1
√
2

|0⟩+ei3π/4|1⟩

= 1
√
2
 1
√
2
(|0⟩+|1⟩)+ei3π/4 1
√
2
(|0⟩−|1⟩)

= 1
2
h
1+ei3π/4
|0⟩+

1−ei3π/4
|1⟩
i
,
(2.9)
where in the third line, we used ieiπ/4 = eiπ/2eiπ/4 = ei3π/4. On the Bloch sphere,
this state is in the southern hemisphere:
x
y
z
108
2 One Quantum Bit
If we measure this qubit in the Z-basis {|0⟩,|1⟩}, the probability of getting |0⟩is

1
2

1+ei3π/4
2
= 1
2

1+ei3π/4 1
2

1+e−i3π/4
= 1
4

1+e−i3π/4 +ei3π/4 +e0
= 1
4

2+2cos 3π
4

= 1
2
 
1−
√
2
2
!
≈0.146,
where to go from the second to the third line, we used Euler’s formula Eq. (2.6) so
that eiθ +e−iθ = 2cosθ. Similarly, the probability of getting |1⟩is

1
2

1−ei3π/4
2
= 1
4

2−2cos 3π
4

= 1
2
 
1+
√
2
2
!
≈0.854.
As expected, since the state is in the southern hemisphere, there is a greater proba-
bility of getting |1⟩when measuring the qubit.
Exercise 2.31. Calculate Y 51H99T 36Z25|0⟩.
Exercise 2.32. Prove that HXH = Z by showing that HXH|0⟩and Z|0⟩result in the same state,
and HXH|1⟩and Z|1⟩result in the same state. Such an equation is called a circuit identity.
Exercise 2.33. Answer the following:
(a) Calculate HTHTH|0⟩.
(b) If you measure this in the Z-basis {|0⟩,|1⟩}, what is the probability that you get |0⟩and the
probability that you get |1⟩?
2.6.4 General One-Qubit Gates
You may have noticed that all the quantum gates from the last section were rotations
by some angle around some axis. This is true in general:
One-qubit quantum gates are rotations on the Bloch sphere.
This is because rotations on the Bloch sphere satisfy the two properties that we
require of quantum gates: they are linear maps, and they keep the total probability
equal to 1. Proving that rotations are linear requires some math, which we sketch
below. Rotations keep the total probability equal to 1 because a qubit is a point on
the Bloch sphere, so if we rotate it, it remains a point on the Bloch sphere.
2.6 Quantum Gates
109
Mathematically, say we rotate by angle θ about some axis of rotation, which we
can specify in terms of the x-, y-, and z-axes. We denote the direction of the x-axis
by ˆx, the direction of the y-axis by ˆy, and the direction of the z-axis by ˆz. Then, we
can denote the axis of rotation by ˆn, and
ˆn = nx ˆx+ny ˆy+nzˆz.
Note ˆn is a unit vector, meaning it must has length 1, i.e., n2
x +n2
y +n2
z = 1.
For example, the Hadamard gate is a rotation by θ = 180◦= π radians about the
axis halfway between the x- and z-axes, and we can express this axis by
ˆn = 1
√
2
ˆx+ 1
√
2
ˆz,
Note n2
x +n2
y +n2
z = 1/2+0+1/2 = 1, as expected.
Now, we state as a fact (without proof) that a rotation by angle θ about axis
ˆn = (nx,ny,nz) can be written in terms of I, X, Y, and Z:
U = eiγ

cos
θ
2

I −isin
θ
2

(nxX +nyY +nzZ)

,
(2.10)
where γ is a global phase that we can set to anything (or drop), since it does not have
any physical relevance.
Returning to our example of the Hadamard gate, with θ = π and ˆn = (1/
√
2,0,
1/
√
2), we have
U = eiγ

cos
π
2

I −isin
π
2
 1
√
2
X +0Y + 1
√
2
Z

= −ieiγ 1
√
2
(X +Z).
To show that U is the Hadamard gate, let us see how it acts on |0⟩and |1⟩:
U|0⟩= −ieiγ 1
√
2
(X +Z)|0⟩= −ieiγ 1
√
2
(X|0⟩+Z|0⟩)
= −ieiγ 1
√
2
(|1⟩+|0⟩) = −ieiγ|+⟩,
U|1⟩= −ieiγ 1
√
2
(X +Z)|1⟩= −ieiγ 1
√
2
(X|1⟩+Z|1⟩)
= −ieiγ 1
√
2
(|0⟩−|1⟩) = −ieiγ|−⟩.
We can drop the global phase of −ieiγ, so U|0⟩= |+⟩and U|1⟩= |−⟩, which is the
Hadamard gate. Alternatively, we can choose γ = π/2, and then since −i = ei3π/2,
the global phase is −ieiγ = ei3π/2eiπ/2 = ei2π = 1.
110
2 One Quantum Bit
We can also use Eq. (2.10) to prove that rotations are linear. Note U is a sum of I,
X, Y, and Z with numbers as coefficients; we call this a linear combination of I, X,
Y, and Z. Since each of these gates distribute over superpositions, U also distributes
over superpositions, and so it is linear:
U(α|0⟩+β|1⟩) = eiγ

cos
θ
2

I −isin
θ
2

(nxX +nyY +nzZ)

(α|0⟩+β|1⟩)
= αeiγ

cos
θ
2

I −isin
θ
2

(nxX +nyY +nzZ)

|0⟩
+βeiγ

cos
θ
2

I −isin
θ
2

(nxX +nyY +nzZ)

|1⟩)
= αU|0⟩+βU|1⟩.
Exercise 2.34. Consider a rotation by 45◦about the z-axis.
(a) What is ˆn?
(b) Use Eq. (2.10) to express the rotation U is terms of I, X, Y, and Z.
(c) Find U|0⟩.
(d) Find U|1⟩.
(e) Show that U is the T gate, up to a global phase.
Exercise 2.35. Consider a rotation by 180◦around the axis equidistant from the x-, y-, and z-axes.
Below, the first picture shows the axis on the Bloch sphere. It may be a little difficult to visualize,
however, so another description of it using a cube is shown below in the second picture. The cube
has a corner at the origin and edges of length s along the x-, y-, and z-axes. Then, the axis of rotation
goes through the origin and the point (s,s,s), and the axis of rotation is drawn as a thicker line.
x
y
z
x + y + z
x
y
z
(s, s, s)
s
s
s
(a) Draw the Bloch sphere and show where |0⟩goes after applying the rotation. Do this without
any calculations by visualizing the rotation on the Bloch sphere.
(b) Draw the Bloch sphere and show where |1⟩goes after applying the rotation. Do this without
any calculations by visualizing the rotation on the Bloch sphere.
(c) What is ˆn? Hint: It should have equal components in the x-, y-, and z-axes, and it should be a
unit vector.
(d) Use Eq. (2.10) to express the rotation U is terms of I, X, Y, and Z.
(e) Find U|0⟩.
(f) Find U|1⟩.
2.7 Quantum Circuits
111
2.7 Quantum Circuits
2.7.1 Circuit Diagrams
Just like we can draw a classical circuit diagram consisting of bits and logic gates,
we can draw quantum circuit diagrams consisting of qubits and quantum gates. For
example, HSTH|0⟩is
|0⟩
H
T
S
H
The circuit is read left-to-right, just like a classical circuit diagram. So, we start with
a single qubit in the |0⟩state and apply a Hadamard gate H to it, followed by a T
gate, then a phase gate S, and finally another H gate. It is implied that we measure
the qubit at the end of the circuit. We can also explicitly draw the measurement as a
meter:
|0⟩
H
T
S
H
2.7.2 Quirk
A great web-based tool for simulating quantum circuits is Quirk at https://alga
ssert.com/quirk:
There are a variety of quantum gates that users can drag and drop onto the circuit,
and users can also make custom gates. Each qubit is initially a |0⟩, as shown below:
112
2 One Quantum Bit
The state of each qubit is visualized a couple ways. First, the probability that a
measurement of the qubit yields |1⟩is zero, and this is labeled as “Off.” Second,
the Bloch sphere representation of the qubit is shown, and the state is a point at the
north pole, as expected.
Now if we apply the X gate, Quirk shows the following:
Note Quirk uses ⊕to denote a single X gate, whereas we use X (later, we will
also use ⊕when the X gate is controlled by another qubit). Since X|0⟩= |1⟩, the
probability of measuring |1⟩is 1. This is labeled as “On,” and the Bloch sphere now
shows the state at the south pole, as expected.
Now consider H|0⟩= |+⟩:
In Quirk, it shows a 50% chance of being |1⟩when measured in the Z-basis, and it
lies on the x-axis of the Bloch sphere, as expected.
Finally, let us simulate HSTH|0⟩, which we earlier computed by hand to have a
0.146 probability of being |0⟩and a 0.854 probability of being |1⟩. Simulating it in
Quirk,
Quirk displays that the probability of measuring |1⟩is 85.4% = 0.854, in agreement
with our previous calculations from Eq. (2.9).
Exercise 2.36. Answer the following about HYTHX|0⟩.
(a) Draw HYTHX|0⟩as a quantum circuit.
(b) Using Quirk, sketch where HYTHX|0⟩appears on the Bloch sphere.
(c) Using Quirk, if you measure HYTHX|0⟩in the Z-basis, what is the probability that you get
|0⟩and the probability that you get |1⟩?
2.8 Summary
The smallest unit of quantum information is the qubit. Besides having two orthog-
onal states |0⟩and |1⟩, a qubit can be a superposition of them with complex am-
plitudes. The norm-square of the amplitudes gives the probability of measuring the
qubit as a 0 or 1, and depending on the outcome, the qubit collapses to |0⟩or |1⟩.
Qubits can also be measured in other bases. A qubit can be visualized on the Bloch
2.8 Summary
113
sphere. Qubits are operated on by quantum gates, which are linear maps that keep
the total probability equal to 1. A single-qubit gate is a rotation on the Bloch sphere.
A quantum circuit is a drawing to depict what quantum gates act on a qubit, and an
online simulator for quantum circuits is Quirk.
Chapter 3
Linear Algebra
So far, we have done quantum computing using elementary algebra and using the
fact that quantum gates are linear, so they distribute across superpositions. This can
be tedious, however, such as when calculating HYTHX(α|0⟩+β|1⟩). Fortunately,
there is an easier way to do the math of quantum computing using linear algebra,
where numbers are arranged in columns, rows, and tables (called matrices). Ulti-
mately, linear algebra is a tool. Learning how to use a new tool may be difficult at
first, but ultimately, it makes the job easier. We will show how linear algebra can be
used for many of the calculations from the previous chapter.
We had a similar progression with classical computing. In Chapter 1, we could
do all of classical computing using truth tables, including to show that ABC+ABC+
ABC + ABC = A+B + AC. Boolean algebra makes such calculations easier, but it
requires becoming proficient enough with the tool. Linear algebra is to quantum
computing as boolean algebra is to classical computing.
3.1 Quantum States
3.1.1 Column Vectors
We write |0⟩and |1⟩as column vectors, which are vertical lists of numbers:
|0⟩=

1
0

,
|1⟩=

0
1

.
Our notation is to write vectors in large parenthesis, but some people use square
brackets instead. These are called column vectors because they have a single col-
umn, and they have length 2 because they have two entries. Writing |0⟩and |1⟩, this
way, it is easy to write superpositions of them. A generic qubit with amplitudes α
and β would be
115
116
3 Linear Algebra
|ψ⟩= α|0⟩+β|1⟩= α

1
0

+β

0
1

=

α
0

+

0
β

=

α
β

.
For example, |i⟩can be written as a column vector:
|i⟩= 1
√
2
|0⟩+ i
√
2
|1⟩= 1
√
2

1
0

+ i
√
2

0
1

=

1/
√
2
0

+

0
i/
√
2

=

1/
√
2
i/
√
2

= 1
√
2

1
i

.
Exercise 3.1. A qubit is in the following state:
1
2|0⟩−
√
3
2 |1⟩.
Write this state as a column vector.
Exercise 3.2. A qubit is in the following state:
√
3/2
1/2

.
If you measure this qubit in the Z-basis {|0⟩,|1⟩}, what states can you get and with what probabil-
ities?
3.1.2 Row Vectors
Next, the transpose of a column vector is obtained by rewriting it as a row vector,
and it is denoted by ⊺(a superscript letter T). So,

α
β
⊺
=
 α β

.
In quantum computing, we typically use the conjugate transpose, which is obtained
by taking the complex conjugate of each component of the transpose. It is denoted
by † (a dagger):

α
β
†
=
 α∗β ∗
.
This is used so frequently in quantum mechanics that bra-ket notation has a special
way of writing it: with an angle bracket and a vertical bar, called a bra:
⟨ψ| =
 α∗β ∗
.
3.1 Quantum States
117
Then, a bra is the conjugate transpose of a ket, and conversely, a ket is the conjugate
transpose of a bra. Flipping between kets and bras is called “taking the dual”, and the
dual of a ket is its bra version, and the dual of a bra is its ket version. For example,
the dual of |i⟩=

1/
√
2
i/
√
2

is
⟨i| =
 1/
√
2 −i/
√
2

.
We can also take the conjugate transpose of |0⟩=

1
0

and |1⟩=

0
1

to get the
Z-basis vectors as bras:
⟨0| =
 1 0

,
⟨1| =
 0 1

.
Then, ⟨ψ| can be written as
⟨ψ| =
 α∗β ∗
=
 α∗0

+
 0 β ∗
= α∗ 1 0

+β ∗ 0 1

= α∗⟨0|+β ∗⟨1|.
Notice this has amplitudes α∗and β ∗, so when we go from |ψ⟩to ⟨ψ|, we need
to take the complex conjugate of the amplitudes. For example, taking the dual of
|i⟩=
1
√
2|0⟩+
i
√
2|1⟩, we get
⟨i| = 1
√
2
⟨0|−i
√
2
⟨1|.
To summarize, to go between kets and bras,
|ψ⟩=

α
β

⇐⇒
⟨ψ| =
 α∗β ∗
,
|ψ⟩= α|0⟩+β|1⟩⇐⇒⟨ψ| = α∗⟨0|+β ∗⟨1|.
Exercise 3.3. Consider the following two states |a⟩and |b⟩:
|a⟩=
√
3
2 |0⟩+ 1
2|1⟩,
|b⟩= 2
3|0⟩+ 1−2i
3
|1⟩.
Answer the following questions:
(a) What is ⟨a| in terms of ⟨0| and ⟨1|?
(b) What is ⟨a| as a row vector?
(c) What is ⟨b| in terms of ⟨0| and ⟨1|?
(d) What is ⟨b| as a row vector?
118
3 Linear Algebra
3.2 Inner Products
3.2.1 Inner Products Are Scalars
Say we have two states,
|ψ⟩=

α
β

,
and
|φ⟩=

γ
δ

.
One way to multiply |ψ⟩and |φ⟩is by taking their inner product, and it is defined
as ⟨ψ| times |φ⟩:
⟨ψ||φ⟩=
 α∗β ∗
γ
δ

.
Typically, we combine the two vertical bars into one and write the inner product as
⟨ψ|φ⟩=
 α∗β ∗
γ
δ

.
We call ⟨ψ|φ⟩a bra-ket or bracket, and the word bracket is the origin of the terms
bra and ket. To evaluate the inner product, from linear algebra, we multiply a row
vector and a column vector by multiplying the first entry of each vector together,
multiplying the second entry of each vector together, and adding them (i.e., taking
their dot product):
⟨ψ|φ⟩= α∗γ +β ∗δ.
Note the result is just a number, or scalar. So, an inner product is also called a scalar
product.
The inner product of |φ⟩and |ψ⟩is just the complex conjugate of the inner prod-
uct of |ψ⟩and |φ⟩:
⟨φ|ψ⟩= ⟨ψ|φ⟩∗.
To prove this, we can just do a simple calculation:
⟨φ|ψ⟩=
 γ∗δ ∗
α
β

= γ∗α +δ ∗β = (γα∗+δβ ∗)∗= (α∗γ +β ∗δ)∗
= ⟨ψ|φ⟩∗.
Inner products have several uses, which we will see next and throughout this
chapter.
Exercise 3.4. Consider
|a⟩= 3+i
√
3
4
|0⟩+ 1
2|1⟩,
|b⟩= 1
4|0⟩+
√
15
4
|1⟩.
(a) Find ⟨a|b⟩.
3.2 Inner Products
119
(b) Find ⟨b|a⟩.
(c) What is the relationship between your answers to parts (a) and (b)?
3.2.2 Orthonormality
Several properties can be expressed using the inner product:
• First, let us take the inner product of |ψ⟩= α|0⟩+β|1⟩with itself:
⟨ψ|ψ⟩=
 α∗β ∗
α
β

= |α|2 +|β|2 = 1.
So, ⟨ψ|ψ⟩is just the total probability, and if ⟨ψ|ψ⟩= 1, the state |ψ⟩is normal-
ized.
• Consider the inner product of the Z-basis states |0⟩and |1⟩:
⟨0|1⟩=
 1 0

0
1

= 1·0+0·1 = 0+0 = 0.
Next, consider the inner product of the X-basis states |+⟩and |−⟩:
⟨+|−⟩= 1
√
2
 1 1
 1
√
2

1
−1

= 1
2
 1 1

1
−1

= 1
2 (1−1) = 0.
Finally, consider the inner product of the Y-basis states |i⟩and |−i⟩:
⟨i|−i⟩= 1
√
2
 1 −i
 1
√
2

1
−i

= 1
2
 1 −i

1
−i

= 1
2
 1+i2
= 0,
where we used i2 = (√−1)2 = −1.
In all of these, the outcome was zero. In fact, any two states on opposite sides of
the Bloch sphere have zero inner product (Exercise 3.8). We say that states with
zero inner product are orthogonal. Thus, orthogonal states are distinct measure-
ment outcomes.
• These two properties, normalized and orthogonal, can be combined into a sin-
gle word, orthonormal. So |0⟩and |1⟩are orthonormal because each state is
individually normalized, and they are orthogonal to each other. Similarly, |+⟩
and |−⟩are orthonormal, and |i⟩and |−i⟩are orthonormal.
Exercise 3.5. Consider a qubit in the following state
|ψ⟩= A(2|0⟩+3i|1⟩).
(a) Calculate ⟨ψ|ψ⟩.
(b) Find a value of A that normalizes |ψ⟩.
Exercise 3.6. Determine if each pair of states is orthogonal or not.
120
3 Linear Algebra
(a) |+⟩and |−⟩.
(b) |0⟩and |+⟩.
(c) 1+
√
3i
4
|0⟩+
√
2−i
2
|1⟩and
√
2+i
2
|0⟩+ −1+
√
3i
4
|1⟩.
Exercise 3.7. Consider
|a⟩= 3+i
√
3
4
|0⟩+ 1
2|1⟩,
|b⟩= 1
4|0⟩+x|1⟩.
(a) Find x so that |a⟩and |b⟩are orthogonal.
(b) Find x so that |b⟩is normalized.
(c) For what values of x (if any) are |a⟩and |b⟩orthonormal?
Exercise 3.8. Say we have two qubits |a⟩and |b⟩. We can parameterize them in spherical coordi-
nates (θ,φ) on the Bloch sphere:
|a⟩= cos
θa
2

|0⟩+eiφa sin
θa
2

|1⟩,
|b⟩= cos
θb
2

|0⟩+eiφb sin
θb
2

|1⟩.
Now say |a⟩and |b⟩lie on opposite points of the Bloch sphere. This means θb = π −θa and
φb = φa +π. Show that ⟨a|b⟩= 0, i.e., they are orthogonal. Possibly useful trigonometric identities:
sin(u±v) = sin(u)cos(v)±cos(u)sin(v),
cos(u±v) = cos(u)cos(v)∓sin(u)sin(v).
3.2.3 Projection, Measurement, and Change of Basis
Next, inner products can be used to find the amplitudes of quantum states, which
can be norm-squared to yield measurement probabilities. The amplitudes can also
be used to change the basis. As an example, consider a qubit in the following state:
|ψ⟩=
√
3
2 |0⟩+ 1
2|1⟩.
It appears on the Bloch sphere as shown below:
3.2 Inner Products
121
x
y
z
|0⟩
|1⟩
|+⟩
|−⟩
|i⟩
|−i⟩
•
√
3
2 |0⟩+ 1
2|1⟩
In Section 2.3.3, we measured this qubit in the Z-basis, X-basis, Y-basis, and a
fourth basis. Let us see how to reproduce the results using inner products.
First, we want to find the possible measurement outcomes and their probabili-
ties if we measure |ψ⟩in the {|0⟩,|1⟩} basis. Although we can just “read off” the
amplitudes of |ψ⟩with respect to |0⟩and |1⟩and take the norm-square of each to
find the probabilities, we can also find the amplitudes using inner products and the
orthonormality of {|0⟩,|1⟩}. For example, the amplitude of |0⟩is
⟨0|ψ⟩= ⟨0|
 √
3
2 |0⟩+ 1
2|1⟩
!
=
√
3
2 ⟨0|0⟩
|{z}
1
+1
2 ⟨0|1⟩
|{z}
0
=
√
3
2 .
When calculating this, the amplitude from |1⟩vanishes because |0⟩and |1⟩are or-
thogonal (i.e., ⟨0|1⟩= 0). We get just the amplitude from |0⟩because |0⟩is normal-
ized (i.e., ⟨0|0⟩= 1). Similarly, the amplitude of |1⟩is
⟨1|ψ⟩= ⟨1|
 √
3
2 |0⟩+ 1
2|1⟩
!
=
√
3
2 ⟨1|0⟩
|{z}
0
+1
2 ⟨1|1⟩
|{z}
1
= 1
2.
Taking the norm-square of each of these, the possible measurement outcomes are
|0⟩with probability 3/4 and |1⟩with probability 1/4. Since ⟨0|ψ⟩and ⟨1|ψ⟩are the
amplitudes of |0⟩and |1⟩, respectively, we can write the state of the qubit as
|ψ⟩= ⟨0|ψ⟩|0⟩+⟨1|ψ⟩|1⟩.
This technique is far more useful when we cannot just read off the amplitudes,
such as when measuring in other bases. Let us measure in the X-basis {|+⟩,|−⟩}
now. The amplitude of |+⟩is
⟨+|ψ⟩= 1
√
2
(⟨0|+⟨1|)
 √
3
2 |0⟩+ 1
2|1⟩
!
122
3 Linear Algebra
= 1
√
2
 √
3
2 ⟨0|0⟩
|{z}
1
+1
2 ⟨0|1⟩
|{z}
0
+
√
3
2 ⟨1|0⟩
|{z}
0
+1
2 ⟨1|1⟩
|{z}
1
!
= 1
√
2
 √
3
2 + 1
2
!
=
√
3+1
2
√
2
.
This agrees with our calculation in Section 2.3.3. Similarly, the amplitude of |−⟩is
⟨−|ψ⟩, but now let us calculate it using linear algebra:
⟨−|ψ⟩= 1
√
2
 1 −1
√
3/2
1/2

= 1
√
2
 √
3
2 −1
2
!
=
√
3−1
2
√
2
.
Again, this agrees with Section 2.3.3. This approach is especially convenient be-
cause we can do the calculations using a computer algebra system that supports
linear algebra, like Mathematica or SageMath:
• In Mathematica,
psi={{Sqrt[3]/2},{1/2}};
minus={{1/Sqrt[2]},{-1/Sqrt[2]}};
ConjugateTranspose[minus].psi
The first line defines a column vector named psi (for |ψ⟩), and the second line
defines a column vector named minus (for |−⟩). To take their inner product,
we take the conjugate transpose of minus (which is ⟨−|) and perform a vector
multiplication (denoted by the period) with psi. The result of this is
√
3−1
2
√
2
,
as expected.
• In SageMath,
sage: psi = vector([sqrt(3)/2,1/2])
sage: minus = vector([1/sqrt(2),-1/sqrt(2)])
sage: minus.conjugate()*psi
1/4*sqrt(3)*sqrt(2) - 1/4*sqrt(2)
The first line defines a vector psi (for |ψ⟩), and the second line defines a vector
minus (for |−⟩). Note we do not need to specify whether it is a column vector
or row vector in SageMath. It will automatically transpose the vector to what-
ever shape is needed. In the third line, we calculate minus.conjugate() (for
⟨−|) and multiply it onto psi using an asterisks, yielding the fourth line as the
answer. Let us simplify it:
(1/4)
√
3
√
2−(1/4)
√
2 =
√
3
√
2
4
−
√
2
4 =
√
3−1
2
√
2
.
3.2 Inner Products
123
This matches what we expected.
Since ⟨+|ψ⟩and ⟨−|ψ⟩are the amplitudes of |+⟩and |−⟩, respectively, we can
write the state of the qubit in the {|+⟩,|−⟩} basis as
|ψ⟩= ⟨+|ψ⟩|+⟩+⟨−|ψ⟩|−⟩.
In Section 2.3.3, we also measured |ψ⟩in the Y-basis and in a fourth basis. These
will be left as Exercise 3.9.
In general,
For an orthonormal basis {|a⟩,|b⟩}, the state of a qubit can be written as
|ψ⟩= α|a⟩+β|b⟩,
where α = ⟨a|ψ⟩and β = ⟨b|ψ⟩.
Finally, we end with some terminology. We have been saying that ⟨a|ψ⟩is the
amplitude of |ψ⟩in |a⟩. We can also say that ⟨a|ψ⟩is the amount of |ψ⟩that is in
|a⟩. Or, ⟨a|ψ⟩is the amount that |ψ⟩and |a⟩overlap. In mathematical language,
⟨a|ψ⟩is the projection of |ψ⟩onto |a⟩or |b⟩.
Exercise 3.9. Consider a qubit in the following state
|ψ⟩=
√
3
2 |0⟩+ 1
2|1⟩.
Consider measuring this qubit in the Y-basis {|i⟩,|−i⟩} and the orthonormal basis {|a⟩,|b⟩}, where
|a⟩=
√
3
2 |0⟩+ i
2|1⟩,
|b⟩= i
2|0⟩+
√
3
2 |1⟩.
(a) Calculate ⟨i|ψ⟩.
(b) Calculate ⟨−i|ψ⟩.
(c) If you measure the qubit in the Y-basis, what states can you get and with what probabilities?
(d) Calculate ⟨a|ψ⟩.
(e) Calculate ⟨b|ψ⟩.
(f) If you measure the qubit in the {|a⟩,|b⟩} basis, what states can you get and with what proba-
bilities?
Hint: Your answers should agree with Section 2.3.3.
Exercise 3.10. Consider a qubit in the following state
|ψ⟩= 3+i
√
3
4
|0⟩−1
2|1⟩,
which lies on the Bloch sphere at (θ,φ) = (π/3,5π/6).
(a) If you measure it in the Z-basis {|0⟩,|1⟩}, what states can you get and with what probabilities?
124
3 Linear Algebra
(b) If you measure it in the X-basis {|+⟩,|−⟩}, what states can you get and with what probabili-
ties?
(c) If you measure it in the Y-basis {|i⟩,|−i⟩}, what states can you get and with what probabili-
ties?
Exercise 3.11. A qubit is in the state
|ψ⟩= 1
√
6

1−2i
1

.
(a) Express this state in the {|+⟩,|−⟩} basis.
(b) Express this state in the {|i⟩,|−i⟩} basis.
3.3 Quantum Gates
3.3.1 Gates as Matrices
Recall a quantum gate U generally turns |0⟩and |1⟩into superpositions of |0⟩and
|1⟩:
U|0⟩= a|0⟩+b|1⟩=

a
b

,
U|1⟩= c|0⟩+d|1⟩=

c
d

.
We can arrange the resulting amplitudes side-by-side, resulting in a matrix, which
is a rectangular array/table of numbers:
U =

a
b
 
c
d

=

a c
b d

.
This is a 2×2 matrix because it has two rows and two columns. Plugging this matrix
into U|0⟩and U|1⟩, we get
U|0⟩=

a c
b d

1
0

=

a
b

,
U|1⟩=

a c
b d

0
1

=

c
d

.
These correctly suggest that we can multiply a matrix and a vector in the following
manner: To get the first (second) entry, we take the first (second) row of the matrix
and multiply it component-by-component with the vector, then add the results:
3.3 Quantum Gates
125

a c
b d

1
0

=

a·1+c·0
b·1+d ·0

=

a
b

,

a c
b d

0
1

=

a·0+c·1
b·0+d ·1

=

c
d

.
This is exactly the way matrices and vectors multiply in linear algebra.
Of course, U can also act on superpositions. If a qubit is in the state
|ψ⟩= α|0⟩+β|1⟩=

α
β

,
then applying U transforms this to
U|ψ⟩= α(a|0⟩+b|1⟩)+β(c|0⟩+d|1⟩)
= (aα +cβ)|0⟩+(bα +dβ)|1⟩
=

aα +cβ
bα +dβ

.
Let us show that the matrix representation of this yields the expected result:
U|ψ⟩=

a c
b d

α
β

=

aα +cβ
bα +dβ

.
This is exactly what we expect.
In the language of linear algebra, quantum gates are matrices. Of course, the
matrix must ensure that the total probability remains 1, so in the previous example,
we must have |aα +cβ|2 +|bα +dβ|2 = 1. This yields the following point:
Quantum gates are matrices that keep the total probability equal to 1.
For example, we previously showed in Section 2.6.1 that the following is a valid
quantum gate because it keeps the total probability equal to 1:
U|0⟩=
√
2−i
2
|0⟩−1
2|1⟩=
 √
2−i
2
−1
2
!
,
U|1⟩= 1
2|0⟩+
√
2+i
2
|1⟩=
 
1
2
√
2+i
2
!
.
Then, as a matrix,
U =
 √
2−i
2
1
2
−1
2
√
2+i
2
!
.
126
3 Linear Algebra
In Quirk, we can create this quantum gate by clicking the “Make Gate” button at
the top of the page. A dialog box will pop up with different options, and we want to
create a gate from a matrix:
We enter the matrix, give it a name, and then click “Create Matrix Gate.” Then, it
will appear in the bottom-right toolbar under “Custom Gates:”
We can drag this onto the main circuit like any other gate. Here it is, along with a T
and H gate:
From this, we see that HTU|0⟩has a 57.3% chance of collapsing to |1⟩, and hence
a 42.7% chance of collapsing to a |0⟩.
Exercise 3.12. Consider an operator U performs the following mapping on the Z-basis states:
U|0⟩= 1
√
2

1
−i

,
U|1⟩= 1
√
2

−i
1

.
(a) What is U as a matrix?
(b) What is U

α
β

?
(c) From your answer to (b), is U a valid quantum gate? Explain your reasoning.
3.3 Quantum Gates
127
Exercise 3.13. A quantum gate U performs the following mapping on the Z-basis states:
U|0⟩=
1
2
√
3 [(3+i)|0⟩−(1+i)|1⟩],
U|1⟩=
1
2
√
3 [(1−i)|0⟩+(3−i)|1⟩].
(a) What is U as a matrix?
(b) Create U as a custom gate in Quirk. Using Quirk, if you measure HUH|0⟩, what are the
possible outcomes, and with what probabilities?
3.3.2 Common One-Qubit Gates as Matrices
Previously, we introduced several common one-qubit gates, including the identity
gate I, Pauli X, Y, and Z gates, the phase gate S, the T gate, and the Hadamard gate
H. Now, each of these can be represented as a matrix:
Gate
Action on Computational Basis
Matrix Representation
Identity
I|0⟩= |0⟩
I =

1 0
0 1

I|1⟩= |1⟩
Pauli X
X|0⟩= |1⟩
X =

0 1
1 0

X|1⟩= |0⟩
Pauli Y
Y|0⟩= i|1⟩
Y =

0 −i
i 0

Y|1⟩= −i|0⟩
Pauli Z
Z|0⟩= |0⟩
Z =

1 0
0 −1

Z|1⟩= −|1⟩
Phase S
S|0⟩= |0⟩
S =

1 0
0 i

S|1⟩= i|1⟩
T
T|0⟩= |0⟩
T =
1
0
0 eiπ/4

T|1⟩= eiπ/4|1⟩
Hadamard H
H|0⟩=
1
√
2(|0⟩+|1⟩)
H =
1
√
2

1 1
1 −1

H|1⟩=
1
√
2(|0⟩−|1⟩)
Of particular note is
I =

1 0
0 1

.
This is called the 2×2 identity matrix. When it acts on a vector, it does nothing. For
example,

1 0
0 1

α
β

=

1α +0β
0α +1β

=

α
β

.
128
3 Linear Algebra
The same is true of large matrices. An N × N matrix with 1’s on the diagonal and
0’s everywhere else is called the N ×N identity matrix.
Exercise 3.14. Recall from Eq. (2.10) than a single-qubit gate is a rotation by some angle θ about
some axis ˆn = (nx,ny,nz). Consider a quantum gate that rotates by 90◦about the y-axis. Using
Eq. (2.10) with γ = 0, what is this gate as a matrix?
3.3.3 Sequential Quantum Gates
Using linear algebra, we can compute the effect of a sequence of quantum gates. For
example, we had previously shown in Eq. (2.9) that
HSTH|0⟩= 1
2
h
1+ei3π/4
|0⟩+

1−ei3π/4
|1⟩
i
.
Now, we can perform this same calculation using linear algebra by multiply each
matrix onto the vector:
HSTH|0⟩= 1
√
2

1 1
1 −1

1 0
0 i
1
0
0 eiπ/4
 1
√
2

1 1
1 −1

1
0

= 1
2

1 1
1 −1

1 0
0 i
1
0
0 eiπ/4

1
1

= 1
2

1 1
1 −1

1 0
0 i
 1
eiπ/4

= 1
2

1 1
1 −1

1
ei3π/4

= 1
2

1+ei3π/4
1−ei3π/4

.
We can also compute this using any computing system that supports linear algebra,
such as Mathematica or SageMath:
• In Mathematica,
zero = {{1}, {0}};
H = 1/Sqrt[2] {{1, 1}, {1, -1}};
S = {{1, 0}, {0, I}};
T = {{1, 0}, {0, Eˆ(I Pi/4)}};
H.S.T.H.zero
The first line defines a column vector named zero, and the second, third, and
fourth lines define the quantum gates as matrices. The fifth line multiplies them
together. Note a period (.) must be used for matrix multiplication; an asterisk
(*) denotes element-by-element multiplication. The output of this code is
1
2 + 1
2ie
iπ
4

,
1
2 −1
2ie
iπ
4

,
3.3 Quantum Gates
129
which is precisely what we calculated by hand since i = eiπ/2, and so ieiπ/4 =
eiπ/2eiπ/4 = ei(π/2+π/4) = ei3π/4.
• In SageMath,
sage: zero = vector([1,0]).column()
sage: H = 1/sqrt(2) * Matrix([[1,1],[1,-1]])
sage: S = Matrix([[1,0],[0,i]])
sage: T = Matrix([[1,0],[0,eˆ(i*pi/4)]])
sage: H*S*T*H*zero
[ (1/4*I - 1/4)*sqrt(2) + 1/2]
[-(1/4*I - 1/4)*sqrt(2) + 1/2]
The first line defines a column vector named zero, and the next three lines define
the quantum gates as matrices. The fifth line multiplies them together, and the
final two lines are the output, which is a column vector:
   i
4 −1
4
√
2+ 1
2
−
  i
4 −1
4
√
2+ 1
2

.
Since (i/4−1/4)
√
2 = (i−1)/(2
√
2) and (i−1)/
√
2 = ei3π/4, this becomes
1
2

1+ei3π/4
1−ei3π/4

,
which is exactly what we had before.
Exercise 3.15. In Section 3.3.1, we simulated the following circuit in Quirk:
where
U =
 √
2−i
2
1
2
−1
2
√
2+i
2
!
.
Calculate HTU|0⟩.
3.3.4 Circuit Identities
In Exercise 2.32, we proved the circuit identity HXH = Z by showing that HXH|0⟩
and Z|0⟩result in the same state, and HXH|1⟩and Z|1⟩result in the same state. We
can prove HXH = Z another way using linear algebra:
HXH = 1
√
2

1 1
1 −1

0 1
1 0
 1
√
2

1 1
1 −1

= 1
2

1 1
1 −1

0 1
1 0

1 1
1 −1

.
130
3 Linear Algebra
To continue this calculation, let us multiply the two matrices on the right (X and H).
The procedure is very similar to multiplying a matrix onto a column vector, except
we now have two column vectors. So, we distribute the middle matrix across the
two column vectors of the rightmost matrix.
HXH = 1
2

1 1
1 −1
 
0 1
1 0

1
1
 
0 1
1 0

1
−1
!
= 1
2

1 1
1 −1
 
1
1
 
−1
1
!
= 1
2

1 1
1 −1

1 −1
1 1

.
Now, we can multiply these two matrices by again distributing the left matrix so that
it multiples both columns of the right vector:
HXH = 1
2
 
1 1
1 −1

1
1
 
1 1
1 −1

−1
1
!
= 1
2
 
2
0
 
0
−2
!
= 1
2

2 0
0 −2

=

1 0
0 −1

= Z.
So, we have proved HXH = Z. We can also perform these calculations using a
computer algebra system that supports linear algebra, such as Mathematica or Sage-
Math:
• In Mathematica,
H = 1/Sqrt[2] {{1, 1}, {1, -1}};
X = {{0, 1}, {1, 0}};
H.X.H
This defines the H and X gates as matrices and then multiplies them together,
and the output is
{{1,0},{0,-1}}.
This is precisely Z as a matrix.
• In SageMath,
sage: H = 1/sqrt(2) * Matrix([[1,1],[1,-1]])
sage: X = Matrix([[0,1],[1,0]])
sage: H*X*H
[ 1
0]
[ 0 -1]
This defines the H and X gates as matrices and then multiplies them together,
and the output is a 2×2 matrix, which is precisely Z.
3.3 Quantum Gates
131
Exercise 3.16. Prove that XY = iZ two different ways:
(a) Show that XY|0⟩= iZ|0⟩and XY|1⟩= iZ|1⟩.
(b) Multiply XY as matrices and show that it equals iZ.
3.3.5 Unitarity
Recall from Section 3.3.1 that if a quantum gate U transforms |0⟩and |1⟩as follows
U|0⟩= a|0⟩+b|1⟩=

a
b

,
U|1⟩= c|0⟩+d|1⟩=

c
d

,
then U can be written as a 2×2 matrix:
U =

a c
b d

.
If we apply it to a state |ψ⟩= α|0⟩+β|1⟩, we get
U|ψ⟩=

a c
b d

α
β

=

aα +cβ
bα +dβ

.
We see that U|ψ⟩is a column vector, so we can also write it as a ket |Uψ⟩:
U|ψ⟩= |Uψ⟩.
Now, consider the conjugate transpose of |Uψ⟩:
⟨Uψ| =
 a∗α∗+c∗β ∗b∗α∗+d∗β ∗
=
 α∗β ∗
a∗b∗
c∗d∗

=
 α∗β ∗
a c
b d
†
= ⟨ψ|U†,
where the second equality comes from the convention for multiplying a row vector
and a matrix, where the first column of the matrix is multiplied by the row vector
according to the usual rule to yield the first entry, and the second column of the
matrix is multiplied by the row vector according to the usual rule to yield the second
entry. As another proof, a property of the (conjugate) transpose is that (AB)† = B†A†,
and since |ψ⟩† = ⟨ψ|, we have
⟨Uψ| = (|Uψ⟩)† = (U|ψ⟩)† = |ψ⟩†U† = ⟨ψ|U†.
To summarize,
132
3 Linear Algebra
|Uψ⟩= U|ψ⟩,
⟨Uψ| = ⟨ψ|U†.
(3.1)
Using this, we can come up with an easy way to determine whether a matrix
keeps the total probability equal to 1. Consider a quantum gate (matrix) U. If it acts
on |ψ⟩, we have
U|ψ⟩= |Uψ⟩.
For U to be a quantum gate, this must be normalized. That is, the inner product of
|Uψ⟩with itself must equal 1:
⟨Uψ|Uψ⟩= 1
⟨ψ|U†U|ψ⟩= ⟨ψ|ψ⟩
U†U = I.
A matrix that satisfies this property U†U = I (and UU† = I) is called unitary. Thus,
Quantum gates are unitary matrices, and unitary matrices are quantum gates.
This is why we typically use U to denote a quantum gate. It stands for unitary.
As an example application of this, is the following matrix a quantum gate?
U = 1
√
2

1 i
−i 1

We can just check whether it is unitary, so whether U†U = I or not.
U†U = 1
√
2

1 i
−i 1
 1
√
2

1 i
−i 1

=

1 i
−i 1

̸= I.
So no, it is not a quantum gate.
Exercise 3.17. Is
U = 1
√
2

1
i
i −1

a quantum gate? If so, what is U|0⟩, and what is U|1⟩?
Exercise 3.18. Is
U = 1
√
2

1 1
i −i

a quantum gate? If so, what is U|0⟩, and what is U|1⟩?
3.3.6 Reversibility
A matrix M is reversible or invertible if there exists a matrix denoted M−1 such that
3.4 Outer Products
133
M−1M = MM−1 = I.
So, if we multiply a vector by both a matrix and its inverse, nothing happens to the
vector because this is equivalent to multiplying it by the identity matrix.
Now, since a quantum gate U must be unitary, it satisfies
U†U = UU† = I.
Then, the inverse of U is simply U†, i.e., U−1 = U†. So, a quantum gate is always
reversible, and its inverse is its conjugate transpose:
A quantum gate U is always reversible, and its inverse is U†.
If we have a qubit and we applied a quantum gate U, we can undo the gate by
applying U†:
U†U|ψ⟩= I|ψ⟩= |ψ⟩.
Exercise 3.19. Consider the following quantum gate, written as a 2×2 matrix:
U =
 1+
√
3
2
√
2 +i 1−
√
3
2
√
6
1−
√
3
2
√
6 +i 1−
√
3
2
√
6
−1+
√
3
2
√
6
+i 1−
√
3
2
√
6
1+
√
3
2
√
2 +i −1+
√
3
2
√
6
!
(a) What is the inverse of U, written as a 2×2 matrix?
(b) A qubit is in the state
|ψ⟩=
√
3
2 |0⟩+ 1
2|1⟩.
What is U†U|ψ⟩? Hint: You can answer this without any messy calculations.
3.4 Outer Products
3.4.1 Outer Products Are Matrices
Consider two states
|ψ⟩= α|0⟩+β|1⟩,
|φ⟩= γ|0⟩+δ|1⟩.
Instead of multiplying |ψ⟩and |φ⟩as an inner product ⟨ψ|φ⟩, where the bra is on
the left and the ket is on the right, another way to multiply them is by having the ket
on the left and the bra on the right, which is called an outer product:
|ψ⟩⟨φ| =

α
β
 γ∗δ ∗
.
134
3 Linear Algebra
To multiply these vectors according to the rules of linear algebra, we multiply each
row of |ψ⟩by each column of |φ⟩, resulting in
|ψ⟩⟨φ| =

α
β

γ∗

α
β

δ ∗

=

αγ∗
βγ∗
 
αδ ∗
βδ ∗

=

αγ∗αδ ∗
βγ∗βδ ∗

.
The result is a 2×2 matrix. So, whereas inner products result in scalars, outer prod-
ucts result in matrices, and we can add outer products together to construct various
quantum gates.
For example, consider
U = |1⟩⟨0|+|0⟩⟨1|.
Let us find how this acts on |ψ⟩= α|0⟩+β|1⟩and show that it is a valid quantum
gate.
U|ψ⟩= (|1⟩⟨0|+|0⟩⟨1|)(α|0⟩+β|1⟩)
= α|1⟩⟨0|0⟩
|{z}
1
+β|1⟩⟨0|1⟩
|{z}
0
+α|0⟩⟨1|0⟩
|{z}
0
+β|0⟩⟨1|1⟩
|{z}
1
= α|1⟩+β|0⟩
= β|0⟩+α|1⟩.
The total probability of this is |β|2 + |α|2 = 1, so this is a valid quantum gate.
Applying U swapped |0⟩and |1⟩, so it is just the X gate. As another approach, we
can find U as a matrix:
U = |1⟩⟨0|+|0⟩⟨1| =

0
1
 1 0

+

1
0
 0 1

=

0 0
1 0

+

0 1
0 0

=

0 1
1 0

.
This is precisely the matrix for the X gate. To confirm that it is a valid quantum gate,
we simply show that it is unitary, i.e., if U†U = I:
U†U =

0 1
1 0

0 1
1 0

=

1 0
0 1

= I.
The outer product of |φ⟩and |ψ⟩is just the conjugate transpose of the outer
product of |ψ⟩and |φ⟩:
|φ⟩⟨ψ| = |ψ⟩⟨φ|†.
We can prove this through a simple calculation:
|φ⟩⟨ψ| =

γ
δ
 α∗β ∗
=

γα∗γβ ∗
δα∗δβ ∗

=

αγ∗αδ ∗
βγ∗βδ ∗
†
= |ψ⟩⟨φ|†.
In the above equtaion, one may use parenthesis to clarify that the entire outer product
is conjugated and transposed, not just the bra, i.e., |φ⟩⟨ψ| = (|ψ⟩⟨φ|)†.
Exercise 3.20. Consider the following outer product
3.4 Outer Products
135
|i⟩⟨−|.
(a) What is it as a matrix?
(b) Is this a valid quantum gate?
Exercise 3.21. Consider the following sum of outer products:
1
√
2
|0⟩⟨0|+ 1
√
2
|0⟩⟨1|+ 1
√
2
|1⟩⟨0|−1
√
2
|1⟩⟨1|.
(a) What is it as a matrix?
(b) Is this a valid quantum gate?
3.4.2 Completeness Relation
Recall from Section 3.2.3 that for any orthonormal basis {|a⟩,|b⟩}, the state of a
qubit can be written as
|ψ⟩= α|a⟩+β|b⟩,
where α = ⟨a|ψ⟩and β = ⟨b|ψ⟩. Substituting these values,
|ψ⟩= ⟨a|ψ⟩
| {z }
scalar
|a⟩+⟨b|ψ⟩
| {z }
scalar
|b⟩.
As indicated above, the inner products are just scalars/numbers, so instead of multi-
ply them onto the vectors |a⟩and |b⟩on the left, we can equivalently multiply them
on the right:
|ψ⟩= |a⟩⟨a|ψ⟩
| {z }
scalar
+|b⟩⟨b|ψ⟩
| {z }
scalar
.
Both of these terms are a ket times a bra times a ket. To make this more clear, we
can write them as
|ψ⟩= |a⟩⟨a||ψ⟩+|b⟩⟨b||ψ⟩.
Now, notice we have two outer products, |a⟩⟨a| and |b⟩⟨b|. Since they are both
multiplying |ψ⟩, we can factor to get
|ψ⟩=
 |a⟩⟨a|+|b⟩⟨b|

|ψ⟩.
For this to be true for all |ψ⟩, we must have
|a⟩⟨a|+|b⟩⟨b| = I.
This is called the completeness relation, and it indicates the state of any qubit can be
expressed in terms of |a⟩and |b⟩, a property we call completeness. We say {|a⟩,|b⟩}
forms a complete orthonormal basis. All the bases we have discussed (any two states
on opposite sides on the Bloch sphere) are complete.
Let us box this:
136
3 Linear Algebra
A complete orthonormal basis {|a⟩,|b⟩} satisfies the completeness relation
|a⟩⟨a|+|b⟩⟨b| = I.
Exercise 3.22. Verify that {|+⟩,|−⟩} is a complete orthonormal basis by showing that
|+⟩⟨+|+|−⟩⟨−| = I.
Exercise 3.23. Verify that {|0⟩,|+⟩} is a not a complete orthonormal basis by showing that
|0⟩⟨0|+|+⟩⟨+| ̸= I.
3.5 Summary
The mathematical language of quantum computing is linear algebra. Quantum states
are represented by column vectors called kets, and the conjugate transpose of a ket
is a bra. Multiplying a bra and a ket is an inner product that yields the projection or
amplitude of the states onto each other. A state whose inner product with itself is
1 is normalized, and states with zero inner product are orthogonal. Quantum gates
are unitary matrices, which satisfy U†U = I. Unitary matrices are always reversible
with U−1 = U†. Multiplying a ket and a bra is an outer product, which is a matrix.
A complete orthonormal basis satisfies the completeness relation, meaning the sum
of the outer products of each basis vector with itself equals the identity matrix.
Chapter 4
Multiple Quantum Bits
In Chapter 2, we explored the qubit, what happens when it is measured, and how
quantum gates act on it. In Chapter 3, we upgraded our tools for working with a
qubit by introducing linear algebra. Now, we are positioned to explore systems con-
sisting of multiple qubits. Sometimes, these qubits are disjoint, but other times, the
qubits are intertwined together. We will learn how quantum gates acting on multi-
qubit systems can be used to perform computations, namely adding binary numbers.
This same addition problem was explored in Chapter 1 using classical computers,
providing a comparison. Then, we will explore sets of universal quantum gates and
how to correct for errors in quantum computers.
4.1 Entanglion: A Quantum Computing Board Game
4.1.1 Mechanics
IBM Research released an open-source board game called Entanglion to teach the
fundamental ideas and mechanics of quantum computing. It is available at https:
//entanglion.github.io, and anyone can download and print the game board
and pieces. The complete rules are available on the website, but let us summarize
the most important parts here, since they reflect the rules of quantum computing.
Entanglion is a two-player collaborative game, and the goal is to collect, as
a team, eight components to build a quantum computer that are scattered across
different planets, while avoiding detection by the planetary defenses. There are
three galaxies in the Entanglion universe: Centarious, Superious, and Entanglion,
as shown in Fig. 4.1. Centarious has two planets, Zero and One, and Superious also
has two planets, Plus and Minus. On the other hand, Entanglion has eight planets,
each holding one of the components to build a quantum computer.
Each player has one spaceship, and one is red while the other is blue. Players
determine the starting locations of their spaceships by rolling a die that only has 0
137
138
4 Multiple Quantum Bits
Fig. 4.1: The universe for the Entanglion board game. It consists of three galaxies:
Centarious, Superious, and Entanglion. The two players’ spaceships (red and blue)
move across the board from planet to planet according to the labeled paths. Dashed
paths correspond to the red player, and solid paths correspond to the blue player.
Inside the Entanglion galaxy, both spaceships move together.
and 1 as the outcomes. This is called the Centarious die because the outcomes of 0
and 1 correspond to the planets Zero and One, both in the galaxy Centarious.
The players take turns moving their spaceships to different planets by playing en-
gine cards H, X, CNOT, and SWAP. As shown in Fig. 4.1, different engine cards are
used for transitions between different planets. In Centarious and Superious, play-
ers’ spaceships can be on different planets. To move into the Entanglion galaxy, one
player must be in Centarious, and the other player must be in Superious. Then the
player in Centarious uses a CNOT engine card, and both spaceships move to the
same planet in the Entanglion galaxy. This planet is where a horizontal line from
4.1 Entanglion: A Quantum Computing Board Game
139
the Centarious planet intersects with the vertical line from the Superious planet, as
shown by the lines ▷pppppppppp ◁in Fig. 4.1. For example, if the red player is at One,
and the blue player is at Plus, and the red player uses a CNOT engine card, then
the red player moves horizontally from One to Psi Plus, and the blue player moves
vertically from Plus to Psi Plus. Inside the Entanglion galaxy, the spaceships always
move together as a pair, so they are always at the same planet.
Anytime the spaceships move to a planet in the Entanglion galaxy, or when a
player attempts to retrieve a component to build a quantum computer, there is a
chance they will be detected by the planetary defenses. The roll of an eight-sized die
determines this, and if the spaceships are detected, both of them move to a random
planet in Centarious determining by rolling the Centarious die.
The game also contains a shuffled deck of event cards, which are played when-
ever the spaceships are detected by planetary defenses, or after six engine cards
have been played. The cards are named after important scientists who contributed to
quantum physics and quantum computing, or after quantum effects. The mechanics
of these cards do not precisely correlate with actual quantum computing, so we limit
our discussion of them here.
Exercise 4.1. Refer to the Entanglion game board in Fig. 4.1.
(a) When can a player use CNOT to move between planets Zero and One?
(b) If the red player is at planet Zero and the blue player is at planet Minus, and the red player
uses a CNOT engine card, where do the players move?
(c) How can the players move between planets Psi Plus and Omega Three?
4.1.2 Connection to Quantum Computing
The rules of Entanglion reflect how quantum computers work. We will explore these
connections in detail throughout this chapter, but here is a quick summary:
• The red and blue spaceships are qubits.
• The planets are various states that qubits can be in. Centarious contains the
classical states |0⟩and |1⟩, Superious contains the two superposition states |+⟩
and |−⟩, and Entanglion contains eight entangled states, where the states of the
two qubits are intertwined, so the spaceships move together.
• The engine cards H, X, CNOT, and SWAP are quantum gates that are applied to
the qubits. This transforms the qubits to different states, or moves the spaceships
to different planets.
• Detection by planetary defenses corresponds to a measurement. Measuring a
qubit yields a classical 0 or 1 with some probability, so the spaceships move to
planet Zero or One according to a roll of the Centarious die.
Exercise 4.2. Read https://medium.com/qiskit/designing-a-quantum-computi
ng-board-game-de4a450cad8c and answer the following questions:
(a) How many major iterations of the board game were there?
(b) The emphasis of the game was on
mastery.
140
4 Multiple Quantum Bits
(c) What win rates for AI players corresponded to an adequate level of challenge for human
players?
(d) Entanglion is a play on what word?
4.2 States and Measurement
4.2.1 Tensor Product
When we have multiple qubits, we write their states as a tensor product ⊗. For
example, two qubits, both in the |0⟩state, are written
|0⟩⊗|0⟩,
and this is pronounced “zero tensor zero.” Often, we compress the notation and leave
out the tensor product in both writing and speech:
|0⟩|0⟩.
We frequently compress the notation further still:
|00⟩.
With two qubits, the Z-basis is {|00⟩,|01⟩,|10⟩,|11⟩}. A general state is a super-
position of these basis states:
c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩.
If we measure these two qubits in the Z-basis, we get |00⟩with probability |c0|2,
|01⟩with probability |c1|2, |10⟩with probability |c2|2, or |11⟩with probability |c3|2.
Thus, the total probability is |c0|2 +|c1|2 +|c2|2 +|c3|2, and it should equal 1.
With three qubits, there are eight Z-basis states |000⟩, |001⟩, |010⟩, |011⟩, |100⟩,
|101⟩, |110⟩, and |111⟩. Sometimes, these binary strings are written as decimal num-
bers |0⟩,|1⟩,...,|7⟩. Inspired by this, let us call the right qubit the zeroth qubit, the
middle qubit the first qubit, and the left qubit the second qubit, so a Z-basis state
takes the form
|b2b1b0⟩.
Then, the decimal representation of this is
22b2 +21b1 +20b0.
In other words, we label qubits right-to-left, starting with zero. This convention,
where the rightmost qubit is the zeroth qubit, is called little endian. Quirk and many
quantum programming languages, including those in Chapter 5, also use little en-
dian. In contrast, the opposite convention, where the leftmost qubit is the zeroth
4.2 States and Measurement
141
qubit, is called big endian. Of note, Nielsen and Chuang’s standard advanced text-
book uses the big endian convention. Disputes over which convention is “better”
has raged classical computing for decades, and the same debates carry into quantum
computing. The reality is that you should be able to use both, but for consistency,
we use little endian throughout this textbook. Next, the general state of three qubits
is a superposition of these basis vectors:
7
∑
j=0
cj|j⟩= c0|0⟩+c1|1⟩+···+c7|7⟩,
and the probability of getting |j⟩when measuring in the Z-basis is |cj|2, so ∑j |cj|2 =
1.
With n qubits, there are N = 2n Z-basis states, which we can label as n-bit strings
or by the decimal numbers 0 through N −1. As an n-bit string,
|bN−1 ...b1b0⟩=
2N−1bN−1 +···+21b1 +20b0

.
Of course, the general state of n-qubits is a superposition of these Z-basis states:
N−1
∑
j=0
cj|j⟩= c0|0⟩+c1|1⟩+···+cN−1|N −1⟩.
This has N amplitudes c0 through cN−1. Thus, if we have just n = 300 qubits, then
we must keep track of N = 2300 ≈2.04 × 1090 amplitudes, which is more than the
number of atoms in the visible universe (1078 to 1082). This is evidence, but not a
proof, that it is difficult for classical computers to simulate quantum computers. It
is evidence because classical computers cannot keep track of this many amplitudes,
but it is not a proof because it is unknown whether quantum computers need all these
amplitudes. That is, if quantum computers can function with much fewer amplitudes
(a polynomial number instead of an exponential number in n), a classical computer
would be able to keep track of all of them. In terms of complexity classes, the
exponential number of amplitudes in a general entangled state is evidence that P ̸=
BQP.
We can also use powers to simplify the notation. If we have n qubits, each in the
state |0⟩, we can write the state as
|0⟩⊗n = |0⟩⊗|0⟩⊗···⊗|0⟩
|
{z
}
n
= |0⟩|0⟩...|0⟩
|
{z
}
n
= |00...0
| {z }
n
⟩= |0n⟩.
With a single qubit, we could parameterize a state as
cos θ
2 |0⟩+eiφ sin θ
2 |1⟩,
with the coordinates (θ,φ) interpreted as a point on the Bloch sphere. With two
qubits, however, we have four complex amplitudes c0, c1, c2, c3 (although one can
142
4 Multiple Quantum Bits
be made real by factoring out an global phase), and unfortunately, this is too many
parameters to represent in three-dimensions. There is no Bloch sphere representa-
tion for a general multi-qubit state.
The tensor product also works for bras, so
⟨0|⊗⟨0| = ⟨0|⟨0| = ⟨00|.
Then, the inner product of, say ⟨01| and |00⟩, is obtained by matching up qubits. For
example,
⟨01|00⟩= ⟨0|0⟩
|{z}
1
·⟨1|0⟩
|{z}
0
= 0.
So |01⟩and |00⟩are orthogonal.
Exercise 4.3. Calculate the following inner products:
(a) ⟨10|11⟩.
(b) ⟨+−|01⟩.
(c) ⟨1+0|1−0⟩.
4.2.2 Kronecker Product
In linear algebra, the tensor product is simply the Kronecker product, which is ob-
tained by multiplying each term of the first matrix/vector by the entire second ma-
trix/vector. For example, with two qubits,
|00⟩= |0⟩|0⟩= |0⟩⊗|0⟩=

1
0

⊗

1
0

=




1

1
0

0

1
0




=




1
0
0
0



.
|01⟩= |0⟩|1⟩= |0⟩⊗|1⟩=

1
0

⊗

0
1

=




1

0
1

0

0
1




=




0
1
0
0



.
|10⟩= |1⟩|0⟩= |1⟩⊗|0⟩=

0
1

⊗

1
0

=




0

1
0

1

1
0




=




0
0
1
0



.
|11⟩= |1⟩|1⟩= |1⟩⊗|1⟩=

0
1

⊗

0
1

=




0

0
1

1

0
1




=




0
0
0
1



.
Then,
4.2 States and Measurement
143
c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩=




c0
c1
c2
c3



.
Similarly, with three qubits, its state can be written as a column vector with eight
elements:
7
∑
j=0
cj|j⟩= c0|0⟩+c1|1⟩+···+c7|7⟩=





c0
c1
...
c7




.
With n qubits, the vector has N = 2n elements:
|ψ⟩=
N−1
∑
j=0
cj|j⟩= c0|0⟩+c1|1⟩+···+cN−1|N −1⟩=





c0
c1
...
cN−1




.
With bras, the Kronecker product is still the tensor product. For example,
⟨00| = ⟨0|⊗⟨0| =
 1 0

⊗
 1 0

=
 1
 1 0

0
 1 0

=
 1 0 0 0

.
So, a general quantum state of n-qubits, written as a bra, is
⟨ψ| =
N−1
∑
j=0
c∗
j⟨j| = c∗
0⟨0|+c∗
1⟨1|+···+c∗
N−1⟨N −1| =
 c∗
0 c∗
1 ··· c∗
N−1

.
Exercise 4.4. Verify that
|1⟩⊗|1⟩⊗|0⟩=











0
0
0
0
0
0
1
0











.
Exercise 4.5. Consider a two-qubit state
|ψ⟩= 1
2|00⟩+
i
√
2
|10⟩+
√
3+i
4
|11⟩.
(a) What is |ψ⟩as a (column) vector?
(a) What is ⟨ψ| as a (row) vector?
Exercise 4.6. Show that {|00⟩,|01⟩,|10⟩,|11⟩} is a complete orthonormal basis for the state of two
qubits by showing that it satisfies the completeness relation
|00⟩⟨00|+|01⟩⟨01|+|10⟩⟨10|+|11⟩⟨11| = I,
144
4 Multiple Quantum Bits
where I is the 4×4 identity matrix:
I =




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1



.
4.2.3 Measuring Individual Qubits
Say we have two qubits in the state
1
√
2
|00⟩+ 1
2|01⟩+
√
3
4 |10⟩+ 1
4|11⟩.
If we measure both qubits, we would get |00⟩with probability 1/2, |01⟩with prob-
ability 1/4, |10⟩with probability 3/16, or |11⟩with probability 1/16.
Now, instead of measuring both qubits, let us only measure the left qubit. This
yields |0⟩or |1⟩with some probabilities, and the state collapses to some state, so the
outcomes are
|0⟩with some probability, and the state collapses to something,
|1⟩with some probability, and the state collapses to something.
The probability of getting |0⟩when measuring the left qubit is given by the sum of
the norm-squares of the amplitudes of |00⟩and |01⟩, since those both have the left
qubit as |0⟩. That is, the probability of getting |0⟩is

1
√
2

2
+

1
2

2
= 3
4.
Similarly, if the outcome is |1⟩, then from the |10⟩and |11⟩states, the probability is

√
3
4

2
+

1
4

2
= 1
4.
Then, the results of the measurement are:
|0⟩with probability 3
4, and the state collapses to something,
|1⟩with probability 1
4, and the state collapses to something.
Now for the states after measurement, if the outcome is |0⟩, then the state collapses
to the parts where the left qubit is |0⟩, so it becomes
4.2 States and Measurement
145
A
 1
√
2
|00⟩+ 1
2|01⟩

,
where A is a normalization constant. Similarly, if the outcome is |1⟩, then the state
collapses to the terms where the left qubit is |1⟩, so it becomes
B
 √
3
4 |10⟩+ 1
4|11⟩
!
.
where B is a normalization constants. Normalizing these, we get A = 2/
√
3 and
B = 2, so measuring the left qubit yields
|0⟩with probability 3
4, and the state collapses to
r
2
3|00⟩+ 1
√
3
|01⟩,
|1⟩with probability 1
4, and the state collapses to
√
3
2 |10⟩+ 1
2|11⟩.
We can apply these ideas to any number of qubits. For example, if we have three
qubits in the state
c0|000⟩+c1|001⟩+c2|010⟩+c3|011⟩+c4|100⟩+c5|101⟩+c6|110⟩+c7|111⟩,
and we measure the left and middle qubits, the possible outcomes are
|00⟩with probability |c0|2 +|c1|2, collapses to c0|000⟩+c1|001⟩
p
|c0|2 +|c1|2
,
|01⟩with probability |c2|2 +|c3|2, collapses to c2|010⟩+c3|011⟩
p
|c2|2 +|c3|2
,
|10⟩with probability |c4|2 +|c5|2, collapses to c4|100⟩+c5|101⟩
p
|c4|2 +|c5|2
,
|11⟩with probability |c6|2 +|c7|2, collapses to c6|110⟩+c7|111⟩
p
|c6|2 +|c7|2
.
Exercise 4.7. Two qubits are in the state
i
√
10
|00⟩+ 1−2i
√
10
|01⟩+ eiπ/100
√
10
|10⟩+
√
3
√
10
|11⟩.
If we measure the qubits in the Z-basis {|00⟩,|01⟩,|10⟩,|11⟩}, what are the possible outcomes and
with what probabilities?
Exercise 4.8. Normalize the following quantum state:
A
1
2|00⟩+i|01⟩+
√
2|10⟩−|11⟩

.
146
4 Multiple Quantum Bits
4.2.4 Sequential Single-Qubit Measurements
We have answered the question of what happens when we measure just a single qubit
or a subset of qubits. Now, let us take this a step further and consider what happens
if we measure the qubits, one after another. For example, in the last section, we
started with two qubits in the state
1
√
2
|00⟩+ 1
2|01⟩+
√
3
4 |10⟩+ 1
4|11⟩.
If we first measure the left qubit, we get
|0⟩with probability 3
4, and the state collapses to
r
2
3|00⟩+ 1
√
3
|01⟩,
|1⟩with probability 1
4, and the state collapses to
√
3
2 |10⟩+ 1
2|11⟩.
Now if we measure the right qubit after this, the possible outcomes for the sequence
of measurements are |00⟩, |01⟩, |10⟩, and |11⟩. The probability of getting |00⟩is the
probability of first getting |0⟩for the left qubit, which was 3/4, times the probability
of getting |0⟩for the right qubit, which is 2/3 because the state collapsed after the
first measurement. Multiplying these, the probability of getting |00⟩is (3/4)(2/3) =
2/4 = 1/2. We can perform this calculation for every possible outcome:
Prob(|00⟩) = Prob(first left |0⟩)Prob(then right |0⟩) = 3
4
2
3 = 1
2,
Prob(|01⟩) = Prob(first left |0⟩)Prob(then right |1⟩) = 3
4
1
3 = 1
4,
Prob(|10⟩) = Prob(first left |1⟩)Prob(then right |0⟩) = 1
4
3
4 = 3
16,
Prob(|11⟩) = Prob(first left |1⟩)Prob(then right |1⟩) = 1
4
1
4 = 1
16.
Notice these outcomes and probabilities are exactly the same as if we had measured
both qubits at the same time, as they should be. Measuring both qubits is the same
as measuring one after another, assuming the state was not modified between the
two measurements.
Exercise 4.9. Consider the two-qubit state
1
4|00⟩+ 1
2|01⟩+ 1
√
2
|10⟩+
√
3
4 |11⟩.
If you measure only the left qubit, what are the resulting states, and with what probabilities?
Exercise 4.10. Consider the three-qubit state
1
6|000⟩+
1
3
√
2
|001⟩+ 1
√
6
|010⟩+ 1
2|011⟩+ 1
6|100⟩+ 1
3|101⟩+ 1
6|110⟩+ 1
√
3
|111⟩.
4.3 Entanglement
147
If you measure only the left and right qubits, but not the middle qubit, what are the resulting states,
and with what probabilities?
4.3 Entanglement
4.3.1 Product States
Some quantum states can be factored into (the tensor product of) individual qubit
states. For example,
1
2 (|00⟩−|01⟩+|10⟩−|11⟩) = 1
√
2
(|0⟩+|1⟩)
|
{z
}
|+⟩
⊗1
√
2
(|0⟩−|1⟩)
|
{z
}
|−⟩
= |+⟩⊗|−⟩
= |+⟩|−⟩.
To confirm this to yourself, work it out in reverse order by multiplying out the states
and showing that you get the original state. Such factorizable states are called prod-
uct states or simply separable states. Each single-qubit state can be visualized on
the Bloch sphere, so |+⟩|−⟩would be two Bloch spheres, with the first at the x-axis,
and the other at the −x-axis:
x
y
z
x
y
z
Let us work through an example of how to factor a state. Say two qubits are in
the state
1
2
√
2
√
3|00⟩−
√
3|01⟩+|10⟩−|11⟩

.
We want to write this as the product of two single-qubit states,
|ψ1⟩|ψ0⟩,
where
|ψ1⟩= α1|0⟩+β1|1⟩,
|ψ0⟩= α0|0⟩+β0|1⟩.
Then,
148
4 Multiple Quantum Bits
|ψ1⟩|ψ0⟩= (α1|0⟩+β1|1⟩)(α0|0⟩+β0|1⟩)
= α1α0|00⟩+α1β0|01⟩+β1α0|10⟩+β1β0|11⟩.
Matching up the coefficients with our original state,
α1α0 =
√
3
2
√
2
,
α1β0 = −
√
3
2
√
2
,
β1α0 =
1
2
√
2
,
β1β0 = −1
2
√
2
.
Using these equations, let us solve for the variables in terms of one of them. Starting
with the first equation, we can solve for α1 in terms of α0:
α1 =
√
3
2
√
2α0
.
Plugging this into the second equation, we can solve for β0 in terms of α0:
β0 = −α0.
For the third equation, we can solve for β1 in terms of α0:
β1 =
1
2
√
2α0
.
Finally, plugging in β1 = 1/2
√
2α0 and β0 = −α0 into the fourth equation, we get
−1
2
√
2
= −1
2
√
2
,
which is a true statement, so it is satisfied, although it does not tell us anything new.
So, we have solved for α1, β1, and β0 in terms of α0, and this is actually sufficient.
Plugging into the product state,
|ψ1⟩|ψ0⟩= (α1|0⟩+β1|1⟩)(α0|0⟩+β0|1⟩)
=
 
√
3
2
√
2α0
|0⟩+
1
2
√
2
1
α0
|1⟩
!
(α0|0⟩−α0|1⟩).
We see that α0 cancels, yielding
|ψ1⟩|ψ0⟩=
 √
3
2
√
2
|0⟩+
1
2
√
2
|1⟩
!
(|0⟩−|1⟩).
Moving the factor of 1/
√
2 to the right qubit so that both qubits are normalized,
|ψ1⟩|ψ0⟩=
 √
3
2 |0⟩+ 1
2|1⟩
! 1
√
2
|0⟩−1
√
2
|1⟩

.
4.3 Entanglement
149
Thus, the left qubit is in the state
√
3
2 |0⟩+ 1
2|1⟩, and the right qubit is in the state |−⟩.
In general, a product state of n qubits can be written
(αn−1|0⟩+βn−1|1⟩)⊗···⊗(α1|0⟩+β1|1⟩)⊗(α0|0⟩+β0|1⟩).
This only has 2n amplitudes, so a classical computer can efficiently store the ampli-
tudes of product states. If quantum computers only used product states, they would
be efficiently simulated by classical computers.
4.3.2 Entangled States
There exist quantum states that cannot be factored into product states. These are
called entangled states. For example, with two qubits,
Φ+
= 1
√
2
(|00⟩+|11⟩)
cannot be written as |ψ1⟩|ψ0⟩. As a proof, let us try writing it as a product state
using the procedure from the last section:
|ψ1⟩|ψ0⟩= (α1|0⟩+β1|1⟩)(α0|0⟩+β0|1⟩)
= α1α0|00⟩+α1β0|01⟩+β1α0|10⟩+β1β0|11⟩.
Matching the coefficients, we get
α1α0 = 1
√
2
,
α1β0 = 0,
β1α0 = 0,
β1β0 = 1
√
2
.
The second equation requires α1 = 0 or β0 = 0. If α1 = 0, then the first equation
gives 0 = 1/
√
2, which is false. If β0 = 0, then the fourth equation gives 0 = 1/
√
2.
Thus, there is no solution to these four equations, so |Φ+⟩cannot be written as a
product state. It is an entangled state. This property that the state of the qubits are
intertwined is called entanglement.
Since an entangled state cannot be factored, a general entangled state of n qubits
would have N = 2n amplitudes c0 through cN−1:
|ψ⟩=
N−1
∑
j=0
cj|j⟩= c0|0⟩+c1|1⟩+···+cN−1|N −1⟩=





c0
c1
...
cN−1




.
In the Entanglion board game, the planets within the Entanglion galaxy corre-
spond to two-qubit states that are entangled. Planet Phi Plus is precisely |Φ+⟩.
We will discuss entanglement in more detail in Chapter 6.
150
4 Multiple Quantum Bits
Exercise 4.11. Are each of the following states a product state or entangled state? If it is a product
state, give the factorization.
(a)
1
√
2
(|01⟩+|10⟩).
(b)
1
√
2
(|10⟩+i|11⟩).
Exercise 4.12. Are each of the following states a product state or entangled state? If it is a product
state, give the factorization.
(a) 1
4

3|00⟩−
√
3|01⟩+
√
3|10⟩−|11⟩

.
(b)
1
√
3
|0⟩|+⟩+
r
2
3|1⟩|−⟩.
4.4 Quantum Gates
4.4.1 One-Qubit Quantum Gates
Say we have multiple qubits, and we want to apply a single-qubit gate (like I, X,
Y, Z, S, T, or H) to just a single qubit. For example, say we have two qubits in the
|00⟩= |0⟩⊗|0⟩state, and we want to apply the Hadamard gate to the left qubit,
but leave the right qubit alone (i.e., apply the identity gate to it). We write the gates
using a tensor product, so we write
(H ⊗I)(|0⟩⊗|0⟩) = H|0⟩⊗I|0⟩
= |+⟩⊗|0⟩
= 1
√
2
(|0⟩+|1⟩)⊗|0⟩
= 1
√
2
(|0⟩⊗|0⟩+|1⟩⊗|0⟩).
Compressing the notation and also writing the result as a column vector,
(H ⊗I)|00⟩= 1
√
2
(|00⟩+|10⟩) = 1
√
2




1
0
1
0



.
To draw as a quantum circuit, we use the convention that the rightmost qubit corre-
sponds to the top row of the quantum circuit, and the leftmost qubit corresponds to
the bottom row of the quantum circuit:
|0⟩
I
|0⟩
H
or
|0⟩
|0⟩
H
4.4 Quantum Gates
151
We follow this convention so that it matches Quirk, and in Chapter 5 the IBM Quan-
tum Composer. Nielsen and Chuang follows the opposite convention, where the
leftmost qubit corresponds to the top row of the quantum circuit.
We can find H ⊗I as a matrix a couple different ways. First, we can find how
H ⊗I acts on each of the basis states |00⟩, |01⟩, |10⟩, |11⟩. We already found how it
acts on |00⟩above. Continuing with the rest,
(H ⊗I)|01⟩= 1
√
2
(|01⟩+|11⟩) = 1
√
2




0
1
0
1



,
(H ⊗I)|10⟩= 1
√
2
(|00⟩−|10⟩) = 1
√
2




1
0
−1
0



,
(H ⊗I)|11⟩= 1
√
2
(|01⟩−|11⟩) = 1
√
2




0
1
0
−1



.
As in Section 3.3.1, we can write H ⊗I as a matrix by combining the column vectors
for (H ⊗I)|00⟩,...,(H ⊗I)|11⟩as a 4×4 grid:
H ⊗I = 1
√
2




1 0 1
0
0 1 0
1
1 0 −1 0
0 1 0 −1



.
The second way to find this matrix is by taking the Kronecker product of H and I:
H ⊗I = 1
√
2

1 1
1 −1

⊗

1 0
0 1

= 1
√
2




1·

1 0
0 1

1·

1 0
0 1

1·

1 0
0 1

−1·

1 0
0 1





= 1
√
2




1 0 1
0
0 1 0
1
1 0 −1 0
0 1 0 −1



.
This matches what we previously obtained. We can also find the Kronecker product
using Mathematica or SageMath:
• In Mathematica,
H=1/Sqrt[2]*{{1,1},{1,-1}};
eye={{1,0},{0,1}};
KroneckerProduct[H,eye]
152
4 Multiple Quantum Bits
• In SageMath,
sage: H = 1/sqrt(2) * Matrix([[1,1],[1,-1]])
sage: eye = Matrix([[1,0],[0,1]])
sage: H.tensor_product(eye)
As another example, to act on the left qubit with H and the right qubit with X,
we would write H ⊗X, so
(H ⊗X)|0⟩|0⟩= |+⟩|1⟩= 1
√
2
(|01⟩+|11⟩).
As a quantum circuit, we would draw this as
|0⟩
X
|0⟩
H
Simulating this in Quirk, we get
This is consistent with the state |+⟩|1⟩. Since the right/top qubit is |1⟩, Quirk cor-
rectly shows that the probability of getting |1⟩when measuring it is 100% (On), and
it correctly draws the state at the south pole of the Bloch sphere. Similarly, the left-
/bottom qubit is |+⟩, and Quirk correctly shows that the probability of measuring it
to be |1⟩is 50%, and it correctly draws the state at the x-axis of the Bloch sphere. In
additional, Quirk also depicts the amplitudes on the real-imaginary plane, labeled
“Final amplitudes.” There are four boxes, and the top-left box depicts the amplitude
of |00⟩, which is zero, and the top-right box depicts the amplitude of |01⟩, which is
1/
√
2. Since this is real, it corresponds to a vector pointing along the real axis of the
real-imaginary plane. The background is also half filled, indicating a probability of
|1/
√
2|2 = 1/2. Mousing over, we get
and the amplitude is also explicitly given as 0.70711 = 1/
√
2, which has a phase
or angle of 0◦on the real-imaginary plane since it is purely real, and a norm-square
magnitude of 50%. The bottom-left box depicts the amplitude of |10⟩, which is zero,
and finally the bottom-right box depicts the amplitude of |11⟩, which is 1/
√
2.
4.4 Quantum Gates
153
As a third example, if we have n qubits, and we want to apply H to all n qubits,
we can write H ⊗H ⊗···⊗H as H⊗n. For example,
H⊗n|0⟩⊗n = |+⟩⊗n.
Note one-qubit gates are unable to create entangled states because each qubit
evolves independently of the others. To create entanglement, we need quantum gates
that operate on multiple qubits at a time.
Exercise 4.13. In this problem, you will prove some of the game mechanics of Entanglion. Please
refer to Fig. 4.1 for the game board. If the players are on planet Psi Plus, and either player uses an
X engine card, they both move to planet Phi Plus, and vice versa. Similarly, if the players are on
planet Psi Minus, and either player uses an X engine card, they both move to planet Phi Minus,
and vice versa. These planets correspond to the following states:
Φ+
= 1
√
2
(|00⟩+|11⟩),
Φ−
= 1
√
2
(|00⟩−|11⟩),
Ψ +
= 1
√
2
(|01⟩+|10⟩),
Ψ −
= 1
√
2
(|01⟩−|10⟩).
(a) Show that when the X gate is applied to either qubit of |Ψ +⟩, the result is |Φ+⟩, up to a global
phase.
(b) Show that when the X gate is applied to either qubit of |Φ+⟩, the result is |Ψ +⟩, up to a global
phase.
(c) Show that when the X gate is applied to either qubit of |Ψ −⟩, the result is |Φ−⟩, up to a global
phase.
(d) Show that when the X gate is applied to either qubit of |Φ−⟩, the result is |Ψ −⟩, up to a global
phase.
Exercise 4.14. Answer the following questions.
(a) What is H ⊗X as a 4×4 matrix?
(b) Consider
|ψ⟩= 1
4|00⟩+ 1
2|01⟩+ 1
√
2
|10⟩+
√
3
4 |11⟩.
What is (H ⊗X)|ψ⟩? Hint: You may use a computer.
4.4.2 Two-Qubit Quantum Gates
Quantum gates can also operate on two qubits at the same time. Some important
examples include:
• The CNOT gate or controlled-NOT gate inverts the right qubit if the left qubit
is 1:
154
4 Multiple Quantum Bits
CNOT|00⟩= |00⟩,
CNOT|01⟩= |01⟩,
CNOT|10⟩= |11⟩,
CNOT|11⟩= |10⟩.
The left qubit is called the control qubit, and the right qubit is called the target
qubit. Note the control qubit is unchanged by CNOT, whereas the target qubit
becomes the XOR (exclusive OR) of the inputs:
CNOT|a⟩|b⟩= |a⟩|a⊕b⟩.
Thus, CNOT is a quantum XOR gate. Also, since the X gate is the NOT gate,
the CNOT gate is also called the CX gate or controlled-X gate.
In Entanglion (see Fig. 4.1), the player who uses the CNOT engine card is the
target qubit, and the other player is the control qubit. So, you can move between
planets Zero and One by playing a CNOT engine card when the other player is
at One.
Acting on a superposition,
CNOT(c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩)
= c0CNOT|00⟩+c1CNOT|01⟩+c2CNOT|10⟩+c3CNOT|11⟩
= c0|00⟩+c1|01⟩+c2|11⟩+c3|10⟩
= c0|00⟩+c1|01⟩+c3|10⟩+c2|11⟩.
So, the amplitudes of |10⟩and |11⟩are swapped.
As a matrix, the columns correspond to CNOT acting on |00⟩, |01⟩, |10⟩, and
|11⟩:
CNOT =




1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0



.
For example, acting on a general superposition,
CNOT(c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩) =




1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0








c0
c1
c2
c3



=




c0
c1
c3
c2



.
So, the amplitudes of |10⟩and |11⟩are swapped, as expected.
As a quantum circuit, CNOT spans two qubits or two lines:
•
4.4 Quantum Gates
155
The solid dot indicates control, and the ⊕denotes the target, which is the XOR
of the control and the target. Simulating this in Quirk, we drag an X gate onto the
top line and a “Control” solid dot, which is in the top Toolbox under “Probes,”
onto the bottom line:
We also clicked on the initial state of the control qubit to change it to |1⟩(alter-
natively, we could leave the initial state as |0⟩and apply X to it, resulting in |1⟩).
This triggers the CNOT, changing the target from |0⟩to |1⟩. The result is that
both qubits are “On” with 100% probability. They are both at the south poles of
their Bloch spheres, and the amplitude of |11⟩is 1.
To further clarify the control and target qubits, we may write CNOT with sub-
scripts:
CNOTi j = CNOT with qubit i as the control and qubit j as the target.
Since we label the qubits from right-to-left starting with 0, we have been using
CNOT = CNOT10.
If we instead want the control and target to be flipped, it would be CNOT01, and
we would draw the circuit as
•
To simulate this in Quirk, we just put the control on the zeroth qubit and the X
gate on the first qubit:
We set the control qubit to |1⟩, and so the CNOT gate flipped the target to |1⟩.
Another way to flip the control and target qubits is to apply Hadamard gates to
both sides of the CNOT:
H
H
=
•
H
•
H
In other words,
(H ⊗H)CNOT(H ⊗H) = CNOT01
156
4 Multiple Quantum Bits
We can prove this circuit identity using either elementary algebra or linear alge-
bra. First, using elementary algebra, the right-hand-side of equation yields the
following when applied to a superposition of the Z-basis states:
CNOT01 (c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩)
= c0|00⟩+c1|11⟩+c2|10⟩+c3|01⟩
= (c0|00⟩+c3|01⟩+c2|10⟩+c1|11⟩).
Let us show that the left-hand-side yields the same state:
c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩
H⊗H
−−−→c0|++⟩+c1|+−⟩+c2|−+⟩+c3|−−⟩
= c0
2 (|00⟩+|01⟩+|10⟩+|11⟩)+ c1
2 (|00⟩−|01⟩+|10⟩−|11⟩)
+ c2
2 (|00⟩+|01⟩−|10⟩−|11⟩)+ c3
2 (|00⟩−|01⟩−|10⟩+|11⟩)
= 1
2 (c0 +c1 +c2 +c3)|00⟩+ 1
2 (c0 −c1 +c2 −c3)|01⟩
+ 1
2 (c0 +c1 −c2 −c3)|10⟩+ 1
2 (c0 −c1 −c2 +c3)|11⟩
CNOT
−−−→1
2 (c0 +c1 +c2 +c3)|00⟩+ 1
2 (c0 −c1 +c2 −c3)|01⟩
+ 1
2 (c0 +c1 −c2 −c3)|11⟩+ 1
2 (c0 −c1 −c2 +c3)|10⟩
= 1
2 (c0 +c1 +c2 +c3)|00⟩+ 1
2 (c0 −c1 +c2 −c3)|01⟩
+ 1
2 (c0 −c1 −c2 +c3)|10⟩+ 1
2 (c0 +c1 −c2 −c3)|11⟩
H⊗H
−−−→1
4 (c0 +c1 +c2 +c3)|++⟩+ 1
4 (c0 −c1 +c2 −c3)|+−⟩
+ 1
4 (c0 −c1 −c2 +c3)|−+⟩+ 1
4 (c0 +c1 −c2 −c3)|−−⟩
= 1
4 (c0 +c1 +c2 +c3)(|00⟩+|01⟩+|10⟩+|11⟩)
+ 1
4 (c0 −c1 +c2 −c3)(|00⟩−|01⟩+|10⟩−|11⟩)
+ 1
4 (c0 −c1 −c2 +c3)(|00⟩+|01⟩−|10⟩−|11⟩)
+ 1
4 (c0 +c1 −c2 −c3)(|00⟩−|01⟩−|10⟩+|11⟩)
= c0|00⟩+c3|01⟩+c2|10⟩+c1|11⟩.
This is the same state, and so we have proved the circuit identity. It was rather
tedious, however. Proving the circuit identity using linear algebra is easier. First,
4.4 Quantum Gates
157
note that
CNOT01 =




1 0 0 0
0 0 0 1
0 0 1 0
0 1 0 0



,
since its columns show that |00⟩stays |00⟩, |01⟩becomes |11⟩, |10⟩stays |10⟩,
and |11⟩becomes |01⟩. Now, let us show that (H ⊗H)CNOT(H ⊗H) corre-
sponds to the same matrix. First,
H ⊗H = 1
√
2

1 1
1 −1

⊗1
√
2

1 1
1 −1

= 1
2




1 1
1
1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1



.
Then,
(H ⊗H)CNOT(H ⊗H)
= 1
2




1 1
1
1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1








1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0




1
2




1 1
1
1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1




= 1
4




1 1
1
1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1








1 1
1
1
1 −1 1 −1
1 −1 −1 1
1 1 −1 −1




= 1
4




4 0 0 0
0 0 0 4
0 0 4 0
0 4 0 0




=




1 0 0 0
0 0 0 1
0 0 1 0
0 1 0 0



.
This is precisely CNOT01, and so we have proved the circuit identity using
linear algebra. We also could have computed it using Mathematica or SageMath.
Simulating the identity in Quirk,
158
4 Multiple Quantum Bits
Since the top qubit is initially |1⟩, and it is now the control qubit, the bottom
qubit gets flipped to |1⟩. So, both qubits are “On.”
The CNOT gate is important because it can produce entanglement. For example,
CNOT|+⟩|0⟩= CNOT 1
√
2
(|00⟩+|10⟩) = 1
√
2
(|00⟩+|11⟩) =
Φ+
,
CNOT|−⟩|0⟩= CNOT 1
√
2
(|00⟩−|10⟩) = 1
√
2
(|00⟩−|11⟩) =
Φ−
,
CNOT|+⟩|1⟩= CNOT 1
√
2
(|01⟩+|11⟩) = 1
√
2
(|01⟩+|10⟩) =
Ψ +
,
CNOT|−⟩|1⟩= CNOT 1
√
2
(|01⟩−|11⟩) = 1
√
2
(|01⟩−|10⟩) =
Ψ −
.
In Section 4.3.2, we proved that |Φ+⟩is entangled. It can be shown that the
other three states, |Φ−⟩, |Ψ +⟩, and |Ψ −⟩, are also entangled. So, in each of the
above four calculations, we started with product states and ended up with en-
tangled states. This demonstrates that CNOT can create entanglement. The four
states, |Φ+⟩, |Φ−⟩, |Ψ +⟩, and |Ψ −⟩, are known as the Bell states or EPR states
or EPR pairs (for Einstein, Podolsky, and Rosen). They form an orthonormal
basis called the Bell basis (see Exercise 4.19), and they will be important in
Chapter 6.
In Entanglion (see Fig. 4.1), the player who uses the CNOT engine card is the
target qubit, and the other player is the control qubit. So, playing a CNOT engine
card while at planet Zero, while your teammate is at planet Plus, causes both of
you to move to planet Phi Plus. Similarly, the spaceships go from planets Zero
and Minus to Phi Minus, One and Plus to Psi Plus, and One and Minus to Psi
Minus.
Exercise 4.15. Prove the following circuit identities, such as by finding the matrix representation
of each circuit.
(a) CNOT(X ⊗I) = (X ⊗X)CNOT.
=
X
X
•
•
X
(b) CNOT(I ⊗X) = (I ⊗X)CNOT.
X
=
X
•
•
(c) CNOT(Z ⊗I) = (Z ⊗I)CNOT.
=
Z
•
•
Z
(d) CNOT(I ⊗Z) = (Z ⊗Z)CNOT.
4.4 Quantum Gates
159
Z
=
Z
•
•
Z
Exercise 4.16. Consider the following circuit, which consists of two CNOTs.
A
•
A′
B
•
B′
C
C′
(a) What is the truth table for this circuit?
(b) How does it compare to the reversible circuit for XOR in Exercise 1.43?
Exercise 4.17. Recall CNOT flips the right qubit if the left qubit is 1. The anti-controlled-NOT
gate flips the right qubit if the left qubit is 0. As a quantum circuit, the anti-control is drawn as an
open dot instead of a solid dot. Prove that it can be obtained from an ordinary CNOT by applying
an X gate to each side of the control:
=
X
•
X
Exercise 4.18. If we apply CNOT in the Z-basis {|00⟩,|01⟩,|10⟩,|11⟩}, the left qubit acts as the
control and the right qubit acts as the target. In this problem, we will prove that in the X-basis
{|++⟩,|+−⟩,|−+⟩,|−−⟩, if the right qubit is |−⟩, the left qubit gets flipped between |+⟩and |−⟩,
so the control and target are reversed. That is,
CNOT|+⟩|+⟩= |+⟩|+⟩,
CNOT|+⟩|−⟩= |−⟩|−⟩,
CNOT|−⟩|+⟩= |−⟩|+⟩,
CNOT|−⟩|−⟩= |+⟩|−⟩.
To prove these four equations, we start with the circuit identity from the main text:
(H ⊗H)CNOT(H ⊗H) = CNOT01.
Then, we multiply on the left and on the right by H ⊗H:
(H ⊗H)(H ⊗H)CNOT(H ⊗H)(H ⊗H) = (H ⊗H)CNOT01(H ⊗H).
Since H2 = I, this becomes
(I ⊗I)CNOT(I ⊗I) = (H ⊗H)CNOT01(H ⊗H).
Dropping the identity matrices,
CNOT = (H ⊗H)CNOT01(H ⊗H).
Now it is straightforward to prove how CNOT acts in the X-basis. Beginning with |++⟩,
CNOT|+⟩|+⟩= (H ⊗H)CNOT01(H ⊗H)|+⟩|+⟩
= (H ⊗H)CNOT01|0⟩|0⟩
= (H ⊗H)|0⟩|0⟩
= |+⟩|+⟩.
Work out how CNOT acts on the remaining three basis states |+−⟩, |−+⟩, and |−−⟩.
160
4 Multiple Quantum Bits
Exercise 4.19. Prove that the Bell basis satisfies the completeness relation:
Φ+
Φ++
Φ−
Φ−+
Ψ +
Ψ ++
Ψ −
Ψ − = I,
where I is the 4×4 identity matrix.
• Just like CNOT, the controlled-U gate applies some quantum gate U to the right
qubit if the left qubit is 1:
CU|00⟩= |00⟩,
CU|01⟩= |01⟩,
CU|10⟩= |1⟩⊗U|0⟩,
CU|11⟩= |1⟩⊗U|1⟩.
To get the matrix representation of CU, first say U acts on a single qubit as
U|0⟩= a|0⟩+b|1⟩,
U|1⟩= c|0⟩+d|1⟩.
So, U as a 2×2 matrix is
U =

a c
b d

.
Then,
CU|00⟩= |00⟩,
CU|01⟩= |01⟩,
CU|10⟩= |1⟩⊗(a|0⟩+b|1⟩) = a|10⟩+b|11⟩,
CU|11⟩= |1⟩⊗(c|0⟩+d|1⟩) = c|10⟩+d|11⟩.
Representing each of these as column vectors and putting them together, CU as
a 4×4 matrix is
CU =




1 0 0 0
0 1 0 0
0 0 a b
0 0 c d



.
This agrees with
CNOT = CX =




1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0



.
Some examples are controlled-Z and controlled-phase:
Z
•
S
•
4.4 Quantum Gates
161
Exercise 4.20. What is the controlled-Z gate as a matrix?
• The SWAP gate simply swaps the two qubits:
SWAP|00⟩= |00⟩,
SWAP|01⟩= |10⟩,
SWAP|10⟩= |01⟩,
SWAP|11⟩= |11⟩.
In other words,
SWAP|a⟩|b⟩= |b⟩|a⟩.
This gate cannot produce entanglement because, if the qubits are in a product
state, swapping the factors results in a product state. Acting on a superposition,
SWAP(c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩)
= c0SWAP|00⟩+c1SWAP|01⟩+c2SWAP|10⟩+c3SWAP|11⟩
= c0|00⟩+c1|10⟩+c2|01⟩+c3|11⟩
= c0|00⟩+c2|01⟩+c1|10⟩+c3|11⟩.
So, the amplitudes of |01⟩and |10⟩are swapped.
As a matrix, the columns correspond to SWAP acting on |00⟩, |01⟩, |10⟩, and
|11⟩:
SWAP =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



.
For example, acting on a general superposition,
SWAP(c0|00⟩+c1|01⟩+c2|10⟩+c3|11⟩) =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1








c0
c1
c2
c3



=




c0
c2
c1
c3



.
So, the amplitudes of |01⟩and |10⟩are swapped, as expected.
As a quantum circuit, we can draw a SWAP gate using a vertical line with ×’s
at each end, or by literally swapping the wires:
|a⟩
×
|b⟩
|b⟩
×
|a⟩
or
|a⟩
|a⟩
|b⟩
|b⟩
In Quirk, “Swap” is located in the top Toolbox under “Half Turns”:
162
4 Multiple Quantum Bits
We also included an X gate so that the top qubit is a |1⟩. This swaps with the
bottom qubit, which then swaps with the middle qubit, so the result is that the
middle qubit is |1⟩.
A SWAP gate can also be created using three CNOT gates:
× =
•
×
•
•
Or as an equation,
SWAP = (CNOT)(CNOT01)(CNOT).
As a proof, we can work through what each CNOT does and show that the result
is a SWAP:
|a⟩|b⟩CNOT
−−−→|a⟩|a⊕b⟩
CNOT01
−−−−→|a⊕a⊕b⟩|a⊕b⟩= |(a⊕a)⊕b⟩|a⊕b⟩
= |0⊕b⟩|a⊕b⟩= |b⟩|a⊕b⟩CNOT
−−−→|b,a⊕b⊕b⟩= |b⟩|a⟩.
As another proof, we can multiply the three CNOTs as matrices and show that
we get the matrix of a SWAP:




1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0








1 0 0 0
0 0 0 1
0 0 1 0
0 1 0 0








1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0



=




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



= SWAP.
Exercise 4.21. Entanglion contains four yellow planets besides the Bell States. Please see the game
board at Fig. 4.1. They are labeled Omega Zero through Omega Four. These are not standard names,
but they correspond to the quantum states
|ω0⟩= 1
2 (|00⟩−|01⟩+|10⟩+|11⟩),
|ω1⟩= 1
2 (−|00⟩+|01⟩+|10⟩+|11⟩),
|ω2⟩= 1
2 (|00⟩+|01⟩+|10⟩−|11⟩),
|ω3⟩= 1
2 (|00⟩+|01⟩−|10⟩+|11⟩).
The blue player corresponds to the left qubit, and the red player corresponds to the right qubit.
(a) Show that when the SWAP gate is applied to |ω0⟩, we get |ω3⟩.
4.4 Quantum Gates
163
(b) Show that when X is applied to the left qubit of |ω1⟩, we get |ω3⟩.
(c) Show that when CNOT01 is applied to |ω2⟩, we get |ω0⟩.
(d) Show that when CNOT = CNOT10 is applied to |ω3⟩, we get |ω2⟩.
Exercise 4.22. The Mølmer-Sørensen (MS) gate is a two-qubit gate that can be naturally imple-
mented on trapped ion quantum computers. It transforms Z-basis states by
|00⟩→1
√
2
(|00⟩+i|11⟩),
|01⟩→1
√
2
(|01⟩−i|10⟩),
|10⟩→1
√
2
(|10⟩−i|01⟩),
|11⟩→1
√
2
(|11⟩+i|00⟩).
(a) What is the MS gate as a matrix?
(b) Show that MS8 = I. (You may use a computer.)
4.4.3 Toffoli Gate
A three-qubit gate that often appears in quantum computing is the Toffoli gate, or
controlled-controlled-NOT gate, that we discussed in Section 1.5.3. Since it is re-
versible, it is a quantum gate, and it flips the right qubit if the left and middle qubits
are 1:
Toffoli|000⟩= |000⟩,
Toffoli|001⟩= |001⟩,
Toffoli|010⟩= |010⟩,
Toffoli|011⟩= |011⟩,
Toffoli|100⟩= |100⟩,
Toffoli|101⟩= |101⟩,
Toffoli|110⟩= |111⟩,
Toffoli|111⟩= |110⟩.
Or
Toffoli|a⟩|b⟩|c⟩= |a⟩|b⟩|ab⊕c⟩.
Recall from Section 1.5.3 that the Toffoli gate is universal for classical comput-
ing, and any efficient classical algorithm can be converted into an efficient algorithm
only utilizing Toffoli gates. Since the Toffoli gate is a quantum gate, quantum com-
puters can efficiently do everything a classical computer can efficiently do. In terms
of complexity classes, P is contained within BQP.
As a matrix, the columns correspond to Toffoli acting on |000⟩,|001⟩,...,|111⟩:
164
4 Multiple Quantum Bits
Toffoli =












1 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 1
0 0 0 0 0 0 1 0












.
In Section 1.5.3, we drew the Toffoli gate as a box. In quantum computing, we
typically draw the Toffoli gate similarly to the CNOT gate, with solid dots indicating
the control qubits and ⊕indicating the target:
•
•
In Quirk, we simply drag two control dots onto the circuit, along with the X gate:
We made the bottom two qubits both in the |1⟩state, so the Toffoli gate flips the top
qubit to |1⟩.
Exercise 4.23. Show that the Toffoli gate can be constructed from the one-qubit gates Hadamard
H, phase S, T, and T †, plus the two-qubit CNOT gate:
H
T †
T
T †
T
H
•
=
•
•
T †
T †
S
•
•
•
•
•
T
Just do the matrix multiplications on the computer.
Exercise 4.24. Consider the anti-Toffoli gate, which was introduced in Exercise 1.41. In quan-
tum computing, it is typically drawn like the anti-CNOT gate from Exercise 4.17, with open dots
indicating the anti-controls:
(a) How does the anti-Toffoli gate act on each basis state?
(b) What is the anti-Toffoli gate as a matrix?
4.4 Quantum Gates
165
4.4.4 No-Cloning Theorem
Classically, it is easy to copy or clone information by reading each bit and writing
it somewhere. In quantum computing, cloning qubits is more complicated. Say we
have a qubit in some superposition state. If we measure it in the Z-basis, we get |0⟩
or |1⟩with some probability. So, we do not learn the original superposition state.
Furthermore, the measurement collapses the state to |0⟩or |1⟩, meaning we lost
whatever superposition state we originally had.
To investigate this in greater detail, say we have a qubit in a known quantum
state, such as |+⟩. Since we know its state, we can produce additional copies of it:
|+⟩|0⟩I⊗H
−−→|+⟩|+⟩.
We went from having one copy to two. So, copying a known quantum state is no
problem.
The issue is copying an unknown quantum state. Say we have a qubit in an un-
known quantum state |ψ⟩= α|0⟩+β|1⟩, and we would like to make a copy of it:
|ψ⟩|0⟩→|ψ⟩|ψ⟩.
Is there a quantum gate U that allows us to copy or clone a general unknown qubit?
U would need to satisfy
U|ψ⟩|0⟩= |ψ⟩|ψ⟩.
Expressing this using linear algebra, we require




U11 U12 U13 U14
U21 U22 U23 U24
U31 U32 U33 U34
U41 U42 U43 U44





α
β

⊗

1
0

=

α
β

⊗

α
β





U11 U12 U13 U14
U21 U22 U23 U24
U31 U32 U33 U34
U41 U42 U43 U44








α
0
β
0



=




α2
αβ
αβ
β 2








U11α +U13β
U21α +U23β
U31α +U33β
U41α +U43β



=




α2
αβ
αβ
β 2




There are many possible solutions, such as
U11 = α,
U13 = 0,
U21 = 0,
U23 = α,
U31 = 0,
U33 = α,
U41 = 0,
U43 = β,
166
4 Multiple Quantum Bits
but this requires knowing α and β, which we do not know. Any general solution
requires knowing α and β, so there is no operator U that allows us to copy a general,
unknown quantum state.
As another “proof,” U|ψ⟩|0⟩= |ψ⟩|ψ⟩is akin to going from ψ to ψ2, and this is
quadratic, not linear. The mathematics we are using is called linear algebra because
matrices are linear. Vectors are transformed by linear transformations.
This result is called the no-cloning theorem. While classical information can be
cloned, quantum information can not generally be cloned.
Using this theorem, some scientists have proposed quantum software that cannot
be copied or pirated, and quantum money that cannot be copied or counterfeited,
but that is beyond the scope of this textbook.
Exercise 4.25. Say there is a unitary U that is able to clone qubits in two known states |ψ⟩and
|φ⟩. That is,
U|ψ⟩|0⟩= |ψ⟩|ψ⟩,
U|φ⟩|0⟩= |φ⟩|φ⟩.
For example, an operator that can clone both |0⟩and |1⟩is CNOT, since CNOT|00⟩= |00⟩and
CNOT|10⟩= |11⟩. Taking the inner product of the previous two equations,
⟨ψ|⟨0|U†U|φ⟩|0⟩= (⟨ψ|⟨ψ|)(|φ⟩|φ⟩)
(⟨ψ|⟨0|)(|φ⟩|0⟩) = (⟨ψ|⟨ψ|)(|φ⟩|φ⟩)
⟨ψ|φ⟩⟨0|0⟩= ⟨ψ|φ⟩⟨ψ|φ⟩
⟨ψ|φ⟩= (⟨ψ|φ⟩)2 .
For ⟨ψ|φ⟩to be equal to its square, it must equal 0 or 1. Thus, |ψ⟩= |φ⟩, or |ψ⟩and |φ⟩are
orthogonal. Thus, an operator can only clone states that are orthogonal.
Does there exist a quantum operator U that can clone both
(a) |+⟩and |−⟩?
(b) |i⟩and |−i⟩?
(c) |0⟩and |+⟩?
4.5 Quantum Adders
In Section 1.3, after defining classical bits and logic gates, we demonstrated how
to compute something: the sum of two binary numbers, each of length n. Now that
we have defined qubits and quantum gates, let us also construct quantum circuits
that add binary numbers. Before we do that, however, let us review the classical
ripple-carry adder.
4.5.1 Classical Adder
First, to review, we can add two binary numbers as follows:
4.5 Quantum Adders
167
(carry) 1 1 1 0 0
1011
“+” 1110
(sum) 11001
Or, in terms of variables,
(carry) c4 c3 c2 c1 c0
a3a2a1a0
“+”
b3b2b1b0
(sum) s4s3 s2 s1 s0
where the initial carry in is c0 = 0. In general, if the binary numbers have length n,
then the output has length n+1.
Classically, we can add binary numbers using the ripple-carry adder from Sec-
tion 1.3.5:
FA
FA
FA
FA
C0 = 0
A0
B0
A1
B1
A2
B2
A3
B3
S0
S1
S2
S3
C4 = S4
where FA denotes a full adder:
FA
Cin
A
B
S
Cout
Each full adder has three inputs: a carry in Cin and two bits A and B. From these, it
computes the sum S = A⊕B⊕Cin and the carry out is Cout = AB+Cin(A⊕B).
168
4 Multiple Quantum Bits
4.5.2 Making the Classical Adder a Quantum Gate
This full adder is not reversible, since it does not have enough outputs to uniquely
determine the inputs. So, it is not a quantum gate. There are several ways, however,
to convert it to a quantum gate.
1. From Exercise 1.45, we can turn the full adder into a reversible circuit by taking
the XOR of each of its outputs with and extra bit:
Cin
Cin
A
A
B
B
FA
S
Cout
D
S ⊕D
E
Cout ⊕E
This entire circuit can be drawn as a single gate with five inputs and five outputs:
FA
Rev
Cin
A
B
D
E
Cin
A
B
S ⊕D
Cout ⊕E
This gate is reversible, so it is a quantum gate. As we will discuss later in Sec-
tion 4.6, it is always possible to break up a large quantum gate like this into the
smaller gates we are familiar with, but it could take many smaller gates, and the
best way to do this is an open area of research.
2. From Section 1.3.3, a full adder can be made from two XOR gates, two AND
gates, and one OR gate:
4.5 Quantum Adders
169
A
B
Cin
S
Cout
One approach is to replace all five of these logic gates with (more than five)
NAND gates. Then, we can implement each NAND gate using a Toffoli gate,
which is reversible and a quantum gate. While this works in principle, the pro-
cedure can be wasteful, leading to extra gates and qubits.
3. Adapting the previous method, instead of converting all the logic gates into
NAND gates and then Toffoli gates, we convert each logic gate into a re-
versible/quantum gate more directly. The basic logic gates are NOT, AND, OR,
XOR, NAND, and NOR. From Section 2.6.3, the X gate is simply the NOT
gate. From Section 1.5.3, the Toffoli gate can implement AND and NAND by
setting its third bit to 0 or 1, respectively. From Exercise 1.41, the anti-Toffoli
gate can implement NOR and OR by setting its third bit to 0 or 1, respectively.
From Exercise 4.16, two CNOT gates can be used to implement XOR. These
results are summarized in the following table:
170
4 Multiple Quantum Bits
Classical
Reversible/Quantum
NOT
A
A
X-Gate
A
X
A
AND
A
B
AB
Toffoli
A
•
A
B
•
B
0
AB
OR
A
B
A + B
anti-Toffoli
A
A
B
B
1
A + B
XOR
A
B
A ⊕B
CNOTs
A
•
A
B
•
B
0
A ⊕B
NAND
A
B
AB
Toffoli
A
•
A
B
•
B
1
AB
NOR
A
B
A + B
anti-Toffoli
A
A
B
B
0
A + B
Replacing each gate in the full adder, we get the following circuit:
Cin
•
•
Cin
A
•
•
A
B
•
•
B
0
A ⊕B
•
•
A ⊕B
0
A ⊕B ⊕Cin
S
0
AB
AB
0
Cin(A ⊕B)
Cin(A ⊕B)
0
X
Cout
The first two CNOTs implement an XOR gate (A⊕B), and the next two CNOTs
also implement an XOR gate, computing A⊕B⊕Cin = S. Next, the Toffoli gate
implements an AND gate (AB), and another Toffoli gate implements another
AND gate [Cin(A ⊕B)]. Finally, an X gate turns an extra bit from a 0 to a 1,
and together with an anti-Toffoli gate, they implement an OR gate, yielding
AB +Cin(A ⊕B) = Cout. Since these are all reversible gates, it is a quantum
circuit.
4.5 Quantum Adders
171
Notice this method uses several extra bits. Besides the inputs (Cin, A, and B) and
the outputs (S and Cout), three extra bits were used for intermediate calculations:
A ⊕B, AB, and Cin(A ⊕B). These extra bits are called ancilla bits or ancillary
bits, and in quantum circuits, they should be cleaned up by turning them back
into zeros. This is so they can be reused in later parts of a circuit and so that they
do not cause unintended entanglement. One method for cleaning up ancillary
bits is called uncomputation, where we apply in reverse order the inverses of
the gates that were used to calculate the ancillas. Since the Toffoli and CNOT
gates are their own inverses, the full adder becomes
Cin
•
•
•
Cin
A
•
•
•
•
A
B
•
•
•
•
B
0
•
•
•
0
0
S
0
0
0
0
0
X
Cout
Simulating in Quirk (see https://bit.ly/34BY6AD), we get
We see that with all three inputs Cin, A, and B set to 1, both the sum S and carry-
out Cout are 1, as expected, and each of the three ancilla qubits correctly start
and end in |0⟩.
4. We can come up with a more clever implementation that uses fewer gates and
bits. Let us do this over the next several sections.
172
4 Multiple Quantum Bits
4.5.3 Quantum Setup
Quantumly, we can encode the binary numbers in two quantum registers |a⟩and |b⟩.
One way to add them reversibly is to replace |b⟩with the sum:
|a⟩|b⟩→|a⟩|s⟩,
where s = a + b. For example, using the quantum adder, 1011 + 1110 = 11010,
would be
|1011⟩|01110⟩→|1011⟩|11010⟩.
By adding this way, it is always possible to determine the inputs: a is the left register,
and b can be obtained by subtracting a from s. Since the sum can have length n+1,
this means our second register needs an extra qubit |bn⟩that is initially |0⟩:
|a⟩= |an−1⟩...|a1⟩|a0⟩,
|b⟩= |bn = 0⟩|bn−1⟩...|b1⟩|b0⟩.
where s = a+b.
In the intermediate steps of the computation, the quantum adder also needs to
keep track of carry bits, so we also have a quantum register of length n for the carry
bits:
|c⟩= |cn−1⟩...|c1⟩|c0⟩.
Initially, this ancillary carry register contains all zeros, and at the end of our com-
putation, we should restore them to all zeros. Putting these together, we want our
quantum adder to map
|a⟩|b⟩|c⟩→|a⟩|s⟩|c⟩.
4.5.4 Quantum Sum
Since CNOT|a⟩|b⟩= |a⟩|a⊕b⟩, we can implement the sum using two CNOTs:
|ci⟩
|ci⟩
•
|ci⟩
|ai⟩
•
|ai⟩
|ai⟩
|bi⟩
|ai ⊕bi⟩
|ai ⊕bi ⊕ci⟩= |si⟩
In the above circuit, the first CNOT turns |bi⟩into |ai ⊕bi⟩, and the second turns it
into |ai ⊕bi ⊕ci⟩, which is |si⟩: We can combine this into a single quantum gate:
4.5 Quantum Adders
173
|ci⟩
S
|ci⟩
|ai⟩
|ai⟩
|bi⟩
|ai ⊕bi ⊕ci⟩= |si⟩
There are also several ways to create custom gates in Quirk. One way is to first
create the sum by dragging and dropping controls and X gates:
Next, we can click on the “Make Gate” button at the top of the page. A dialog box
will pop up with different options, and we want to create a gate from the circuit we
just drew:
We can either turn the whole circuit into a gate, or we can just select the first two
columns (1:2). Let us also name the gate “S.” If we click “Create Custom Gate,” we
return to the main screen, and now our gate appears at the bootm right toolbar under
“Custom Gates:”
174
4 Multiple Quantum Bits
We can drag this onto the main circuit like any other gate:
By changing the initial state of the qubits to |1⟩, we can also test the circuit to verify
that it adds correctly.
4.5.5 Quantum Carry
Recall from Exercise 1.30) that the OR gate that is used to calculate Cout can be
replaced by an XOR gate. That is,
Cout = AB⊕Cin(A⊕B).
To implement this, recall the Toffoli gate is
Toffoli|a⟩|b⟩|c⟩= |a⟩|b⟩|ab⊕c⟩.
This allows us to create the AND of A and B, XORed with C. Then, a quantum carry
circuit is
|ci⟩
|ci⟩
|ci⟩
•
|ci⟩
|ai⟩
•
|ai⟩
•
|ai⟩
|ai⟩
|bi⟩
•
|bi⟩
|ai ⊕bi⟩
•
|ai ⊕bi⟩
|ci+1⟩
|aibi ⊕ci+1⟩
|aibi ⊕ci+1⟩
|aibi ⊕ci(ai ⊕bi) ⊕ci+1⟩
In the above circuit, when ci+1 = 0, this carry gate transforms it to aibi ⊕ci(ai ⊕bi),
which is precisely Cout. Combining all this into a single quantum gate,
4.5 Quantum Adders
175
|ci⟩
C
|ci⟩
|ai⟩
|ai⟩
|bi⟩
|ai ⊕bi⟩
|ci+1⟩
|aibi ⊕ci(ai ⊕bi) ⊕ci+1⟩
Note in the third row that bi →ai ⊕bi, so we will need to uncompute this later.
4.5.6 Quantum Ripple-Carry Adder
Now, let us construct a quantum adder that was proposed by Vedral, Barenco, and
Ekert in 1996. We order the wires to alternate between ci, ai, and bi:
c0 = 0
a0
b0
c1 = 0
a1
b1
c2 = 0
a2
b2
c3 = 0
a3
b3
b4 = 0
For the first operation, we can either calculate the sum s0 using our sum circuit S,
or we can calculate the carry c1 using our carry circuit C. If we begin by calculating
s0, then we no longer have b0, but we need b0 to calculate the carry c1. So, let us
calculate the carry first:
176
4 Multiple Quantum Bits
c0 = 0
C
c0 = 0
a0
a0
b0
a0 ⊕b0
c1 = 0
c′
1
a1
a1
b1
b1
c2 = 0
c2 = 0
a2
a2
b2
b2
c3 = 0
c3 = 0
a3
a3
b3
b3
b4 = 0
b4 = 0
Now that we have c′
1, we can either add it to a1 and b1, or we can calculate the carry
c′
2. Again, if we add first, then we no longer have b1 to calculate the carry. So, let us
calculate the next carry, and repeating this argument, we calculate all the carries:
c0 = 0
C
c0 = 0
a0
a0
b0
a0 ⊕b0
c1 = 0
C
c′
1
a1
a1
b1
a1 ⊕b1
c2 = 0
C
c′
2
a2
a2
b2
a2 ⊕b2
c3 = 0
C
c′
3
a3
a3
b3
a3 ⊕b3
b4 = 0
s4
Note the last carry corresponds to the leftmost bit of the sum s4. Now, to calculate
s3 using our sum circuit S, we need the inputs to be c′
3, a3, and b3, but currently
the third input is a3 ⊕b3. To make this third input simply b3, we CNOT a3 with it,
resulting in a3 ⊕(a3 ⊕b3) = (a3 ⊕a3)⊕b3 = 0⊕b3 = b3:
4.5 Quantum Adders
177
c0 = 0
C
c0 = 0
a0
a0
b0
a0 ⊕b0
c1 = 0
C
c′
1
a1
a1
b1
a1 ⊕b1
c2 = 0
C
c′
2
a2
a2
b2
a2 ⊕b2
c3 = 0
C
c′
3
a3
•
a3
b3
b3
b4 = 0
s4
Now, we can use our sum circuit S to calculate b3:
c0 = 0
C
c0 = 0
a0
a0
b0
a0 ⊕b0
c1 = 0
C
c′
1
a1
a1
b1
a1 ⊕b1
c2 = 0
C
c′
2
a2
a2
b2
a2 ⊕b2
c3 = 0
C
S
c′
3
a3
•
a3
b3
s3
b4 = 0
s4
Next, we need to undo c′
3 so that we just have c3 = 0. We can do this by inverting
the carry gate:
178
4 Multiple Quantum Bits
C†
•
=
•
•
•
•
Note since C is a quantum gate, it is unitary, so its inverse is equal to its conjugate
transpose (i.e., C−1 = C†). Applying this,
c0 = 0
C
c0 = 0
a0
a0
b0
a0 ⊕b0
c1 = 0
C
c′
1
a1
a1
b1
a1 ⊕b1
c2 = 0
C
C†
c′
2
a2
a2
b2
b2
c3 = 0
C
S
c3 = 0
a3
•
a3
b3
s3
b4 = 0
s4
This also converted a2 ⊕b2 back to b2, so we can again use the sum circuit to find
s2 = a2 ⊕b2 ⊕c′
2:
4.5 Quantum Adders
179
c0 = 0
C
c0 = 0
a0
a0
b0
a0 ⊕b0
c1 = 0
C
c′
1
a1
a1
b1
a1 ⊕b1
c2 = 0
C
C†
S
c′
2
a2
a2
b2
s2
c3 = 0
C
S
c3 = 0
a3
•
a3
b3
s3
b4 = 0
s4
Repeating this process, we can apply C† to convert c′
2 back to c2 = 0 and a1 ⊕b1
back to b1, and then use the sum circuit to compute s1, and so forth, resulting in the
following complete circuit:
c0 = 0
C
C†
S
c0 = 0
a0
a0
b0
s0
c1 = 0
C
C†
S
c1 = 0
a1
a1
b1
s1
c2 = 0
C
C†
S
c2 = 0
a2
a2
b2
s2
c3 = 0
C
S
c3 = 0
a3
•
a3
b3
s3
b4 = 0
s4
This is our quantum ripple-carry adder, and |b⟩has been replaced by |s⟩(while
keeping |a⟩and |c⟩unchanged), as we wanted.
Note the qubits in this circuit have a different order. Rather than taking
|a⟩|b⟩|c⟩→|a⟩|s⟩|c⟩,
180
4 Multiple Quantum Bits
our circuit takes
|b4⟩|b3⟩|a3⟩|c3⟩|b2⟩|a2⟩|c2⟩|b1⟩|a1⟩|c1⟩|b0⟩|a0⟩|c0⟩
to
|s4⟩|s3⟩|a3⟩|c3⟩|s2⟩|a2⟩|c2⟩|s1⟩|a1⟩|c1⟩|s0⟩|a0⟩|c0⟩.
Let us verify our quantum circuit in Quirk by adding |a⟩= |1011⟩and |b⟩=
|01110⟩, which should result in |s⟩= |11001⟩. With the qubit ordering from above,
where all the carry qubits are |0⟩at the start and end of the computation, the quantum
ripple-carry adder should take
|0110100110010⟩→|1110000010110⟩.
You can view the circuit in Quirk by going to https://bit.ly/39NzEf9. It is also
shown below:
We see that with the input |0110100110010⟩, the output is |1110000010110⟩, as
expected.
Exercise 4.26. Simulate the quantum ripple-carry adder in Quirk, and use it to add 1111+1011.
Exercise 4.27. We can use an adder to subtract binary numbers by using the fact that
a−b = a+b.
4.5 Quantum Adders
181
In Quirk, modify your circuit from Exercise 4.26 to subtract 1111 −1011. Do this by adding X
gates to each bit of the input a (this gives a). Then, the adder computes a+b. Then, add X gates to
each bit of s, except for the extra bit s4, since it is not needed and should stay 0. This gives a+b.
Exercise 4.28. While teaching a course on quantum computing in Fall 2018, one of my Creighton
University students, Lorenzo Riva, proposed the following change to the quantum ripple-carry
adder:
c0 = 0
C
C†
S
c0 = 0
a0
a0
b0
s0
c1 = 0
C
C†
S
c1 = 0
a1
a1
b1
s1
c2 = 0
C
C†
S
c2 = 0
a2
a2
b2
s2
c3 = 0
C
•
c3 = 0
a3
a3
b3
s3
b4 = 0
s4
That is, the CNOT between a3 and b3, and the bottommost S (sum, not the S =
√
Z gate), can be
replaced by a single CNOT between c3 and b3.
(a) Explain why this simplification is correct.
(b) If each binary number has length n, how many Toffoli gates and how many CNOT gates does
this circuit use?
Exercise 4.29. In this exercise, we will learn about another quantum adder that does not need
carry bits. It is called Draper’s adder, and it uses the “quantum Fourier transform,” which will be
discussed later in the textbook in Section 7.7.3.
First, let us define a single-qubit gate Rr that rotates about the z-axis of the Bloch sphere by
angle 360◦/2r. For example, R1 = Z is a rotation by 180◦, R2 = S is a rotation by 90◦, R3 = T is a
rotation by 45◦, and R4 is a rotation by 22.5◦. Rotations about axes can be created in Quirk using
the “Make Gate” feature, e.g., R4 is
182
4 Multiple Quantum Bits
We also have the conjugate transpose (or inverse) of the rotation, which we denote R†
r, and it rotates
about the z-axis by −360◦/2r. For example, R†
4 rotates by −22.5◦, and since it is a negative angle,
it rotates the “other way.”
Draper’s quantum adder transforms
|a⟩|b⟩→|a+b⟩|b⟩,
and it does not use any carry qubits. Instead of using the ripple-carry adder, The circuit for the
adder is a little long, so we break it up over three parts:
Part 1
b0
. . .
b1
. . .
b2
. . .
b3
. . .
a0
•
•
•
H
. . .
a1
•
•
H
R2
. . .
a2
•
H
R2
R3
. . .
a3
H
R2
R3
R4
. . .
Part 2
. . .
•
•
•
•
. . .
. . .
•
•
•
. . .
. . .
•
•
. . .
. . .
•
. . .
. . .
R1
. . .
. . .
R1
R2
. . .
. . .
R1
R2
R3
. . .
. . .
R1
R2
R3
R4
. . .
Part 3
4.5 Quantum Adders
183
. . .
b0
. . .
b1
. . .
b2
. . .
b3
. . .
H
•
•
•
s0
. . .
R†
2
H
•
•
s1
. . .
R†
3
R†
2
H
•
s2
. . .
R†
4
R†
3
R†
2
H
s3
Implement Draper’s adder in Quirk and use it to add |a⟩= |0111⟩and |b⟩= |0011⟩.
4.5.7 Circuit Complexity
Generalizing this, adding two n-bit strings uses n carry gates C, n−1 inverses of the
carry gate C†, n sum gates S, and an extra CNOT gate. Each C and C† gate uses two
Toffoli gates and one CNOT gate, and each S gate uses two CNOT gates. The total
number of quantum gates to add two n-bit strings is summarized in the following
table:
Gate
No. of Gates Total No. of Toffolis Total No. of CNOTs
C
n
2n
n
C†
n−1
2(n−1)
n−1
S
n
0
2n
Extra CNOT
1
0
1
4n−2
4n
Altogether, the quantum ripple-carry adder uses 4n−2 Toffoli gates and 4n CNOT
gates, which is linear in n, i.e., Θ(n), so the algorithm is efficient.
Exercise 4.30. How many Toffoli gates and how many CNOT gates does the quantum ripple-carry
adder need to add two (a) 4-bit strings, (b) 8-bit strings.
Exercise 4.31. To add two binary numbers of length 4, our quantum ripple-carry adder used 13
qubits. How many qubits does the quantum ripple-carry adder need to add two binary numbers of
length n?
184
4 Multiple Quantum Bits
4.5.8 Adding in Superposition
Note our quantum ripple-carry adder is a quantum circuit, so it can also act on
superpositions. For example, if |a⟩is an equal superposition of 6 and 3, i.e.,
|a⟩= 1
√
2
(|0110⟩+|0011⟩),
and if |b⟩is 11, then
|b⟩= |01011⟩,
then the quantum ripple-carry adder turns |a⟩|b⟩into
1
√
2
(|0110⟩|10001⟩+|0011⟩|01110⟩).
It may appear as though we solved two addition problems at once, i.e., in “paral-
lel,” since both 6 +11 = 17 and 3 +11 = 14 appear in the answers as |10001⟩and
|01110⟩, respectively, but this is not the case. When we measure the result, we get
one sum with probability 1/2 or the other sum with probability 1/2. In contrast, in
parallel computing, two computers calculate both answers at the same time, so we
get both sums at the end.
It is incorrect to think of a quantum computer as a massively parallel classical
computer because we must measure and only get one result. In fact, this misunder-
standing is so common that it might be best to avoid the term “parallel” altogether
when describing quantum computing.
We have seen our first quantum algorithm: the ripple-carry adder. We will get to
many more quantum algorithms in Chapter 7, but there are several other topics to
cover first.
Exercise 4.32. Read “Quantum Computing: A Soccer Analogy” at
https://medium.com/@thomaswong_8663/quantum-computing-a-soccer-a
nalogy-1335644a1472
Answer the following questions and fill in the blanks.
(a) Who is the author of the article, and what is their relationship with the author of this textbook?
(b) “Analogously, the essence of quantum computing is to change the rules so that a computer
can now use its “
.” That is, the rules of the game are changed from
the laws of classical physics to the laws of
physics. As a result, a
quantum computer can solve
problems faster by using its “hands.” For
other problems, using one’s “feet” is better, so a quantum computer is
for these problems.”
4.6 Universal Quantum Gates
185
4.6 Universal Quantum Gates
4.6.1 Definition
A set of quantum gates that allows us to approximate any quantum gate to any
desired precision is called a universal gate set. Recall in Section 1.2.5 that we used
the same term to describe a set of logic gates that can reproduce all classical gates.
It is usually clear from the context. For example, if the Hadamard gate is involved,
then we must be discussing quantum gates because there is no classical Hadamard
gate. Or, we might describe a set as “universal for classical computing” or “universal
for quantum computing.”
Proving that a set of gates is universal is a more advanced topic, which we do
not discuss in this textbook. Nielsen and Chuang is a good resource for additional
details. Instead, we provide some intuition below.
4.6.2 Components of a Universal Gate Set
There are several components that we need for a set of quantum gates to be universal.
• Superposition. We must be able to produce superpositions. For example the
Hadamard gate can create superpositions, such as H|0⟩= |+⟩. Other gates are
not. Z, S, and T only apply phases; they do not create superpositions of |0⟩and
|1⟩. Similarly, the X and CNOT gates only flip |0⟩and |1⟩, so they cannot create
superpositions. Y only applies phases and flips, so again superpositions cannot
be created by it.
• Entanglement. We must be able to entangle qubits. One-qubit gates, such as H,
cannot do this since they only act on a single qubit. A gate must act on at least
two qubits to produce entanglement. CNOT can produce entanglement since
CNOT|+⟩|0⟩= |Φ+⟩. Not all two qubit gates produce entanglement, however.
The SWAP gate cannot generate entanglement since it only swaps two qubits.
• Complex amplitudes. CNOT and H only contain real numbers, so they do not
produce states with complex amplitudes.
• Contain more than the Clifford group. The Clifford group1 is the set of gates
{CNOT,H,S}, and although this set satisfies all of the previous requirements
(entanglement, superposition, and complex amplitudes), the Gottesman-Knill
theorem says that a quantum circuit containing only these gates is efficiently
simulated by a classical computer. That is, the Clifford group is only as powerful
as a classical computer, so a universal quantum gate set should contain more
than this.
1 Mathematically, the Clifford group is the normalizer of the Pauli group, which is generated by
the Pauli matrices.
186
4 Multiple Quantum Bits
It is unknown if these are sufficient requirements for a set of quantum gates to be
universal. It may be that a set satisfies all of these properties, but is still not universal.
Exercise 4.33. What property are each of the following gate sets lacking to be universal for quan-
tum computing?
(a) {Toffoli,H,Z}.
(b) {H,X,Y,Z,S,T}.
(c) {SWAP,H,S,T}.
4.6.3 Examples of Universal Gate Sets
Some examples of universal gate sets are:
• {CNOT,all single-qubit gates} is universal for quantum computing.2
• {CNOT,H,T} is universal for quantum computing.3 That is, although the Clif-
ford group {CNOT,H,S} is not universal for quantum computing, replacing S
with T does yield a universal gate set. H and T are sufficient to approximate all
one-qubit gates.
• {CNOT,Rπ/8,S} is universal for quantum computing, where
Rπ/8 =

cos
  π
8

−sin
  π
8

sin
  π
8

cos
  π
8


.
Although the Clifford group {CNOT,H,S} is not universal for quantum com-
puting, replacing H with Rπ/8 does yield a universal gate set.
• {Toffoli,H,S} is universal for quantum computing.4 Although {CNOT,H,S} is
not universal for quantum computing, replacing CNOT with Toffoli does yield
a universal gate set.
• H plus almost any two-qubit unitary.
Exercise 4.34. The Clifford group {CNOT,H,S} is not universal for quantum computing. Give
three ways to modify the Clifford group so that it is universal for quantum computing.
4.6.4 Solovay-Kitaev Theorem
The Solovay-Kitaev theorem says that with any universal gate set, we can approx-
imate a quantum gate on n qubits to precision ε using Θ(2n logc(1/ε)) gates for
2 A. Barenco, C. H. Bennett, R. Cleve, D. P. DiVincenzo, N. H. Margolus, P. W. Shor, T. Sleator, J.
A. Smolin, and H. Weinfurter. Elementary gates for quantum computation. Phys. Rev. A 52, 3457
(1995).
3 P. O. Boykin, T. Mor, M. Pulver, V. Roychowdhury, and F. Vatan. A new universal and fault-
tolerant quantum basis. Information Processing Letters, 75, 101 (2000).
4 A. Y. Kitaev. Quantum computations: Algorithms and error correction. RMS: Russian Mathe-
matical Surveys 52, 1191 (1997).
4.6 Universal Quantum Gates
187
some constant c. The dependence on the number of qubits 2n is what we might ex-
pect since an operator on n qubits is a matrix of 2n ×2n entries. The dependence on
the precision logc(1/ε) is great! The precision ε is the “distance” (in some measure-
ment or metric) that the approximate quantum gate is to the actual quantum gate,
and we want it to be small. So 1/ε is big, but taking the logarithm of it makes it
small. A logarithm to a constant power, such as logc, is a polynomial of a logarithm,
so is also called polylog. This is also considered small. Thus, this dependence means
our approximation quickly converges on the actual quantum gate.
4.6.5 Quantum Computing without Complex Numbers
Recall any complex number z has a real part x and an imaginary part y, i.e., z = x+iy.
Since x and y are real numbers, this means we can express any complex number as
two real numbers (x,y) and keep track of the fact that they play different roles. So
in theory, we can formulate all of quantum computing just in terms of real numbers.
Then, a universal set of quantum gates technically does not need to produce states
with complex amplitudes. For example, the following sets are also universal for
quantum computing:
• {Toffoli,any single-qubit gate that is basis-changing} is universal for quantum
computing.5 A gate is basis-changing if it changes the Z-basis {|0⟩,|1⟩} to an-
other basis. For example, H is basis-changing, since it changes between the
Z-basis {|0⟩,|1⟩} and the X-basis {|+⟩,|−⟩}, so {Toffoli,H} is universal for
quantum computing. In contrast, Z is not basis-changing since Z|0⟩= |0⟩and
Z|1⟩= −|1⟩≡|1⟩.
As Dorit Aharonov said, “This is perhaps the simplest universal set of gates that
one can hope for [...] It shows that one only needs to add the Hadamard gate to
make a ‘classical’ set of gates quantum universal.”6
• The controlled-Hadamard gate {CH} is universal for quantum computing.7 In
this gate, H is applied to the right qubit if the left qubit is 1. That is, it acts on
Z-basis states as
CH|00⟩= |00⟩,
CH|01⟩= |01⟩,
CH|10⟩= |1+⟩= 1
√
2
(|10⟩+|11⟩),
5 Y. Shi, Both Toffoli and controlled-NOT need little help to do universal quantum computation,
arXiv:quant-ph/0205115 (2002).
6 D. Aharonov, A Simple Proof that Toffoli and Hadamard are Quantum Universal, arXiv:quant-
ph/0301040 (2003).
7 D. J. Shepherd, T. Franz, and R. F. Werner, Universally Programmable Quantum Cellular Au-
tomaton, Phys. Rev. Lett. 97, 020502 (2006).
188
4 Multiple Quantum Bits
CH|11⟩= |1−⟩= 1
√
2
(|10⟩−|11⟩).
This can simulate both the Toffoli gate and the Hadamard gate, so from the
previous bullet, it is universal.
• {CNOT,any single-qubit gate whose square is basis-changing} is universal for
quantum computing.8 An example of a single-qubit gate whose square is basis-
changing is the following gate U:
U|0⟩=
√
3
2 |0⟩+ 1
2|1⟩,
U|1⟩= −1
2 |0⟩+
√
3
2 |1⟩.
Next, if we apply U again, meaning twice to the Z-basis states, we get
U2|0⟩= U
 √
3
2 |0⟩+ 1
2|1⟩
!
= 1
2|0⟩+
√
3
2 |1⟩,
U2|1⟩= U
 
−1
2 |0⟩+
√
3
2 |1⟩
!
= −
√
3
2
|0⟩+ 1
2|1⟩.
This is an orthonormal basis, so U2 is basis-changing. CNOT and U together
form a universal set of quantum gates.
In contrast, the square of the Hadamard gate is not basis-changing because ap-
plying the Hadamard gate twice does nothing:
H2|0⟩= |0⟩,
H2|1⟩= |1⟩.
So, {CNOT,H} is not universal, whereas from the first bullet point, {Toffoli,H}
is universal.
4.7 Quantum Error Correction
4.7.1 Decoherence
Recall a qubit can be represented by a point on the Bloch sphere:
8 Y. Shi (2002).
4.7 Quantum Error Correction
189
x
y
z
|0⟩
|1⟩
The north pole corresponds to |0⟩and the south pole corresponds to |1⟩. For a clas-
sical bit, these would be the only possible states, and the only error is for the bit to
completely flip between the north and south poles. For a qubit, however, every loca-
tion on the Bloch sphere is a different state. For example, beginning at |0⟩, instead
of completely flipping to |1⟩, a qubit could experience a partial bit flip error, where
it only rotates a little toward |1⟩:
x
y
z
|0⟩
|1⟩
Since a full bit flip corresponds to the X gate, and the X gate is a rotation about
the x-axis by π = 180◦, a partial bit flip corresponds to rotating about the x-axis by
some angle. So, in the above figure, the state is moving leftward, down the Bloch
sphere, in the yz-plane. This small change is an error.
To further complicate matters, a qubit’s state is not just its latitude up and down
the Bloch sphere, but also its longitude around the Bloch sphere. For example, if a
qubit initially in the |+⟩state gets bumped to the side, we get a different state:
190
4 Multiple Quantum Bits
x
y
z
|+⟩
This is called a phase flip error, because rotations around the z-axis correspond
changes in the relative phase. For example, |+⟩= (|0⟩+|1⟩)/
√
2 and |−⟩= (|0⟩−
|1⟩)/
√
2 lie on opposite sides of the equator.
Since qubits are more sensitive to errors than classical bits, small interactions
with the environment can move the qubit to a different location on the Bloch sphere.
This process is called decoherence. In practice, decoherence is the biggest obstacle
to building large-scale quantum computers, since it is very difficult to isolate a qubit
from its environment while making it accessible for quantum gates and measure-
ments.
Next, we will see how to correct for bit-flip errors and then phase-flip errors.
Then, we will combine both types of error correction into what is known as the
Shor code.
4.7.2 Bit-Flip Code
To make it possible to correct bit-flip errors, we use three physical qubits to encode
each logical qubit:
|0L⟩= |000⟩,
|1L⟩= |111⟩,
where subscript L denotes a logical qubit. A logical qubit is, in general, a superpo-
sition of |0L⟩and |1L⟩:
α|0L⟩+β|1L⟩= α|000⟩+β|111⟩.
A way to create this encoding is given in Exercise 4.35.
For the moment, let us first consider the case where a bit is completely flipped
(ε = 1). For example, say the left qubit flips, so
α|000⟩+β|111⟩→α|100⟩+β|011⟩
= β|011⟩+α|100⟩.
4.7 Quantum Error Correction
191
We would like to detect this error and correct it. Classically, we could just measure
the bits, see which one disagrees with the others, and then flip it back to correct
it. Quantumly, however, if we measure the bits (or even just a single bit), the state
collapses to |100⟩or |011⟩, and we lose the superposition. So, instead of measuring
the bits, we follow Section 1.6.3 and measure the parity of adjacent qubits. Recall
that the parity of two bits, a and b, can be calculated using Exclusive OR. That is,
parity(a,b) = a ⊕b. Also recall that CNOT|a⟩|b⟩= |a⟩|a⊕b⟩. Then, we can use
two CNOTs to calculate the the parity of two qubits, putting the answer in an ancilla
qubit:
|a⟩
•
|a⟩
|b⟩
•
|b⟩
|0⟩
|0 ⊕a⟩= |a⟩
|a ⊕b⟩
With three qubits, we can calculate the parities of adjacent qubits by doing this
twice:
|a⟩
•
|a⟩
|b⟩
•
•
|b⟩
|c⟩
•
|c⟩
|0⟩
|a ⊕b⟩
|0⟩
|b ⊕c⟩
In this example, the parity of the left two qubits is 1, and the parity of the right two
qubits is 0. This tells us that the left two qubits differ, and the right two qubits are the
same. Then, we know the left qubit has flipped, and we inferred this without directly
measuring and collapsing the state. This is called an error syndrome. To correct
the error, we can simply apply (X ⊗I ⊗I), which results in β|111⟩+ α|000⟩=
α|000⟩+β|111⟩, thus correcting the error.
Now, say there is a partial flip. In Section 2.6.4, it was stated that on the Bloch
sphere, a rotation by angle θ about the axis ˆn = (nx,ny,nz) is given by Eq. (2.10):
eiα

cos
θ
2

I −isin
θ
2

(nxX +nyY +nzZ)

,
(2.10 revisited)
where eiα is a global phase, so α can be chosen as we please. Now, a partial bit flip
corresponds to a rotation about the x-axis by some angle θ, so we have ˆn = (1,0,0).
We also choose α = π/2. Then, the rotation corresponds to
icos
θ
2

I +sin
θ
2

X.
Letting ε = sin(θ/2), we get cos(θ/2) =
q
1−sin2(θ/2) =
√
1−ε2, so the rota-
tion is
192
4 Multiple Quantum Bits
i
p
1−ε2I +εX = i
p
1−ε2

1 0
0 1

+ε

0 1
1 0

=

i
√
1−ε2
ε
ε
i
√
1−ε2

.
Thus, the partial bit flip maps
|0⟩→i
p
1−ε2|0⟩+ε|1⟩,
|1⟩→ε|0⟩+i
p
1−ε2|1⟩.
When θ = π, ε = 1, and we get |0⟩→|1⟩and |1⟩→|0⟩, which is a complete bit
flip, or the X gate.
For example, if the left qubit partially flips,
α|000⟩+β|111⟩→α

i
p
1−ε2|000⟩+ε|100⟩

+β

i
p
1−ε2|111⟩+ε|011⟩

= αi
p
1−ε2|000⟩+αε|100⟩+βi
p
1−ε2|111⟩+βε|011⟩
= αi
p
1−ε2|000⟩+βε|011⟩+αε|100⟩+βi
p
1−ε2|111⟩.
Now, we measure the parity of adjacent qubits. Labeling the qubits |q2q1q0⟩, we get
the following possible outcomes with corresponding probabilities:
• parity(q2,q1) = 0 and parity(q1,q0) = 0 with probability
αi
p
1−ε2

2
+
βi
p
1−ε2

2
= |α|2  1−ε2
+|β|2  1−ε2
=
 |α|2 +|β|2 1−ε2
= 1−ε2,
and the state collapses to
A

αi
p
1−ε2|000⟩+βi
p
1−ε2|111⟩

= α|000⟩+β|111⟩,
where A = 1/i
√
1−ε2 is a normalization constant. We see that the resulting
state is already corrected, so we do not need to do anything further to correct
the error. That is, the measurement fixed the error.
• parity(q2,q1) = 1 and parity(q1,q0) = 0 with probability
|βε|2 +|αε|2 = |α|2|ε|2 +|β|2|ε|2
=
 |α|2 +|β|2
|ε|2
= |ε|2,
and the state collapses to
4.7 Quantum Error Correction
193
B(βε|011⟩+αε|100⟩) = β|011⟩+α|100⟩,
where B = 1/ε is a normalization constant. To correct this state, we apply (X ⊗
I ⊗I) so that it becomes
β|111⟩+α|000⟩= α|000⟩+β|111⟩,
so we have corrected the error.
• parity(q2,q1) = 0 and parity(q1,q0) = 1 with probability 0.
• parity(q2,q1) = 1 and parity(q1,q0) = 1 with probability 0.
Finally, we need to reset the ancilla qubits to |0⟩so that we can reuse them, since
we want to repeatedly do error correction to fix any errors that appear. We can do
this by conditionally applying an X gate. If we measured a parity to be 0, we know
that the ancilla qubit is |0⟩, so we leave it alone. If we measured the parity to be 1,
we know that the ancilla qubit is |1⟩, and so we apply an X gate to it, turning it into
a |0⟩.
To summarize, when we have a partial bit flip, the measurement forces it to be
corrected or to become a complete bit flip, which we can correct by applying an X
gate. Here is the quantum circuit for this:
|q0⟩
•
X
|q1⟩
•
•
X
|q2⟩
•
X
|0⟩
•
•
|0⟩
|0⟩
•
•
|0⟩
The first four columns are the CNOTs that calculate the parities of adjacent qubits.
Then, we measure these parities, as shown by the meter symbols, which results
in classical bits. We denote these classical bits/wires using double lines. We end
with three X gates conditioned on these classical bits/parities. If both parities are 1,
then q1 flipped, so we apply an X gate to it to correct it. If parity(q2,q1) = 0 and
parity(q1,q0) = 1, then q0 flipped, so we apply an X gate to it to correct it. Finally,
if parity(q2,q1) = 1 and parity(q1,q0) = 0, then q2 flipped, so we apply an X gate
to it to correct it. We end by resetting the ancillas to |0⟩, indicated by the boxes with
|0⟩in them. Simulating this in Quirk (see https://bit.ly/3jZ4zKQ),
194
4 Multiple Quantum Bits
In this simulation, the logical qubit starts as |0L⟩= |000⟩. In the first column of
the circuit, we can visualize the states on Bloch spheres and confirm that we have
|000⟩. In the second column, we introduce a bit-flip error by applying a bit-flip Xt,
with t varying from 0 to 360◦, to q2. In the third row, we again visualize the states
on the Bloch sphere, confirming that |q2⟩̸= |0⟩. Then, we have our error correction
circuit, and at the very end of the circuit, the top three qubits are restored to |000⟩,
as expected. Note Quirk does not have a “reset” tool, so the ancillas have not yet
been restored to |0⟩. This would need to be done to repeat the circuit.
We can modify the above circuit using the principle of deferred measurement,
which says,
Intermediate measurements that are used to control operations can be moved
after the operations, and the controls can be replaced by quantum controls.
Then, the previous quantum circuit to correct bit flips is equivalent to
|q0⟩
•
X
|q1⟩
•
•
X
|q2⟩
•
X
|0⟩
•
•
•
|0⟩
|0⟩
•
•
|0⟩
Phrased another way, we can collapse and then do the controlled operations, or we
can do the controlled operations in superposition, and then collapse. Let us prove
this for our previous example, where the qubits started in the state α|000⟩+β|111⟩,
but then the left qubit partially flips with amplitude ε. From earlier, if we include
the ancilla qubits, the state after the first four CNOTs is
αi
p
1−ε2|00000⟩+βε|10011⟩+αε|10100⟩+βi
p
1−ε2|00111⟩.
Recall the qubits are ordered as |parity(q2,q1)⟩|parity(q1,q0)⟩|q2⟩|q1⟩|q0⟩. Now, if
we apply the controlled- and anti-controlled-X gates to correct the answers, the state
becomes
4.7 Quantum Error Correction
195
αi
p
1−ε2|00000⟩+βε|10111⟩+αε|10000⟩+βi
p
1−ε2|00111⟩
= i
p
1−ε2|00⟩(α|000⟩+β|111⟩)+ε|10⟩(α|000⟩+β|111⟩)
=

i
p
1−ε2|00⟩+ε|10⟩

(α|000⟩+β|111⟩).
Measuring the ancilla qubits now, we get:
• parity(q2,q1) = 0 and parity(q1,q0) = 0 with probability 1 −ε2, and the state
collapses to
|00⟩(α|000⟩+β|111⟩).
• parity(q2,q1) = 1 and parity(q1,q0) = 0 with probability ε2, and the state col-
lapses to
|10⟩(α|000⟩+β|111⟩).
This is the same result as before, where we first measured the ancilla qubits and
then applied the controlled- and anti-controlled-X gates. Finally, if the second
outcome occurs, we apply an X gate to the left ancilla to reset it to 0, yielding
|00⟩(α|000⟩+β|111⟩).
Simulating the circuit with deferred measurement in Quirk (see https://bit.
ly/3EyuBMK),
The top three qubits have been successfully corrected to |000⟩. Again, we have not
reset the ancilla qubits at the end of this circuit, which would be necessary to repeat
the error correction scheme.
Exercise 4.35. In this exercise, we will work through encoding a qubit in the bit-flip code. Say we
have a single qubit in the state
|ψ⟩= α|0⟩+β|0⟩.
We want to encode this using the bit-flip code. we add two more qubits to our system, all initially
in |0⟩, so our three qubits are in the state
|ψ00⟩= (α|0⟩+β|1⟩)|00⟩= α|000⟩+β|100⟩.
Starting with this state, we apply the following quantum circuit:
|0⟩
|0⟩
=
|ψ⟩
•
•
•
196
4 Multiple Quantum Bits
Show the final state, after the circuit, is
α|000⟩+β|111⟩= α|0L⟩+β|1L⟩.
Exercise 4.36. A logical qubit is in the state
|ψ⟩=
√
3
2 |0L⟩+ 1
2|1L⟩,
where we encode the logical qubit using three physical qubits:
|ψ⟩=
√
3
2 |000⟩+ 1
2|111⟩.
You suspect that a partial bit flip has occurred to one of the bits, so you measure the parity of
adjacent qubits.
(a) If parity(q2,q1) = 0 and parity(q1,q0) = 0, what quantum gate(s) should you apply to fix the
error, if any?
(b) If parity(q2,q1) = 0 and parity(q1,q0) = 1, what quantum gate(s) should you apply to fix the
error, if any?
(c) If parity(q2,q1) = 1 and parity(q1,q0) = 0, what quantum gate(s) should you apply to fix the
error, if any?
(d) If parity(q2,q1) = 1 and parity(q1,q0) = 1, what quantum gate(s) should you apply to fix the
error, if any?
4.7.3 Phase-Flip Code
We can similarly correct phase-flip errors by using three physical qubits to encode
each logical qubit, but instead of using three |0⟩’s and |1⟩’s, we use three |+⟩’s and
|−⟩’s, i.e.,
|0L⟩= |+++⟩,
|1L⟩= |−−−⟩,
so a general superposition is
α|0L⟩+β|1L⟩= α|+++⟩+β|−−−⟩.
A way to create this encoding is given in Exercise 4.37. The reason why we use |+⟩
and |−⟩is because a complete phase flip (the Z gate) switches between these states:
|+⟩= 1
2 (|0⟩+|1⟩) Z−→1
2 (|0⟩−|1⟩) = |−⟩,
|−⟩= 1
2 (|0⟩−|1⟩) Z−→1
2 (|0⟩+|1⟩) = |+⟩.
Say the left qubit experiences a complete phase flip:
α|+++⟩+β|−−−⟩→α|−++⟩+β|+−−⟩.
Then, we detect and correct this just like we did for the bit-flip error, except working
in the X-basis. So, we measure the parity of consecutive qubits in the X-basis, which
4.7 Quantum Error Correction
197
is 0 if the number of minuses is even and 1 if the number of minuses is odd. In
Exercise 4.38 and Exercise 4.39, you will show that the parities can be calculated
using
|a⟩
H
•
H
|a⟩
|b⟩
H
•
•
H
|b⟩
|c⟩
H
•
H
|c⟩
|0⟩
|a ⊕b⟩
|0⟩
|b ⊕c⟩
In our example where the left qubit experienced a phase flip, we get parity 1 for the
left two qubits and parity 0 for the right two qubits, implying that the first qubit is
flipped. So we apply (Z ⊗I ⊗I), restoring α|+++⟩+β|−−−⟩.
Now for partial phase flips, a partial phase flip corresponds to a rotation about the
z-axis by some angle θ, so again using Eq. (2.10) with α = π/2 and ε = sin(θ/2),
but now with ˆn = (0,0,1), we get that the rotation is
i
p
1−ε2I +εZ = i
p
1−ε2

1 0
0 1

+ε

1 0
0 −1

=

i
√
1−ε2 +ε
0
0
i
√
1−ε2 −ε

.
Thus, the partial phase flip maps
|0⟩→

i
p
1−ε2 +ε

|0⟩,
|1⟩→

i
p
1−ε2 −ε

|1⟩.
Note when θ = π, ε = 1, and we get |0⟩→|0⟩and |1⟩→−|1⟩, which is a complete
phase flip, or the Z gate. Let us see how a partial phase flip transforms |+⟩and |−⟩:
|+⟩= 1
√
2
(|0⟩+|1⟩)
→1
√
2
h
i
p
1−ε2 +ε

|0⟩+

i
p
1−ε2 −ε

|1⟩
i
→i
p
1−ε2 1
√
2
(|0⟩+|1⟩)+ε 1
√
2
(|0⟩−|1⟩)
→i
p
1−ε2|+⟩+ε|−⟩,
|−⟩= 1
√
2
(|0⟩−|1⟩)
→1
√
2
h
i
p
1−ε2 +ε

|0⟩−

i
p
1−ε2 −ε

|1⟩
i
198
4 Multiple Quantum Bits
→ε 1
√
2
(|0⟩+|1⟩)+i
p
1−ε2 1
√
2
(|0⟩+|1⟩)
→ε|+⟩+i
p
1−ε2|−⟩.
Using this, if we have a logical qubit in the state
α|0L⟩+β|1L⟩= α|+++⟩+β|−−−⟩,
a partial phase flip on the left qubit transforms this to
α

i
p
1−ε2|+++⟩+ε|−++⟩

+β

ε|+−−⟩+i
p
1−ε2|−−−⟩

= αi
p
1−ε2|+++⟩+αε|−++⟩+βε|+−−⟩+βi
p
1−ε2|−−−⟩
= αi
p
1−ε2|+++⟩+βε|+−−⟩+αε|−++⟩+βi
p
1−ε2|−−−⟩.
Now, we measure the parity of adjacent qubits in the X-basis (i.e., whether the num-
ber of |−⟩’s are even or odd). We get:
• parity(q2,q1) = 0 and parity(q1,q0) = 0 with probability
αi
p
1−ε2

2
+
βi
p
1−ε2

2
= |α|2  1−ε2
+|β|2  1−ε2
=
 |α|2 +|β|2 1−ε2
= 1−ε2,
and the state collapses to
A

αi
p
1−ε2|+++⟩+βi
p
1−ε2|−−−⟩

= α|+++⟩+β|−−−⟩,
where A = 1/i
√
1−ε2 is a normalization constant. We see that the resulting
state is already corrected, so we do not need to do anything further to correct
the error. That is, the measurement fixed the error.
• parity(q2,q1) = 1 and parity(q1,q0) = 0 with probability
|βε|2 +|αε|2 = |α|2|ε|2 +|β|2|ε|2
=
 |α|2 +|β|2
|ε|2
= |ε|2,
and the state collapses to
B(βε|+−−⟩+αε|−++⟩) = β|+−−⟩+α|−++⟩,
where B = 1/ε is a normalization constant. To correct this state, we apply (Z ⊗
I ⊗I) so that it becomes
4.7 Quantum Error Correction
199
β|−−−⟩+α|+++⟩= α|+++⟩+β|−−−⟩,
so we have corrected the error.
• parity(q2,q1) = 0 and parity(q1,q0) = 1 with probability 0.
• parity(q2,q1) = 1 and parity(q1,q0) = 1 with probability 0.
To summarize, when we have a partial phase flip, the measurement forces it to be
corrected or to become a complete bit flip, which we can correct by applying a Z
gate. The quantum circuit for this procedure is shown below:
|q0⟩
H
•
H
Z
|q1⟩
H
•
•
H
Z
|q2⟩
H
•
H
Z
|0⟩
•
•
|0⟩
|0⟩
•
•
|0⟩
Simulating this in Quirk (https://bit.ly/3e7dNQR):
The top three qubits are |0L⟩= |+++⟩, and the bottom two qubits will be used to
calculate the parities. In the first column of the circuit, we can visualize the states
on Bloch spheres and confirm that we have |+++⟩. Next, we simulate an error by
applying a phase-flip Zt, with t varying from 0 to 360◦), to the middle qubit. Now,
another set of Bloch spheres confirms that the middle qubit has changed. We want
to correct this so that we end up with |+++⟩again. The rest of our circuit is the
same as the bit-flip circuit, except we apply Hadamard gates before and after it so
that we work in the X-basis. In the output, we see that we have restored |+++⟩.
We can move the phase flip to any of the top three qubits, and our error-correcting
circuit will restore the state to |+++⟩. Note we also need to reset the ancilla qubits.
Exercise 4.37. In this exercise, we will work through an exercise for encoding a qubit in the phase-
flip code. Say we have a single qubit in the state
|ψ⟩= α|0⟩+β|0⟩.
We want to encode this using the bit-flip code. we add two more qubits to our system, all initially
in |0⟩, so our three qubits are in the state
|ψ00⟩= (α|0⟩+β|1⟩)|00⟩= α|000⟩+β|100⟩.
200
4 Multiple Quantum Bits
Starting with this state, we apply the following quantum circuit:
|0⟩
H
H
|0⟩
H
=
H
|ψ⟩
•
•
H
•
H
Show the final state, after the circuit, is
α|+++⟩+β|−−−⟩= α|0L⟩+β|1L⟩.
Exercise 4.38. Consider the following quantum circuit:
|a⟩
H
•
H
|b⟩
H
•
H
|0⟩
Show that...
(a) If |a⟩= |+⟩and |b⟩= |+⟩, find the resulting state at the end of the circuit.
(b) If |a⟩= |+⟩and |b⟩= |−⟩, find the resulting state at the end of the circuit.
(c) If |a⟩= |−⟩and |b⟩= |+⟩, find the resulting state at the end of the circuit.
(d) If |a⟩= |−⟩and |b⟩= |−⟩, find the resulting state at the end of the circuit.
(e) Using your answers to the previous parts, explain why this circuit calculates the parity in the
X-basis.
Exercise 4.39. Using two copies of the circuit from Exercise 4.38, we can calculate the parity of
adjacent qubits in the X-basis using the following circuit:
|a⟩
H
•
H
|a⟩
|b⟩
H
•
H
H
•
H
|b⟩
|c⟩
H
•
H
|c⟩
|0⟩
|a ⊕b⟩
|0⟩
|b ⊕c⟩
Explain how this is equivalent to the circuit from the text.
Exercise 4.40. A logical qubit is in the state
α|0L⟩+β|1L⟩,
where |0L⟩and |1L⟩are encoded using the phase-flip code:
|0L⟩= |+++⟩,
|1L⟩= |−−−⟩.
That is, the physical qubits are in the state
α|+++⟩+β|−−−⟩.
Now, the left physical qubit suffers a slight phase flip, causing the state to become
α

i
p
1−ε2|+++⟩+ε|−++⟩

+β

i
p
1−ε2|−−−⟩+ε|+−−⟩

.
To detect/correct this, you measure the parity of the left two qubits and the parity of the right two
qubits, both in the X-basis.
4.7 Quantum Error Correction
201
(a) What is the probability that both parities are even? If this probability is nonzero, say you
get this outcome. What is the state after the measurement? What gate(s) should you apply to
correct the error, if any?
(b) What is the probability that the parity of the left two qubits is odd and the parity of the right
two qubits is even? If this probability is nonzero, say you get this outcome. What is the state
after the measurement? What gate(s) should you apply to correct the error, if any?
(c) What is the probability that the parity of the left two qubits is even and the parity of the right
two qubits is odd? If this probability is nonzero, say you get this outcome. What is the state
after the measurement? What gate(s) should you apply to correct the error, if any?
(d) What is the probability that both parities are odd? If this probability is nonzero, say you get
this outcome. What is the state after the measurement? What gate(s) should you apply to
correct the error, if any?
4.7.4 Shor Code
We can combine the phase-flip code and bit-flip code to correct both kinds of errors.
We begin with the phase-flip code, so we can correct phase-flip errors. That is,
|0L⟩= |+++⟩
= 1
√
2
(|0⟩+|1⟩) 1
√
2
(|0⟩+|1⟩) 1
√
2
(|0⟩+|1⟩)
=
1
23/2 (|0⟩+|1⟩)(|0⟩+|1⟩)(|0⟩+|1⟩).
Then, so that we can correct bit-flip errors, we replace each of the three qubits with
three qubits using the bit-flip encoding, i.e., |0⟩→|000⟩and |1⟩→|111⟩, so that
each logical qubit is encoded using nine physical qubits:
|0L⟩=
1
23/2 (|000⟩+|111⟩)(|000⟩+|111⟩)(|000⟩+|111⟩).
Similarly, we begin with |1L⟩= |−−−⟩and replace |0⟩→|000⟩and |1⟩→|111⟩:
|1L⟩=
1
23/2 (|000⟩−|111⟩)(|000⟩−|111⟩)(|000⟩−|111⟩).
Then, the state of a general logical qubit is
α|0L⟩+β|1L⟩=
α
23/2 (|000⟩+|111⟩)(|000⟩+|111⟩)(|000⟩+|111⟩)
+ β
23/2 (|000⟩−|111⟩)(|000⟩−|111⟩)(|000⟩−|111⟩).
This encoding is called the Shor code, and it is named after its inventor, Peter Shor,
who proposed it in 1995 and, by doing so, invented quantum error correction. It uses
nine physical qubits to encode one logical qubit. A way to create this encoding is
given in Exercise 4.41.
202
4 Multiple Quantum Bits
Exercise 4.41. A qubit can be encoded using the Shor code by first encoding it in the three-qubit
phase-flip code (Exercise 4.37) followed by encoding each of the three qubits using the three-qubit
bit-flip code (Exercise 4.35), which results in nine qubits total. Applying these encodings one after
another, a method called concatenation, yields the following circuit:
|0⟩
|0⟩
|0⟩
H
•
H
•
|0⟩
|0⟩
=
|0⟩
H
•
H
•
|0⟩
|0⟩
|ψ⟩
•
H
•
•
H
•
In the above circuit, the large dashed box to the left is the phase-flip encoding, which turns
|0⟩→|+++⟩and |1⟩→|−−−⟩. Then, the three dashed boxes in the middle are each the bit-flip
encoding, which turns |0⟩→|000⟩and |1⟩→|111⟩.
If the initial state of the circuit is |ψ00000000⟩, where |ψ⟩= α|0⟩+β|1⟩, show that:
(a) The state of the circuit after the first column (after the CNOT with two targets) is
α|000⟩|000⟩|000⟩+β|100⟩|100⟩|100⟩.
(b) The state of the circuit after the the second column (after the Hadamard gates) is
α
√
2
(|000⟩+|100⟩)(|000⟩+|100⟩)(|000⟩+|100⟩)
+ β
√
2
(|000⟩−|100⟩)(|000⟩−|100⟩)(|000⟩−|100⟩).
(c) The final state of the circuit is
α
23/2 (|000⟩+|111⟩)(|000⟩+|111⟩)(|000⟩+|111⟩)
+ β
23/2 (|000⟩+|111⟩)(|000⟩+|111⟩)(|000⟩+|111⟩).
This is precisely α|0L⟩+β|1L⟩.
Let us see how to correct bit flips and phase flips using the Shor code, beginning
with bit flips. First, remember that the qubits are ordered q8q7 ...q0. Say q8 and q3
both experience complete bit flips. Then, the state of the system is
α
23/2 (|100⟩+|011⟩)(|001⟩+|110⟩)(|000⟩+|111⟩)
+ β
23/2 (|100⟩−|011⟩)(|001⟩−|110⟩)(|000⟩−|111⟩).
To detect this, we measure the parities of adjacent qubits within each triplet. In this
example, we would get:
4.7 Quantum Error Correction
203
left triplet:
parity(q8,q7) = 1, parity(q7,q6) = 0,
middle triplet:
parity(q5,q4) = 0, parity(q4,q3) = 1,
right triplet:
parity(q2,q1) = 0, parity(q1,q0) = 0.
This tells us that the eighth qubit and third qubit have flipped, so we can apply X
gates to to those two qubits to correct them. Similarly, if there is a partial bit flip,
measuring all the parities to be zero collapses the state and automatically corrects it,
or if there is a discrepancy, we apply X to the appropriate bit to correct it. This also
works with partial bit flips. Measuring the parities of adjacent qubits might collapse
the state and correct the errors, or it might collapse the state into a full bit flip, which
we correct as previously described.
Exercise 4.42. A logical qubit is encoded using nine physical qubits in the Shor code. In each
triplet, you measure the parity of adjacent qubits and get the following results:
left triplet:
parity(q8,q7) = 0, parity(q7,q6) = 1,
middle triplet:
parity(q5,q4) = 1, parity(q4,q3) = 0,
right triplet:
parity(q2,q1) = 1, parity(q1,q0) = 1.
Are there any bit flip errors? If so, which bits flipped, and what can you do to correct them?
Exercise 4.43. Bit flips can be corrected in the Shor code using the following quantum circuit:
|q0⟩
•
X
|q1⟩
•
•
X
|q2⟩
•
X
|q3⟩
•
X
|q4⟩
•
•
X
|q5⟩
•
X
|q6⟩
•
X
|q7⟩
•
•
X
|q8⟩
•
X
|0⟩
•
•
|0⟩
•
•
|0⟩
•
•
|0⟩
|0⟩
•
•
|0⟩
•
•
|0⟩
•
•
|0⟩
The first third of the circuit measures the parities of adjacent qubits in the top three qubits, correct
any errors, and reset the ancillas. The middle third of the circuit calculates the parities of adjacent
qubits in the next triplet, correcting any errors. Finally, it does the same for the last triplet of qubits.
Using Quirk, simulate this circuit by inserting it into the following circuit (see https://bi
t.ly/3D1kRKI):
204
4 Multiple Quantum Bits
The first part of this circuit applies the Hadamard gate to |q8⟩, turning it into |+⟩. Then, it uses
the circuit in Exercise 4.41 to encode this in the Shor code. Then, the Xt gates applies a partial bit
flip to one qubit in each triplet. In the middle section, you should add the previously given circuit.
Although there is no reset feature in Quirk, you can use the postselection tool as a workaround. In
Quirk, it is drawn as the outer product |0⟩⟨0|). It measures a qubit in the {|0⟩,|1⟩} basis, and if the
result is |0⟩, the calculation continues. Otherwise, the simulation starts over. For our purposes, it
has the effect of guaranteeing that the ancilla qubit is |0⟩before proceeding. A true “reset” feature
would allow us to continue with the ancilla as |0⟩without the risk of restarting the simulation. In
the last section of the circuit, we undo the Shor encoding and Hadamard gate so that all the qubits
are |0⟩again. Verify that your circuit does this.
Next, let us see how the Shor code also allows us to correct phase flips. Say q3
experiences a complete phase flip. Then, the state of the system is
α
23/2 (|000⟩+|111⟩)(|000⟩−|111⟩)(|000⟩+|111⟩)
+ β
23/2 (|000⟩−|111⟩)(|000⟩+|111⟩)(|000⟩−|111⟩).
Then, we can measure the “phase parity” of adjacent triplets, i.e., whether the num-
ber of (|000⟩−|111⟩)/
√
2 triplets is even or odd. This is similar to the phase flip
code, where we measured the parity in the X basis to determine if the number of
|−⟩’s was even or odd. How to measure this parity is shown in Exercise 4.44). In
our example, we would get
parity(triplet2,triplet1) = 1,
parity(triplet1,triplet0) = 1.
This indicates that the middle triplet needs to be flipped, so we apply the Z gate
to any one of the three qubits in that triplet. That is, we can apply the Z gate to
either q5, q4, or q3, correcting the error. Similarly, when there is a partial phase flip,
if we measure all the phase parities and get zero, the state collapsed and corrected
the error, and if there was a discrepancy in phase parities, we apply a Z gate to the
appropriate triplet to correct it.
4.7 Quantum Error Correction
205
By alternating between correcting bit-flip errors and phase-flip errors, the Shor
code corrects all quantum errors, assuming each triplet experiences at most one bit-
flip error per correction cycle, and at most one triplet experiences a phase-flip error
per correction cycle.
A quantum computer that accumulates errors slowly enough that errors can be
corrected is called fault tolerant. Depending on the error correcting code that is used,
the maximum correctable error rate can vary, and this is an area of active research.
At the time of this writing, a fault tolerant quantum computer does not yet exist, and
one could argue that building one is the “holy grail” of the field.
Exercise 4.44. Consider the following circuit, which computes the phase parity of adjacent triplets
in the Shor code:
|q0⟩
|q1⟩
|q2⟩
•
H
•
H
•
|q3⟩
|q4⟩
|q5⟩
•
H
•
•
H
•
|q6⟩
|q7⟩
|q8⟩
•
H
•
H
•
|0⟩
|0⟩
Comparing this circuit with Exercise 4.41, the first two layers of this circuit partially unencodes
the qubit. Then the middle four layers (the CNOTs) calculates the parity of adjacent triplets in two
ancilla qubits. Then, the final two layers reencodes the qubit. In this exercise, we will work through
this for an example.
(a) A qubit α|0L⟩+β|1L⟩is encoded using the nine-qubit Shor code, but a physical qubit in the
middle triplet experienced a phase flip, so the state of the nine qubits is
α
23/2 (|000⟩+|111⟩)(|000⟩−|111⟩)(|000⟩+|111⟩)
+ β
23/2 (|000⟩−|111⟩)(|000⟩+|111⟩)(|000⟩−|111⟩).
Show that after the first two columns of the circuit (the CNOTs with two targets and the
Hadamards), the state of the nine qubits is
α|000100000⟩+β|100000100⟩.
(b) Show that after the middle layers of the circuit (CNOTs), the ancilla qubits now store the
phase parities of adjacent triplets.
(c) Show that at the end of the circuit, the state of the nine qubits is again
206
4 Multiple Quantum Bits
α
23/2 (|000⟩+|111⟩)(|000⟩−|111⟩)(|000⟩+|111⟩)
+ β
23/2 (|000⟩−|111⟩)(|000⟩+|111⟩)(|000⟩−|111⟩).
(d) From the parities in (b), what gate should be applied to fix the phase flip, and to which qubit?
Exercise 4.45. Construct a quantum circuit that corrects when a triplet experiences a phase flip
error in the Shor code. To check your answer, simulate your circuit in Quirk by inserting it into the
following circuit (see https://bit.ly/3kmbTAm):
The first part of this circuit applies the Hadamard gate to |q8⟩, turning it into |+⟩. Then, it uses the
circuit in Exercise 4.41 to encode this in the Shor code. Then, the Zt gates applies a partial bit flip
to one triplet. In the middle section, you should add the previously given circuit. As described in
Exercise 4.45, postselect on |0⟩as a workaround for resetting the ancilla qubits. In the last section
of the circuit, we undo the Shor encoding and Hadamard gate so that all the qubits are |0⟩again.
Verify that your circuit does this. Try moving the Zt gate around to different triplets to ensure that
the phase flip is corrected in all instances, as long as at most one triplet experiences a phase flip.
Exercise 4.46. You have a logical qubit encoded in nine physical qubits using the Shor code. Let
us label the qubits q8q7 ...q0. They are grouped into three triplets (triplet2,triplet1,triplet0).
(a) You begin by detecting bit flip errors. Within each triplet, you measure the parity of adjacent
qubits in the Z-basis. Here are the results:
left triplet:
parity(q8,q7) = 1, parity(q7,q6) = 1,
middle triplet:
parity(q5,q4) = 0, parity(q4,q3) = 1,
right triplet:
parity(q2,q1) = 1, parity(q1,q0) = 0.
Are there any bit flip errors? If so, which bits flipped, and what can you do to correct them?
(b) Next, you measure the parities of adjacent triplets in the H3-basis. Here are the results:
parity(triplet2,triplet1) = 0,
parity(triplet1,triplet0) = 1.
Was there a phase flip error? If so, which triplet flipped, and what can you do to correct it?
4.8 Summary
207
4.8 Summary
The state of multiple qubits is written as as a tensor product. With n qubits, there
are 2n orthonormal basis states, and a general state is a superposition of these basis
states. In a product state, measuring one qubit cannot affect the others, while in an
entangled state, measuring one qubit can affect the other qubits. A quantum gate on
n qubits is a 2n ×2n unitary matrix. There are various ways to add binary numbers
on a quantum computer. A universal set of quantum gates can approximate any
quantum gate to any desired precision. Quantum bits can suffer from both bit-flip
and phase-flip errors, but they can be corrected, so building a quantum computer
“only” requires really good qubits, not perfect qubits.
Chapter 5
Quantum Programming
Quantum computing is currently emerging from the research lab onto the market-
place. Many companies are building prototype quantum processors, and although
these devices are not yet good enough for fault-tolerant quantum computation,
they may still have uses. These rudimentary quantum processors are called noisy
intermediate-scale quantum (NISQ) devices, where noisy means they suffer from
too much decoherence to be fault-tolerant, and intermediate-scale means they have
a moderate number of qubits, say roughly fifty to a few hundred. NISQ devices were
used to demonstrate quantum computational supremacy, which we briefly discussed
in Section 1.8.3.
Many companies have made their rudimentary quantum processors available for
people to experiment with. In this chapter, we will learn how to program IBM’s
quantum computers over the internet. This is not an endorsement of their prod-
uct or services, and other companies have similar tools for programming their own
quantum devices, which you are encouraged to explore on your own. Rather, IBM
has made several of their quantum processors freely available to the public, mak-
ing them a prudent choice for a textbook. Furthermore, after learning one quantum
programming toolkit, it will be easier to learn others, as there are many similarities
across them.
5.1 IBM Quantum Experience
5.1.1 Services
IBM was the first to make their quantum processors available over the internet (over
the “cloud”), and their online platform is called IBM Quantum Experience. It can
be accessed at https://quantum-computing.ibm.com. Their smaller quantum
processors are available to the public, and access to their larger, newer processors is
available commercially. When we log in, we first see the Dashboard:
209
210
5 Quantum Programming
We can go to the Services page to view a list of quantum processors available to us.
To get to the Services page, we can click on the menu icon in the top-left corner of
the Dashboard, then click “Services:”
This brings us to the Services page:
5.1 IBM Quantum Experience
211
On the Services page, we can click the “Systems” tab and then filter by “Your sys-
tems:”
This is the list of quantum processors that are available to us. If we click on a pro-
cessor, such as ibmq manila, we can see more information about it:
212
5 Quantum Programming
We see that this quantum processor has five qubits arranged in a line. This arrange-
ment, or topology, can affect which quantum gates can be naturally applied. For
example, we can naturally apply CNOT between qubits 0 and 1. If we want to apply
CNOT between qubits 0 and 2, however, we would need to, for example, SWAP
qubits 2 and 1, apply CNOT between 0 and 1, then SWAP 1 back with 2.
5.1.2 Circuit Composer
The Circuit Composer provides a drag-and-drop interface for programming quan-
tum circuits. To get to the Circuit Composer, we can click on the menu icon in the
top-left corner and then click “Composer:”
For example, let us program the following quantum circuit:
5.1 IBM Quantum Experience
213
|0⟩
|0⟩
|0⟩
H
•
•
This circuit produces the following state:
|000⟩H⊗I⊗I
−−−−→1
√
2
(|000⟩+|100⟩)
CNOT21
−−−−→1
√
2
(|000⟩+|110⟩)
CNOT20
−−−−→1
√
2
(|000⟩+|111⟩).
This state is known as the Greenberger–Horne–Zeilinger state (GHZ state). It is an
entangled state, and we will revisit it in the next chapter. If we measure it, we find
that all the qubits are 0 with probability 1/2 or all 1 with probability 1/2.
Using the Circuit Composer, we can create this circuit by dragging a Hadamard
gate and two CNOT gates onto the circuit:
214
5 Quantum Programming
To change the control and target of CNOTs, we double-clicked on them and mod-
ified which qubit was the control and which was the target. For example, for
CNOT21, the control and target were set as shown below:
In the Circuit Composer, we also deleted some qubits so that there are only three.
At the bottom of the webpage, the Circuit Composer automatically simulated the
circuit, showing histograms indicating that the circuit yields |000⟩with probability
50% or |111⟩with probability 50%, as expected.
To run this on an actual quantum processor, we need to add at least one measure-
ment. Let us measure all three qubits by adding measurement gates to the Circuit
Composer:
5.1 IBM Quantum Experience
215
Now, the histograms at the bottom of the screen have changed. Instead of giving
both |000⟩and |111⟩, we only get one of them. This is because when measuring
the qubits, we only get |000⟩or |111⟩, not both. The Circuit Composer is choosing
one of them using a pseudo-random number generator. At the top-right corner of the
screen, we can change the “Visualization seed,” which is a number that the pseudo-
random number generator starts with to generate pseudo-random numbers.
5.1.3 Quantum Processor
We can run the quantum circuit on one of IBM’s actual quantum processors. At the
top of the Circuit Composer, there is a button that says “Setup and run.” Clicking it
shows the following menu:
From this menu, we can select the quantum system on which to run the circuit. The
number of shots defaults to 1024, meaning it will run our circuit 1024 times and
return a histogram of the measurement outcomes. Ideally, we expect to get |000⟩
512 times and |111⟩512 times. We also see our job limit. Each user is limited to
having five jobs in the queue at a time. Clicking “Run on ibmq xxx” adds our job
216
5 Quantum Programming
to the queue. We can see its status by clicking the “Jobs” tab on the left side of the
screen:
When it is done, we can click it to see the results. Here is what we got for the
histogram:
Theoretically, we expect 000 or 111, each half the time. Due to a limited number of
shots and decoherence in the quantum processor, however, the results deviated from
our expectations. The results page also shows the actual quantum circuit that was
run, which is called the transpiled circuit:
5.1 IBM Quantum Experience
217
That is, it might not be possible to run our original quantum circuit on the device due
to the topology or gate set available to the processor, so the software will transpile
or convert our circuit to an equivalent one that can be physically run.
Exercise 5.1. Using the drag-and-drop Circuit Composer in IBM Quantum Experience, run the
following circuit on one of IBM’s quantum processors:
|0⟩
X
•
|1⟩
|0⟩
X
•
|1⟩
|0⟩
|1⟩
(a) Which processor did you use?
(b) Draw the topology of your processor (the arrangement of the qubits and their connections to
each other).
(c) Draw the transpiled circuit.
(d) Draw the resulting histogram showing the probability of each measurement outcome.
218
5 Quantum Programming
5.1.4 Simulator
Sometimes, it can take a long time for a job to make it through the queue for an
actual quantum processor. Or, the available quantum processors have too few qubits.
In these cases, using a simulator rather than an actual quantum processor may be
favorable. Let us try this for the previous circuit that creates the GHZ state. Clicking
“Setup and run,” let us run the circuit on ibm qasm simulator:
This results in the following histogram:
5.2 Quantum Assembly Language
219
Out of the 1024 shots, the simulator measured |000⟩493 times and |111⟩531 times.
Due to the limited number of shots and the use of a pseudo-random number gen-
erator to simulate the results, the results were not perfectly 50% each, but they are
pretty close.
Exercise 5.2. Simulate the circuit from Exercise 5.1 using ibm qasm simulator. Draw the resulting
histogram showing the probability of each measurement outcome.
5.2 Quantum Assembly Language
5.2.1 OpenQASM
Rather than dragging and dropping quantum gates to create a circuit, they can also
be written using programming languages. We can describe quantum circuits using
OpenQASM, where Open refers to the specification being open or freely available,
and QASM (pronounced kazm) stands for quantum assembly language. Despite the
name “assembly language,” it is really more of a hardware description language
like Verilog (see Section 1.3), where we defined registers and wires, listed logic
gates with their inputs and outputs, and defined modules/functions. A document
describing OpenQASM is available at https://arxiv.org/abs/1707.03429.
Here is an example of a simple OpenQASM program:
OPENQASM 2.0;
qreg q[3];
creg c[3];
U(pi,0,pi) q[0];
CX q[0], q[1];
measure q -> c;
220
5 Quantum Programming
The first line specifies that it is an OpenQASM program, version 2.0. Then we define
a quantum register or array named q consisting of 3 qubits, |q2⟩|q1⟩|q0⟩. All of these
qubits are initially |0⟩, so q is initially |000⟩. This is followed by a classical register
named c consisting of 3 bits, also indexed c2c1c0, and all these bits are initially 0.
Next, we apply a one-qubit quantum gate U(pi,0,pi) to qubit q[0], where the
one-qubit gate is parameterized as
U(θ,φ,λ) =

e−i(φ+λ)/2 cos(θ/2) −e−i(φ−λ)/2 sin(θ/2)
ei(φ−λ)/2 sin(θ/2)
ei(φ+λ)/2 cos(θ/2)

.
With appropriate choices for the angles, any one-qubit gate can be written this way,
up to a global phase. Technically, this is a rotation about the z-axis of the Bloch
sphere by λ, followed by a rotation about the y-axis by θ, followed by another
rotation about the z-axis, but by φ. In this example, when (θ,φ,λ) = (π,0,π), we
get
U(π,0,π) =

e−iπ/2 cos(π/2) −eπ/2 sin(π/2)
e−iπ/2 sin(π/2) eiπ/2 cos(π/2)

=

0 −i
−i 0

= −i

0 1
1 0

= −iX ≡X,
where in the last step, ≡means “equivalent to” because the global phase of −i can
be dropped. So, this gate transforms q[0] from |000⟩to −i|001⟩, but the global
phase can be ignored, so it is just |001⟩. Next, CNOT (CX) is applied with q[0]
as the control and q[1] as the target, transforming the state from |001⟩to |011⟩.
Finally, q is measured, and the resulting bits are placed in the classical register c.
So, c[2] = 0, c[1] = 1, and c[0] = 1.
U(θ,φ,λ) and CX are the only two gates that OpenQASM has built-in because
they form a universal gate set. That is, recall from Section 4.6 that the set {CNOT,
all single-qubit gates} is universal for quantum computing. We can, however, define
our own gates so that they are easier to use. Rewriting our previous code,
OPENQASM 2.0;
// Define the Pauli X gate.
gate x a
{
U(pi,0,pi) a;
}
qreg q[3];
creg c[3];
x q[0];
CX q[0], q[1];
measure q -> c;
5.2 Quantum Assembly Language
221
Comments are preceded by two slashes //, and names must start with a lowercase
letter, so our gate is called x, not X.
5.2.2 Quantum Experience Standard Header
Rather than writing all one-qubit gates in the form U(θ,φ,λ) or defining them our-
selves, it would be convenient if commonly used quantum gates like X, Y, Z, H,
and others were predefined. Thankfully, these and many of the gates used by IBM
Quantum Experience are defined in the library qelib1.inc, called the IBM Quan-
tum Experience standard header, which we can include in OpenQASM. So our
previous code can be written as
OPENQASM 2.0;
include "qelib1.inc";
qreg q[3];
creg c[3];
x q[0]
cx q[0], q[1];
measure q -> c;
Note CX has been replaced by cx, since we are now using CNOT defined in the
Quantum Experience standard header instead of the CNOT that is native to Open-
QASM.
Exercise 5.3. Recall OpenQASM parameterizes an arbitrary single-qubit gate as
U(θ,φ,λ) =

e−i(φ+λ)/2 cos(θ/2) −e−i(φ−λ)/2 sin(θ/2)
ei(φ−λ)/2 sin(θ/2)
ei(φ+λ)/2 cos(θ/2)

.
For convenience, let us define:
u1(λ) = U(0,0,λ),
u2(φ,λ) = U(π/2,φ,λ),
u3(θ,φ,λ) = U(θ,φ,λ).
Now go to https://arxiv.org/abs/1707.03429 and download a PDF of the preprint
that specifies OpenQASM. Go to Section 3.1 on the “Quantum Experience standard header.” In
terms of u1, u2, u3, and CNOT (CX), what is
(a) The H gate?
(b) The T † gate?
(c) A rotation about the x-axis by angle θ?
(d) The controlled-Z gate?
(e) The Toffoli gate?
222
5 Quantum Programming
5.2.3 OpenQASM in IBM Quantum Experience
Besides dragging and dropping quantum gates, IBM Quantum Experience also sup-
ports programming using OpenQASM. From the previous circuit for the GHZ state,
we can go to the menu and select “View” and then “Code Editor:”
Then, the Code Editor will appear on the right side of the screen:
5.2.4 Quantum Adder
Now, let us write some OpenQASM code to add 1110 + 1011 = 11001 using the
quantum ripple-carry adder in Section 4.5.6 and simulate it in IBM Quantum Expe-
5.2 Quantum Assembly Language
223
rience. We can define our own quantum gates to implement the sum S, carry C, and
inverse carry C†. Note s is the S gate, so we cannot use it as an identifier/name.
OPENQASM 2.0;
// Include standard gates from IBM Quantum Experience.
include "qelib1.inc";
// Define the quantum sum gate.
gate sum cin, a, b
{
cx a, b;
cx cin, b;
}
// Define the quantum carry gate.
gate carry cin, a, b, cout
{
ccx a, b, cout;
cx a, b;
ccx cin, b, cout;
}
// Define the inverse of the quantum carry gate.
gate carrydg cin, a, b, cout
{
ccx cin, b, cout;
cx a, b;
ccx a, b, cout;
}
// Declare the quantum registers.
qreg c[4];
qreg a[4];
qreg b[5];
// Declare the classical registers.
creg bc[5];
// Set the input states by applying X gates.
x a[1];
x a[2];
x a[3]; // a = 1110
x b[0];
x b[1];
x b[3]; // b = 1011
// Add the numbers so that |a>|b> becomes |a>|a+b>.
carry c[0], a[0], b[0], c[1];
carry c[1], a[1], b[1], c[2];
carry c[2], a[2], b[2], c[3];
carry c[3], a[3], b[3], b[4];
cx a[3], b[3];
sum c[3], a[3], b[3];
224
5 Quantum Programming
carrydg c[2], a[2], b[2], c[3];
sum c[2], a[2], b[2];
carrydg c[1], a[1], b[1], c[2];
sum c[1], a[1], b[1];
carrydg c[0], a[0], b[0], c[1];
sum c[0], a[0], b[0];
// Measure the sum and put it in the classical register.
measure b -> bc;
In the Circuit Composer, if we click the button on the left to view the “Composer
files,” there is a button to upload an OpenQASM circuit:
Uploading it, we get a new circuit:
5.2 Quantum Assembly Language
225
The circuit is too tall to include all of it in the above picture. Also, the histograms at
the bottom of the circuit are not available because our circuit uses too many qubits.
If we look at the Bloch spheres, however, at the end of the circuit, we see that
b4b3b2b1b0 = 11001, so the addition circuit works, as expected. We can also run the
circuit on the ibm qasm simulator backend, which yields the following results:
Again, the sum is 11001, as expected. Finally, we can run it on an actual quantum
processor, yielding the following:
The histogram cannot fit all the results. If we click the three dots in the upper-right
corner of the histogram, we can download the results as a CSV file. Here’s what we
get:
Computational basis states,Probabilities
00000,27.832
00001,16.113
00010,8.398
00011,4.199
00100,7.031
00101,3.125
00110,1.27
00111,1.367
01000,5.176
01001,2.246
01010,1.367
01011,0.391
01100,1.074
01101,0.586
01110,0.098
226
5 Quantum Programming
01111,0.293
10000,7.324
10001,3.711
10010,2.246
10011,0.879
10100,1.758
10101,0.781
10110,0.488
10111,0.293
11000,1.074
11001,0.488
11011,0.195
11100,0.098
11101,0.098
So, out of 1024 shots, the probability of getting the correct answer of 11001 is
0.488%, or less than half a percent. That is quite bad. To understand why, let us look
at the transpiled circuit. It is so long that we split it across six images:
5.2 Quantum Assembly Language
227
Since the transpiled circuit is so long, our qubits are noisy, and we do not yet have
error correction, the errors just kept accumulating. It is unsurprising, then, that our
histogram was so wrong.
Exercise 5.4. Consider the following quantum circuit:
|0⟩
•
H
•
|0⟩
•
|0⟩
H
•
Z
(a) Program this circuit using OpenQASM.
(b) Import your program into the Circuit Composer.
(c) Run your circuit on an actual quantum device. Which device did you choose, and what results
did you get?
228
5 Quantum Programming
5.3 Qiskit
5.3.1 Circuit Composer
Besides the Circuit Composer and OpenQASM editor, IBM has provided another
way to program their quantum processors. It is called Qiskit, where QIS stands for
quantum information science, and kit refers to a software development kit (SDK).
Qiskit is pronounced “kiz kit,” although some variants exist, like, “quiz kit” and
“kiss kit.” Qiskit is not a programming language, but is rather a toolkit or package
for the Python programming language. Qiskit is the most powerful way to program
IBM’s quantum computers because it provides more functionality than the other
approaches, and it also allows users to use Python’s vast network of packages and
libraries. More information about Qiskit is available at qiskit.org.
You can use Qiskit inside IBM Quantum Experience. To view a circuit as Qiskit
code, in the Circuit Composer, just select “Qiskit” in the Code Editor. For the GHZ
state, we get the following:
5.3 Qiskit
229
Here is the code again:
from qiskit import QuantumRegister, ClassicalRegister,
,→QuantumCircuit
from numpy import pi
qreg_q = QuantumRegister(3, ’q’)
creg_c = ClassicalRegister(3, ’c’)
circuit = QuantumCircuit(qreg_q, creg_c)
circuit.h(qreg_q[2])
circuit.cx(qreg_q[2], qreg_q[1])
circuit.cx(qreg_q[2], qreg_q[0])
circuit.measure(qreg_q[0], creg_c[0])
circuit.measure(qreg_q[1], creg_c[1])
circuit.measure(qreg_q[2], creg_c[2])
The first line imports from the Qiskit package functions to define quantum registers,
classical registers, and quantum circuits. In the second line, we import from the
numpy package the number pi. Although it is not used in this circuit, it is used in
many circuits, so it is included here for convenience.
In the next block of lines, the code defines a quantum register of length 3, la-
beled q, with the variable name qreg q. Then, the three qubits would be qreg q[0],
qreg q[1], and qreg q[2]. Similarly, the next line defines a classical register of
length 3, labeled c, with the variable name creg c, so the bits are creg c[0],
creg c[1], and creg c[2]. After that, a quantum circuit is created containing the
quantum and classical registers, and we name it circuit.
Finally, in the last block of 6 lines, we add a Hadamard gate to our quantum
circuit, and it is applied to qubit qreg q[2]. Then, we add a CNOT (CX) gate, with
qreg q[2] as the control and qreg q[1] as the target. Then, we add another CNOT
gate, again with qreg q[2] as the control, but now with qreg q[0] as the target. In
the final three lines, we add measurements to the circuit, and the result of measuring
qubit qreg q[0] is placed in the classical bit creg c[0], and so forth.
5.3.2 Quantum Lab
In the Circuit Composer, the Qiskit code is “read only,” so it cannot be modified. To
modify it, we click “Open in Quantum Lab.” This opens a Jupyter notebook, where
Python code can be executed and the results displayed in an interactive manner:
230
5 Quantum Programming
The first cell contains mostly the code from before, but now it starts and ends with
some additional code:
from ibm_quantum_widgets import CircuitComposer
[same code as before]
editor = CircuitComposer(circuit=circuit)
editor
The first line loads a package that will allow us to view the Circuit Composer from
within the Quantum Lab. The second-to-last line creates the Circuit Composer as
an object named editor, and the last line displays the editor. We can run this cell by
selecting the cell and clicking the ▷Run button, or by pressing Shift+Enter on your
keyboard. When we do, we get the following:
5.3 Qiskit
231
After running the cell, a second, empty cell appears below. We can put any Python
code we would like. For example, we can draw the circuit without using the en-
tire Circuit Composer using the draw function within QuantumCircuit to draw a
picture of our quantum circuit:
QuantumCircuit.draw(circuit)
The output of this is
232
5 Quantum Programming
As another example, we can print the OpenQASM code for the quantum circuit:
print(circuit.qasm())
The output of this is
OPENQASM 2.0;
include "qelib1.inc";
qreg q[3];
creg c[3];
h q[2];
cx q[2],q[1];
cx q[2],q[0];
measure q[0] -> c[0];
measure q[1] -> c[1];
measure q[2] -> c[2];
5.3.3 Simulator
Now, let us simulate this quantum circuit. The simulators are contained in a Qiskit
library called Aer, which we can import and then view the backends:
# Import the Qiskit Aer library
from qiskit import Aer
# View the Aer backends, which are all simulators.
Aer.backends()
The output of this is:
[AerSimulator(’aer_simulator’),
AerSimulator(’aer_simulator_statevector’),
AerSimulator(’aer_simulator_density_matrix’),
AerSimulator(’aer_simulator_stabilizer’),
AerSimulator(’aer_simulator_matrix_product_state’),
AerSimulator(’aer_simulator_extended_stabilizer’),
AerSimulator(’aer_simulator_unitary’),
AerSimulator(’aer_simulator_superop’),
QasmSimulator(’qasm_simulator’),
StatevectorSimulator(’statevector_simulator’),
UnitarySimulator(’unitary_simulator’),
PulseSimulator(’pulse_simulator’)]
Let us use qasm simulator as the backend and execute the circuit on it:
# Choose the qasm_simulator backend.
backend = Aer.get_backend(’qasm_simulator’)
# Import the Qiskit execute function.
from qiskit import execute
# Execute the quantum circuit on the backend, creating a job.
job = execute(circuit, backend)
5.3 Qiskit
233
We can use the job monitor function to see if the job is completed. It needs to be
imported from Qiskit.
# Check the status of the job.
from qiskit.tools.monitor import job_monitor
job_monitor(job)
If the job has not yet run, the job monitor will periodically update itself. Once the
job is done, we can get a count of the results and print them:
# Get a count of the results.
count = job.result().get_counts()
# Print the counts.
print(count)
The output of this is
{’000’: 497, ’111’: 527}
We expect that each result should appear 512 times (half of 1024 shots), but just like
flipping a coin 1024 times may not yield heads 512 times and tails 512 times, there
is some deviation because of statistics, not because of errors. This is a simulation,
not actual noisy quantum hardware.
We can also plot a histogram of the counts by importing the qiskit.visualizations
package and calling the plot histogram function:
# Import visualizations from Qiskit.
from qiskit.visualization import *
# Plot the count as a histogram.
plot_histogram(count)
This outputs the following image:
234
5 Quantum Programming
5.3.4 Quantum Processor
Now, let us run the quantum circuit on one of IBM’s actual quantum processors.
First, we need to load our IBM Quantum account to see what quantum processors
are available to us:
# Import the Qiskit IBMQ library.
from qiskit import IBMQ
# Load our IBMQ account.
provider = IBMQ.load_account()
# List our backends.
provider.backends()
This prints the quantum processors that are available to us, which includes the public
ones and any that we have paid access to. The output is
[<IBMQSimulator(’ibmq_qasm_simulator’) from IBMQ(hub=’ibm-q’,
,→group=’open’, project=’main’)>,
<IBMQBackend(’ibmqx2’) from IBMQ(hub=’ibm-q’, group=’open’,
,→project=’main’)>,
<IBMQBackend(’ibmq_16_melbourne’) from IBMQ(hub=’ibm-q’,
,→group=’open’, project=’main’)>,
<IBMQBackend(’ibmq_vigo’) from IBMQ(hub=’ibm-q’, group=’open
,→’, project=’main’)>,
<IBMQBackend(’ibmq_ourense’) from IBMQ(hub=’ibm-q’, group=’
,→open’, project=’main’)>,
<IBMQBackend(’ibmq_valencia’) from IBMQ(hub=’ibm-q’, group=’
,→open’, project=’main’)>,
<IBMQBackend(’ibmq_armonk’) from IBMQ(hub=’ibm-q’, group=’
,→open’, project=’main’)>,
<IBMQBackend(’ibmq_athens’) from IBMQ(hub=’ibm-q’, group=’
,→open’, project=’main’)>,
<IBMQBackend(’ibmq_santiago’) from IBMQ(hub=’ibm-q’, group=’
,→open’, project=’main’)>]
As before, let us pick a particular hardware backend and execute our circuit. We have
previously imported the necessary functions and libraries, so we can just execute the
commands:
# Choose a quantum processor as the backend.
backend = provider.get_backend(’ibmq_athens’)
# Execute the job.
job = execute(circuit, backend)
Again, we can use the job monitor function to see if the job is completed. It is
already imported, so we do not need to import it again:
job_monitor(job)
Once it has finished, we can get a count of the results, print them, and print a his-
togram:
5.3 Qiskit
235
count = job.result().get_counts()
print(count)
plot_histogram(counts)
The output is
{’000’: 499, ’001’: 7, ’010’: 8, ’011’: 13, ’100’: 1, ’101’:
,→22, ’110’: 17, ’111’: 457}
Most of the time, the result of the quantum circuit is 000 or 111, but due to decoher-
ence, some other results are also obtained.
Exercise 5.5. In IBM Quantum Experience, go to the Quantum Lab through the menu:
In the Quantum Lab, create a new Jupyter notebook for Qiskit (the arrow below indicates which
button to click):
The notebook should be blank. Starting with this, import the necessary Qiskit libraries and func-
tions and create the following quantum circuit:
236
5 Quantum Programming
|0⟩
S
H
T
H
|0⟩
H
•
Note you should add measurements at the end of the circuit.
(a) Simulate the circuit using qasm simulator. What histogram of results do you get?
(b) Run it on a sufficiently large quantum processor. Which processor did you choose? What
histogram of results did you get?
5.4 Other Quantum Programming Languages
OpenQASM and Qiskit were both developed by IBM to work with their quantum
devices. They are not the only options for quantum programming, however. Here
are a few others, which you are free to explore on your own, and there are more
beyond this list:
• Braket is Amazon’s quantum computing platform, and they have their own
Python software development kit for programming the devices on their plat-
form.
• Cirq is Google’s Python software library for programming their quantum com-
puters.
• Microsoft developed a programming language called Q# (pronounced “Q-
sharp”) specifically for programming quantum computers.
• Quil is a quantum “instruction set architecture” made by Rigetti for program-
ming its quantum computers. It is similar to OpenQASM. PyQuil is a library
for programming Quil in Python.
5.5 Summary
Quantum computing is progressing from an academic research interest to a nascent
industry, and the existence of this chapter on quantum programming is evidence
of this. Actual quantum devices are being developed, and the tools described in
this chapter provide an introduction for how to use them. We will continue using
the IBM Quantum Experience for the rest of this textbook as we learn about more
advanced protocols and algorithms.
Chapter 6
Entanglement and Quantum Protocols
In this chapter, we will explore entanglement in greater detail. We will see that if
two qubits are entangled, measuring one affects the other, but there are limits to how
quickly one can communicate using this behavior. We will see that if two people,
Alice and Bob, share entangled qubits, they can exploit this entanglement to send
information to each other using various protocols. With superdense coding, Alice
can send one qubit to Bob, but Bob will be able to discern two classical bits worth
of information. With quantum teleportation, Alice will be able to teleport the state
of a qubit to Bob by only sending him classical information. We will also see a
quantum protocol that does not use entanglement: quantum key distribution, where
Alice and Bob will be able to agree on a secret key or code, and the laws of quantum
mechanics prevent a third party from learning the key.
6.1 Measurements
In Section 4.3, we learned about product states, which can be factored into the tensor
product of single-qubit states, and entangled states, which could not. For example,
1
2 (|00⟩−|01⟩+|10⟩−|11⟩) = 1
√
2
(|0⟩+|1⟩)
|
{z
}
|+⟩
⊗1
√
2
(|0⟩−|1⟩)
|
{z
}
|−⟩
= |+⟩⊗|−⟩
= |+⟩|−⟩
is a product state, and
Φ+
= 1
√
2
(|00⟩+|11⟩)
is an entangled state.
237
238
6 Entanglement and Quantum Protocols
6.1.1 Product States
Now, if we measure a single qubit in a product state, it does not affect the other
qubit. For example, if we measure the left qubit of |+⟩|−⟩, we get |0⟩or |1⟩, each
with probability 1/2. Thus, the resulting states are |0⟩|−⟩or |1⟩|−⟩. The right qubit
remains |−⟩, unaltered by the measurement of the left qubit. A product state has no
entanglement.
6.1.2 Maximally Entangled States
If we measure a single qubit in an entangled state, it can affect the other qubits. For
example, consider the entangled state |Φ+⟩= (|00⟩+|11⟩)/
√
2. If we measure the
left qubit, we get |0⟩or |1⟩, each with probability 1/2, and the state collapses to |00⟩
or |11⟩, respectively. So, if we measure the left qubit and get |0⟩, we know that the
right qubit is also in the state |0⟩, and similarly, if we measures the left qubit and get
|1⟩, we know that the right qubit is also in the state |1⟩.
This is another way to identify entanglement. A measurement of one qubit affects
the other qubit.
In this example, |Φ+⟩has the maximum amount of entanglement, since mea-
suring one qubit completely determines what the other qubit will be. That is, if we
measure the left qubit and get |0⟩, we know with certainty that a measurement of
the second qubit will also yield |0⟩. The same holds for |1⟩. We say such entangled
states are maximally entangled.
With two qubits, there are four maximally entangled states. They are the Bell
states:
Φ+
= 1
√
2
(|00⟩+|11⟩),
Φ−
= 1
√
2
(|00⟩−|11⟩),
Ψ +
= 1
√
2
(|01⟩+|10⟩),
Ψ −
= 1
√
2
(|01⟩−|10⟩).
6.1.3 Partially Entangled States
Now, consider the following state of two qubits:
√
3
2
√
2
|00⟩+
√
3
2
√
2
|01⟩+
√
3
4 |10⟩+ 1
4|11⟩.
6.1 Measurements
239
This state is entangled because it cannot be factored into the tensor product of single-
qubit states (see Exercise 6.1). If we measure only the left qubit, we get
• 0 with probability 3/4, and the state collapses to
1
√
2
(|00⟩+|01⟩) = |0⟩1
√
2
(|0⟩+|1⟩).
• 1 with probability 1/4, and the state collapses to
√
3
2 |10⟩+ 1
2|11⟩= |1⟩
 √
3
2 |0⟩+ 1
2|1⟩
!
.
We see that measuring the left qubit does affect the right qubit because in one case,
the right qubit collapses to (|0⟩+ |1⟩)/
√
2, while in the other case, the right qubit
collapses to (
√
3|0⟩+ |1⟩)/2. So, there is entanglement. Next, if we measure the
right qubit, we may get 0 or 1, with probabilities 50:50 or 0.75:0.25, depending on
which state the qubit was in. So, even though measuring the left qubit affected the
right qubit, it did not completely determine what a measurement of the right qubit
would yield. Since we do not know exactly what the right qubit will be, the original
state is not maximally entangled. We say it is partially entangled.
Various ways to quantify the amount of entanglement have been proposed, called
entanglement measures. They all agree that product states have no entanglement,
and they largely agree on which states are maximally entangled. They disagree on
the degree to which partially entangled states are entangled. This is an area of active
research and is beyond the scope of this textbook.
Exercise 6.1. Using the techniques from Section 4.3, show that
√
3
2
√
2
|00⟩+
√
3
2
√
2
|01⟩+
√
3
4 |10⟩+ 1
4|11⟩
cannot be factored into the tensor product of single-qubit states.
Exercise 6.2. Consider the following state of two qubits:
√
3
2
√
2
|00⟩+
1
2
√
2
|01⟩+
1
2
√
2
|10⟩+
√
3
2
√
2
|11⟩
If you measure the left qubit, what outcomes can you get, what are the corresponding probabilities
of those outcomes, and what does the state collapse to for each outcome? Is this state a product
state, partially entangled state, or maximally entangled state?
Exercise 6.3. Consider the following state of two qubits:
1
√
2
(|01⟩+|10⟩).
If you measure the left qubit, what outcomes can you get, what are the corresponding probabilities
of those outcomes, and what does the state collapse to for each outcome? Is this state a product
state, partially entangled state, or maximally entangled state?
240
6 Entanglement and Quantum Protocols
6.2 Bell Inequalities
6.2.1 EPR Paradox and Local Hidden Variables
In 1935, Einstein, Podolsky, and Rosen (EPR) published a paper where they took
entangled measurements a step further. Einstein is famous for his Theory of Relativ-
ity, among other breakthroughs, and one idea that he pioneered was that the speed
of light acted as a universal speed limit, that nothing can travel through space faster
than light. This notion that no influence can propagate faster than light is called
locality. The EPR paradox uses locality to question the meaning of quantum me-
chanics.
As a modern take on the EPR experiment, we take two qubits and prepare them
in the |Φ+⟩state (such as by starting with |00⟩, applying H ⊗I to get |+⟩|0⟩, and
then applying CNOT to get |Φ+⟩). Now, we separate the two qubits so that two
scientists, conventionally named Alice and Bob (A and B), each have one qubit:
1
√
2 (|00⟩+ |11⟩)
Alice
Bob
Now, say Alice measures her qubit, and then Bob measures his qubit so quickly
after Alice’s measurement that any influence from Alice’s qubit would have to travel
faster than light in order to affect Bob’s qubit. Alice and Bob can be sufficiently far
apart to make this feasible.
According to the laws of quantum mechanics, whenever Alice measures |0⟩, Bob
should also measure |0⟩, since the moment Alice measures |0⟩, the state of the qubits
is now |00⟩. The same goes for |1⟩and |1⟩. EPR proposed that for this to be true,
either Alice’s measurement was able to influence Bob’s measurement faster than
light, or the measurement outcomes were predetermined by some hidden variable
that quantum mechanics did not account for, meaning quantum mechanics was in-
complete. EPR’s proposal became known as the EPR paradox.
EPR considered the former option absurd, and Einstein even called the faster-
than-light collapse of the quantum state “spooky action-at-a-distance” to deride it.
EPR advocated the latter option, that the measurement outcomes are determined
before the measurement in a way that quantum mechanics does not account for. This
belief is called realism, that the qubits have actual, real values before measurement.
Note EPR did not argue against quantum mechanics’ correctness, just its com-
pleteness. That is, they did agree that whenever Alice measured |0⟩, Bob would
also measure |0⟩, and the same goes for |1⟩. But they argued that a local hidden
variable theory must exist that explains these outcomes assuming locality, and this
more complete theory would replace quantum mechanics.
The hardware necessary to do this experiment was not available at the time, and
would not be for nearly fifty years in the early 1980’s, long after Einstein’s death in
1955. Even if scientists were able to do the experiment at the time, it would not have
resolved the paradox. If scientists did the experiment and Alice and Bob’s qubits
agreed as predicted by quantum mechanics, it would show that quantum mechanics
6.2 Bell Inequalities
241
was correct, but we would still not know if it meant that Alice’s qubit could influence
Bob’s qubit faster than light, or if there is a hidden variable not accounted for by
quantum mechanics that determines the measurement outcomes. If scientists did
the experiment and Alice and Bob’s qubits disagreed from the the predictions of
quantum mechanics, then it would mean that quantum mechanics was simply wrong,
not incomplete.
Almost thirty years into the EPR paradox, however, an important result was
found that would allow scientists to determine which was right, quantum mechanics
or local realism.
Exercise 6.4. Watch “Misconceptions About the Universe” by Veritasium at https://www.yo
utube.com/watch?v=XBr4GkRnY04. Fill in the blanks:
(a) “There was a time when the universe was expanding so rapidly that parts of it were mov-
ing apart from each other faster than the speed of light. That time is
.”
(b) “Relativity says nothing can move through
faster than light, but that
doesn’t stop
itself from expanding however it likes.”
6.2.2 Bell Inequalities and the CHSH Inequality
In 1964, John Stewart Bell proved an important result that would further distinguish
quantum mechanics from the local hidden variable theories that EPR, and others,
proposed. Bell calculated the measurement statistics of quantum mechanics for a
general problem. Then, he calculated the measurement statistics of any local hidden
variable theory and showed that they must obey an inequality. The amazing result is
that the quantum mechanical statistics disobeyed or violated the inequality, giving a
way to experimentally determine whether nature followed quantum mechanics or a
local hidden variable theory. If the experiment agreed with quantum mechanics, then
nature is not described by any local hidden variable theory, and if the experiment
agreed with the inequality, the quantum mechanics is not simply incomplete, it is
wrong, because it made an incorrect prediction. An experiment that tests this is
called a Bell test.
The general result that Bell proved is beyond the scope of this textbook, but we
will focus on a particular Bell inequality proposed by Clauser, Horne, Shimony, and
Holt (CHSH). In this experiment, Alice and Bob each have one qubit in the |Φ+⟩
state:
1
√
2 (|00⟩+ |11⟩)
Alice
Bob
Alice will either measure her qubit in the Z-basis A = {|0⟩,|1⟩} or the X-basis A′ =
{|+⟩,|−⟩}. Of course, measuring A is just a typical measurement. Measuring in the
X-basis can be done by first applying H, which turns |+⟩to |0⟩and |−⟩to |1⟩, and
then measuring in the Z-basis:
242
6 Entanglement and Quantum Protocols
A
A′
H
Bob will measure his qubit in one of two bases. The first is
B =
(
1
p
4+2
√
2
h
(1+
√
2)|0⟩+|1⟩
i
,
1
p
4−2
√
2
h
(1−
√
2)|0⟩+|1⟩
i)
,
and they appear on the Bloch sphere on the x+z-axis:
x
y
z
x + z
•
1
p
4 + 2
√
2
h
(1 +
√
2)|0⟩+ |1⟩
i
•
1
p
4 −2
√
2
h
(1 −
√
2)|0⟩+ |1⟩
i
The other basis for Bob is
B′ =
(
1
p
4+2
√
2
h
(−1−
√
2)|0⟩+|1⟩
i
,
1
p
4−2
√
2
h
(−1+
√
2)|0⟩+|1⟩
i)
,
and they appear on the Bloch sphere on the −x+z-axis:
x
y
z−x + z
•
1
p
4 + 2
√
2
h
(−1 −
√
2)|0⟩+ |1⟩
i
•
1
p
4 −2
√
2
h
(−1 +
√
2)|0⟩+ |1⟩
i
To measure in these bases, we apply the following gates, and then measure in the
Z-basis:
6.2 Bell Inequalities
243
B
S
H
T
H
B′
S
H
T †
H
These circuits convert the basis states to |0⟩and |1⟩, and then we measure in the
Z-basis (see Exercise 6.5).
Exercise 6.5. Consider Bob’s two measurement bases, B and B′:
B =
(
1
p
4+2
√
2
h
(1+
√
2)|0⟩+|1⟩
i
,
1
p
4−2
√
2
h
(1−
√
2)|0⟩+|1⟩
i)
.
B′ =
(
1
p
4+2
√
2
h
(−1−
√
2)|0⟩+|1⟩
i
,
1
p
4−2
√
2
h
(−1+
√
2)|0⟩+|1⟩
i)
,
(a) Show that the following quantum circuit converts the basis states of B to |0⟩and |1⟩, up to a
global phase.
B
S
H
T
H
(b) Show that the following quantum circuit converts the basis states of B′ to |0⟩and |1⟩, up to a
global phase.
B′
S
H
T †
H
You can do these calculations by hand if you would like, but I recommend using a computer algebra
system.
So, there are four combinations of bases, AB, AB′, A′B, and A′B′. For each of
these four bases, Alice and Bob run the experiment many times, and they get some
probability distribution for them both getting 0, getting 0 and 1, getting 1 and 0, and
both getting 1:
P00,
P01,
P10,
P11.
Now Alice and Bob interpret their measurement results as +1 and −1. So, if Alice
measures |0⟩, she records it as +1, and when she gets |1⟩, she records it as −1. Bob
does the same thing. If we multiply their measurement results, then |00⟩would be
(1)(1) = 1, |01⟩would be (0)(−1) = −1, |10⟩would be (−1)(0) = −1, and |11⟩
would be (−1)(−1) = 1. Then, the average outcome are these values multiplied by
their probabilites, added together:
E(A,B) = (1)P00 +(−1)P01 +(−1)P10 +(1)P11
= P00 −P01 −P10 +P11.
This is called the quantum correlation, and it is simply the average or expected
value of the product of their measurement results. Finally, consider the following
quantity that comes from adding/subtracting the quantum correlations for the four
measurement bases:
S = E(A,B)+E(A,B′)+E(A′,B)−E(A′,B′).
244
6 Entanglement and Quantum Protocols
The CHSH inequality is a Bell inequality that says for any local hidden variable
theory,
|S| ≤2.
The derivation of this is beyond the scope of this textbook. We can calculate, how-
ever, what quantum mechanics predicts S should be. First, if Alice measures in the
Z-basis A and Bob measures in the basis B, then the circuit we apply is
S
H
T
H





|Φ+⟩
Then, the state at the end of the circuit, before the measurements, is
(I ⊗H)(I ⊗T)(I ⊗H)(I ⊗S)
Φ+
.
We could calculate this by hand, but Mathematica or SageMath might be easier:
• Using Mathematica,
PhiPlus = {{1/Sqrt[2]}, {0}, {0}, {1/Sqrt[2]}};
H = 1/Sqrt[2] {{1, 1}, {1, -1}};
S = {{1, 0}, {0, Eˆ(I Pi/2)}};
T = {{1, 0}, {0, Eˆ(I Pi/4)}};
Eye = IdentityMatrix[2];
KroneckerProduct[Eye, H] . KroneckerProduct[Eye, T] .
KroneckerProduct[Eye, H] . KroneckerProduct[Eye, S] .
,→PhiPlus
The output is
1+eiπ/4
2
√
2
|00⟩+ 1−eiπ/4
2
√
2
|01⟩+i1−eiπ/4
2
√
2
|10⟩+i1+eiπ/4
2
√
2
|11⟩.
• Using SageMath,
sage: PhiPlus = vector([1/sqrt(2), 0, 0, 1/sqrt(2)]).column()
sage: H = 1/sqrt(2) * Matrix([[1,1],[1,-1]])
sage: S = Matrix([[1,0],[0,eˆ(i*pi/2)]])
sage: T = Matrix([[1,0],[0,eˆ(i*pi/4)]])
sage: Eye = Matrix([[1,0],[0,1]])
sage: Eye.tensor_product(H) * Eye.tensor_product(T) * Eye.
,→tensor_product(H)
....: * Eye.tensor_product(S) * PhiPlus
[
-1/8*sqrt(2)*(-(I + 1)*sqrt(2) - 2)]
[
-1/8*sqrt(2)*((I + 1)*sqrt(2) - 2)]
[ -1/8*sqrt(2)*((I - 1)*sqrt(2) - 2*I)]
[-1/8*sqrt(2)*(-(I - 1)*sqrt(2) - 2*I)]
Since (1+i)/
√
2 = eiπ/4, the output simplifies to
1+eiπ/4
2
√
2
|00⟩+ 1−eiπ/4
2
√
2
|01⟩+i1−eiπ/4
2
√
2
|10⟩+i1+eiπ/4
2
√
2
|11⟩.
6.2 Bell Inequalities
245
Then, the probability of getting each state is
P00 =

1+eiπ/4
2
√
2

2
= 2+
√
2
8
,
P01 =

1−eiπ/4
2
√
2

2
= 2−
√
2
8
,
P10 =
i1−eiπ/4
2
√
2

2
= 2−
√
2
8
,
P11 =
i1+eiπ/4
2
√
2

2
= 2+
√
2
8
.
Then, the quantum correlation is
E(A,B) = P00 −P01 −P10 +P11
= 2+
√
2
8
−2−
√
2
8
−2−
√
2
8
+ 2+
√
2
8
= 4
√
2
8
= 1
√
2
.
Similarly, if we calculate the quantum correlation for the other measurement bases,
we would get
E(A,B′) = 1
√
2
,
E(A′,B) = 1
√
2
,
E(A′,B′) = −1
√
2
.
Note the negative sign for E(A′,B′).
Exercise 6.6. Alice and Bob share an entangled pair of qubits in the |Φ+⟩=
1
√
2 (|00⟩+|11⟩) state.
If Alice measures her qubit in the X-basis A′ = {|+⟩,|−⟩} and Bob measures his qubit in the basis
B′ =
(
1
p
4+2
√
2
h
(−1−
√
2)|0⟩+|1⟩
i
,
1
p
4−2
√
2
h
(−1+
√
2)|0⟩+|1⟩
i)
,
find
(a) P00.
(b) P01.
(c) P10.
(d) P11.
(e) E(A′,B′) = P00 −P01 −P10 +P11.
You can do these calculations by hand if you would like, but I recommend using a linear algebra
on a computer, like Mathematica.
Then,
S = E(A,B)+E(A,B′)+E(A′,B)−E(A′,B′) = 4
√
2
= 2
√
2 ≈2.83.
246
6 Entanglement and Quantum Protocols
Thus, we see that quantum mechanics violates the CHSH inequality, since quantum
mechanics predicts S = 2.83, while any local hidden variable theory predicts |S| < 2.
Now, the true value of S is a matter of experiment. If we get a value of S greater than
2, then local hidden variable theories are wrong. If we get a value less that 2, then
quantum mechanics is not only incomplete, but wrong because it made an incorrect
prediction.
Tsirelson proved that the value for S that we just calculated is the maximum
amount that S can be using quantum mechanics, i.e.,
|S| ≤2
√
2.
This inequality is known as Tsirelson’s inequality, and it is the maximum amount
that the CHSH inequality can be violated by quantum mechanics.
6.2.3 Quantum Processor Experiment
Since we have access to IBM’s quantum processors, let us try running the CHSH
experiment to see if we get a value of |S| less than or greater than 2.
We need to run four experiments, one for each of the measurement bases AB,
AB′, A′B, and A′B′. Beginning with AB:
In this circuit, we have two qubits that are initially |00⟩. We apply H ⊗I to get |+0⟩
and then CNOT to get |Φ+⟩. Now Alice measures in the Z-basis A, and Bob mea-
sures in the B basis. We are not interested in the simulation because the simulation
is simply following the laws of quantum mechanics, and we want to test if quantum
mechanics is actually correct experimentally or not. Running this on ibmq athens,
we get the following results:
6.2 Bell Inequalities
247
Note you also ran this circuit on a quantum processor in Exercise 5.5.
Next, for AB′, Alice again measures in the Z-basis, but now Bob measures in the
B′ basis:
Executing this on ibmq athens,
Third, for A′B, Alice now measures in the X-basis, and Bob measures in the B
basis:
Executing this on ibmq athens,
248
6 Entanglement and Quantum Protocols
Finally, for A′B′, Alice measures in the X-basis, and Bob measures in the B′ basis:
Executing this on ibmq athens,
Let us put all these probabilities in a table:
Basis
P00
P01
P10
P11
E
AB 0.40820 0.08203 0.08398 0.42578 0.66797
AB′ 0.44727 0.08984 0.07422 0.38867 0.67188
A′B 0.39453 0.09375 0.09180 0.41992 0.62890
A′B′ 0.09180 0.40820 0.41797 0.08203 -0.65234
Then, our quantity of interest is
S = E(A,B)+E(A,B′)+E(A′,B)−E(A′,B′)
= 0.66797+0.67188+0.62890−(−0.65234)
= 2.62109.
Since this is greater than 2, quantum mechanics is right. Nature is not described by
a local hidden variable theory.
6.2 Bell Inequalities
249
6.2.4 Other Experiments
Our previous experiment using a quantum processor is not very precise, however.
We do not know how much time occurred between the measurements of the two
qubits, so we have not guaranteed that we measured them so closely that an influ-
ence would need to travel faster than light. Far more precise experiments, not using
quantum processors, were performed in the early 1980’s by Alain Aspect and others
(see Exercise 6.7), and Alice and Bob’s qubits were sufficiently far apart that the
collapse of one would need to travel faster than light to affect Bob’s qubit. They
showed that the Bell inequalities were indeed violated, meaning the universe is not
locally realistic (i.e., not described by local hidden variable theories). Since then,
even more experiments have been done that have closed various “loopholes” that
skeptics have raised (see Exercise 6.8).
Exercise 6.7. Go to https://doi.org/10.1103/PhysRevLett.49.91 and download
the PDF.
(a) From the abstract, the scientists measured pairs of what?
(b) From Eq. (2), Bell’s inequalities say that for “realistic local theories,” the quantity S must be
between what two values?
(c) From Eq. (6), quantum mechanics predicts what value of S for the experiment?
(d) From Eq. (5), what value of S did they actually measure?
(e) Did their measured value for S in (d) agree or disagree with Bell’s inequalitiy for realistic
local theories in (b)?
Exercise 6.8. Go to https://doi.org/10.1038/nature15759, which is a paper on the
first “loophole-free” Bell test. From the abstract, answer the following questions.
(a) The scientists used the spins of what particle to perform their Bell test?
(b) What is a loophole that their experiment closed/addressed?
(c) How far apart were their spins?
(d) Which Bell inequality did they test, and what bound on S does it place?
(e) What observed value of S did they measure?
(f) What p-value did they obtain? That is, what is the probability that a local hidden variable
theory could produce the data they observed?
6.2.5 No-Signaling Principle
From the above experiments, the collapse of an entangled state occurs faster than
light. While this seems to violate a fundamental tenant of physics, that nothing can
travel through space faster than light, it does not permit information to be transmitted
faster than light. So, while the universe permits “spooky action at a distance,” it does
not permit “spooky communication at a distance.”
Say Alice and Bob share seven maximally entangled pairs of qubits, with each
pair in the state |Φ+⟩. Alice is on Earth, and Bob travels to another galaxy with his
qubits.
250
6 Entanglement and Quantum Protocols
1
√
2 (|00⟩+ |11⟩)
1
√
2 (|00⟩+ |11⟩)
1
√
2 (|00⟩+ |11⟩)
1
√
2 (|00⟩+ |11⟩)
Alice
Bob
1
√
2 (|00⟩+ |11⟩)
1
√
2 (|00⟩+ |11⟩)
1
√
2 (|00⟩+ |11⟩)
Alice wants to send the ASCII letter “Q” (i.e., the bit string 1010001) to Bob. Clas-
sically, Alice would need to physically send the bits, such as via radio waves trans-
mitted through space, or by writing a letter and sending it on a spaceship. Either
way, her message (the letter “Q”) cannot travel faster than light. But quantumly, can
they use their entangled qubits to communicate faster than light?
What might Alice try doing? If she measures each of her seven qubits, she gets
|0⟩or |1⟩for each qubit, and Bob’s qubits would match hers. But since she gets
each value with probability 1/2, Alice and Bob get matching random strings, not
1010001 that encodes the letter Q.
In hopes of getting something less random, suppose Alice measures a qubit in
the Z-basis {|0⟩,|1⟩} to try to communicate 0, and in the X-basis {|+⟩,|−⟩} to try
to communicate 1. So to send 1010001, Alice would measure her first qubit in the
X-basis, her second qubit in the Z-basis, her third qubit in the X-basis, her fourth
through sixth qubits in the Z-basis, and her seventh qubit in the X-basis. When
she measures in the Z-basis, she gets |0⟩or |1⟩, each with probability 1/2. If she
measures in the X-basis, then we can show that (see Exercise 6.9):
Φ+
= 1
√
2
(|++⟩+|−−⟩),
and so she gets |+⟩or |−⟩, each with probability 1/2. Bob’s qubits match Alice’s
exactly. To determine the message, Bob needs to determine which basis the qubits
were measured in. But there is no way for him to do that. If he measures in the same
basis as Alice, he gets the same result as Alice. But if he measures in the wrong basis
(X when Alice used Z, or Z when Alice used X), he gets an answer with probability
6.2 Bell Inequalities
251
1/2. Either way, he does not know if the answer he got was correct or random unless
he calls Alice on the phone and asks her, which is a classical communication bound
by the speed of light.
In fact, no matter what Alice and Bob try, they cannot use entanglement to com-
municate faster than light. This is called the no-signaling principle, and it is a state-
ment of no faster-than-light communication or no superluminal communication.
Exercise 6.9. Show that |Φ+⟩=
1
√
2 (|00⟩+|11⟩), in the X-basis, is
1
√
2
(|++⟩+|−−⟩).
6.2.6 Other Theories
The Bell tests show that nature is not locally realistic. Whether locality is wrong,
realism is wrong, or both, however, is still a matter of debate.
Our explanation of quantum computing and quantum mechanics, that quantum
states are superpositions and measurement collapses the state, is known as the
Copenhagen interpretation. It was formulated by Niels Bohr and Werner Heisen-
berg, two of the “fathers” of quantum mechanics, in the 1920’s, and is named after
Copenhagen, Denmark because that is where they worked. It is the most popular
interpretation of quantum mechanics. Following this interpretation, the EPR para-
dox would be resolved by the understanding that quantum states are superpositions
and do not take “real” values until measurement, and the collapse occurs instanta-
neously, although information cannot be communicated faster than light.
It is not the only interpretation, however, and others explain the EPR paradox
differently:
• Some physicists theorize that entangled particles are connected through worm-
holes or Einstein-Rosen bridges. Then, the collapse of entangled states does not
occur faster than light because these bridges provide shortcuts across spacetime.
• In pilot wave theory or de Broglie-Bohm theory or Bohmian mechanics, an ac-
tual, real particle interacts with a wave that guides it, and the wave evolves by
the laws of quantum mechanics. This maintains realism but abandons locality.
From the Bell tests, our universe is not described by a local hidden variable
theory, but that does not exclude the possibility that nature obeys a non-local
hidden variable theory. Pilot wave theory is a non-local hidden variable theory.
• In the many-worlds interpretation, quantum states do not collapse. Rather, when
a measurement is made, parallel universes are created, one where each possible
outcome occurred. Both outcomes are equally real in each universe. This is local
and realistic.
Exercise 6.10. Watch “Quantum Entanglement and the Great Bohr-Einstein Debate” by PBS
Spacetime at https://www.youtube.com/watch?v=tafGL02EUOA. Fill in the blanks.
252
6 Entanglement and Quantum Protocols
(a) “This notion that the universe exists independent of the mind of the observer is called
in physics.”
(b) “Niels Bohr insisted that it was meaningless to assign reality to the universe in the absence of
observation; in the intervals between measurements, quantum systems truly exist as a fuzzy
mixture of all possible properties, what we call a
of states.”
(c) “Albert Einstein insisted on an objective reality, a reality independent of our observation of it.
He insisted that the wave function, and by extension quantum mechanics, is
.
There must exist what we call
that reflect a more
physical underlying reality.”
(d) “[EPR] proposed a quantum scenario that showed in order to abandon the assumption of
realism, you also had to to abandon a concept almost as sacred—
.
is the idea that each bit of the universe only acts on its immediate sur-
roundings. This is fundamental to Einstein’s relativity, which tells us that the chain of cause
and effect can’t propagate any faster than the
.”
(e) “Quantum mechanics requires that we describe the particle pair with a single combined
wave function that encompasses all possible states of both particles. We call such particles
an
. Now, according to the Copenhagen interpre-
tation, any measurement of one particle automatically collapses the entire entangled wave
function and so affects the results of measurements of other particle. That’s an influence that
could theoretically be transmitted
across any distance, and even back in
time, violating locality and possibly violating causality. Einstein et al. thought this very silly.
They thought that every special point in the universe must be real and physical and defined
by knowable quantities,
that
could affect each other no faster than the speed of light.”
(f) “Measurement
the alignment of the measured particle.”
(g) “Scenario one, if Einstein was right, imagine the response of each particle to all pos-
sible spin measurements is encoded in each particle at the moment of their creation as
local to each particle.
we
do later to one particle will then effect the other. When we later measure the spins of both
particles, there will be a correlation in the results because the particles were once connected.
But there’ll be
due to our choice of measurement
axis.”
(h) “Scenario two—Bohr was right. What if between creating and measurement, the electron and
positron only exist as a wave function of all possible states. In that case, measurement of one
particle spin should cause the entire wave function to collapse, to take on
. Both particles should then manifest opposite spins along whichever
axis we choose for one of the particles. That should lead to a correlation between our choice
of measurement axis for the first particle and the spin of direction then measured for the
second. This is exactly the
at a distance that made
Einstein so uncomfortable.”
(i) “John Steward Bell figured out a set of observable results, the so-called
, that we expect to see in the case that Einstein was right and quantum
mechanics needs local hidden variables. But if an entanglement experiment violates the Bell
inequalities, then local realism is also violated.”
(j) “But in the early ’80s, French physicist Alain Aspect succeeded. Instead of looking at the en-
tangled spins of an electron-positron pair, he used photon pairs with entangled polarizations.
[...] And Aspect showed that there was a correlation between the choice of polarization mea-
surement axis for one photon and the final polarization direction and its entangled partner.
The Bell inequalities were
. The experiment was even set up so that the
influence had to travel between the photons at
the
speed of light.”
(k) “It’s now been thoroughly confirmed that the Bell inequalities are violated, suggesting that the
wave function cannot have
.”
6.3 Monogamy of Entanglement
253
(l) “The results of these entanglement experiments do seem to violate local realism. But that may
mean a violation of
, or just of
.”
(m) “Non-locality requires that entangled particles affect each other instantaneously. That sounds
blasphemous to anyone who accepts Einstein’s theory of relativity. However, non-locality and
relativity can actually be perfectly
. Relativity requires that causality is
preserved, so no faster than light
flow. But none of these entanglement
experiments allow any real information to be transmitted between particles. It’s only possible
to see the influence between the entangled partners after measurements have been made and
those measurements are compared.”
(n) “The
interpretation remains consistent with all quantum observations.”
6.3 Monogamy of Entanglement
6.3.1 Classical Correlations
Classical correlations can be shared among multiple parties. For example, say there
are three people, Alice, Bob, and Charlie, and they each have a bit. Say Alice and
Bob’s bits are perfectly correlated: if Alice’s bit is 0, then Bob’s bit is also 0, and if
Alice’s bit is 1, then Bob’s bit is also 1:
Alice Bob
0
0
1
1
Classically, it is possible for Charlie’s bit to be perfectly correlated with Alice’s bit
as well:
Alice Bob Charlie
0
0
0
1
1
1
This satisfies two properties. First, if we remove Charlie, we retain perfect corre-
lation between Alice and Bob. Second, if we remove Bob, Alice and Charlie have
the same correlation as Alice did with Bob. Put another way, if we swap Bob and
Charlie, we get the same distribution. Together, Alice is perfectly correlated with
both Bob and Charlie.
6.3.2 Quantum Entanglement
Is this possible for quantum entanglement? That is, if Alice, Bob, and Charlie have
qubits, is it possible for them to satisfy the two properties above? Say Alice and
Bob’s qubits are in the maximally entangled state
Φ+
= 1
√
2
(|0⟩⊗|0⟩+|1⟩⊗|1⟩).
254
6 Entanglement and Quantum Protocols
We want to add Charlie’s qubit |ψc⟩so that if we remove him, Alice and Bob’s
qubits are still in the |Φ+⟩state. Then the three-qubit state must look like
1
√
2
(|0⟩⊗|0⟩+|1⟩⊗|1⟩)⊗|ψc⟩.
But now the second property is not satisfied—swapping Bob and Charlie yields a
different state. Alice is not entangled with them to the same degree. In fact, Alice is
maximally entangled with Bob, but she is not entangled at all with Charlie!
From this result, we say that entanglement is monogamous, like a monogamous
relationship between people is exclusive. Two people perfectly “entangled” with
each other (Alice and Bob) are not entangled at all with a third party (Charlie).
Finally, note if Alice and Bob are partially entangled, then it is possible for there
to be some entanglement with Charlie. A proper treatment of this involves express-
ing quantum states, not as kets or vectors, but as density matrices, which also allow
probabilistic mixtures of kets, but that is beyond the scope of this textbook.
Exercise 6.11. The Greenberger–Horne–Zeilinger state (GHZ state) is an entangled state of three-
qubits. It is:
1
√
2
(|000⟩+|111⟩).
(a) Verify that the GHZ state is produced by the following quantum circuit:
|0⟩
|0⟩
•
|0⟩
H
•
(b) If we measure the left qubit, what are the possible resulting states and with what probabilities?
(c) Are the resulting states after the measurement entangled?
Exercise 6.12. The W state is an entangled state of three qubits. It is:
|W⟩= 1
√
3 (|001⟩+|010⟩+|100⟩).
(a) Verify that the W state is produced by the following quantum circuit:
|0⟩
X
|0⟩
H
|0⟩
U
where
U =


q
2
3 −1
√
3
1
√
3
q
2
3

.
Also, as a reminder, the hollow circle is anti-control. So the second gate applies the Hadamard
gate to the middle qubit if the bottom qubit is 0.
(b) If we measure the left qubit, what are the possible resulting states and with what probabilities?
(c) Are the resulting states after the measurement entangled?
6.4 Superdense Coding
255
6.4 Superdense Coding
6.4.1 The Problem
Alice wants to send classical information to Bob, say one of four possible restaurant
options: American, Chinese, Italian, or Mexican. She can either send her preference
using classical bits or qubits, and we will see that if Alice and Bob share entangle-
ment, Alice only needs to send half the number of qubits as she would bits.
6.4.2 Classical Solution
Classically, Alice would have to send two bits to Bob, since two bits have four
possible states 00, 01, 10, or 11. One bit would not suffice, since it only has two
states 0 or 1.
6.4.3 Quantum Solution
Quantumly, Alice can send just one qubit, but it needs to be entangled with a second
qubit that Bob already has. Say Alice and Bob share a pair of entangled qubits in
the |Φ+⟩state:
1
√
2 (|00⟩+ |11⟩)
Alice
Bob
Depending on which of the four options Alice wants to communicate to Bob, she
can apply quantum gates to her qubit, then send her one qubit to Bob so that Bob
ends up with both qubits:
• If Alice wants to send 00, she does nothing to her qubit, and sends it to Bob so
that he has both qubits.
• If Alice wants to send 01, she applies the X gate to her qubit, which transforms
|Φ+⟩to
Ψ +
= 1
√
2
(|10⟩+|01⟩).
Then she sends her qubit to Bob, so that he has both qubits.
• If Alice wants to send 10, she applies the Z gate to her qubit, which transforms
|Φ+⟩to
Φ−
= 1
√
2
(|00⟩−|11⟩).
Then she sends her qubit to Bob, so that he has both qubits.
256
6 Entanglement and Quantum Protocols
• Finally, if Alice wants to send 11, she applies both X and Z to her qubit. Apply-
ing X transforms |Φ+⟩to |Ψ +⟩, and appling Z transforms |Ψ +⟩to
Ψ −
= 1
√
2
(|01⟩−|10⟩).
Then Alice sends her qubit to Bob, so that he has both qubits.
Now Bob has both qubits, and they are in one of four states:
Φ+
= 1
√
2
(|00⟩+|11⟩),
Ψ +
= 1
√
2
(|01⟩+|10⟩),
Φ−
= 1
√
2
(|00⟩−|11⟩),
Ψ −
= 1
√
2
(|01⟩−|10⟩).
Since these four states are orthonormal, they form a measurement basis called the
Bell basis. Bob can measure the two qubits this Bell basis to distinguish them, thus
determining what Alice wanted to send. This is called a Bell measurement.
Another way to understand the Bell measurement is to apply CNOT and then
H ⊗I, then measuring in the Z-basis. That is,
Φ+ CNOT
−−−→1
√
2
(|00⟩+|10⟩) = |+⟩|0⟩H⊗I
−−→= |00⟩,
Ψ + CNOT
−−−→1
√
2
(|01⟩+|11⟩) = |+⟩|1⟩H⊗I
−−→= |01⟩,
Φ− CNOT
−−−→1
√
2
(|00⟩−|10⟩) = |−⟩|0⟩H⊗I
−−→= |10⟩,
Ψ − CNOT
−−−→1
√
2
(|01⟩−|11⟩) = |−⟩|1⟩H⊗I
−−→= |11⟩.
Computationally, this protocol still requires two qubits, as it must because
Holevo’s theorem says that n qubits can only store n bits of classical information.
Yet as a communication protocol, it only requires one qubit to be sent.
Generalizing this, if Alice and Bob share n pairs of entangled qubits (so there are
2n qubits total), then Alice can measure each of her n qubits or not depending on
what she wants to send, then send them to Bob.
Exercise 6.13. Verify that |Φ+⟩, |Ψ +⟩, |Φ−⟩, and |Ψ −⟩are orthonormal to each other. That is,
calculate ⟨Φ+|Φ−⟩, etc.
Exercise 6.14. Say Alice wants to send one of sixteen possible states to Bob.
(a) How many classical bits would Alice need to send to Bob?
(b) How many qubits would Alice need to send to Bob if they share entanglement?
6.5 Quantum Teleportation
257
(c) How many qubits total would it take, counting both Alice’s and Bob’s qubits?
Exercise 6.15. Consider the following quantum circuit that implements superdense coding.
|0⟩
|0⟩
H
•
X
Z
•
H
The bottom qubit is Alice’s, and the top qubit is Bob’s. In the first dotted square, Alice and Bob
create the maximally entangled state |Φ+⟩by applying H ⊗I and CNOT. Bob then goes away on a
trip. Alice wants to send Bob a message, so in the second dotted square, she applies X followed by
Z to her qubit. Then, she sends her qubit to Bob. Bob now has both qubits. To read Alice’s message,
in the third square, he applies CNOT followed by H ⊗I, and then measures in the Z-basis.
(a) What result should Bob get for his measurement?
(b) Run the circuit on an actual quantum processor using IBM Quantum Experience. Which pro-
cessor did you use? What histogram of results do you get?
(c) How would you modify the circuit so that Alice sends |01⟩to Bob?
6.5 Quantum Teleportation
6.5.1 The Problem
In the previous section on superdense coding, we used qubits to send classical in-
formation. In this section, we consider the opposite. We can only send bits, but we
want to send quantum information. In particular, Alice would like to send a qubit’s
unknown state |ψ⟩= α|0⟩+β|1⟩to Bob.
6.5.2 Classical Solution
Since Alice does not know the state |ψ⟩, she cannot describe it to Bob. If she mea-
sures the qubit, she will collapse it to |0⟩or |1⟩, and she will not know the original
superposition α|0⟩+β|1⟩. So, in general, Alice cannot tell Bob the state |ψ⟩.
If Alice had many qubits, each in the state |ψ⟩, then she could measure each
one, possibly in different bases, to get a sense of what |ψ⟩might be. This is called
quantum state tomography. The more qubits Alice has in the state |ψ⟩, the more
accurately she can determine |ψ⟩, and then she can tell Bob her best guess of the
state. In the problem we are trying to solve, however, Alice only has one qubit in
the state |ψ⟩, and from the no-cloning theorem, she cannot make extra copies.
Even if she did know the state, the amplitudes are in general complex numbers
that can take rational or irrational values, and the irrational values would take an
infinite number of bits to express. So, Alice would need to send Bob many bits to
describe the state of the qubit. Next, we will see how entanglement can be used so
that Alice can get Bob the state |ψ⟩by only telling him two bits.
258
6 Entanglement and Quantum Protocols
6.5.3 Quantum Solution
If Alice and Bob already share entanglement, they can use it to teleport the quantum
state. Say they share a pair of entangled qubits in the state |Φ+⟩. Alice has one of
these qubits, plus the one she wants to send |ψ⟩. Bob has the other entangled qubit.
|ψ⟩= α|0⟩+ β|1⟩
1
√
2 (|00⟩+ |11⟩)
Alice
Bob
Altogether, the three qubits are in the state
|ψ⟩
Φ+
= α|0⟩
Φ+
+β|1⟩
Φ+
= α|0⟩1
√
2
(|00⟩+|11⟩)+β|1⟩1
√
2
(|00⟩+|11⟩)
= 1
√
2
[α (|000⟩+|011⟩)+β (|100⟩+|111⟩)].
The left two qubits belong to Alice, and the right qubit belongs to Bob. First, Alice
applies a CNOT gate to her two qubits, resulting in the state
1
√
2
[α (|000⟩+|011⟩)+β (|110⟩+|101⟩)].
Next, she applies a Hadamard gate to her left qubit, yielding
1
√
2
[α (|+00⟩+|+11⟩)+β (|−10⟩+|−01⟩)]
= 1
2 [α (|0⟩+|1⟩)(|00⟩+|11⟩)+β (|0⟩−|1⟩)(|10⟩+|01⟩)]
= 1
2
h
|00⟩(α|0⟩+β|1⟩)+|01⟩(β|0⟩+α|1⟩)
+|10⟩(α|0⟩−β|1⟩)+|11⟩(−β|0⟩+α|1⟩)
i
.
Then, Alice measures her two qubits. She gets 00, 01, 10, 11, each with probability
1/4. So after the measurement, the possible states are
|00⟩(α|0⟩+β|1⟩),
|01⟩(β|0⟩+α|1⟩),
|10⟩(α|0⟩−β|1⟩),
|11⟩(−β|0⟩+α|1⟩).
Now Alice tells Bob the results of her measurement, which are two classical bits.
This is a classical communication. Bob uses this information to possibly apply quan-
tum gates to his qubit.
6.5 Quantum Teleportation
259
• If Alice’s measurement was 00, Bob does nothing because his qubit is now in
the state |ψ⟩that Alice wanted to send him.
• If Alice’s measurement was 01, then Bob applies an X gate to his qubit, trans-
forming it into |ψ⟩.
• If Alice’s measurement was 10, then Bob applies a Z gate to his qubit, trans-
forming it into |ψ⟩.
• Finally, if Alice’s qubit was 11, then Bob applies an X gate followed by a Z
gate, transforming it into |ψ⟩.
So Bob’s qubit is now in the state |ψ⟩, achieving the goal of transferring the state of
Alice’s qubit to Bob’s qubit. This is called quantum teleportation. Note that Alice’s
qubit was not physically transferred to Bob, only information about what state it
was in. In the process, Alice had to measure her qubit, destroying the quantum in-
formation. This is necessary because of the no-cloning theorem. Furthermore, even
though the state of Bob’s qubit changed instantly when Alice measured her qubits
and collapsed the state, this information was not useful until Alice told Bob the re-
sult of her measurement, which is a classical communication bounded by the speed
of light. So, quantum teleportation cannot be performed faster than light, which is
consistent with the no-signaling principle.
Quantum teleportation is implemented by the following circuit:
X
Z
|ψ⟩
•
|ψ⟩
•
H
•





|Φ+⟩
The top qubit is Bob’s, and the bottom two qubits are Alice’s. The bottom qubit
starts in the unknown state |ψ⟩, which Alice wants to teleport to Bob, and the top
two qubits start in the entangled state |Φ+⟩. Alice applies a CNOT to her qubits
followed by H on her bottom qubit. Then she measures her qubits, getting 00, 01, 10,
or 11. If her right qubit was 1, Bob applies X, and if her left qubit was 1, Bob applies
Z. These are controlled-X and controlled-Z gates. This completes the teleportation,
and Bob’s qubit ends up in the state |ψ⟩.
Using the principle of deferred measurement (introduced on page 194), we can
move the measurements after the controls and replace the classical controls with
quantum ones:
X
Z
|ψ⟩
•
|ψ⟩
•
H
•





|Φ+⟩
Simulating this in Quirk at https://bit.ly/3pn1hCj,
260
6 Entanglement and Quantum Protocols
This begins with Xt and Zt, which put the bottom qubit in some state |ψ⟩depicted
on the Bloch sphere. Then, we have the teleportation circuit, and at the end, the three
Bloch spheres show that the top qubit is in the state |ψ⟩, while the bottom qubit is
no longer in this state.
Exercise 6.16. Alice wants to teleport a qubit in an unknown state |ψ⟩= α|0⟩+ β|1⟩to Bob.
Instead of sharing two entangled qubits in the |Φ+⟩state, they share two entangled qubits in the
|Ψ +⟩state:
|ψ⟩= α|0⟩+ β|1⟩
1
√
2 (|01⟩+ |10⟩)
Alice
Bob
Altogether, the initial state of the system is
|ψ⟩
Ψ +
= (α|0⟩+β|1⟩) 1
√
2
(|01⟩+|10⟩)
= 1
√
2
(α|001⟩+α|010⟩+β|101⟩+β|110⟩).
So, the left two qubits are Alice’s, and the right qubit is Bob’s.
(a) Show that if Alice applies CNOT to her two qubits, followed by H to her left qubit, the state
of the system becomes
1
2

|00⟩(β|0⟩+α|1⟩)+|01⟩(α|0⟩+β|1⟩)
+|10⟩(−β|0⟩+α|1⟩)+|11⟩(α|0⟩−β|1⟩)

.
(b) Next, Alice measures both of her qubits. What values can she get, with what probabilities,
and what does the state collapse to in each case?
(c) Finally, Alice tells Bob the results of her measurement. For each possible result, what should
Bob do to his qubit so that is α|0⟩+β|1⟩, the state that Alice wanted to teleport to him?
Exercise 6.17. Alice wants to teleport a qubit in an unknown state |ψ⟩= α|0⟩+ β|1⟩to Bob.
Instead of sharing two entangled qubits in a Bell state, they share three entangled qubits in the
GHZ state:
|GHZ⟩= 1
√
2
(|000⟩+|111⟩).
The left two qubits are with Alice, and the right qubit is with Bob.
|ψ⟩= α|0⟩+ β|1⟩
1
√
2 (|000⟩+ |111⟩)
Alice
Bob
6.5 Quantum Teleportation
261
Altogether, the initial state of the system is
|ψ⟩|GHZ⟩= (α|0⟩+β|1⟩) 1
√
2
(|000⟩+|111⟩)
= 1
√
2
(α|0000⟩+α|0111⟩+β|1000⟩+β|1111⟩).
So, the left three qubits are Alice’s, and the right qubit is Bob’s.
(a) Show that if Alice applies CNOT21 (recall the qubits are numbered right-to-left starting with
zero), followed by CNOT32, followed by H ⊗I ⊗I ⊗I, the state of the system becomes
1
2

|000⟩(α|0⟩+β|1⟩)+|010⟩(β|0⟩+α|1⟩)
+|100⟩(α|0⟩−β|1⟩)+|110⟩(−β|0⟩+α|1⟩)

.
(b) Next, Alice measures all three of her qubits. What values can she get, with what probabilities,
and what does the state collapse to in each case?
(c) Finally, Alice tells Bob the results of her measurement. For each possible result, what should
Bob do to his qubit so that is α|0⟩+β|1⟩, the state that Alice wanted to teleport to him?
Exercise 6.18. Alice wants to teleport a qubit in an unknown state |ψ⟩= α|0⟩+ β|1⟩to Charlie,
and Bob is helping her. They share three entangled qubits in the GHZ state:
|GHZ⟩= 1
√
2
(|000⟩+|111⟩).
The left qubit is Alice’s, the middle qubit is Bob’s, and the right qubit is Charlie’s.
|ψ⟩= α|0⟩+ β|1⟩
1
√
2 (|000⟩+ |111⟩)
Alice
Bob
Charlie
Altogether, the initial state of the system is
|ψ⟩|GHZ⟩= (α|0⟩+β|1⟩) 1
√
2
(|000⟩+|111⟩)
= 1
√
2
(α|0000⟩+α|0111⟩+β|1000⟩+β|1111⟩).
So, the left two qubits are Alice’s, the second-to-right qubit is Bob’s, and the right qubit is Charlie’s.
(a) Show that if Alice applies CNOT to her qubits (so the far left qubit is the control and the
second-to-left qubit is the target) and then the Hadamard gate to her left qubit, the state of the
system becomes
1
2

|00⟩(α|00⟩+β|11⟩)+|01⟩(β|00⟩+α|11⟩)
+|10⟩(α|00⟩−β|11⟩)+|11⟩(−β|00⟩+α|11⟩)

.
(b) Next, Alice measures her two qubits and makes the results known. What values can she get,
with what probabilities, and what does the state collapse to in each case?
262
6 Entanglement and Quantum Protocols
(c) After Alice has completed the above, Bob applies the Hadamard gate to his qubit. Show that
the state of the system after Bob does this is the following four states, depending on the result
of Alice’s measurement:
1
√
2
|00⟩

|0⟩(α|0⟩+β|1⟩)+|1⟩(α|0⟩−β|1⟩)

,
1
√
2
|01⟩

|0⟩(β|0⟩+α|1⟩)+|1⟩(β|0⟩−α|1⟩)

,
1
√
2
|10⟩

|0⟩(α|0⟩+β|1⟩)+|1⟩(α|0⟩−β|1⟩)

,
1
√
2
|11⟩

|0⟩(−β|0⟩+α|1⟩)+|1⟩(−β|0⟩−α|1⟩)

.
(d) Then, Bob measures his qubit and makes his result known. For each of the above states, what
does the state collapse to? For each possible outcome, what quantum gate(s) should Charlie
apply to his qubit so that it is α|0⟩+β|1⟩, the state that Alice wanted to teleport to him?
6.6 Quantum Key Distribution
6.6.1 Encryption
Alice and Bob would like to send private messages to each other over the internet.
This means others can see the bits they send, but the meaning should be hidden from
everyone except Alice and Bob.
To do this, Alice and Bob need to have a secret key or code that only they know.
Using this secret key, they can encrypt their messages to each other.
For example, say Alice and Bob share a secret key of fourteen random bits that
only they know:
key = 11010110011011.
Alice wants to send “Hi” to Bob, which in ASCII is 1001000 1101001. This is called
the plaintext. If she sends these bits, everyone will know that the message is “Hi.”
So instead, she takes the XOR of each bit of the message with each bit of the secret
key, yielding the ciphertext
plaintext = 1001000 1101001
⊕
key = 1101011 0011011
ciphertext = 0100011 1110010
Now Alice sends this ciphertext to Bob over the internet. If someone intercepts it
along the way, like Eve the eavesdropper, then she will not be able to determine the
original plaintext since she does not have the secret key. Now Bob has the ciphertext,
and he takes the XOR of it with the secret key, which he knows:
6.6 Quantum Key Distribution
263
ciphertext = 0100011 1110010
⊕
key = 1101011 0011011
plaintext = 1001000 1101001
Bob decodes the plaintext as “Hi,” receiving the message.
This scheme is called a one-time pad, and it assumes that the secret key is only
used once and is random. If the secret key is used more than once, then Eve might
be able to discern the key, and if it is not random, Eve might be able to guess the
pattern. But as long as these assumptions are satisfied, it is information-theoretically
secure, meaning it is secure from a mathematical standpoint, but perhaps not secure
from someone breaking into Bob’s office and stealing the secret key.
Due to this information-theoretic security, the one-time pad is used for the most
critical of communications, such as the Moscow–Washington hotline that allows
direct, secure communication between Russia and the United States. This hotline
was developed during the Cold War, and is still used today. Since the secret keys
cannot just be sent over the internet for all to see, they have to be delivered in-person
to each country’s embassy.
For those who need strong security, but not at the level of a one-time pad, the
Advanced Encryption Standard (AES) can be used. The secret key can be shorter
than the message, and the ciphertext is created through a series of substitutions,
shifts, and mixes. Nevertheless, a secret key must be established.
Exercise 6.19. You are Bob, and you and Alice are communicating using a one-time pad. You
receive from Alice the ciphertext
1101010101001100000110010010110100110110011001100,
and you know the secret key
0101001011101111000101100000000010101100000101001.
What is the plaintext binary string, and what does it encode in ASCII?
6.6.2 Classical Solution: Public Key Cryptography
A classical way to send a secret message, such as to establish a secret key, is the RSA
cryptosystem, which is an acronym for its inventors Rivest, Shamir, and Adleman.1
It is an example of a public-key cryptosystem, where each user reveals some public
information while still maintaining some private (secret) information.
Say Alice wants to send a message to Bob. To do this securely, Bob prepares
some public and private information. The public information allows anyone to send
him encrypted messages. The private information allows Bob, and only Bob, to
1 Actually, RSA was invented a few years earlier by Clifford Cocks, a mathematician working for
a British intelligence agency, but his work was classified and not revealed until decades later.
264
6 Entanglement and Quantum Protocols
decrypt the message. To do this, Bob begins by choosing two distinct prime numbers
p and q, with some conditions that are beyond the scope of this textbook. A prime
number is any whole number greater than 1 whose only factors are 1 and itself.
Prime numbers can be listed using Mathematica or SageMath:
• In Mathematica, the first ten prime numbers can be listed using
Table[Prime[i], {i, 10}]
The output is
{2,3,5,7,11,13,17,19,23,29}.
• In SageMath, the prime numbers less than 30 can be listed using
sage: list(primes(30))
[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
We can also use Mathematica or SageMath to check if a number is prime.
• In Mathematica, we can check if a number is prime using the PrimeQ function,
where the “Q” stands for “query” meaning to ask. With this function, we are
asking if a particular number is prime. For example, consider
PrimeQ[2003]
The output of this is True, so 2003 is a prime number. As another example,
consider
PrimeQ[2005]
The output of this is False, so 2005 is not a prime number.
• In SageMath, we can check if a number is prime using the is prime function.
For example,
sage: is_prime(2003)
True
sage: is_prime(2005)
False
For example, say Bob chooses p = 17 and q = 41. Bob keeps these two numbers a
secret, but he does reveal their product. That is, bob computes n = pq = 17·41 = 697
and makes the number 697 known to Alice and the world. This product n is one of
Bob’s two public keys, and he publishes it so that Alice can use it to send him a
secret message. The length of n in bits is called the key length, and at the time of
this writing, the recommended key length of RSA is 2048-bits. For example, 697 in
binary is 1010111001, so we are using a key length of 10, which is much too short
in practice.
Bob then computes the product φ = (p−1)(q−1). Continuing our example, Bob
computes φ = 16·40 = 640. Using this, he then finds some integer e between 1 and
φ that is relatively prime or coprime to φ, meaning the greatest common divisor
of e and φ is 1, which we write as gcd(e,φ) = 1. Put another way, e and φ share
no common factors except 1. Note there is a fast method called Euclid’s algorithm
6.6 Quantum Key Distribution
265
from 300 BC/BCE for calculating the greatest common divisor of two integers, and
the number of steps is at most five times the number of digits of the smaller integer.
See Exercise 6.20 for a description of the algorithm. We can use Euclid’s algorithm
to find an e such that gcd(e,φ) = 1, or we can just use Mathematica or SageMath:
• In Mathematica, we can list all the numbers between 2 and 639, inclusive, that
are relatively prime to 640 using the following:
Table[If[GCD[i, 640] == 1, i, Nothing], {i, 2, 639}]
The output of this is
{3,7,9,11,13,17,19,21,23,27,29,31,33,37,39,41,43,47,49,51,53,57,59,
61,63,67,69,71,73,77,79,81,83,87,89,91,93,97,99,101,103,107,109,
111,113,117,119,121,123,127,129,131,133,137,139,141,143,147,149,
151,153,157,159,161,163,167,169,171,173,177,179,181,183,187,189,
191,193,197,199,201,203,207,209,211,213,217,219,221,223,227,229,
231,233,237,239,241,243,247,249,251,253,257,259,261,263,267,269,
271,273,277,279,281,283,287,289,291,293,297,299,301,303,307,309,
311,313,317,319,321,323,327,329,331,333,337,339,341,343,347,349,
351,353,357,359,361,363,367,369,371,373,377,379,381,383,387,389,
391,393,397,399,401,403,407,409,411,413,417,419,421,423,427,429,
431,433,437,439,441,443,447,449,451,453,457,459,461,463,467,469,
471,473,477,479,481,483,487,489,491,493,497,499,501,503,507,509,
511,513,517,519,521,523,527,529,531,533,537,539,541,543,547,549,
551,553,557,559,561,563,567,569,571,573,577,579,581,583,587,589,
591,593,597,599,601,603,607,609,611,613,617,619,621,623,627,629,
631,633,637,639}
• In SageMath, we can list all the numbers less than 640 that are relatively prime
to 640 using the coprime integers function:
sage: phi = 640
sage: phi.coprime_integers(phi)
[1, 3, 7, 9, 11, 13, 17, 19, 21, 23, 27, 29, 31, 33, 37, 39,
41, 43, 47, 49, 51, 53, 57, 59, 61, 63, 67, 69, 71, 73, 77,
79, 81, 83, 87, 89, 91, 93, 97, 99, 101, 103, 107, 109, 111,
113, 117, 119, 121, 123, 127, 129, 131, 133, 137, 139, 141,
143, 147, 149, 151, 153, 157, 159, 161, 163, 167, 169, 171,
173, 177, 179, 181, 183, 187, 189, 191, 193, 197, 199, 201,
203, 207, 209, 211, 213, 217, 219, 221, 223, 227, 229, 231,
233, 237, 239, 241, 243, 247, 249, 251, 253, 257, 259, 261,
263, 267, 269, 271, 273, 277, 279, 281, 283, 287, 289, 291,
293, 297, 299, 301, 303, 307, 309, 311, 313, 317, 319, 321,
323, 327, 329, 331, 333, 337, 339, 341, 343, 347, 349, 351,
353, 357, 359, 361, 363, 367, 369, 371, 373, 377, 379, 381,
383, 387, 389, 391, 393, 397, 399, 401, 403, 407, 409, 411,
266
6 Entanglement and Quantum Protocols
413, 417, 419, 421, 423, 427, 429, 431, 433, 437, 439, 441,
443, 447, 449, 451, 453, 457, 459, 461, 463, 467, 469, 471,
473, 477, 479, 481, 483, 487, 489, 491, 493, 497, 499, 501,
503, 507, 509, 511, 513, 517, 519, 521, 523, 527, 529, 531,
533, 537, 539, 541, 543, 547, 549, 551, 553, 557, 559, 561,
563, 567, 569, 571, 573, 577, 579, 581, 583, 587, 589, 591,
593, 597, 599, 601, 603, 607, 609, 611, 613, 617, 619, 621,
623, 627, 629, 631, 633, 637, 639]
Note we require e > 1, so we should ignore the first entry.
Bob can choose any of these numbers. Say he chooses e = 3. This number e is Bob’s
second public key, and he publishes it. It is called e because later, Alice will use it
as an exponent.
Finally, Bob computes d = e−1 mod φ, where mod refers to the modulus or re-
mainder when dividing by φ.2 For example, the 12-hour clock is modulo 12, since
7 +8 = 15 = 3 mod 12. Continuing our example, we can compute d using Mathe-
matica or SageMath:
• In Mathematica, the inverse of a number modulo some other number can be
found using
ModularInverse[3,640]
The output is 427.
• In SageMath, the inverse of a number modulo some other number can be found
using
sage: e = 3
sage: phi = 640
sage: e.inverse_mod(phi)
427
So, Bob computed d = 3−1 mod 640 = 427. Since d is the inverse of e modulo φ, if
we multiply e and d modulo φ, we should get 1, i.e., ed = 1 mod φ. The number d is
Bob’s private key. It will allow him to decrypt messages sent to him, so he keeps it
a secret. No one else knows d because even though Bob published e, one also needs
to know φ in order to calculate d. Bob may now throw away p and q, but keeping
it does allow him to decrypt a little faster using the Chinese remainder theorem, but
that is beyond the scope of this book.
Alice wants to send Bob a plaintext message that is encoded as a number M, such
that 0 < M < n. She finds Bob’s public keys n and e and computes the ciphertext
C = Me mod n, and then she sends C to Bob. For example, Alice wants to send
M = 104 to Bob, which in binary is 1101000. She can compute the ciphertext using
Mathematica or SageMath:
• In Mathematica,
PowerMod[104,3,697]
2 From number theory, the inverse of e mod φ exists precisely because e was chosen to be relatively
prime to φ.
6.6 Quantum Key Distribution
267
The output is 603.
• In SageMath,
sage: power_mod(104,3,697)
603
Thus, C = 1043 mod 697 = 603, and alice sends the number 603 to Bob.
When Bob receives the ciphertext C, he computes Cd mod n = (Me)d mod n =
Med mod n = M1 mod n = M, receiving the message.3 Continuing our example,
Bob computes 603427 mod 697. Computing this in Mathematica or SageMath,
• In Mathematica,
PowerMod[603,427,697]
The output is 104.
• In SageMath,
sage: power_mod(603,427,697)
104
So, 603427 mod 697 = 104, which is the message.
The sequence of steps in this example is summarized below:
Alice
Bob
Chooses primes p = 17, q = 41.
Calculates n = pq = 697.
Publishes n.
Computes φ = 16·40 = 640.
Chooses e = 3 since gcd(3,640) = 1.
Publishes e.
Computes d = 3−1 mod 640 = 427.
Chooses message M = 104.
Computes C = 1043 mod 697 = 603.
Alice Sends to C to Bob.
Computes 603427 mod 697 = 104 = M.
Decryption is hard for an eavesdropper, Eve, to do, because she does not know
the secret key d. She only knows Bob’s public keys, n and e. To find the secret key
d = e−1 mod φ, she needs to know φ = (p−1)(q−1), but this requires knowing p
and q, which involves factoring n. There is no known efficient, classical algorithm
for factoring large numbers, although a quantum computer can efficiently factor
numbers using Shor’s algorithm, which is the culminating algorithm in this text-
book. As described in Section 1.7.2 on complexity classes, this is evidence, but not
proof, that BQP is larger than P.
3 The plaintext and ciphertext are modulo n, but the exponents are modulo φ. It can be proved
using Fermat’s Little Theorem that with these moduli, Med mod n = M mod n, but the proof is
beyond the scope of this textbook.
268
6 Entanglement and Quantum Protocols
Exercise 6.20. Euclid’s algorithm is a method for finding the greatest common divisor of two
integers by converting the problem into finding the greatest common divisor of successively smaller
and smaller pairs of integers until we get one that is easy to find.
For example, say we want to find gcd(1122,422). We begin by dividing the larger number by
the smaller number. That is, 1122/442 is 2 with a remainder of 238, which we can write as
1122 = 2·442+238.
The claim is that the greatest common divisor of the dividend 1122 and divisor 442 is equal to
the greatest common divisor of the divisor 442 and the remainder 238, i.e., gcd(1122,442) =
gcd(442,238). This is because they have the same common divisors, including their greatest com-
mon divisor.4
Using this fact, let us instead find gcd(442,238). Dividing these numbers, 442/238 is 1 with a
remainder of 204, so we can write:
442 = 1·238+204.
Using the same fact from before, the greatest common divisor of the dividend and divisor is equal
to the greatest common divisor of the divisor and remainder, i.e., gcd(442,238) = gcd(238,204).
So, we instead find gcd(238,204). Again, we divide these numbers. Since 238/204 is 1 with a
remainder of 34, we write,
238 = 1·204+34.
Again using the fact, gcd(238,204) = gcd(204,34).
Now, we instead find gcd(204,34). Dividing, 204/34 is 6 with no remainder, so
204 = 6·34+0.
We again use the fact, gcd(204,34) = gcd(34,0).
Finally, we have gcd(34,0) = 34. This is because 34 is the largest integer that divides both 34
and 0 with no remainder. Since this is also the greatest common divisor of our original question,
we have
gcd(1122,442) = 34.
(a) Use Euclid’s algorithm to find gcd(51,57).
(b) Use Euclid’s algorithm to find gcd(34,39).
Exercise 6.21. You are Bob, and you and Alice are communicating using RSA cryptography. You
picked two prime numbers p = 59 and q = 127. Find
(a) Your public key n.
(b) A valid public key e.
(c) Your private key given your choice in part (b).
Exercise 6.22. You are Alice, and you and Bob are communicating using RSA cryptography. Bob
has secret keys n = 2035153 and e = 5. You want to send him a message M = 1234567 using
RSA. What ciphertext C do you send him?
Exercise 6.23. You are Bob, and you and Alice are communicating using RSA cryptography. Alice
sends you some ciphertext C = 1873198. Your public keys are n = 2035153 and e = 5, and your
secret key is d = 1219277. What is the plaintext message (number) that Alice sent?
4 As a proof, let us show that any divisor of 1122 and 422 is also a divisor of their remainder
238, and any divisor of 422 and 238 is also a divisor of 1122. First, say d is a common divisor of
1122 and 422. Then, d is is also a divisor of 1122 −2 · 442, since division is distributive. Since
238 = 1122 −2 · 442, d is also a divisor of 238. Going the other direction, say e is a common
divisor of 422 and 238. Then, e is also a divisor of 2 · 2 · 442 + 238, since division is distributive.
Since 2·442+238 = 1122, e is also a divisor of 1122. So, gcd(1122,442) = gcd(442,238).
6.6 Quantum Key Distribution
269
Exercise 6.24. In 1991, the RSA Factoring Challenge was created to encourage research into fac-
toring by giving values of n of increasing sizes and offering prize money for people who factored
them. Visit
https://en.wikipedia.org/wiki/RSA_Factoring_Challenge
to learn more about the challenge.
(a) How many bits long is the first RSA number, and when was it factored?
(b) What is the largest RSA number that has been factored? How many bits does it have? When
was it factored?
(c) If the RSA Factoring Challenge were still active, how much prize money would you earn for
factoring a 2048-bit long value of n?
6.6.3 Quantum Solution: BB84
A quantum method for establishing a shared secret key was introduced by Bennett
and Brassard in 1984, and it is called the BB84 protocol. It is an example of quantum
key distribution (QKD). Even though it does not use entanglement, it is a quantum
protocol, so it is included in this chapter. There do exist other protocols for QKD that
do utilize entanglement, such as E91, which is named after Ekert who discovered it
in 1991, but they are beyond the scope of this textbook.
In BB84, Alice begins with a bunch of random bits, and for each bit, she ran-
domly chooses either the Z-basis {|0⟩,|1⟩} or the X-basis {|+⟩,|−⟩}. For example,
Alice’s Bits
0 1 0 1 1 0 1 1 1
Alice’s Bases Z Z X Z X X X Z Z
If the bit she wants to send is a 0, and she picked the Z-basis, then she sends Bob
|0⟩, and if she picked the X-basis, then she sends Bob |+⟩. If Alice instead wants to
send the bit 1, and she picked the Z-basis, then she sends Bob |1⟩, and if she picked
the X-basis, then she sends Bob |−⟩. Continuing the example,
Alice’s Bits
0
1
0
1
1
0
1
1
1
Alice’s Bases Z
Z
X
Z
X
X
X
Z
Z
Alice Sends
|0⟩|1⟩|+⟩|1⟩|−⟩|+⟩|−⟩|1⟩|1⟩
Bob receives the qubits, and he randomly measures each one in either the Z-basis
or X-basis. If the basis he picked was the same as Alice’s, then he will get the
same result as Alice. If he picked the opposite basis, however, then he will get
each possible result with probability 1/2. For example, if Alice sends |0⟩and Bob
measures in the Z-basis, he is certain to get |0⟩. But if he measures in the X-basis,
he gets |+⟩with probability 1/2 or |−⟩with probability 1/2. He interprets |0⟩and
|+⟩as 0, and |1⟩and |−⟩as 1. Continuing the example,
270
6 Entanglement and Quantum Protocols
Alice’s Bits
0
1
0
1
1
0
1
1
1
Alice’s Bases
Z
Z
X
Z
X
X
X
Z
Z
Alice Sends
|0⟩|1⟩|+⟩|1⟩|−⟩|+⟩|−⟩|1⟩|1⟩
Bob’s Bases
Z
X
X
Z
Z
X
Z
X
Z
Bob’s Measurement |0⟩|−⟩|+⟩|1⟩|0⟩|+⟩|1⟩|+⟩|1⟩
Bob’s Bits
0
1
0
0
0
0
1
0
1
Now Alice and Bob call each other and openly (publicly) share what basis they
used for each measurement. If they used the same basis, then they know that their
measurement outcomes should agree, and they have a shared secret bit. If they used
different bases, then their measurement outcomes might agree or disagree, and they
discard these bits. Continuing the example.
Alice’s Bits
0
1
0
1
1
0
1
1
1
Alice’s Bases
Z
Z
X
Z
X
X
X
Z
Z
Alice Sends
|0⟩|1⟩|+⟩|1⟩|−⟩|+⟩|−⟩|1⟩|1⟩
Bob’s Bases
Z
X
X
Z
Z
X
Z
X
Z
Bob’s Measurement |0⟩|−⟩|+⟩|1⟩|0⟩|+⟩|1⟩|+⟩|1⟩
Bob’s Bits
0
1
0
0
0
0
1
0
1
Public Discussion of Basis
Shared Secret Key
0
0
1
0
1
So, their shared secret key is 00101.
To ensure Eve did not measure the qubits along the way, Alice and Bob can
reveal a fraction of their shared secret key and make sure they agree. For example,
if Alice and Bob want 256 bits in their shared secret key, they can generate 306 bits
using BB84, reveal 50 of them to ensure there was no eavesdropper, and then use
the remaining 256 for their shared secret key.
If Alice and Bob reveal 50 bits of their shared secret key, what is the probability
they will catch Eve, if Eve is measuring every qubit along the way? To answer this,
let us start with revealing one bit. Say Alice and Bob are revealing a bit where they
both used the Z-basis, and say Alice sent a qubit in the state |0⟩. Then, Alice will
reveal that her bit is 0, while Bob could reveal that his bit is 0 or 1, and we determine
the probabilities of these outcomes using the following diagram:
Alice
Z, |0⟩
Eve
Eve
Z, |0⟩
Eve
X, |±⟩
1/2
1/2
Bob
Z, |0⟩
Bob
Z, |0⟩
Bob
Z, |1⟩
1/2
1/2
1
2
1
4
1
4
3
4 undetected
1
4 detected
On the left side of the diagram, Alice is using the Z-basis, and she sends a qubit in
the |0⟩state. In the middle of the diagram, Eve intercepted the qubit and measured
6.6 Quantum Key Distribution
271
it in either the Z-basis or the X-basis, each with probability 1/2. If Eve measured
in the Z-basis, she got |0⟩, and then forwarded the qubit to Bob. Bob measured the
qubit in the Z-basis, so he also got |0⟩. This is the top row of the above diagram. In
this scenario, which occurs with probability 1/2, Alice and Bob both reveal that they
got the bit 0, and Eve’s eavesdropping was undetected. Now, if Eve measured in the
X-basis instead, then she collapsed the state to |+⟩or |−⟩and forwarded it to Bob. In
the above figure, this is the bottom row. Bob then measured the qubit in the Z-basis,
getting |0⟩with probability 1/2 or |1⟩with probability 1/2. Overall, each of these
outcomes occur with probability 1/4. If Bob got |0⟩, Eve is undetected, but if he
got |1⟩, Alice and Bob will realize there was an eavesdropper when they reveal their
results. Overall, Eve has a probability of 3/4 of being undetected and a probability
of 1/4 of being detected, as indicated by the curly brace in the above figure, when
Alice and Bob reveal this bit of their shared secret key. In other words, there is a
probability of 3/4 that Alice and Bob both have 0 as their bits, and probability 1/4
that Alice has 0 and Bob has 1.
If Alice and Bob share n bits of their shared secret key, the probability that Eve is
undetected for all n bits is (3/4)n. Then, the probability that Eve is detected is one
minus this, or
Probability Alice and Bob detect Eve = 1−
3
4
n
.
Thus, if Alice and Bob share 50 bits of their shared secret key, the probability that
they detect Eve is 1−(3/4)50 = 0.99999943, which is very close to certainty.
In order to use BB84 in practice, we need the ability to send qubits to each other
through a network. This is called a quantum network, and building a quantum net-
work is an area of active research.
Exercise 6.25. You are Alice, and you and Bob are establishing a secret key using BB84. You have
the following random bits and random bases. What qubits do you send to Bob?
Alice’s Bits
1 0 0 1 0 0 0 1 1
Alice’s Bases X X Z Z Z X X X X
Alice Sends
? ? ? ? ? ? ? ? ?
Exercise 6.26. You are Bob, and you and Alice are establishing a secret key using BB84. You
choose the following random bases to measure each qubit in, and you got the following results.
Bob’s Bases
X
X
Z
X
Z
Z
X
X
Z
Bob’s Measurement |+⟩|−⟩|0⟩|−⟩|0⟩|1⟩|1⟩|+⟩|1⟩
Bob’s Bits
0
1
0
1
0
1
1
0
1
Next, you call Alice and learn that she used the following bases:
Alice’s Bases Z Z Z Z Z X X Z Z
What is your shared secret key?
Exercise 6.27. Alice and Bob want to catch a possible eavesdropper with a probability of 99%.
How many bits of their shared secret key should they reveal?
272
6 Entanglement and Quantum Protocols
6.7 Summary
Entanglement is a feature of quantum states that does not exist with classical bits.
Entangled qubits can influence each other faster than the speed of light, but this
influence cannot be used to communicate faster than light. Entanglement is monog-
amous, meaning if two qubits are maximally entangled, they cannot be entangled
at all with a third qubit. Using superdense coding, entanglement does allow Alice
to send Bob 2n bits of information by only physically sending him n qubits. En-
tanglement also allows the state of a qubit to be teleported with the aid of classical
communication. Finally, the BB84 quantum key distribution protocol does not use
entanglement, but it illustrates how a shared secret key can be established between
Alice and Bob, and the security is guaranteed by the laws of physics.
Chapter 7
Quantum Algorithms
In Chapters 1 and 2, we saw a classical algorithm and a quantum algorithm for
adding two binary numbers, each of length n. The classical ripple-carry adder in
Section 1.3.4 used 5n −3 logic gates, whereas from Section 4.5.7, the quantum
ripple-carry adder used 4n−2 Toffoli gates and 4n CNOT gates, for a total of 8n−2
gates. Thus, the quantum algorithm uses more gates than the classical algorithm.
Furthermore, if we decompose the Toffoli gates into one- and two-qubit gates, the
number of quantum gates would be even greater. This is a disappointing result. We
want quantum computers to be faster (i.e., use fewer gates) than classical computers.
In this chapter, we consider quantum algorithms that are actually better than their
classical counterparts.
7.1 Circuit vs Query Complexity
7.1.1 Circuit Complexity
The most precise way to quantify the complexity of a quantum circuit is to count
the least number of quantum gates required to implement it, relative to some uni-
versal set of quantum gates. This is called its circuit complexity. For example, if we
permit only one- and two-qubit quantum gates, then recall from Exercise 4.23 that
the Toffoli gate can be decomposed into
H
T †
T
T †
T
H
•
=
•
•
T †
T †
S
•
•
•
•
•
T
This has sixteen one- and two-qubit gates, but it is not the circuit complexity of the
Toffoli gate. In the top row, the last T and H gates can be combined into a single
one-qubit gate, reducing the number of one- and two-qubit gates to fifteen. Yet this
273
274
7 Quantum Algorithms
is still not the circuit complexity. A circuit that uses even fewer one- and two-qubit
gates is
H
P
P †
P
H
•
=
•
•
•
•
•
•
where P is some one-qubit gate. This only uses seven one- and two-qubit gates. It
is an active area of research to determine whether Toffoli can be simplified further.
If only CNOT and one-qubit gates are permitted, however, it has been proved that
Toffoli requires at least six CNOT gates (plus one-qubit gates). Suffice it to say that
circuit complexity is generally hard to find.
In terms of circuit complexity, an efficient quantum algorithm is one with a poly-
nomial circuit complexity. For example, the quantum adder is efficient since its
4n −2 Toffoli gates can each be decomposed into seven one- and two-qubit gates.
Adding the 4n CNOT gates, this results in 7(4n−2)+4n = 32n−14 one- and two-
qubit gates, which is polynomial (linear) in n.
It is also difficult to find the circuit complexity of classical circuits. It can be
hard to determine if a logic circuit has been fully simplified, and it depends on what
gates are allowed. For example, should the final circuit only consist of AND, OR,
and NOT, or can it also include XOR? Should it only consist of NAND gates? Are
three-bit logic gates allowed, or only one- and two-bit gates?
7.1.2 Query Complexity
Since circuit complexity can be hard to find, we often turn to query complexity
instead. The query complexity of a problem is the number of calls to a function, or
queries to an oracle or black box needed to solve the problem. We give an input to
the function or oracle or black box, and it returns an output, without us knowing
its inner workings. Hence, it is opaque or black. It is significantly easier to find the
query complexity of a problem, or when that is not possible, mathematically prove
upper or lower bounds on it.
For example, a problem we will explore later this chapter is brute-force search-
ing. Say we are searching a database of 100 items for one particular item, and we
have an oracle that tells us whether an item is the correct one or not. We can query,
“Oracle, is item number 1 correct?” If the oracle replies, “Yes,” we have found our
item. If the oracle replies “No,” we can inquire about another item: “Oracle, is item
number 2 correct?” We can continue in this manner until the oracle says “Yes.”
Mathematically, the oracle is just a function f(x) that outputs 0 (no) or 1 (yes). So,
evaluating f(1) is inquiring whether item number 1 is correct. If f(1) = 1, we have
found the correct item, but if f(1) = 0, we can inquire about another item, like f(2),
and so on. The query complexity is the number of times we need to evaluate f(x)
7.1 Circuit vs Query Complexity
275
in order to find the item. As we will see later in Section 7.6, if the database has N
entries, a classical computer takes O(N) queries, but a quantum computer only takes
O(
√
N) queries using Grover’s algorithm. We call such an improvement or speedup
in the number of oracle queries an oracle separation.
The first part of this chapter will cover quantum algorithms with oracle separa-
tions, meaning they take fewer queries than classical computers to solve problems.
These algorithms are generally easier to understand. Then, the second part of this
chapter will cover quantum algorithms with better circuit complexities, which are
generally more advanced. Before we start looking at oracular problems, i.e., prob-
lems with an oracle, let us discuss next how an oracle f(x) acts in a quantum com-
puter.
7.1.3 Quantum Oracles
An oracle is simply a boolean function, meaning a function that acts on bits. Then,
it can be defined using a truth table, and it can be constructed using logic gates.
For it to be a quantum oracle, however, it needs to be reversible. Fortunately, from
Section 1.5.4, we can turn it into a reversible circuit by XORing its output with an
extra bit. For example, if the function is f(x), the reversible circuit is
x
x
Oracle
f(x)
y
y ⊕f(x)
Since this entire circuit is reversible, it is a quantum gate. Let us call the gate Uf to
emphasize that it is unitary. We can draw it as
|y⟩
Uf
|y ⊕f(x)⟩
|x⟩
|x⟩
That is, the quantum oracle Uf acts as
|x⟩|y⟩
Uf
−→|x⟩|y⊕f(x)⟩.
Note we can find f(0) and f(1) by setting y = 0. That is,
|0⟩|0⟩
Uf
−→|0⟩|0⊕f(0)⟩= |0⟩|f(0)⟩,
|1⟩|0⟩
Uf
−→|1⟩|0⊕f(1)⟩= |1⟩|f(1)⟩.
The extra qubit |y⟩is called an answer qubit or target qubit, and |x⟩is called the
input qubit.
276
7 Quantum Algorithms
Exercise 7.1. Consider a classical oracle f(x) = x, where x is a bit, and x is the NOT of x. We want
to turn this into a quantum oracle Uf that acts according to
|x⟩|y⟩
Uf
−→|x⟩|y⊕f(x)⟩.
Answer the following questions about this operator:
(a) What is the truth table of the quantum oracle?
x y x y⊕f(x)
0 0 ?
?
0 1 ?
?
1 0 ?
?
1 1 ?
?
(b) Is the operation reversible?
(c) What is the quantum oracle as a 4×4 matrix?
(d) Verify that the matrix is unitary.
Exercise 7.2. Go to https://bit.ly/3m3Zcei. By following this link, you should have ac-
cess to a custom gate called Uf .
This is the oracle. It acts on two qubits according to
|y⟩
Uf
|y ⊕f(x)⟩
|x⟩
|x⟩
(a) Using Quirk, query the oracle with appropriate inputs to find f(0).
(b) Using Quirk, query the oracle with appropriate inputs to find f(1).
7.1.4 Phase Oracle
If we query a quantum oracle the standard way described above, the input qubit
|x⟩is unchanged while the answer qubit |y⟩becomes |y⊕f(x)⟩. There is a way to
query the quantum oracle, however, that causes the answer qubit |y⟩to be unchanged
while multiplying |x⟩by a phase. It works by setting |y⟩= |−⟩, which can be done by
initializing the answer qubit to |0⟩, applying X to turn it into |1⟩, and then applying
H to turn it into |−⟩. That is, writing both the input and answer qubits,
|x⟩|0⟩I⊗X
−−→|x⟩|1⟩I⊗H
−−→|x⟩|−⟩.
7.1 Circuit vs Query Complexity
277
Note it is possible to prepare the answer qubit |y⟩in the state |−⟩because the state of
a qubit can be a superposition of |0⟩and |1⟩; with a classical answer bit, this would
be impossible. Now, let us expand |x⟩|−⟩and see what happens when we query the
oracle:
|x⟩|−⟩= |x⟩1
√
2
(|0⟩−|1⟩)
= 1
√
2
(|x⟩|0⟩−|x⟩|1⟩)
Uf
−→1
√
2
(|x⟩|0⊕f(x)⟩−|x⟩|1⊕f(x)⟩)
=
( 1
√
2 (|x⟩|0⟩−|x⟩|1⟩),
f(x) = 0
1
√
2 (|x⟩|1⟩−|x⟩|0⟩),
f(x) = 1
=
(
|x⟩|−⟩,
f(x) = 0
−|x⟩|−⟩,
f(x) = 1
= (−1)f(x)|x⟩|−⟩.
We can interpret this as the answer qubit staying in the |−⟩state while the input
qubit goes from |x⟩to (−1)f(x)|x⟩. That is, the input qubit acquires a phase. This
is called phase kickback. Often, we drop the answer qubit, since it stays in the |−⟩
state, and only write the input qubit:
|x⟩
Uf
−→(−1)f(x)|x⟩.
This is called a phase oracle, where the qubit |x⟩is multiplied by a phase (−1)f(x).
The phase oracle will be very useful for the oracular problems we will cover.
Exercise 7.3. Quantum oracles are quantum gates, so they act across superpositions. Consider an
input qubit in the superposition state
√
3
2 |0⟩+ 1
2|1⟩,
and an answer qubit in the state |−⟩. Show that the quantum oracle acts by
 √
3
2 |0⟩+ 1
2|1⟩
!
|−⟩
Uf
−→
 √
3
2 (−1)f(0)|0⟩+ 1
2(−1)f(1)|1⟩
!
|−⟩.
Exercise 7.4. We saw that when the answer qubit is in the state |−⟩, we get phase kickback. Let
us explore what happens if the answer qubit is in the state |+⟩. Suppose an input qubit and answer
qubit are in the state |x⟩|+⟩, where x is a bit. If we apply the quantum oracle Uf to this, which maps
|x⟩|y⟩to |x⟩|y⊕f(x)⟩, what do we get if
(a) f(x) = 0?
(b) f(x) = 1?
(c) How do your answers in parts (a) and (b) compare to the initial state |x⟩|+⟩?
278
7 Quantum Algorithms
7.2 Parity
7.2.1 The Problem
For the first algorithm, we have two unknown bits b0 and b1, and we want to find
the parity of the two bits. That is, we want to find b0 ⊕b1, or equivalently, whether
the number of 1’s is even or odd. To do this, we are given an oracle f(x) = bx that
takes as input an index x ∈{0,1} and returns the corresponding bit. That is,
f(0) = b0,
f(1) = b1.
We will show that to find the parity of b0 and b1, we must query this oracle twice
classically, but only once quantumly.
7.2.2 Classical Solution
Classically, we need to know both bits in order to find b0 ⊗b1. So, we need to query
the oracle twice, once to find b0 and again to find b1:
b0 = f(0),
b1 = f(1).
Then we take the XOR of b0 and b1 to find the parity. Thus, the classical query
complexity is 2.
7.2.3 Quantum Solution: Deutsch’s Algorithm
Quantumly, it only takes 1 query using Deutsch’s algorithm. It uses one input qubit
and one answer qubit, and the algorithm is shown in the following quantum circuit:
|−⟩
Uf
|−⟩
|0⟩
H
H
Parity
Let us work through each step of the circuit. Ignoring the answer qubit, we first
apply the Hadamard gate to the input qubit:
|0⟩H−→1
√
2
(|0⟩+|1⟩).
Next, we query the oracle, which acts as a phase oracle because the answer qubit is
|−⟩. From Exercise 7.3, we get
7.2 Parity
279
1
√
2
h
(−1)f(0)|0⟩+(−1)f(1)|1⟩
i
.
To show why this is helpful, let us rewrite this. First, we substitute f(0) = b0 and
f(1) = b1:
1
√
2
h
(−1)b0|0⟩+(−1)b1|1⟩
i
.
Then, we factor out (−1)b0.
(−1)b0 1
√
2
h
|0⟩+(−1)b1−b0|1⟩
i
.
Now, depending on whether b0 and b1 are equal, this is
(
(−1)b0 1
√
2 (|0⟩+|1⟩),
b0 = b1
(−1)b0 1
√
2 (|0⟩−|1⟩),
b0 ̸= b1
.
These are just |+⟩and |−⟩, each with a phase:
(
(−1)b0|+⟩,
b0 = b1
(−1)b0|−⟩,
b0 ̸= b1
.
So, if b0 and b1 are equal, we have |+⟩with an overall phase, and if b0 and b1 are
unequal, we have |−⟩with an overall phase. We can distinguish these by measur-
ing in the X-basis {|+⟩,|−⟩}. Or, we can apply the Hadamard gate again and then
measure in the Z-basis {|0⟩,|1⟩}. Applying the Hadamard gate, we get
(
(−1)b0|0⟩,
b0 = b1
(−1)b0|1⟩,
b0 ̸= b1
.
If we measure this, we either get |0⟩or |1⟩, since the overall phase of (−1)b0 does
not matter. If we get |0⟩, we know that b0 = b1, and so the parity of the bits is 0
(even). On the other hand, if we get |1⟩, we know that b0 ̸= b1, and so the parity of
the bits is 1 (odd). Thus, depending on whether we get |0⟩or |1⟩, we know whether
the parity of the two bits is 0 or 1, and we determined this with just one query to
the oracle. Thus, the quantum query complexity is 1, which is an improvement over
the classical query complexity of 2. While the improvement in query complexity
from 2 to 1 may be small, it is our first concrete example of a quantum computer
outperforming a classical computer.
Note in Deutsch’s algorithm, we never learned the values of b0 and b1 them-
selves, which would require two oracle queries. Instead, we only learned whether
they are equal or opposite, which corresponds to even or odd parity, respectively.
Exercise 7.5. There are two unknown bits b0 and b1, and you want to find the parity of the two bits
by querying an oracle. Go to https://bit.ly/2ILe3cF. By following this link, you should
have access to a custom gate called Uf . This is the oracle, and it acts on two qubits by
280
7 Quantum Algorithms
|y⟩
Uf
|y ⊕f(x)⟩
|x⟩
|x⟩
For this problem, the function f(x) returns bit x, so f(x) = bx.
(a) In Quirk, use Deutsch’s algorithm to find the parity of b0 and b1 using just one query to Uf .
Note you will need to prepare the answer qubit so that it is in the minus state.
(b) In Quirk, query Uf in such a way as to find b0.
(c) In Quirk, query Uf in such a way as to find b1.
(d) Since you now know b0 and b1 from parts (b) and (c), find their parity. Verify that it agrees
with your result from part (a).
(e) In the worst case, how many queries does it take to solve the problem classically?
Exercise 7.6. There are two unknown bits b0 and b1, and you want to find the parity of the two bits
by querying an oracle. Go to https://ibm.co/3GxSWTT. By following this link, you should
have access to a custom gate called Uf :
This is the oracle, and it acts on two qubits by
|y⟩
Uf
|y ⊕f(x)⟩
|x⟩
|x⟩
For this problem, the function f(x) returns bit x, so f(x) = bx.
(a) Program Deutsch’s algorithm in IBM Quantum Experience, and use the quantum simulator to
find the parity of b0 and b1.
(b) Run the circuit on an actual quantum processor using IBM Quantum Experience. Which pro-
cessor did you use, and what histogram of results do you get?
Exercise 7.7. Say you are trying to use Deutsch’s algorithm, but you neglect the last Hadamard
gate. That is, you apply
|0⟩H−→|+⟩
Uf
−→1
√
2
h
(−1)f(0)|0⟩+(−1)f(1)|1⟩
i
.
If you measure the system now, what possible states do you get, and with what probabilities?
7.2.4 Generalization to Additional Bits
What if we have n bits b0,b1,...,bn−1, and we want to find their parity? Classically,
we need to know all n bits, so it takes n queries. Quantumly, we can use Deutsch’s
algorithm to find the parity of pairs of bits:
7.3 Constant vs Balanced Functions
281
b0,b1
| {z }
parity
,b2,b3
| {z }
parity
,...bn−2,bn−1
|
{z
}
parity
.
This takes n/2 queries. Then we can take the XOR of all these parities to get the
parity of all the bits. This takes no additional queries. So, the quantum query com-
plexity is n/2, which is half classical query complexity. Note both the classical and
quantum runtimes are O(n), however, so there is no improvement in their asymptotic
scaling.
Exercise 7.8. You have eight bits, and using Deutsch’s algorithm, you have found the parities of
pairs of bits, shown below:
b0,b1
| {z }
1
,b2,b3
| {z }
1
,b4,b5
| {z }
0
,b6,b7
| {z }
1
.
(a) What is the parity of all eight bits?
(b) How many queries to the oracle did it take to find the parity of all eight bits?
(c) In the worst case, how many queries does it take to solve the problem classically?
Exercise 7.9. You have nine bits, and using Deutsch’s algorithm, you have found the parities of
the first four pairs of bits. Then you queried the oracle for the last bit, revealing whether it’s a 1 or
a 0. This is shown below:
b0,b1
| {z }
0
,b2,b3
| {z }
1
,b4,b5
| {z }
0
,b6,b7
| {z }
0
, b8
|{z}
1
.
(a) What is the parity of all nine bits?
(b) How many queries to the oracle did it take to find the parity of all nine bits?
(c) In the worst case, how many queries does it take to solve the problem classically?
7.3 Constant vs Balanced Functions
7.3.1 The Problem
In this problem, we have a function f(x) that takes as input a binary number x =
xn−1 ...x1x0 of length n and outputs 0 or 1. Mathematically, we can write this as f :
{0,1}n →{0,1}, where {0,1}n denotes bit strings of length n. We additionally have
the promise that f is constant (always outputs 0 or always outputs 1) or balanced
(outputs 0 half the time, and outputs 1 half the time), and the problem is to determine
which we have. Put another way, f outputs 1 none of the time, all of the time, or
half of the time, and the task is to determine if it is none or all of the time (constant)
or half of the time (balanced).
For example, the following function on binary strings of length 3 (i.e., on 3 bits)
is balanced since it outputs 0 half the time and 1 the other half of the time:
282
7 Quantum Algorithms
x2 x1 x0 f(x)
0 0 0
0
0 0 1
0
0 1 0
1
0 1 1
1
1 0 0
0
1 0 1
1
1 1 0
1
1 1 1
0
Note n = 1 is Deutsch’s algorithm since the input is a single bit, and if the func-
tion is constant, the parity is 0 or even, and if the function is balanced, the parity is 1
or odd. So the Deutsch-Jozsa algorithm can be seen as a generalization of Deutsch’s
algorithm.
7.3.2 Classical Solution
Classically, to determine with certainty whether f is constant or balanced, we need
to query half the inputs, plus one, in the worst case scenario. That is, if we query
half the inputs and get zero each time, then we still do not know if just half the
outputs are zero, or if all the outputs are zero. Querying one more input resolves
this. Since there are 2n possible inputs (binary strings of length n), the classical
query complexity is 2n−1 +1. Note this scales exponentially in n, i.e., it is O(2n).
In practice, however, one may accept a classical algorithm that guesses the cor-
rect answer most of the time. For example, say we query f with c = 10 different
random inputs, and we get f = 0 each time. Then, we can guess that f is constant
with some degree of certainty. As we will show next, the probability that our guess
is wrong can be made smaller than any constant using some suitable constant value
for c. Then, such a randomized algorithm can solve the problem with just c queries,
which is O(1).
To show this, say we classically query f for c different random inputs. If we
get the same output every time, we guess that f is constant, and if we get a mix of
0’s and 1’s, we guess that f is balanced. Let us calculate the probability that these
guesses are incorrect. First, if f is constant, we will get the same output for all c
of our inputs, and we will correctly guess that f is constant, so there is no error
in this case. If f is balanced and the c outputs are any mix of 0’s and 1’s, we will
correctly guess that f is balanced, so there is no error in this case, either. If f is
balanced and all c of our outputs are the same, however, we will incorrectly guess
that f is constant, which is an error. Let us find the probability of this error. Say
all c of our outputs are 0, but f is actually balanced. To get the first 0, there are
2n/2 = 2n−1 outputs that are 0 out of a total of 2n outputs, so the probability of
getting a 0 is 2n−1/2n. For the second 0, there are 2n−1 −1 outputs remaining that
are 0 out of a total of 2n −1 outputs remaining, so the probability of getting a second
0 is (2n−1−1)/(2n−1). Continuing this reasoning, the probability of getting c zeros
7.3 Constant vs Balanced Functions
283
is
2n−1
2n
2n−1 −1
2n −1
2n−1 −2
2n −2 ... 2n−1 −c
2n −c
≈2n−1
2n
2n−1
2n
2n−1
2n ... 2n−1
2n
= 1
2
1
2
1
2 ... 1
2 = 1
2c ,
where in the first line, we have approximated the expression for large n. Similarly,
the probability of all c queries yielding 1 even though f is balanced is also 1/2c.
Together, the total probability of incorrectly guessing that f is constant when it is
actually balanced is
1
2c + 1
2c =
1
2c−1 .
This does not depend on n. So, we can bound the error by checking an appropriate
number of inputs. For example, if we want the probability of error to be less than
1%, we only need to query f for c = 8 different inputs, and this is a constant number
of queries regardless of n. Thus, this randomized algorithm takes O(1) queries of f.
From Exercise 1.53, problems that are efficiently solved with bounded error by
such randomized algorithms are contained in the complexity class bounded-error
probabilistic polynomial time (BPP). It is believed that P = BPP, but it is not proven.
Since we gave an efficient randomized algorithm for determining whether f is con-
stant or balanced, this problem is in BPP.
To review, for a classical computer to determine with certainty whether f is con-
stant or balanced, it needs 2n−1 + 1 queries to f in the worst case, which is O(2n).
For a probabilistic classical computer to guess the answer with bounded error, it
only needs a constant number of queries to f, which is O(1).
Exercise 7.10. When determining if an oracle f is constant or balanced,
(a) What is the probability of an error if you evaluate f for c = 7 different inputs?
(b) What is the probability of an error if you evaluate f for c = 8 different inputs?
(c) How many times should f be evaluated to reduce the error probability to less than 0.1%?
7.3.3 Quantum Solution: Deutsch-Jozsa Algorithm
A quantum computer using the Deutsch-Jozsa algorithm can determine with cer-
tainty whether f is constant or balanced using just 1 query to f. This is an exponen-
tial speedup over the exact classical algorithm, but no speedup over the bounded-
error probabilistic classical algorithm.
The Deutsch-Jozsa algorithm is very similar to Deutsch’s algorithm, but we now
have n qubits (plus an answer qubit, which we ignore by using a phase oracle). These
n qubits are initially each in the |0⟩state, and we apply Hadamards to put them in
a superposition of all n-bit strings. Then, we query the oracle on this superposition.
284
7 Quantum Algorithms
Finally, we apply Hadamards to all the qubits to create a state that we measure,
and whose measurement outcome allows us to distinguish whether the function is
constant or balanced. Including the answer qubit, the Deutsch-Jozsa algorithm as a
quantum circuit is
|−⟩
Uf
|0⟩
H
H
|0⟩
H
H
...
...
...
...
|0⟩
H
H



















n
Let us work out the math to show why this determines whether f is constant or
balanced. Ignoring the answer qubit, we begin with n qubits, all in the |0⟩state. First
applying the Hadamard gate to each of these n qubits, we get
|0⟩⊗n H⊗n
−−→|+⟩⊗n
=
1
√
2n (|0⟩+|1⟩)⊗n
=
1
√
2n
∑
x∈{0,1}n
|x⟩.
(7.1)
So, applying Hadamards to the all zero state creates a uniform superposition over
all binary strings. Next, we query the phase oracle:
1
√
2n
∑
x∈{0,1}n
|x⟩
Uf
−→
1
√
2n
∑
x∈{0,1}n
(−1)f(x)|x⟩.
Finally, we again apply the Hadamard gate to each of the n qubits:
1
√
2n
∑
x∈{0,1}n
(−1)f(x)|x⟩H⊗n
−−→
1
√
2n
∑
x∈{0,1}n
(−1)f(x)H⊗n|x⟩.
(7.2)
This is the final state of the algorithm before the measurement, and to interpret this,
let us focus on just one H⊗n|x⟩, where |x⟩is a single n-bit string:
H⊗n|x⟩= H⊗n|xn−1 ...x1x0⟩
= H|xn−1⟩...H|x1⟩H|x0⟩.
Depending on whether xi is 0 or 1, H|xi⟩is either |+⟩or |−⟩. To account for the
difference in sign between |+⟩and |−⟩, we can write H|xi⟩as
H|xi⟩= 1
√
2
h
|0⟩+(−1)xi|1⟩
i
.
7.3 Constant vs Balanced Functions
285
This way, when xi = 0, (−1)xi = 1, and the result is |+⟩, and when xi = 1, (−1)xi =
−1, and the result is |−⟩. Writing each H|xi⟩like this, we get
H|x0⟩H|x1⟩...H|xn−1⟩
= 1
√
2
h
|0⟩+(−1)xn−1|1⟩
i
... 1
√
2
h
|0⟩+(−1)x1|1⟩
i 1
√
2
h
|0⟩+(−1)x0|1⟩
i
.
Multiplying out the terms, this becomes
1
√
2n
h
|0...000⟩+(−1)x0|0...001⟩+(−1)x1|0...010⟩
+(−1)x1(−1)x0|0...011⟩+(−1)x2|0...100⟩+(−1)x2(−1)x0|0...101⟩
+(−1)x2(−1)x1|0...110⟩+(−1)x2(−1)x1(−1)x0|0...111⟩+...
i
.
Writing (−1)x1(−1)x0 as (−1)x1+x0, and similarly elsewhere, we get
1
√
2n
h
|0...000⟩+(−1)x0|0...001⟩+(−1)x1|0...010⟩
+(−1)x1+x0|0...011⟩+(−1)x2|0...100⟩+(−1)x2+x0|0...101⟩
+(−1)x2+x1|0...110⟩+(−1)x2+x1+x0|0...111⟩+...
i
.
This is a sum over all n-bit strings |z⟩= |zn−1 ...z1z0⟩, so it becomes
1
√
2n
∑
z∈{0,1}n
(−1)∑i:zi=1 xi|z⟩.
For the negative sign, the power is the sum of the values of xi such that zi = 1. We
can also write this sum using the dot product x·z,
x·z = xn−1zn−1 +···+x1z1 +x0z0.
In this dot product, the only xi’s that survive are those where zi = 1. Using this
notation, we get
H⊗n|x⟩=
1
√
2n
∑
z∈{0,1}n
(−1)x·z|z⟩.
(7.3)
Plugging this into Eq. (7.2), the final state of the algorithm before measurement is
1
√
2n
∑
x∈{0,1}n
(−1)f(x)H⊗n|x⟩=
1
√
2n
∑
x∈{0,1}n
(−1)f(x) 1
√
2n
∑
z∈{0,1}n
(−1)x·z|z⟩
= ∑
z∈{0,1}n
 
1
2n
∑
x∈{0,1}n
(−1)f(x)+x·z
!
|z⟩.
(7.4)
286
7 Quantum Algorithms
To see how measuring this state lets us determine whether the function is constant or
balanced, let us calculate the probability of getting all zeros |0...00⟩. The amplitude
of |0...00⟩(right before measurement) is
1
2n
∑
x∈{0,1}n
(−1)f(x).
This amplitude depends on whether f(x) is constant or balanced:
• If f(x) is constant, then f(x) always outputs the same value, so f(x) = f(0...00)
for all x, and the amplitude is
1
2n
∑
x∈{0,1}n
(−1)f(0...00) = 1
2n (−1)f(0...00)2n = (−1)f(0...00).
Taking the norm-square of this, if f(x) is constant, the probability of measuring
|0...00⟩is 1.
• If f(x) is balanced, then (−1)f(x) is 1 half the time and −1 the other half the
time, so the amplitude is 0. Hence, if f(x) is balanced, the probability of mea-
suring |0...00⟩is 0, so we are guaranteed to get something other than |0...00⟩
when we measure.
Thus, to determine if f is constant or balanced, we measure the n qubits, and if we
get |0...00⟩, the function is constant, and if we get anything else, the function is
balanced.
Exercise 7.11. Apply H ⊗H ⊗H to |000⟩, and show that the resulting state is a uniform superpo-
sition of all binary strings of length 3. If you measure the qubits, what possible outcomes can you
get, and with what probabilities?
Exercise 7.12. There is a function on three bits f(b0,b1,b2), with the promise that the function
is constant or balanced. You want to determine which by querying an oracle. Go to https:
//bit.ly/38P0Nig. By following this link, you should have access to a custom gate called
Uf . This is the oracle, and it acts on four qubits by
|y⟩
Uf
|y ⊕f(b0, b1, b2)⟩
|b0⟩
|b0⟩
|b1⟩
|b1⟩
|b2⟩
|b2⟩
(a) In Quirk, use the Deutsch-Jozsa algorithm to determine whether f(x) is constant or balanced
using just one query to Uf . Note you will need to prepare the answer qubit so that it is in the
minus state.
(b) In Quirk, query Uf in various ways to determine f(b2,b1,b0):
7.4 Secret Dot Product String
287
b2 b1 b0 f(b2,b1,b0)
0 0 0
?
0 0 1
?
0 1 0
?
0 1 1
?
1 0 0
?
1 0 1
?
1 1 0
?
1 1 1
?
(c) Since you now know f(b2,b1,b0) completely from part (b), verify that it agrees with your
result from part (a).
(d) In the worst case, how many queries does it take to solve the problem classically?
Exercise 7.13. There is a function on three bits f(b0,b1,b2), with the promise that the function
is constant or balanced. You want to determine which by querying an oracle. Go to https:
//ibm.co/3EWbltg. By following this link, you should have access to a custom gate called
Uf . This is the oracle, and it acts on four qubits by
|y⟩
Uf
|y ⊕f(b0, b1, b2)⟩
|b0⟩
|b0⟩
|b1⟩
|b1⟩
|b2⟩
|b2⟩
(a) Program the Deutsch-Jozsa algorithm in IBM Quantum Experience, and use the quantum
simulator to determine if f(b0,b1,b2) is constant or balanced.
(b) Run the circuit on an actual quantum processor using IBM Quantum Experience. Which pro-
cessor did you use, and what histogram of results do you get?
7.4 Secret Dot Product String
7.4.1 The Problem
Deutsch’s algorithm and the Deutsch-Jozsa algorithm both followed the same steps:
apply Hadamard gate(s), query the oracle, apply Hadamard gate(s) again, and then
measure. Since this worked so well, are there any other problems that can be solved
by this procedure?
The answer is yes. There is another problem that a quantum computer can solve
using this procedure, and it is finding a secret n-bit string by querying an oracle
that takes the dot product of the string with the input. That is, we again have a
function f that takes as input a binary string of length n and outputs 0 or 1, so
f : {0,1}n →{0,1}. But now the promise is that f(x) = s·x, where s is some n-bit
string sn−1 ...s1s0, and the dot product of s and x is the sum of the products of their
elements, i.e.,
s·x = sn−1xn−1 +...s1x1 +s0x0.
The problem is to find s, which means finding sn−1 ...s1s0.
288
7 Quantum Algorithms
7.4.2 Classical Solution
Since we need to determine all n bits of s, the classical solution requires n queries,
one to learn each bit of s. For example, if n = 4, then
f(0001) = s3(0)+s2(0)+s1(0)+s0(1) = s0,
f(0010) = s3(0)+s2(0)+s1(1)+s0(0) = s1,
f(0100) = s3(0)+s2(1)+s1(0)+s0(0) = s2,
f(1000) = s3(1)+s2(0)+s1(0)+s0(0) = s3.
It is known that a bounded-error probabilistic algorithm must also take at least n
queries to f, but the details are beyond the scope of this textbook.
7.4.3 Quantum Solution: Bernstein-Vazirani Algorithm
Quantumly, we only need one query using the Bernstein-Vazirani algorithm, which
is a polynomial speedup over classical computers. It follows the exact same steps
as the Deutsch-Jozsa algorithm, where we apply Hadamards, query the oracle, and
then apply Hadamards again, so as a quantum circuit, it is
|−⟩
Uf
|0⟩
H
H
|0⟩
H
H
...
...
...
...
|0⟩
H
H



















n
Let us work out the math to show that this works. Ignoring the answer qubit,
from Eq. (7.4), the state of the n qubits before measurement is
∑
z∈{0,1}n
 
1
2n
∑
x∈{0,1}n
(−1)f(x)+x·z
!
|z⟩.
For the problem of the secret dot product string, f(x) = s · x. Plugging this in, we
get
∑
z∈{0,1}n
 
1
2n
∑
x∈{0,1}n
(−1)(s+z)·x
!
|z⟩.
where s+z denotes bitwise addition (no carry), also known as bitwise XOR. That is,
(s+z)i = si ⊕zi. Now we measure this, and to determine the possible measurement
7.4 Secret Dot Product String
289
outcomes, let us consider the amplitude of getting |s⟩. When z = s, s + z is a bit
string of all zeros. Then the amplitude of |s⟩is
1
2n ∑
x
(−1)0 = 1
2n ∑
x
1 = 1
2n 2n = 1.
Thus, normalization implies that the amplitude of all other states is 0, so the final
state of the qubits is
|s⟩.
Measuring this is certain to yield |s⟩, and we have determined s with just one query
to the oracle.
This is a polynomial speedup over the O(n) queries needed by a classical com-
puter. The above speedup holds for bounded error, so it yields an oracle separa-
tion between the complexity classes P and BQP. However, the problem is efficient
for both classical and quantum computers. The next algorithm, Simon’s algorithm,
gives the first “true” exponential speedup, where the problem is inefficient for a
classical computer, but efficient for a quantum computer, in the number of oracle
queries.
Exercise 7.14. There is a function on six bits f(b5,b4,b3,b2,b1,b0) = s5b5 + ··· + s1b1 + s0b0.
Find s = s5 ...s1s0 by querying an oracle. Go to https://bit.ly/31YCBZu. By following
this link, you should have access to a custom gate called Uf . This is the oracle, and it acts on seven
qubits by
|y⟩
Uf
|y ⊕f(b5, . . . , b0)⟩
|b0⟩
|b0⟩
|b1⟩
|b1⟩
|b2⟩
|b2⟩
|b3⟩
|b3⟩
|b4⟩
|b4⟩
|b5⟩
|b5⟩
(a) In Quirk, use the Bernstein-Vazirani algorithm to determine s using just one query to Uf . Note
you will need to prepare the answer qubit so that it is in the minus state.
(b) In Quirk, query Uf in various ways to determine each bit of s. Verify that it agrees with your
result from part (a).
(c) In the worst case, how many queries does it take to solve the problem classically?
Exercise 7.15. There is a function on four bits f(x3,x2,x1,x0) = s3x3 + s2x2 + s1x1 + s0x0. Find
s = s3s2s1s0 by querying an oracle. Go to https://ibm.co/3INITfq. By following this link,
you should have access to a custom gate called Uf . This is the oracle, and it acts on five qubits by
290
7 Quantum Algorithms
|y⟩
Uf
|y ⊕f(x3, . . . , x0)⟩
|x0⟩
|x0⟩
|x1⟩
|x1⟩
|x2⟩
|x2⟩
|x3⟩
|x3⟩
(a) Program the Bernstein-Vazirani algorithm in IBM Quantum Experience, and use the quantum
simulator to find s = s3s2s1s0.
(b) Run the circuit on an actual quantum processor using IBM Quantum Experience. Which pro-
cessor did you use, and what histogram of results do you get?
Exercise 7.16. In the Bernstein-Vazirani algorithm, recall the final state of the qubits (before mea-
surement) is
1
2n
∑
z∈{0,1}n
 
∑
x∈{0,1}n
(−1)(s+z)·x
!
|z⟩.
Say n = 3 and consider z ̸= s such that s+z = 001 (using bitwise addition). Show that the amplitude
of this choice of |z⟩is zero by filling in the following table, and then computing the sum of the last
column.
x
(s+z)·x (−1)(s+z)·x
000
?
?
001
?
?
010
?
?
011
?
?
100
?
?
101
?
?
110
?
?
111
?
?
∑
x
(−1)(s+z)·x:
?
7.4.4 Recursive Problem
The problem of finding a hidden dot product string can be made recursive, meaning
we embed the problem in a bigger instance of the problem, which is embedded in
a bigger instance of the problem, and so forth. The details are beyond the scope of
this textbook, but if we have k levels, then a classical computer takes Ω(nk) queries
to solve the problem (recall from Section 1.7.1 that big-Ωis a lower bound, so a
classical computer takes at least this many queries). In contrast, a quantum com-
puter using a recursive version of the Bernstein-Vazirani algorithm, however, can
solve this problem with 2k queries. If we have k = log2(n) levels, then the classi-
cal algorithm takes Ω(nlogn) queries, which is bigger than any polynomial. We call
this superpolynomial. The quantum computer, however, only takes n queries, which
is linear and efficient. Thus, the recursive version of the hidden dot product string
problem shows that a quantum computer can yield a superpolynomial speedup in
queries. This speedup, however, is less than exponential. Next, we will see a prob-
lem with an exponential speedup.
7.5 Secret XOR Mask
291
7.5 Secret XOR Mask
7.5.1 The Problem
In this problem, the oracle takes as input an n-bit string x = xn−1 ...x1x0 and outputs
an n-bit string f(x) = fn−1 ... f1 f0. That is, f : {0,1}n →{0,1}n. We are promised
that
f(x) = f(y)
if and only if the two inputs x and y are related by
x = y⊕s,
and
y = x⊕s
for some “secret” n-bit string s = sn−1 ...s1s0 ̸= 0...00, where ⊕denotes the bitwise
XOR. That is, f(x) = f(y) if and only if
xi = yi ⊕si,
and
yi = xi ⊕si.
The goal is to find the secret n-bit string s = sn−1 ...s1s0. The secret bit string is
called a mask, and since it is used to XOR the inputs, it is called an XOR mask. The
problem is to find the secret XOR mask s = sn−1 ...s1s0.
For example, say n = 3 and s = 110. Then, for each value of x, x⊕s is shown in
the following table:
x
x⊕s
000 110
001 111
010 100
011 101
100 010
101 011
110 000
111 001
Notice these come in pairs. That is, 000 and 110 are a pair, 001 and 111 are a pair,
010 and 100 are a pair, and 011 and 101 are a pair. This is because if y = x⊕s, then
it is automatically true that x = y⊕s. As a proof, we start with
y = x⊕s.
Next, if we XOR both sides with s, we get
y⊕s = x⊕s⊕s.
Since s⊕s = 0, this is
y⊕s = x⊕0.
Thus,
292
7 Quantum Algorithms
y⊕s = x,
or reversing the two sides, x = y ⊕s. Now, from the promise about the oracle, for
each pair x and y, f(x) and f(y) must be the same. For example, here are two possi-
ble truth tables for f(x), satisfying that f(x) = f(y) if and only if y = x⊕s:
x
f(x)
000 011
001 101
010 001
011 000
100 001
101 000
110 011
111 101
x
f(x)
000 110
001 001
010 111
011 000
100 111
101 000
110 110
111 001
Notice that in both examples, f(000) = f(110), f(001) = f(111), f(010) = f(100),
and f(011) = f(101). Also note there are 1680 different possible truth tables for
f(x). This is because we have four pairs that we need to assign outputs to, and there
are 23 = 8 different outputs. For the first pair, we have 8 choices of outputs. For the
second pair, we have 7 choices of outputs. For the third pair, we have 6 choices of
outputs. And for the fourth pair, we have 5 choices of outputs. Altogether, we have
8·7·6·5 = 1680 possible permutations.
Exercise 7.17. Say n = 3 and s = 010.
(a) Find the pairs of n-bit strings x and y such that y = x⊕s.
(b) Give a possible truth table for f(x) that satisfies the promise that f(x) = f(y) if and only if
y = x⊕s.
7.5.2 Classical Solution
Classically, we can find the secret XOR mask s by finding a collision, meaning a
pair x and y such that f maps them to the same string, i.e., f(x) = f(y). From the
promise about f, this implies that x = y⊕s and y = x⊕s, and we can take the XOR
of x and y to find s:
x⊕y = x⊕(x⊕s) = (x⊕x)
| {z }
0
⊕s = s.
One approach is to trying the inputs one-by-one until we find a collision. In the
worst case, we could try half of the inputs without yet seeing a collision. We are
guaranteed, however, that trying one more input will yield a collision, so the query
complexity with this approach is O(2n−1 +1).
We can do better, however. If we query f with random inputs. This prevents f
from being designed to be as worse as possible as previously described, where half
the inputs, plus 1, must be queried to find a collision. Now, say we have queried
7.5 Secret XOR Mask
293
f a total of k times, so we have k values of f. The probability of there being a
collision in these k values of f is given by the number of pairs of values, which is
the combination kC2 = k(k −1)/2 = O(k2). Since this grows quadratically with the
number of queries, one expects to query f roughly
√
2n = 2n/2 times in order to find
a collision. Although this is an improvement, it is still exponential in n.
Exercise 7.18. We have an oracle f : {0,1}n →{0,1}n with a secret XOR mask s. Say n = 4.
Querying the oracle with some various inputs, we find that f(1011) = 0010 and f(0111) = 0010.
What is s?
Exercise 7.19. The task of finding a collision is closely related to a famous problem called the
birthday problem, which is to find the probability that in a room of n people, at least two of them
share the same birthday. We ignore leap years, so there are 365 days in a year. We also assume that
people’s birthdays are randomly distributed. In reality, this is not true, as some birthdays are more
common than others, but this only makes a shared birthday more likely.
To solve this problem, we find the probability that the n people do not share any birthdays. Then,
the probability that at least two people share the same birthday is 1 minus this. To calculate the
probability that no one shares a birthday, we add people to the room one-by-one. The first person
in the room does not share a birthday with anyone else because there is no one else. The second
person in the room has 364 possible birthdays so as to not share a birthday with the first person,
and the probability of this is 364/365. The third person in the room has 363 possible birthdays
so as to not share a birthday with the first two people, and the probability of this is 363/365. The
fourth person has 362 possible birthdays to avoid sharing, which has a probability of 362/365.
Continuing this, the probability of no one sharing a birthday is
364
365
363
365
362
365 ... 365−(n−1)
365
.
Multiplying this by 365/365, we get
365
365
364
365
363
365
362
365 ... 365−(n−1)
365
= 365·364·363·362·(365−(n−1))
365n
.
Thus, the probability that at least two people share the same birthday is
1−365·364·363·362·(365−(n−1))
365n
.
This can be calculated using a computer algebra system. For example, with n = 23 people,
• Using Mathematica,
n=23;
1 - Product[i/365., {i, 365-(n-1), 365}]
The Product function multiplies (365−(n−1))/365 up through 365/365, and the output
of 1 minus this product is 0.507297.
• Using SageMath,
sage: n=23
sage: 1 - prod(i/365. for i in ((365-(n-1))..365))
0.507297234323986
The Product function multiplies (365−(n−1))/365 up through 365/365.
So, there is over a 50% chance that at least two people share the same birthday. This may be higher
than one might expect, so some call the birthday problem the birthday paradox.
294
7 Quantum Algorithms
(a) With n = 30, what is the probability that at least two of them share the same birthday?
(b) With n = 40, what is the probability that at least two of them share the same birthday?
(c) With n = 50, what is the probability that at least two of them share the same birthday?
(d) With n = 60, what is the probability that at least two of them share the same birthday?
7.5.3 Quantum Solution: Simon’s Algorithm
Simon’s algorithm follows the pattern we have seen so far: apply Hadamards, query
the oracle, and apply Hadamards again. But now we have n input qubits and n an-
swer qubits, and we start each of the answer qubits in the |0⟩state (so we are using
the regular quantum oracle, not the phase oracle). For the oracle, it maps
|x⟩|y⟩
Uf
−→|x⟩|y⊕f(x)⟩,
where
|x⟩= |xn−1 ...x1x0⟩,
|y⟩= |yn−1 ...y1y0⟩,
|y⊕f(x)⟩= |yn−1 ⊕fn−1,...,y1 ⊕f1,y0 ⊕f(x)0⟩.
Another difference with Simon’s algorithm is that we will measure all the qubits,
not just the input qubits.
The quantum circuit for Simon’s algorithm is
|0⟩
Uf
|0⟩
...
...
...
...
|0⟩
|0⟩
H
H
|0⟩
H
H
...
...
...
...
|0⟩
H
H



















n



















n
Let us work through the math of what this does. Initially, we have two n-qubit reg-
isters, one for the input qubits, and another for the answer qubits.
|0...00⟩|0...00⟩.
7.5 Secret XOR Mask
295
Now, we apply the Hadamard gate to each of the input qubits, resulting in
|+···++⟩|0...00⟩.
Multiplying out the |+⟩states, we get a uniform superposition over n-bit strings:
1
√
2n
∑
x∈{0,1}n
|x⟩|0...00⟩.
Next, querying the oracle, we get
1
√
2n
∑
x∈{0,1}n
|x⟩|f(x)⟩.
Now, we again apply the Hadamard gate to each of the input qubits, resulting in
1
√
2n
∑
x∈{0,1}n
H⊗n|x⟩|f(x)⟩.
From Eq. (7.3), H⊗n|x⟩is a uniform superposition of bit strings |z⟩multiplied by a
phase of (−1)x·z, so we get
1
√
2n
∑
x∈{0,1}n
1
√
2n
∑
z∈{0,1}n
(−1)x·z|z⟩|f(x)⟩.
Now, let us measure the answer qubits. We will get one particular value of f(x). Let
us call the value f ′. There are two values of x for which f(x) = f ′. Let us call them
x′ and x′′. That is, f(x′) = f(x′′) = f ′. So, x′ and x′′ are a pair of inputs for which
there is a collision. Then, the state will collapse to these two values of x:
1
√
2
1
√
2n
∑
z∈{0,1}n
h
(−1)x′·z +(−1)x′′·zi
|z⟩
 f ′
.
Note the first coefficient went from 1/
√
2n to 1/
√
2 because the number of possible
outcomes for x went from 2n (all possible bit strings) to 2 (|x′⟩and |x′′⟩). Combining
the coefficients,
1
√
2n+1
∑
z∈{0,1}n
h
(−1)x′·z +(−1)x′′·zi
|z⟩
 f ′
.
Next, we measure the input qubits. To determine the possible results, note that
(−1)x′·z = ±1 and (−1)x′′·z = ±1 depending on what x′ and x′′ are. Then, their
sum is either ±2 or 0:
(−1)x′·z +(−1)x′′·z =
(
±2,
x′ ·z = x′′ ·z mod 2,
0,
x′ ·z ̸= x′′ ·z mod 2.
296
7 Quantum Algorithms
Thus, when measuring the input qubits, we only get a value of |z⟩where
x′ ·z = x′′ ·z mod 2.
Adding x′′ ·z to both sides, we get
x′ ·z+x′′ ·z = x′′ ·z+x′′ ·z mod 2.
The right-hand side of this equation is 0 because if we add any bit to itself modulo
2, we get 0. Thus,
x′ ·z+x′′ ·z = 0 mod 2.
Factoring the left-hand side,
(x′ +x′′)·z = 0 mod 2.
Since x′ and x′′ are a pair of inputs for which there is a collision, x′ ⊕x′′ = s. Then,
we have
s·z = 0 mod 2.
Thus, when measuring the input qubits, we get a value of |z⟩= |zn−1 ...z1z0⟩such
that its dot product with s is 0 mod 2. Writing out the dot product,
sn−1zn−1 +···+s1z1 +s0z0 = 0 mod 2.
(7.5)
This is an equation containing all n of our unknowns, the si’s.
If we repeat this process, we will get a |z⟩= |zn−1 ...z1z0⟩that satisfies Eq. (7.5)
and is likely different from the first because there are an exponential number of
them. To see this, the probability of measuring any such |z⟩is

±2
√
2n+1

2
=
4
2n+1 =
1
2n−1 .
Or, put another way, there are 2n−1 possible |z⟩’s whose dot product with s is zero,
and we have the same probability of getting each one.
Repeating the quantum algorithm O(n) times, we can get n different |z⟩’s, each
satisfying Eq. (7.5). Together, they are a system of n equations and n unknowns,
which we can solve for the si’s. Thus, we can find s with O(n) queries to the oracle,
and this was the first exponential oracle separation between classical and quantum
computers.
Exercise 7.20. You are using Simon’s algorithm to find an n = 3 bit string s = s2s1s0. You run the
quantum circuit three times, and you get the following values for |z⟩, such that s·z = 0 mod 2:
|011⟩,|110⟩,|010⟩,
What is s?
Exercise 7.21. You are using Simon’s algorithm to find an n = 3 bit string s = s2s1s0.
(a) How many different values of |z⟩are there, such that s·z = 0 mod 2?
7.6 Brute-Force Searching
297
(b) If you run the quantum circuit three times, what is the probability that all three values of |z⟩
are different?
7.5.4 Summary
We have examined several quantum algorithms that all follow the same proce-
dure: apply Hadamards, query the oracle, and apply Hadamards again. The follow-
ing table summarizes the problems, query complexities, and asymptotic quantum
speedups:
Problem
Classical
Quantum
Quantum
Asymptotic
Queries
Algorithm
Queries
Speedup
n-bit Parity
n
Deutsch
n/2
None
Constant
Exact: 2n−1 +1
Deutsch-Jozsa
1
Exponential
vs Balanced
Bounded: O(1)
None
Dot Product String
n
Bernstein-Vazirani
1
Polynomial
Recursive
Ω(nlog2 n)
Recursive
n
Superpolynomial
Dot Product String
Bernstein-Vazirani
XOR Mask
O(2n/2)
Simon
O(n)
Exponential
We started with the parity problem. The quantum algorithm does offer an improve-
ment in that it takes half as many queries, but asymptotically, both the classical and
quantum algorithms are O(n), so there is no speedup in that sense. Then, we looked
at determining whether the oracle is constant or balanced. Although the quantum
algorithm yields an exponential improvement over the exact classical algorithm, it
is no improvement over the bounded algorithm that is often acceptable in practice.
For a true asymptotic speedup, the problem of finding a secret dot product string
is solved by a quantum computer using polynomially fewer queries, and a recur-
sive version of the problem is solved with superpolynomially fewer queries. Finally,
finding a secret XOR mask takes exponentially fewer queries on a quantum com-
puter, which shows that for oracular problems, quantum computers can yield an
exponential speedup.
7.6 Brute-Force Searching
7.6.1 The Problem
Before moving on to problems where we can calculate the circuit complexity, let us
discuss one more oracular problem where we count the number of oracle queries.
298
7 Quantum Algorithms
It is the problem of brute-force searching. We again have a function f that takes as
input a binary string of length n and outputs 0 or 1, so f : {0,1}n →{0,1}. This
function, however, only outputs 1 for one input, so it outputs 0 for all other inputs.
The problem is to find this one special input, which we will call w (for winner).
A common motivation for this problem is searching a telephone book, which is
a list of people in alphabetical order along with each person’s telephone number:1
Name
Phone Number
Alice
314-1592
Bob
271-8281
Charlie
105-4571
Dave
885-4187
Eve
125-6637
Frank
299-7924
Grace
729-7352
...
...
Zoe
200-2319
Given a name, it is easy to find the corresponding phone number, since the names
are sorted in alphabetical order. For example, we can start in the middle of the list
and determine if the person we are looking for is in the first half or the last half of
the phone book. Say it is the first half. Then we can look at the middle entry of this
half of the phone book and determine if the person we are looking for is in the first
quarter or second quarter of the list. Repeating this, we reduce the number entries by
one-half each time until we find the entry we are looking for. This process is called
binary search, and if the number of entries is N, it takes at most log2 N steps.
The inverse problem, however, is harder to do. Given a phone number, say 299-
7924, finding the name it corresponds to is harder since the numbers are unsorted.
In fact, we might need to look though every phone number until we find a match.
This inverse problem is the brute-force searching problem that we want to solve, and
a classical computer needs O(N) queries to solve it. This problem is also phrased
as searching an unordered database, and it is also called unstructured searching. In
terms of a function, we have f(name) = number, and we want to find the name. So,
this is also the problem of inverting a function, i.e., of starting with an output and
trying to find the input.
It is possible to have a function or oracle that recognizes the correct answer, even
if it does not know what the correct answer is. For example, it is generally hard to
factor numbers (see Section 6.6.2 on RSA cryptography), but it is easy to verify
if the product of numbers equals the number we are trying to factor. Recall from
Section 1.7.2 that the problems whose possible solutions are easy to check comprise
the complexity class NP. So, brute-force searching includes trying to solve problems
in NP by checking each input to see which output is correct.
1 These phone numbers are inspired by the mathematical constants π and e, and the physical
constants ¯h, ε0, µ0, c, α, and gs.
7.6 Brute-Force Searching
299
7.6.2 Classical Solution
Classically, we must query all N = 2n possible bit strings in the worst case. Or on
average, we must query half the bit strings, or N/2, since it is equally likely that the
winner is the first input as the last. Either way, the classical runtime is O(N).
7.6.3 Quantum Solution: Grover’s Algorithm
A quantum computer can solve the brute-force searching problem using only
O(
√
N) queries using Grover’s algorithm. Ignoring the answer qubit, the algorithm
begins with the qubits all in the |+⟩state. Let us call this starting state |s⟩. From
Eq. (7.1), this is a uniform superposition over all n-bit strings:
|s⟩= |+⟩⊗n =
1
√
N
∑
x∈{0,1}n
|x⟩,
where N = 2n. We can create this initial state by applying Hadamard gates to the all-
zeros state, i.e., |+⟩⊗n = H⊗n|0⟩⊗n. Since the initial state is a uniform superposition
over all n-bit strings, it includes the binary string |w⟩we are trying to find, and all
the other ones:
|s⟩=
1
√
N
 
|w⟩+ ∑
i̸=w
|i⟩
!
=
1
√
N |w⟩+ 1
√
N ∑
i̸=w
|i⟩
=
1
√
N |w⟩+
r
N −1
N
1
√N −1 ∑
i̸=w
|i⟩
|
{z
}
|r⟩
=
1
√
N |w⟩+
r
N −1
N
|r⟩
= sinθ|w⟩+cosθ|r⟩,
where |r⟩is defined as the uniform superposition over all n-bit strings that are not
|w⟩, and θ is defined such that
sinθ =
1
√
N ,
cosθ =
r
N −1
N
.
Drawing the initial state in a coordinate plane with |r⟩and |w⟩as the x- and y-axes,
we get
300
7 Quantum Algorithms
|r⟩
|w⟩
|s⟩
θ
Next, we query the phase oracle Uf . Since f(x) = 1 only when x = w, the state
becomes
Uf |s⟩= (−1)1 sinθ|w⟩+(−1)0 cosθ|r⟩
= −sinθ|w⟩+cosθ|r⟩.
Thus, the amplitude of |w⟩is inverted. Drawn in the coordinate plane, this is equiv-
alent to a reflection through |w⟩:
|r⟩
|w⟩
|s⟩
θ
Uf |s⟩
θ
Next, we apply a quantum gate Rs that reflects about |s⟩(more on how to do this,
and how to interpret it, later). Drawn in the rw-plane,
|r⟩
|w⟩
|s⟩
θ
Uf |s⟩
θ
RsUf |s⟩
2θ
We see that the net effect of these two reflections is a rotation by 2θ. If we apply Uf
and Rs again, we rotate by 2θ again:
7.6 Brute-Force Searching
301
|r⟩
|w⟩
|s⟩
θ
Uf |s⟩
θ
RsUf |s⟩
2θ
(RsUf )2|s⟩
2θ
In this manner, we keep rotating by 2θ by applying Uf and Rs until the final state is
close to |w⟩. Say this takes a total of t rotations. Then, since the angle between |r⟩
and |w⟩is 90◦, or π/2 radians,
θ +t(2θ) = π
2
t(2θ) = π
2 −θ
t = π
4θ −1
2.
Assuming N is large,
θ = sin−1
 1
√
N

≈
1
√
N ,
and so
t ≈π
4
√
N −1
2 ≈π
4
√
N.
Thus, the number of queries is O(
√
N), which is a quadratic speedup over the clas-
sical computer’s O(N).
The angle of the final state may not be exactly π/2, so the success probability
may not be exactly 1. This is not an issue, however, for a couple of reasons. First,
for large N, the angle θ is small. So, the final state may only miss |w⟩by a small
amount. Second, there are ways to adjust this algorithm so that the last step rotates
by a different angle, causing the final state to be exactly aligned with |w⟩. This is
beyond the scope of this textbook.
Including the answer qubit, as a quantum circuit, Grover’s algorithm is
302
7 Quantum Algorithms
|−⟩
Uf
Uf
Uf
|+⟩
Rs
Rs
Rs
|+⟩
...
· · ·
...
|+⟩
|+⟩



























n
Exercise 7.22. Prove that when N = 4 (n = 2), the final state of Grover’s algorithm is exactly |w⟩.
Exercise 7.23. Answer the following questions about Grover’s algorithm:
(a) When the n qubits are in their initial state (all |+⟩states), if you measure the qubits, what is
the probability that you get |w⟩? Express your answer in terms of N = 2n.
(b) Say you apply just one step of Grover’s algorithm (one query Uf and one reflection Rs). If
you measure the qubits after this one step, what is the probability that you get |w⟩? Express
your answer in terms of N = 2n. Hint: In the rw-plane, the amplitude of the state in |w⟩is the
sine of the angle between the state and |r⟩.
Exercise 7.24. Go to https://bit.ly/3qoOErN. By following this link, you should have
access to the custom gates Uf and Rs. Uf is the oracle, and it acts on five qubits by
|y⟩
Uf
|y ⊕f(x3, . . . , x0)⟩
|x0⟩
|x0⟩
|x1⟩
|x1⟩
|x2⟩
|x2⟩
|x3⟩
|x3⟩
Rs acts on four qubits, and it reflects about |s⟩.
(a) What is n?
(b) What is N?
(c) For this problem, how many queries does Grover’s algorithm take?
(d) Using Quirk, implement Grover’s algorithm and find |w⟩.
(e) Using Quirk, query Uf in various ways to fill out the following table:
7.6 Brute-Force Searching
303
x
f(x)
0000
?
0001
?
0010
?
0011
?
0100
?
0101
?
0110
?
0111
?
1000
?
1001
?
1010
?
1011
?
1100
?
1101
?
1110
?
1111
?
Do your results agree with your answer to part (d)?
7.6.4 Reflection About Uniform State
Now, let us explore the reflection about |s⟩, which we denoted Rs. Since we want |s⟩
to be unchanged, but states perpendicular to |s⟩to be reflected (i.e., take on a minus
sign), we can write Rs as
Rs = 2|s⟩⟨s|−I.
Recall from Section 3.4 that the outer product |s⟩⟨s| is a matrix, and I is the identity
matrix that acts on n qubits, so it is N × N in size. Let us show that this keeps |s⟩
the same, but flips any state |s⊥⟩that is orthogonal to |s⟩, as we expect a reflection
about |s⟩to do:
Rs|s⟩= 2|s⟩⟨s|s⟩
|{z}
1
−|s⟩= 2|s⟩−|s⟩= |s⟩,
Rs|s⊥⟩= 2|s⟩⟨s|s⊥⟩
| {z }
0
−|s⊥⟩= −|s⊥⟩.
To write Rs in terms of elementary gates, which also proves that it is a valid
quantum gate, recall
|s⟩= |+⟩⊗n = H⊗n0⊗n
= H|0⟩...H|0⟩.
Then, taking the dual using Eq. (3.1),
⟨s| = ⟨0|H† ...⟨0|H† = ⟨0|H ...⟨0|H =

0⊗nH⊗n,
where we used the fact that H† = H. Plugging these into Rs,
304
7 Quantum Algorithms
Rs = 2H⊗n|0n⟩⟨0n|H⊗n −I.
We can also write the n-qubit identity matrix as
I = I ⊗···⊗I = HH ⊗···⊗HH = (H ⊗···⊗H)(H ⊗···⊗H) = H⊗nH⊗n.
Plugging this into Rs, we get
Rs = 2
 H⊗n|0n⟩
 ⟨0n|H⊗n
−H⊗nH⊗n
= H⊗n (2|0n⟩⟨0n|−I)
|
{z
}
R0
H⊗n
= H⊗nR0H⊗n,
where we have defined another operator
R0 = 2|0n⟩⟨0n|−I.
We will discuss R0 in a moment, but the point is that Rs is equal to R0 surrounded
by Hadamards on both sides:
Rs
H
R0
H
H
H
=
...
...
H
H
H
H
Now for R0, let us calculate how it acts on the all 0’s state |0n⟩and how it acts
any other state |a⟩:
R0|s⟩= 2|0n⟩⟨0n|0n⟩
| {z }
1
−|0n⟩= 2|0n⟩−|0n⟩= |0n⟩,
R0|a⟩= 2|0n⟩⟨0n|a⟩
| {z }
0
−|a⟩= −|a⟩.
Thus, R0 is a reflection about the all zeros state |0n⟩. To create a circuit for R0, recall
Z|0⟩= |0⟩and Z|1⟩= −|1⟩. Then the following circuit flips the sign of the all ones
state |1...1⟩only:
7.6 Brute-Force Searching
305
•
•
...
•
Z
If we multiply this on both sides by X gates, the resulting circuit will flip the sign of
the all zeros state |0...0⟩only:
X
•
X
X
•
X
...
...
...
X
•
X
X
Z
X
But, we want the all zeros state to be unchanged, while all other states are flipped.
Using Exercise 2.27, we use ZXZX to flip the sign of the top qubit, which flips the
sign of the entire state.
X
•
X
X
Z
X
Z
X
•
X
...
...
...
...
...
...
...
X
•
X
X
Z
X
Using X2 = I, R0 is
R0
X
•
Z
X
Z
X
•
X
=
...
...
...
X
•
X
X
Z
X
So, we have a quantum circuit for R0. By “sandwiching” this between Hadamard
gates, we get Rs. Then, we can implement Grover’s algorithm.
306
7 Quantum Algorithms
Exercise 7.25. Go to https://bit.ly/3pZacec. By following this link, you should have
access to the custom gate Uf . This is the oracle, and it acts on six qubits in the usual way.
(a) What is n?
(b) What is N?
(c) For this problem, many queries does Grover’s algorithm take?
(d) Using Quirk, create a custom gate for R0.
(e) Using Quirk, create a custom gate for Rs.
(f) Using Quirk, implement Grover’s algorithm and find |w⟩.
7.6.5 Optimality
It is proven that a quantum computer cannot solve the brute-force problem faster
than O(
√
N), so Grover’s algorithm is optimal, the best that a quantum computer can
do. Then, if it takes a classical computer an exponential number of queries to solve
a problem in NP, a quantum computer also takes an exponential number of queries
(albeit a smaller exponential). This is because the square root of an exponential is
still an exponential, e.g.,
√
2n = 2n/2. Thus, quantum computers cannot brute-force
solve NP problems by simply checking all the answers in superposition. If quantum
computers can solve NP problems efficiently, they would have to exploit some other
structure of the problems besides the fact that their potential solutions are efficiently
checkable. All evidence, however, suggests that quantum computers cannot solve
NP problems.
7.7 Discrete Fourier Transform
7.7.1 Application: Analyzing Music
We have finished exploring oracular algorithms. For the rest of this chapter, we
will explore problems where quantum computers have a better gate complexity than
classical computers, meaning the number of elementary gates/steps is less.
In this section, we will explore a method of analyzing data that has wide appli-
cations in science, engineering, and technology: the discrete Fourier transform. To
introduce it, let us look at a specific application: analyzing music and sound.
Sound is vibration. The pluck of a guitar string, flutter of our vocal chords, or
pulse of a speaker causes air molecules to vibrate. These vibrations reverberate
through subsequent air molecules, eventually reaching our ears. A diagram of an
ear is shown below:
7.7 Discrete Fourier Transform
307
Image credit: Adapted from https://commons.wikimedia.org/wiki/File:
Anatomy_of_the_Human_Ear_blank.svg
The air in the ear canal carries these vibrations to the eardrum, a stretchy mem-
brane, causing it to vibrate. The vibrations continue through three bones, the small-
est in the human body, called ossicles. The third ossicle rests on the oval window,
another stretchy membrane, and the vibrations transmit through it into the cochlea,
a spiral-shaped hollow bone. Inside the cochlea are tiny hair receptors that convert
the vibrations into nerve signals that are sent to the brain’s hearing center and inter-
preted as sound.2
The following waveform shows the vibrations of a piano playing a C major chord
(made of middle C and the E and G notes above it) for one second:
0
0.2
0.4
0.6
0.8
1
Time (s)
-1
-0.5
0
0.5
1
Amplitude
The details of the vibrations are hard to see, so let us zoom in to the first 0.05
seconds:
2 In the 2013 Academy Award-winning film Gravity, many of the scenes are silent because there
is no air in space to transmit sound.
308
7 Quantum Algorithms
0
0.01
0.02
0.03
0.04
0.05
Time (s)
-1
-0.5
0
0.5
1
Amplitude
You can download a WAV file of the one-second piano tune at https://tinyur
l.com/2p86zew6. The WAV file, and the previous waveforms, consists of 44100
points for the one second of sound, and we say that the sound was sampled at a rate
of 44100 Hertz (Hz), or 44100 points per second. Since the first plot is one second
long, it has 44100 points, and since the second plot is 0.05 seconds long, it contains
2205 points. The 44100 points can be downloaded from https://tinyurl.com/
jsavt7ez, and they begin and end with the following (x,y) coordinates:
0.00000,-0.46933
0.00002,-0.46011
0.00005,-0.44931
0.00007,-0.41455
0.00009,-0.38632
0.00011,-0.34164
0.00014,-0.28851
...
0.99993,0.12177
0.99995,0.12454
0.99998,0.13571
Now, say we want to find the frequencies that make up the previous C chord, which
correspond to the pitches or notes that make up the sound. If we let the num-
ber of samples be N = 44100 and label the previous amplitudes a0 = −0.46933,
a1 = −0.46011, ..., aN−1 = 0.13571, then the discrete Fourier transform of the
waveform is a sequence of N points φ0, φ1, ..., φN−1 defined to be
φk =
1
√
N
N−1
∑
j=0
aje2πi jk/N.
(7.6)
For example,
φ0 =
1
√
44100

−0.46933e2πi(0)(0)/44100 −0.46011e2πi(1)(0)/44100 +...
+0.13571e2πi(44099)(0)/44100
= −0.0973861,
φ1 =
1
√
44100

−0.46933e2πi(0)(1)/44100 −0.46011e2πi(1)(1)/44100 +...
7.7 Discrete Fourier Transform
309
+0.13571e2πi(44099)(1)/44100
= −0.118737+0.136405i,
φ2 =
1
√
44100

−0.46933e2πi(0)(2)/44100 −0.46011e2πi(1)(2)/44100 +...
+0.13571e2πi(44099)(2)/44100
= −0.106039+0.0597867i,
...
φ44098 =
1
√
44100

−0.46933e2πi(0)(44098)/44100 −0.46011e2πi(1)(44098)/44100 +...
+0.13571e2πi(44099)(44098)/44100
= −0.106039−0.0597867i
φ44099 =
1
√
44100

−0.46933e2πi(0)(44099)/44100 −0.46011e2πi(1)(44099)/44100 +...
+0.13571e2πi(44099)(44099)/44100
= −0.118737−0.136405i.
Calculating these by hand would be incredibly tedious, as each φk contains 44100
terms in its sum. In the next subsection, we will discuss how to calculate these
using a computer algebra system. For now, let us continue interpreting these num-
bers. Notice φ1 is the complex conjugate of φ44099. Similarly, φ2 is the complex
conjugate of φ44098, and so forth, through φ22051 = φ ∗
22049. That is, φk = φ ∗
N−k for
k = 1,2,...,N/2 −1, so φ0 and φN/2 are unique. In general, the φk’s are complex
numbers. Let us take the norm of each of them. We get
|φ0| = 0.097386
|φ1| = 0.180844
|φ2| = 0.121732
...
|φ44098| = 0.121732
|φ44099| = 0.180844.
Due to the symmetry of the discrete Fourier transform, |φ1| = |φ44099|, |φ2| =
|φ44098|, etc. Plotting k on the x-axis and |φk| on the y-axis with k = 0,1,...,22050,
we get the frequency spectrum of the waveform:
310
7 Quantum Algorithms
0
5000
10000
15000
20000
Frequency (Hz)
0
2
4
6
8
10
12
14
Amplitude
This is hard to read, so let us zoom into the first 1000 points of the x-axis:
0
100
200
300
400
500
600
700
800
900
1000
Frequency (Hz)
0
2
4
6
8
10
12
14
Amplitude
Here, the x-axis corresponds to the frequency, which is measured in Hertz (Hz) and
corresponds to the pitch, so a higher frequency is a higher pitch note. The y-axis
corresponds to the strength of note, so there are several frequencies that are stronger
than the rest. The biggest is around 262 Hz. This corresponds to Middle C on the
piano, which has a frequency of 261.6256 Hz. There is another large spike around
330 Hz, and this corresponds to the E key on the piano above Middle C, and this has
a frequency of 329.6276 Hz. Beyond that, there is another spike at 392 Hz, and this
corresponds to the next G key on the piano, which has a frequency of 391.9954 Hz.
These three piano keys were pressed in order to create the music, so they contribute
strongly to the sound. Note the other prominent frequencies are resonances of these
three fundamental frequencies, and they occur at integer multiples of the aforemen-
tioned frequencies. For example, the spike at 522 Hz is twice Middle C’s 262 Hz,
the spike at 660 Hz is twice the E key’s 330 Hz, the spike at 784 Hz is roughly three
times Middle C’s 262 Hz and twice the G key’s 392 Hz, and the spike at 990 Hz is
three times the E key’s 330 Hz.
Exercise 7.26. Consider a sequence of four points
a0 = 0.841,
a1 = 0.909,
a2 = 0.141,
a3 = −0.757.
Calculate the discrete Fourier transform of this (i.e., φ0, φ1, and φ2) using Eq. (7.6). You may use
a calculator, but not a computer algebra system.
7.7 Discrete Fourier Transform
311
Exercise 7.27. Visit https://en.wikipedia.org/wiki/Piano_key_frequencies
and answer the following questions:
(a) What is the scientific name for the E key above Middle C, which has a frequency of 329.6276
Hz?
(b) What is the frequency of the A4 key?
7.7.2 Classical Solution: Fast Fourier Transform
Calculating just one φk using Eq. (7.6) requires adding together N terms. Since there
are N different φk’s, altogether, this is a total of N2 terms. Although this O(N2) run-
time is efficient in the language of computational complexity, since it is a polynomial
in N, in practice, it can be quite slow because of the large number of points that a
long audio recording can contain.
Another way to interpret Eq. (7.6) is as a matrix-vector multiplication. To write
it more cleanly, let us define ω = e2πi/N. Then, Eq. (7.6) becomes
φk =
1
√
N
N−1
∑
j=0
a jω jk.
For example,
φ0 =
1
√
N (a0 +a1 +a2 +···+aN−1),
φ1 =
1
√
N
 a0 +a1ω +a2ω2 +···+aN−1ωN−1
,
φ2 =
1
√
N

a0 +a1ω2 +a2ω4 +···+aN−1ω2(N−1)
,
...
φN−1 =
1
√
N

a0 +a1ωN−1 +a2ω2(N−1) +···+aN−1ω(N−1)2
.
These equations can be written as







φ0
φ1
φ2
...
φN−1







=







1
1
1
...
1
1
ω
ω2
...
ωN−1
1
ω2
ω4
... ω2(N−1)
...
...
...
...
...
1 ωN−1 ω2(N−1) ... ω(N−1)2














a0
a1
a2
...
aN−1







.
(7.7)
Thus, the discrete Fourier transform can be interpreted as a matrix that acts on a
vector of amplitudes. Although this is useful conceptually, it is just as slow as using
312
7 Quantum Algorithms
Eq. (7.6) directly, since we must calculate all N2 terms of the matrix, and then do
the matrix-vector multiplication.
Fortunately, faster classical algorithms for the discrete Fourier transform exist
that only take O(N logN) steps. These are called fast Fourier transform (FFT) al-
gorithms. The precise workings of these algorithms are beyond the scope of this
textbook, but they are used by computer algebra systems like Mathematica and
SageMath.
First, download the waveform from https://tinyurl.com/jsavt7ez. It is a
csv file, which stands for comma-separated values. Its contents are the 44100 (x,y)
points of the waveform:
0.00000,-0.46933
0.00002,-0.46011
0.00005,-0.44931
0.00007,-0.41455
0.00009,-0.38632
0.00011,-0.34164
0.00014,-0.28851
...
0.99993,0.12177
0.99995,0.12454
0.99998,0.13571
Say the filename is waveform.csv.
• Place waveform.csv in the same folder as the Mathematica notebook. Then,
we can import these points into Mathematica using
wave = Import["waveform.csv"];
In the variable wave, the first column of numbers is the time of each sample,
and the second column is the amplitude. We can just get the amplitudes using
wave[[;;,2]], and we can take the discrete Fourier transform of these ampli-
tudes using the Fourier command:
ft = Fourier[wave[[;;,2]]];
Finally, we can plot the first 1000 points of the frequency spectrum using the
following command:
ListPlot[Transpose[{Table[i, {i, 0, 999}], Abs[ft
,→[[1;;1000]]]}],
PlotRange -> All, Joined -> True]
Above, Table[i, {i, 0, 999}] creates a list of numbers 0,1,...,999, and it
serves as the x-axis of the frequency spectrum. Then, ft[[1;;1000]] lists the
first 1000 points of the discrete Fourier transform, and Abs takes their absolute
values, and this is the y-axis of the frequency spectrum. We combine these into
a list that contains all the x-coordinates followed by all the y-coordinates, and
then we take the transpose to obtain a list of (x,y) coordinates, which we then
plot using ListPlot. The parameter PlotRange -> All ensures that we can
see the full y-axis, and Joined -> True joins the points of the scatter plot.
7.7 Discrete Fourier Transform
313
• Place waveform.csv in the same folder as the SageMath instance. In Sage-
Math, we import the points using
sage: import csv
sage: file = open("waveform.csv","r")
sage: wave = list(csv.reader(file, quoting=csv.
,→QUOTE_NONNUMERIC))
The variable wave contains the (time, amplitude) coordinates of the waveform.
To calculate the discrete Fourier transform, we only want the amplitudes. To
get them, we create an empty array called amps of size 44100 using the FFT
command, and then we go through wave and copy its amplitudes to the variable
amp:
sage: amps = FFT(44100)
sage: for j in range(44100):
....:
amps[j] = wave[j][1]
Next, we calculate the discrete Fourier transform using the forward transform
function. Note this does not include the overall factor of 1/
√
N in Eq. (7.6), so
we will need to include it later.
sage: amps.forward_transform()
Now, we want to plot the first 1000 points of the frequency spectrum. To do this,
we create an empty list of 1000 (x,y) coordinates. The x coordinates are just the
frequencies 0,1,...,999, and the y-coordinates are |φk|2, which we divide by
√
N because the forward transform function did not include 1/
√
N.
sage: freq = [[0,0] for k in range(1000)]
sage: for j in range(1000):
....:
freq[j][0] = j
....:
freq[j][1] = abs(vector(amps[j])) / sqrt(44100)
Finally, we plot the frequency spectrum:
sage: list_plot(freq, plotjoined=true)
Launched png viewer for Graphics object consisting of 1
,→graphics primitive
The graph should pop up.
Exercise 7.28. A one-second recording of a piano playing a triad (a three-note chord), sampled
at 44100 Hz, is available at https://tinyurl.com/43yzv7s3, and the waveform can be
downloaded at https://tinyurl.com/hebe2tj9. Using a computer algebra system, an-
swer the following.
(a) Plot the first 1000 points of the frequency spectrum of the waveform.
(b) Determine the frequencies of the three keys that make up the chord. You can estimate them
off the plot.
(c) Visit https://en.wikipedia.org/wiki/Piano_key_frequencies. Using your
answers to part (b), what are the scientific names of the three keys?
314
7 Quantum Algorithms
7.7.3 Quantum Solution: Quantum Fourier Transform
In the last section, we showed in Eq. (7.7) that the discrete Fourier transform can
be written as a matrix-vector multiplication. It turns out that the N × N matrix is
unitary, so it is a valid quantum gate, which we call the quantum Fourier transform
(QFT). Later, we will show how to implement this using single-qubit and two-qubit
quantum gates, which proves that the QFT is unitary. Alternatively, Exercise 7.29
outlines the proof that QFT†QFT = I. Then, if
|ψ⟩=







a0
a1
a2
...
aN−1







= a0|0⟩+···+aN−1|N −1⟩
is a normalized quantum state, then applying the QFT yields another normalized
quantum state
|φ⟩=







φ0
φ1
φ2
...
φN−1







= φ0|0⟩+···+φN−1|N −1⟩.
We call |φ⟩the quantum Fourier transform of |ψ⟩. Put another way, using Eq. (7.6),
the QFT transforms the state
|ψ⟩=
N−1
∑
j=0
aj|j⟩−→|φ⟩=
N−1
∑
k=0
φk|k⟩=
1
√
N
N−1
∑
k=0
N−1
∑
j=0
aje2πijk/N|k⟩.
Examining the above equation, the QFT transforms basis states from
|j⟩−→
1
√
N
N−1
∑
k=0
e2πi jk/N|k⟩.
(7.8)
The QFT is a large quantum gate acting on n qubits, whose general state has
N = 2n amplitudes. We want to implement it, however, using single-qubit and two-
qubit gates. From Section 4.6, the Solovay-Kitaev theorem says we can decompose
a n-qubit gate into a universal gate set up to precision ε using Θ(2n logc(1/ε))
gates, for some constant c. Or, since N = 2n, this is Θ(N logc(1/ε)). Compared to
the classical fast Fourier transform algorithms, which run in O(N logN) time, the
quantum implementation could be a little better or worse by logarithmic factors,
depending on the constant c. This is not the kind of speedup we desire.
Fortunately, there is a more clever way to implement the QFT using single-qubit
and two-qubit gates, and it only takes O(log2 N) of them, which is an exponential
speedup in circuit complexity over the classical fast Fourier transform algorithms.
7.7 Discrete Fourier Transform
315
To construct this implementation, we express j as an n-bit binary number:
j = jn−1 jn−2 ... j1 j0
= jn−12n−1 + jn−22n−2 +···+ j12+ j0.
Then, j/N can be represented using a binary point, which is like a decimal point,
but in base 2:
j
N = jn−12n−1 + jn−22n−2 +···+ j12+ j0
2n
= jn−1
2
+ jn−2
22 +···+
j1
2n−1 + j0
2n
= 0.jn−1 jn−2 ... j1 j0.
Similarly, we can express k as an n-bit binary number:
k = kn−1kn−2 ...k1k0
= kn−12n−1 +kn−22n−2 +···+k12+k0.
Using these, the exponential in Eq. (7.8) is
e2πijk/N = e2πi(j/N)k
= e2πi(0.jn−1 jn−2...j1 j0)(kn−12n−1+kn−22n−2+···+k12+k0)
= e2πi(0.jn−1 jn−2...j1 j0)kn−12n−1e2πi(0.jn−1 jn−2...j1 j0)kn−22n−2 ...
×e2πi(0.jn−1 jn−2...j1 j0)k12e2πi(0.jn−1 jn−2...j1 j0)k0
= e2πi(jn−1 jn−2...j1.j0)kn−1e2πi(jn−1 jn−2...j2.j1 j0)kn−2 ...
×e2πi(jn−1.jn−2...j1 j0)k1e2πi(0.jn−1 jn−2...j1 j0)k0.
We can drop all the bits left of the binary point. To see why, take for example the
first exponential in the previous line:
e2πi(jn−1 jn−2...j1.j0)kn−1 = e2πi(jn−12n−2+jn−22n−3...j1+j0/2)kn−1
= e2πi jn−12n−2kn−1
|
{z
}
1
e2πi jn−22n−3kn−1
|
{z
}
1
...e2πij1kn−1
|
{z
}
1
e2πij0/2kn−1
= e2πi0.j0kn−1.
The exponentials are 1 because they are either e0 = 1 or e2πim = 1 for some positive
integer m. Dropping all the bits left of the binary point, we have
e2πi jk/N = e2πi(0.j0)kn−1e2πi(0.j1 j0)kn−2 ...
×e2πi(0.jn−2...j1 j0)k1e2πi(0.jn−1 jn−2...j1 j0)k0.
316
7 Quantum Algorithms
Plugging this into Eq. (7.8), we get
|j⟩→
1
√
N
N−1
∑
k=0
e2πi jk/N|k⟩
=
1
√
N
N−1
∑
k=0
e2πi(0.j0)kn−1e2πi(0.j1 j0)kn−2 ...
×e2πi(0.jn−2...j1 j0)k1e2πi(0.jn−1 jn−2...j1 j0)k0|k⟩.
Since we are summing over all n-bit binary numbers k, each bit kn−1,kn−2,...,k0
sums through 0 and 1, so this becomes
1
√
N
1
∑
kn−1=0
...
1
∑
k0=0
e2πi(0.j0)kn−1e2πi(0.j1 j0)kn−2 ...
×e2πi(0.jn−2...j1 j0)k1e2πi(0.jn−1 jn−2...j1 j0)k0|kn−1 ...k0⟩.
Since |kn−1 ...k0⟩is shorthand for |kn−1⟩...|k0⟩, we can move the terms to get
1
√
N
1
∑
kn−1=0
...
1
∑
k0=0
e2πi(0.j0)kn−1|kn−1⟩e2πi(0.j1 j0)kn−2|kn−2⟩...
×e2πi(0.jn−2...j1 j0)k1|k1⟩e2πi(0.jn−1 jn−2...j1 j0)k0|k0⟩.
Moving the summations,
1
√
N
1
∑
kn−1=0
e2πi(0.j0)kn−1|kn−1⟩
1
∑
kn−2=0
e2πi(0.j1 j0)kn−2|kn−2⟩...
×
1
∑
k1=0
e2πi(0.jn−2...j1 j0)k1|k1⟩
1
∑
k0=0
e2πi(0.jn−1 jn−2...j1 j0)k0|k0⟩.
Since e0 = 1, if we evaluate the sums, we get
1
√
N

|0⟩+e2πi(0.j0)|1⟩

|0⟩+e2πi(0.j1 j0)|1⟩

...
×

|0⟩+e2πi(0.jn−2...j1 j0)|1⟩

|0⟩+e2πi(0.jn−1 jn−2...j1 j0)|1⟩

.
Finally, since
√
N =
√
2n = (
√
2)n, we get the product state
1
√
2

|0⟩+e2πi(0.j0)|1⟩
 1
√
2

|0⟩+e2πi(0.j1 j0)|1⟩

...
(7.9)
× 1
√
2

|0⟩+e2πi(0.jn−2...j1 j0)|1⟩
 1
√
2

|0⟩+e2πi(0.jn−1 jn−2...j1 j0)|1⟩

.
7.7 Discrete Fourier Transform
317
This is another way of stating the definition of the QFT, but in binary. If we can
create a quantum circuit that converts |j⟩= |jn−1 ... j0⟩to Eq. (7.9), we will have a
quantum circuit for the QFT.
Let us now prove that we can create a circuit for the QFT using Hadamard gates
and controlled rotations. Consider the rightmost term of Eq. (7.9). To begin con-
structing it, we apply the Hadamard gate to |jn−1⟩:
H|jn−1⟩= 1
√
2
 |0⟩+(−1)jn−1|1⟩

= 1
√
2
 |0⟩+(eiπ)jn−1|1⟩

= 1
√
2

|0⟩+e2πi jn−1/2|1⟩

= 1
√
2

|0⟩+e2πi(0.jn−1)|1⟩

.
Next, consider a single-qubit gate that rotates about the z-axis of the Bloch sphere
by 2π/2r radians, which we call Rr. It acts on basis states by
Rr|0⟩= |0⟩,
Rr|1⟩= e2πi/2r|1⟩,
and its matrix representation is
Rr =
1
0
0 e2πi/2r

.
After the previous Hadamard matrix, we apply R2 to qubit n−1, controlled by qubit
n−2. That is, for the state of qubit n−1, the amplitude of |1⟩is multiplied by e2πi/22
if jn−2 = 1, and nothing happens otherwise. That is, the state of the (n−1)th qubit
goes from
1
√
2

|0⟩+e2πi(0.jn−1)|1⟩

→1
√
2

|0⟩+e2πi(0.jn−1)(e2πi/22)jn−2|1⟩

= 1
√
2

|0⟩+e2πi(0.jn−1)e2πi(0.0jn−2)|1⟩

= 1
√
2

|0⟩+e2πi(0.jn−1 jn−2)|1⟩

.
Similarly, we can apply R3 to n −1, controlled by qubit n −3. Then, the state of
qubit n−1 would be
1
√
2

|0⟩+e2πi(0.jn−1 jn−2 jn−3)|1⟩

.
Continuing this through Rn, controlled by qubit 0, the state of qubit n−1 is
1
√
2

|0⟩+e2πi(0.jn−1 jn−2 jn−3...j0)|1⟩

.
318
7 Quantum Algorithms
This is the rightmost factor of Eq. (7.9). Similarly, we can apply Hadamard and
controlled-Rr gates to the other qubits to construct the other factors, resulting in the
following quantum circuit:
j0
...
•
...
•
...
•
H
kn−1
j1
...
•
...
•
... H
R2
kn−2
...
...
jn−2
•
...
H ... Rn−2
Rn−1 ...
k1
jn−1
H
R2 ... Rn−1
Rn
...
...
k0
Note the order of the outputs is reversed, so we need to reverse the order, such as by
using SWAP gates
j0
...
•
...
•
...
•
H ×
k0
j1
...
•
...
•
... H
R2
× k1
...
...
jn−2
•
...
H ... Rn−2
Rn−1 ...
× kn−2
jn−1
H
R2 ... Rn−1
Rn
...
...
×
kn−1
This is our quantum circuit for the QFT. For example, with n = 4 qubits,
j0
•
•
•
H ×
k0
j1
•
•
H
R2
×
k1
j2
•
H
R2
R3
×
k2
j3
H
R2
R3
R4
×
k3
Let us add up the total number of gates in the QFT circuit with n qubits, beginning
with the Hadamard and controlled-Rr gates. The bottom row of the circuit uses n
gates, the row above it uses n−1 gates, and so fourth, until we get to one gate at the
top row. So, the total number of Hadamard and controlled-Rr gates is
n+(n−1)+(n−2)+···+3+2+1 = n(n+1)
2
.
The first equality can be obtained by pairing up terms from the outside in. That is,
the first term n and the last term 1 add up to n+1. Similarly, the second term (n−1)
and the second-to-last term 2 add up to n + 1. Next, the third term (n −2) and the
third-to-last term 3 add up to n + 1. Altogether, there are n/2 pairs, so they total
(n/2)(n+1) = n(n+1)/2. There are also n/2 swap gates to reverse the order of the
outputs. Altogether, the total number of single-qubit and two-qubit gates is
n(n+1)
2
+ n
2 = O(n2) = O(log2 N).
7.7 Discrete Fourier Transform
319
This runtime of O(log2 N) is an exponential speedup over the classical fast Fourier
transform algorithms, which run in O(N logN) time. This speedup, however, comes
with a major caveat. With the classical algorithm, we get all the terms of the discrete
Fourier transform. In contrast, with the QFT, we get a quantum state whose am-
plitudes correspond to the discrete Fourier transform, and we cannot access these
amplitudes all at once. We can only measure the qubits, which yields a bit string
with a probability given by the norm-square of the amplitude. Thus, obtaining ac-
tual speedups using the QFT requires clever application of it, and in the next section,
we will see an example called phase estimation.
Exercise 7.29. The quantum Fourier transform was given as a N × N matrix in Eq. (7.7). In this
problem, we will show that the matrix is unitary, so it is a valid quantum gate. Let M = QFT†QFT
and let Mrs denote the element of matrix M at row r and column s. We want to prove that Mrs = 1
when r = s and Mrs = 0 when r ̸= s, so M is equal to the identity matrix.
(a) Show that
Mrs = 1
N
N−1
∑
k=0
ω−krωks = 1
N
N−1
∑
k=0
ωk(s−r).
(b) Show that Mrs = 1 when r = s.
(c) Show that Mrs = 0 when r ̸= s. Hint: When r ̸= s, Mrs is a geometric series. You may need
to look up the geometric series formula from Algebra II. Also note that ωmN = e2πim = 1 for
integer m.
Exercise 7.30. In this exercise, we will use Quirk to simulate the quantum Fourier transform on
six qubits. Go to https://bit.ly/3DNKSxl to access the following quantum circuit:
The first part of the QFT circuit, which calculates the Fourier transform of the bottom qubit, has
been done for you. Note the Z gate is a rotation about the x-axis by π radians, so we have the
following relations:
R2 = Z1/2 = S, R3 = Z1/4 = T, R4 = Z1/8, R5 = Z1/16, R6 = Z1/32.
In Quirk, fill in the remainder of the QFT circuit.
Exercise 7.31. Using the IBM Quantum Lab, use the following code to create a quantum circuit
for the QFT:
# Number of qubits.
n = 4
320
7 Quantum Algorithms
# Create a quantum circuit.
qc = QuantumCircuit(n)
# Iterate through each target qubit from (n-1) to 0.
for target in range(n-1,-1,-1):
# Apply the Hadamard gate.
qc.h(target)
# Iterate through the control qubits from (target-1) to 0.
for control in range(target-1,-1,-1):
# Calculate "r," the rotation by 2*pi/2**r.
r = target - control + 1
# Apply the controlled phase/rotation.
qc.cp(2*np.pi/2**r, control, target)
# Swap qubits.
for qubit in range(n//2):
qc.swap(qubit, n - qubit - 1)
# Draw the circuit.
qc.draw()
What circuit is shown? Why is it equivalent to the QFT circuit shown in the textbook?
7.7.4 Inverse Quantum Fourier Transform
The inverse quantum Fourier transform (IQFT) undoes the QFT. Since the QFT
performs the mapping in Eq. (7.8), the IQFT does the reverse:
1
√
N
N−1
∑
k=0
e2πi jk/N|k⟩−→|j⟩.
(7.10)
As a quantum circuit, the IQFT can be performed by reversing the order of the gates
the QFT and replacing them with their inverses:
k0
× H
•
...
•
...
•
...
j0
k1 ×
R†
2
H ...
•
...
•
...
j1
...
...
kn−2 ×
... R†
n−1
R†
n−2 ... H
•
...
jn−2
kn−1
×
...
...
R†
n
R†
n−1
R†
2 ... H
jn−1
Since quantum gates are unitary, the inverses are their conjugate transposes. Note
SWAP† = SWAP, H† = H, and R†
r is a rotation about the z-axis of the Bloch sphere
7.8 Phase / Eigenvalue Estimation
321
by −2π/2r radians. The IQFT has the same gate complexity as the QFT, which is
O(n2).
Exercise 7.32. Go to https://bit.ly/3kUXfQN to access the following quantum circuit:
Insert the IQFT circuit in the space provided, and verify that it cancels out the QFT gate that is
already present, so the qubits are unchanged. Use single-qubit gates and controls to construct the
circuit, not the QFT† gate that comes with Quirk. Also, note R†
4 = Z−1/8, and this can be made
with the “Formula Z Rotation” in Quirk, which has a label Z f(t).
Exercise 7.33. Modify the code in Exercise 7.31 so that it creates a circuit for the IQFT.
7.8 Phase / Eigenvalue Estimation
7.8.1 The Problem
We learned in Chapter 3 that a quantum gate can be represented by a unitary matrix,
and a quantum state can be represented by a vector. For example, we can use linear
algebra to see how the X gate transforms the state (
√
3/2)|0⟩+(1/2)|1⟩:
X
 √
3
2 |0⟩+ 1
2|1⟩
!
=

0 1
1 0
√
3/2
1/2

=
 1/2
√
3/2

= 1
2|0⟩+
√
3
2 |1⟩.
Most of the time, when the X gate is applied to a vector, we get a different vector
as the result. There are some special vectors, however, called eigenvectors, where
applying the X gate results in the exact same vector, multiplied by a number called
an eigenvalue. For example, |+⟩is an eigenvector of the X gate, since if we apply
the X gate to it, we get |+⟩multiplied by 1, so its eigenvalue is 1:
X|+⟩=

0 1
1 0

1/
√
2
1/
√
2

=

1/
√
2
1/
√
2

= |+⟩.
Similarly, |−⟩is an eigenvector of the X gate with eigenvalue −1, meaning when
we apply the X gate to it, we get |−⟩multiplied by −1:
X|−⟩=

0 1
1 0

1/
√
2
−1/
√
2

=

−1/
√
2
1/
√
2

= −

1/
√
2
−1/
√
2

= −|−⟩.
322
7 Quantum Algorithms
When the eigenvector is the state of a quantum system, it is often called an eigen-
state. So, |+⟩and |−⟩are eigenstates of the X gate.
Although the eigenvectors and eigenvalues of a matrix are very important in
many areas of science, technology, and engineering, including quantum mechan-
ics and quantum computing, the details of their importance are beyond the scope of
this introductory textbook. How to find eigenvectors and eigenvalues of a matrix are
also beyond the scope of this textbook. Instead, we will focus on a specific problem:
Given a unitary matrix U and one of its eigenvectors |v⟩, find or estimate its
eigenvalue.
From linear algebra, it is known that the eigenvalues of a unitary matrix must have
the form eiθ for some real number θ. For this reason, this problem is called phase
estimation, since finding the eigenvalue is equivalent to finding the phase θ.
Exercise 7.34. Consider the Hadamard gate,
H = 1
√
2

1 1
1 −1

.
(a) Verify that

1+
√
2
1

is an eigenvector of the Hadamard gate with eigenvalue 1.
(b) Verify that

1−
√
2
1

is an eigenvector of the Hadamard gate with eigenvalue −1.
Exercise 7.35. Consider the following unitary matrix
U = 1
√
2




1
0
1
0
0 eiπ/4
0
eiπ/4
1
0
−1
0
0 eiπ/4
0 −eiπ/4



.
Verify that




2+i
√
2+1
1
1



is an eigenvector of U with eigenvalue eiπ/4.
7.8.2 Classical Solution
Since we are promised that |v⟩is an eigenvector of U, and its eigenvalue takes the
form eiθ, then we know that multiplying |v⟩by U will result in |v⟩multiplied by eiθ,
i.e.,
U|v⟩= eiθ|v⟩.
If |v⟩is an N-dimensional vector and U is an N × N matrix, we can write out this
equation as
7.8 Phase / Eigenvalue Estimation
323





U11 U12 ... U1N
U21 U22 ... U2N
...
...
...
...
UN1 UN2 ... UNN










v1
v2
...
vN




= eiθ





v1
v2
...
vN




.
Multiplying out the left-hand side,





U11v1 +U12v2 +···+U1NvN
U21v1 +U22v2 +···+U2NvN
...
UN1v1 +UN2v2 +···+UNNvN




= eiθ





v1
v2
...
vN




.
We can use any row to find eiθ. For example, using the first row,
U11v1 +U12v2 +···+U1NvN = eiθv1.
Thus the eigenvalue is
eiθ = U11v1 +U12v2 +···+U1NvN
v1
.
This takes N multiplications, N −1 additions, and one division, for a total of 2N =
O(N) elementary arithmetic operations.
7.8.3 Quantum Solution
Say the unitary matrix U is an n-qubit quantum gate, so U is an N ×N matrix, where
N = 2n. We assume that we have n qubits whose state is the eigenstate |v⟩:
|v⟩
|{z}
n qubits
.
To estimate the phase of its corresponding eigenvalue to m bits of precision, we also
have m additional qubits, all initially in the |0⟩state:
|0...000⟩
|
{z
}
m qubits
|v⟩
|{z}
n qubits
.
So, the total number of qubits in our circuit is m+n. Let us refer to these groupings
as the “eigenvalue register” and the ”eigenstate register,” since the m qubits will
eventually contain an m-bit approximation of the phase of the eigenvalue, and the
n qubits are in the eigenstate |v⟩. To estimate the phase of the eigenvalue, we apply
the following quantum circuit:
324
7 Quantum Algorithms
U
U 2
U 4
...
U 2m−1
...
...
...
...







n qubits |v⟩







|v⟩
|0⟩
H
•
...
IQFT
jm
|0⟩
H
•
...
jm−1
|0⟩
H
•
...
jm−2
...
...
...
...
...
...
|0⟩
H
...
•
j1























m qubits
Let us go through each step of this circuit to see how it works. First, we apply the
Hadamard gate to each qubit of the eigenvalue register, and we get
|++···+⟩|v⟩= 1
√
2
(|0⟩+|1⟩) 1
√
2
(|0⟩+|1⟩)... 1
√
2
(|0⟩+|1⟩)|v⟩
=
1
√
2m (|0⟩+|1⟩)(|0⟩+|1⟩)...(|0⟩+|1⟩)|v⟩.
Next, we apply a controlled-U gate, where the rightmost qubit of the eigenvalue
register is the control, and the eigenstate register is the target. Since U|v⟩= eiθ|v⟩,
this causes the state to acquire a phase of eiθ when the control qubit is |1⟩:
1
√
2m (|0⟩+|1⟩)...(|0⟩+|1⟩)(|0⟩+|1⟩)

|0⟩+eiθ|1⟩

|v⟩.
Then, we apply the controlled-U2 gate, which cause the second-to-rightmost qubit
of the eigenvalue register to acquire a phase of eiθ twice, which is a phase of e2iθ,
when the control qubit is |1⟩:
1
√
2m (|0⟩+|1⟩)...(|0⟩+|1⟩)

|0⟩+e2iθ|1⟩

|0⟩+eiθ|1⟩

|v⟩.
Then, we apply the controlled-U4 gate, which applies a phase of e4iθ:
1
√
2m (|0⟩+|1⟩)...

|0⟩+e4iθ|1⟩

|0⟩+e2iθ|1⟩

|0⟩+eiθ|1⟩

|v⟩.
Continuing with the controlled gates, we eventually apply controlled-U2m−1, where
the phase is e2m−1iθ:
1
√
2m

|0⟩+e2m−1iθ|1⟩

...

|0⟩+e4iθ|1⟩

|0⟩+e2iθ|1⟩

|0⟩+eiθ|1⟩

|v⟩.
7.8 Phase / Eigenvalue Estimation
325
Now, let us change the variables using θ = 2π j, so if we can find j, we simply
multiply it by 2π to find θ. Substituting, the previous state becomes
1
√
2m

|0⟩+e2πi2m−1 j|1⟩

...
 |0⟩+e2πi4j|1⟩
 |0⟩+e2πi2j|1⟩
 |0⟩+e2πij|1⟩

|v⟩.
Since 0 ≤θ < 2π, we have 0 ≤j < 1. Expressing j as an m-bit binary number
0.j1 j2 ... jm, which is a number less than 1, the state becomes
1
√
2m

|0⟩+e2πi(j1 j2...jm−1.jm)|1⟩

...

|0⟩+e2πi(j1 j2.j3...jm)|1⟩

×

|0⟩+e2πi(j1.j2...jm)|1⟩

|0⟩+e2πi(0.j1...jm)|1⟩

|v⟩.
From Section 7.7.3, we can ignore the bits to the left of the binary point because
they contribute multiples of e2πi = 1, so the state is equivalent to
1
√
2m

|0⟩+e2πi(0.jm)|1⟩

...

|0⟩+e2πi(0.j3...jm)|1⟩

×

|0⟩+e2πi(0.j2...jm)|1⟩

|0⟩+e2πi(0.j1...jm)|1⟩

|v⟩.
Comparing this to Eq. (7.9), this is precisely the QFT of |j1 j2 ... jm⟩, so we can find
|j1 j2 ... jm⟩by taking the IQFT of the eigenvalue register, resulting in:
|j1 j2 ... jm⟩|v⟩.
This completes the quantum circuit for phase estimation. After measuring these
qubits and obtaining j1, j2, ..., jm, we do a little postprocessing. We calculate
j = 0.j1 j2 ... jm
= j1
2 + j2
4 +···+ jm
2m .
Then, the phase of the eigenvalue is θ = 2π j, and the eigenvalue is eiθ.
To estimate the eigenvalue to m bits of precision, we need m Hadamard gates,
m controlled-U p operations, and an IQFT on m qubits that takes O(m2) gates. Al-
together, the number of gates is O(m2). The classical method takes O(N) = O(2n)
elementary arithmetic operations, so depending on the number of bits of precision
m, the quantum method can be faster, although it assumes we can create |v⟩and do
controlled-U p operations.
Exercise 7.36. Go to https://tinyurl.com/fdtm5fas to access the following quantum
circuit:
326
7 Quantum Algorithms
There are two custom three-qubit gates, v and U. The v gate turns |000⟩into |v⟩, which is an
eigenstate of U whose eigenvalue we want to estimate to m = 8 bits. The beginning of the phase
estimation circuit has been started for you.
(a) Using two copies of U, use Quirk’s “Make Gate” “From Circuit” to create the gate U2. Then
using two copies of U2, make U4. Continuing, make U8, U16, U32, U64, and U128.
(b) Fill in the rest of the phase estimation circuit. Hint: Use Quirk’s IQFT function, which is
called QFT†, rather than constructing it from scratch.
(c) What is j = 0.j1 j2 ... j7 as a binary number?
(d) What is j as a decimal number?
(e) What is θ, the phase of the eigenvalue eiθ?
(f) What is the eigenvalue eiθ.
(g) Explain why your value for eiθ is only an estimate, and the actual value may be slightly
different.
7.8.4 Multiple Eigenstates
Say we have two eigenstates of U, which we call |v1⟩and |v2⟩, with corresponding
eigenvalues e2πij1 and e2πi j2. Say we are using the previous phase estimation algo-
rithm but prepare the eigenstate register in the following superposition of |v1⟩and
|v2⟩:
√
3
2 |v1⟩+ 1
2|v2⟩.
We also have the m qubits that each start in the |0⟩state, so the initial state of the
phase estimation circuit is
7.9 Period of Modular Exponentiation
327
|0...000⟩
 √
3
2 |v1⟩+ 1
2|v2⟩
!
=
√
3
2 |0...000⟩|v1⟩+ 1
2|0...000⟩|v2⟩.
Following the same calculation as the previous section, the final state of the phase
estimation circuit is
√
3
2 |j1 j2 ... jm⟩|v1⟩+ 1
2
 j′
1 j′
2 ... j′
m

|v2⟩,
where 0.j1 j2 ... jm is an m-bit approximation of j1 and 0.j′
1 j′
2 ... j′
m is an m-bit ap-
proximation of j2. Then, when we measure the qubits at the end of the circuit, we
get an approximation of j1 with probability 3/4 or an approximation of j2 with
probability 1/4.
Exercise 7.37. Consider three eigenstates ofU, |v1⟩, |v2⟩, and |v3⟩, with corresponding eigenvalues
e2πij1, e2πij2, and e2πij3. If we use the phase estimation algorithm but prepare the eigenstate register
in the following state,
√
3
2
√
2
|v1⟩+ 1
√
2
|v2⟩+
1
2
√
2
|v3⟩,
what is the probability that we get an approximation to j1, j2, and j3?
7.9 Period of Modular Exponentiation
7.9.1 The Problem
Recall from Section 6.6.2 that “mod” refers to modulus, or the remainder when
dividing. For example, 15 = 3 mod 12 because 15 divided by 12 has a remainder
of 3. This is also how a twelve-hour clock works, as 15 o’clock corresponds to 3
o’clock.
Modular exponentiation is taking powers of a number modulo some other num-
ber. For example, consider powers of 2 taken modulo 7:
20 mod 7 = 1 mod 7,
21 mod 7 = 2 mod 7,
22 mod 7 = 4 mod 7,
23 mod 7 = 8 mod 7 = 1 mod 7,
24 mod 7 = 16 mod 7 = 2 mod 7,
25 mod 7 = 32 mod 7 = 4 mod 7,
26 mod 7 = 64 mod 7 = 1 mod 7,
27 mod 7 = 128 mod 7 = 2 mod 7,
28 mod 7 = 256 mod 7 = 4 mod 7,
328
7 Quantum Algorithms
29 mod 7 = 512 mod 7 = 1 mod 7,
...
Notice the results are 1,2,4,... repeated. The period or order r of the modular
exponential is the length of the repeating sequence, so in this example, r = 3. Next,
let us consider another example: powers of 3 taken modulo 10:
30 mod 10 = 1 mod 10,
31 mod 10 = 3 mod 10,
32 mod 10 = 9 mod 10,
33 mod 10 = 27 mod 10 = 7 mod 10,
34 mod 10 = 81 mod 10 = 1 mod 10,
35 mod 10 = 243 mod 10 = 3 mod 10,
36 mod 10 = 729 mod 10 = 9 mod 10,
37 mod 10 = 2187 mod 10 = 7 mod 10,
38 mod 10 = 6561 mod 10 = 1 mod 10,
...
Now, the pattern is 1,3,9,7 repeated, and the period is r = 4. In both of these ex-
amples, the repeated sequences started with a 1. This is always true because a0 = 1
for any positive integer a. Furthermore, the modular exponential ax mod N always
follows a repeated pattern as long as a and N are relatively prime (i.e., their greatest
common divisor is 1, so they share no common factors except 1). This fact comes
from a branch of mathematics called number theory.
Since the repeated sequence always starts with 1, another way to define the period
is as the smallest positive exponent r such that ar mod N = 1 mod N. For example,
with 2x mod 7, r = 3 was the smallest positive exponent to yield 1 mod 7, so it
takes r = 3 terms for the pattern to repeat to 1. For the second example, r = 4 is
the smallest exponent such that 3x mod 10 = 1 mod N. More generally, since the
numbers repeat every r powers, ax+r mod N = ax mod N.
The problem is to find the period of modular exponentials. Since this is a mouth-
ful, we often just call this problem period finding or order finding. Note the period
r must be less than N, and so the challenge is to find the period for large N.
Exercise 7.38. Consider the modular exponential 4x mod 5.
(a) Confirm that 4 and 5 are relatively prime.
(b) Calculate enough terms of 4x mod 5, where x = 0,1,2,..., to see a pattern.
(c) What is the sequence that is repeated?
(d) What is the period?
Exercise 7.39. Consider the modular exponential 4x mod 13.
(a) Confirm that 4 and 13 are relatively prime.
7.9 Period of Modular Exponentiation
329
(b) Calculate enough terms of 4x mod 13, where x = 0,1,2,..., to see a pattern.
(c) What is the sequence that is repeated?
(d) What is the period?
7.9.2 Classical Solution
Finding a single modular exponent is fast using the repeated squaring method. For
example, say we want to find
9143 mod 131.
We do not want to calculate 9143, as this is a very big number. Instead, we want
to calculate it in pieces, taking it modulo 131 as we go. To do this, we express the
exponent in binary:
43 = 1010112
= 1·25 +0·24 +1·23 +0·22 +1·21 +1·20
= 1·32+0·16+1·8+0·4+1·2+1·1.
So, we want to calculate
9143 mod 131 = 911·32+0·16+1·8+0·4+1·2+1·1 mod 131
= 911·32 910·16 911·8 910·4 911·2 911·1 mod 131
=

91321
91160
9181
9140
9121
9111
mod 131 (7.11)
This consists of square powers of 91 modulo 131, and we can calculate them by
starting with 911, then squaring it to get 912, then squaring it to get 914, then squar-
ing it to get 918, and so forth:
911 mod 131 = 91 mod 131,
912 mod 131 = 8281 mod 131 = 28 mod 131,
914 mod 131 = (922)2 mod 131 = 282 mod 131 = 784 mod 131 = 129 mod 131,
918 mod 131 = (924)2 mod 131 = 1292 mod 131 = 16641 mod 131 = 4 mod 131,
9116 mod 131 = (928)2 mod 131 = 42 mod 131 = 16 mod 131,
9132 mod 131 = (9216)2 mod 131 = 162 mod 131 = 256 mod 131 = 125 mod 131.
By repeatedly squaring, we were able to calculate these using relatively small num-
bers. Plugging these into Eq. (7.11), we get
9143 mod 131 = (125)1(16)0(4)1(129)0(28)1(91)1 mod 131
= 125·4·28·91 mod 131
= 1274000 mod 131
330
7 Quantum Algorithms
= 25 mod 131.
In this case, multiplying 125 · 4 · 28 · 91 is small enough to be done on an ordinary
calculator, but if it were not, it could also be multiplied progressively, e.g.,
125·4·28·91 mod 131 = 125(4(28·91)) mod 131
= 125(4(2548)) mod 131
= 125(4(59)) mod 131
= 125(236) mod 131
= 125(105) mod 131
= 13125 mod 131
= 25 mod 131.
To go from the second line to the third, we used 2548 mod 131 = 59 mod 131. Thus,
9143 mod 131 = 25 mod 131, and we were able to calculate this using relatively
small numbers, as opposed to trying to calculate 9143 from the start.
Repeated squaring and other similar methods for calculating modular exponen-
tials have been implemented in computer algebra systems like Mathematica and
SageMath:
• In Mathematica, 9143 mod 131 can be computed using:
PowerMod[91,43,131]
The output is 25, as expected.
• In SageMath, 9143 mod 131 can be computed using:
sage: power_mod(91,43,131)
25
Alternatively, since SageMath is based on Python, we can use Python’s built-in
pow() function:
sage: pow(91,43,131)
25
For the computational complexity of the repeated squaring method, say we are
calculating ax mod N, where x is an n-bit binary number. Then, we start with a and
square it n −1 times, modulo N. Once we have these, we may have to multiply
them together, which following the progressive approach above takes up to n −1
multiplications, modulo N. Together, this is (n −1) + (n −1) = 2(n −1) = O(n)
decimal arithmetic operations modulo N. We may be interested in the number of bit
operations, however, rather than decimal operations. Recall from elementary school
that you can multiply two d-digit numbers by multiplying O(d2) pairs of digits. For
example, to multiply 123 and 456,
7.9 Period of Modular Exponentiation
331
123
× 456
738
6150
+ 49200
56088
That is, we multiplied each digit of 123 by 6, then multiplied each digit of 123
by 5, and then multiplied each digit of 123 by 4, doing the carries along the way.
Altogether, we multiplied 9 pairs of numbers. Then, we added 9 digits together,
ignoring the zeros that we padded on the right. So, the total number of operations
on digits is 9+9 = d2 +d2 = 2d2 = O(d2). Similarly, to multiply two n-bit strings,
this method takes O(n2) multiplications of pairs of bits and additions. For example,
to multiply 101 and 110,
101
× 110
000
1010
+ 10100
11110
Converting to decimal, 101 = 5, 110 = 6, and 11110 = 30, so we get 5 × 6 = 30,
as expected. Now, the repeated squares method takes O(n) multiplications/squares,
and we just saw that each of these takes O(n2) binary multiplications/additions,
and so the total number of elementary binary arithmetic operations for modular
exponentiation is O(n3), which is still a polynomial and is hence efficient.
Although calculating a single modular exponential using the previous repeated
squares method is fast, finding the period is slow because, when N is large, we
may need to calculate many individual modular exponentials before a pattern forms.
There is no known efficient algorithm for period finding.
Computer algebra systems often have functions for finding the period of modular
exponentials. Although they are slow for large N, they are fast for small values.
• In Mathematica, the MultiplicativeOrder function can be used to find the
period of ax mod N. For example, the order of 3x mod 10 is
MultiplicativeOrder[3, 10]
The output is 4, as expected.
• In SageMath, the multiplicative order() function within a modulus object
can be used to find the period of ax mod N. For example, the order of 3x mod 10
is
sage: Mod(3,10).multiplicative_order()
332
7 Quantum Algorithms
4
Exercise 7.40. Use the repeated squares algorithm to calculate 9153 mod 131. Hint: Many of the
numbers were calculated for you in the text. Check your answer using a computer algebra system.
Exercise 7.41. Use the repeated squares algorithm to calculate 8738 mod 197. Check your answer
using a computer algebra system.
7.9.3 Quantum Solution
A quantum computer can efficiently find the period of ax mod N by utilizing a
quantum gate U that performs modular multiplication, which multiplies a number y
by a mod N, so it maps
U|y⟩= |ay mod N⟩.
Since we are working modulo N, y is a number between 0 and N −1. If N can be
written using n bits, then |y⟩would require n qubits. By repeatedly applying U to
|1⟩, we get a to some power:
U0|1⟩= |1 mod N⟩=
a0 mod N

,
U1|1⟩= |a mod N⟩=
a1 mod N

,
U2|1⟩=
a2 mod N

,
U3|1⟩=
a3 mod N

,
...
Ur|1⟩= |ar mod N⟩=
a0 mod N

.
This is exactly the modular exponential ax mod N because exponentiation is re-
peated multiplication. The last term is ar mod N = a0 mod N = 1 mod N because r
is the order of ax mod N, and the sequence repeats itself.
Now, consider a superposition of
a0 mod N

,
a1 mod N

, ...,
ar−1 mod N

with respective coefficients e−2πis(0)/r, e−2πis(1)/r, ..., e−2πis(r−1)/r, where s is an
integer taking values 0, 1, ..., r −1:
|vs⟩= 1
√r

e−2πis(0)/ra0 mod N

+e−2πis(1)/ra1 mod N

+...
+e−2πis(r−2)/rar−2 mod N

+e−2πis(r−1)/rar−1 mod N

= 1
√r
r−1
∑
k=0
e−2πisk/rak mod N
E
.
Let us show that |vs⟩is an eigenvector of U with eigenvalue e2πis/r:
7.9 Period of Modular Exponentiation
333
U|vs⟩= 1
√r
r−1
∑
k=0
e−2πisk/rU
ak mod N
E
= 1
√r

e−2πis(0)/rU
a0 mod N

+e−2πis(1)/rU
a1 mod N

+...
+e−2πis(r−2)/rU
ar−2 mod N

+e−2πis(r−1)/rU
ar−1 mod N

= 1
√r

e−2πis(0)/ra1 mod N

+e−2πis(1)/ra2 mod N

+...
+e−2πis(r−2)/rar−1 mod N

+e−2πis(r−1)/r |ar mod N⟩
|
{z
}
|a0 mod N⟩

= 1
√r

e−2πis(r−1)/ra0 mod N

+e−2πis(0)/ra1 mod N

+e−2πis(1)/ra2 mod N

+···+e−2πis(r−2)/rar−1 mod N

.
Multiplying by 1 = e0 = e2πis/r−2πis/r = e2πis/re−2πis/r, this becomes
U|vs⟩= e2πis/r 1
√r

e−2πis(r)/ra0 mod N

+e−2πis(1)/ra1 mod N

+e−2πis(2)/ra2 mod N

+···+e−2πis(r−1)/rar−1 mod N

.
Note the first coefficient e−2πis(r)/r = e−2πis = 1 since s is an integer, and since
e−2πis(0)/r = 1, the is equation can be written as
U|vs⟩= e2πis/r 1
√r

e−2πis(0)/ra0 mod N

+e−2πis(1)/ra1 mod N

+e−2πis(2)/ra2 mod N

+···+e−2πis(r−1)/rar−1 mod N

= e2πis/r|vs⟩.
Thus, |vs⟩is an eigenvector of U with eigenvalue e2πis/r.
Since |vs⟩is an eigenvector of U, we can use the phase estimation algorithm
from Section 7.8 to estimate its eigenvalue e2πis/r. That is, we can find s/r for some
s, which will allow us to find r, the period of the modular exponential, hence solving
the problem. To do this, however, we need to work out three more items:
1. How to construct the controlled-U gates for the phase estimation algorithm.
2. How to construct the eigenvector |vs⟩for the phase estimation algorithm.
3. How to take the result of the phase estimation, which is an m-bit estimate for
s/r, and find r.
Let us address each of these now.
For the first item, we need controlled-U, controlled-U2, controlled-U4, through
controlled-U2m−1. We choose to approximate the eigenvalue to m = O(n) bits. Writ-
334
7 Quantum Algorithms
ing the control qubit as |z⟩and the target qubits as |y⟩, the operation of CU2 j is:
CU2 j|z⟩|y⟩= |z⟩
az2 jy mod N
E
.
This way, when z = 0, the target remains unchanged as y, and when z = 1, the target
is multiplied by a2j and taken mod N. From Section 7.9.2, repeated squaring is a fast
classical method for computing ax mod N that takes O(n2), and we can convert this
into a reversible circuit and hence a quantum gate. In Section 4.5.2, we discussed
methods for this by converting a classical adder into a quantum adder. The process
would be similar, but we would need to discuss how to square integers and take
the modulo using a classical computer first, so to avoid the lengthy discussion, the
details are beyond the scope of this textbook. Also, the best way to do this is also an
open research question.
For the second item, we need to prepare an eigenvector of U. A trick is, instead
of preparing a single eigenvector of U, we prepare the following equal superposition
of them:
1
√r
r−1
∑
s=0
|vs⟩.
In a moment, we will see that this superposition is easy to construct. First, the
broader picture is that we will use this superposition in the phase estimation al-
gorithm. Since the eigenvalue of |vs⟩is e2πis/r, the phase estimation will yield an
m-bit approximation to s/r for one s = 0,1,...,r −1, where each value of s has a
probability of 1/r.
Now, let us show that the equal superposition is easy to construct. Plugging in
the definition of |vs⟩, the equal superposition becomes
1
√r
r−1
∑
s=0
|vs⟩= 1
√r
r−1
∑
s=0
1
√r
r−1
∑
k=0
e−2πisk/rak mod N
E
= 1
r
r−1
∑
k=0
 
r−1
∑
s=0
e−2πisk/r
!
|
{z
}
r when k=0,
0 otherwise.
ak mod N
E
.
Let us show why the term in parenthesis is r when k = 0 and why it is 0 when k ̸= 0.
First, when k = 0, the term in parenthesis is
r−1
∑
s=0
e−2πisk/r =
r−1
∑
s=0
e0 =
r−1
∑
s=0
1 = r.
Next, when k ̸= 0, let us define ω = e−2πik. Then, the term in parenthesis is
r−1
∑
s=0
e−2πisk/r =
r−1
∑
s=0
ωs = 1+ω +ω2 +···+ωr−1.
7.9 Period of Modular Exponentiation
335
This is a geometric series. We could look up the formula from Algebra II, but let us
quickly derive it. Let us call the series S:
S = 1+ω +···+ωr−1.
If we multiply S by ω, we get
ωS = ω +ω2 +···+ωr.
Subtracting these series,
S−ωS =
 1+ω +···+ωr−1
−
 ω +ω2 +···+ωr
= 1−ωr.
The left-hand side is (1−ω)S, so this becomes
(1−ω)S = 1−ωr.
Dividing, we get a formula for the geometric series S:
S = 1−ωr
1−ω .
Using this formula, let us plug in for ω:
S = 1−e−2πisk
1−e−2πisk/r =
1−1
1−e−2πisk/r = 0,
where in the numerator, we noted that e to any multiple of 2π is equivalent to e0 = 1.
Thus, we have proved that the term in parenthesis from several lines ago is 0 when
k ̸= 0. Thus, the equal superposition that we were considering is equal to
1
√r
r−1
∑
s=0
|vs⟩= 1
r r
a0 mod N

= |1 mod N⟩.
Thus, the equal superposition of the eigenstates |vs⟩is precisely equal to |1 mod N⟩,
which is easily prepared by starting all the qubits as |00...00⟩and then applying
an X gate to the rightmost qubit to yield |00...01⟩= |1 mod N⟩. Then, from Sec-
tion 7.8.4, if we use the phase estimation algorithm, we get an approximation to the
phase of one |vs⟩, with s ∈{0,...,r −1}, with probability 1/r. That is, since |vs⟩is
an eigenstate of U with eigenvalue e2πis/r, the phase estimation yields 0.j1 j2 ... jm,
which is an m-bit approximation to j = s/r.
For example, let us implement this in Quirk to find the order of 3x mod 7. See
https://bit.ly/3otwt4n:
336
7 Quantum Algorithms
Quirk has a built-in modular multiplication gate under the “Modular” toolbox. It is
labeled
×A
mod R. To use this, we need to specify the values of A and R. We can do
this using a tool under the “Inputs” toolbox, and the tools look like
A=#
default and
R=#
default.
On the above circuit, they look like large gray squares with squared off corners.
Next, gates that multiply by modular powers of A can be created using the “Make
Gate” feature, i.e., ×A2 is two copies of ×A, and ×A4 is two copies of ×A2, etc.
The output of the quantum circuit can be hard to read, so we added a “Chance”
display that we resized across all five qubits of the eigenvalue register. By hovering
the mouse cursor over the Chance display, we can see the probability of various
outcomes for the eigenvalue register:
Here are the most likely outcomes for the eigenvalue register:
7.9 Period of Modular Exponentiation
337
Probability Binary Approx. of s/r Decimal Approx. of s/r
16.7963%
|00000⟩
0
11.4759%
|00101⟩
0.1562
11.4760%
|01011⟩
0.3438
16.7963%
|10000⟩
0.5
11.4759%
|10101⟩
0.6562
11.4760%
|11011⟩
0.8438
These are the likely values for our approximation of s/r. For example, we have an
11.4759% chance of measuring the eigenvalue register to be |00101⟩, so 0.00101
is a binary approximation of s/r. Converting this to decimal, we get that s/r is
approximately 0.1562.
Now for the third item, how do we take an approximation to s/r, like 0.1562 from
above, and find s and r? We use a method called continued fractions. A continued
fraction has the form
a0 +
1
a1 +
1
a2 +
1
... + 1
aℓ
for some non-negative integer ℓ. For example, from the table above, consider the
number 0.1562. To express this as a continued fraction, we begin by expressing
0.1562 as 1562/10000, which we express as a mixed number, i.e., a whole number
0 and fractional part 1562/10000:
0.1562 = 1562
10000 = 0+ 1562
10000.
Next, we invert the fractional part to get
0+
1
10000
1562
.
Then, we express 10000/1562 as a mixed number, resulting in
0+
1
6+ 628
1562
.
Again, we invert the fractional part and then express it as a mixed number:
338
7 Quantum Algorithms
0+
1
6+
1
1562
628
= 0+
1
6+
1
2+ 306
628
.
Continuing this, we eventually arrive at
0.1562 = 0+
1
6+
1
2+
1
2+
1
19+ 1
8
.
By listing all the whole numbers, plus the very last denominator, we can write the
continued fraction as [a0,a1,...,a5] = [0,6,2,2,19,8].
A computer algebra system can quickly write a number as a continued fraction:
• In Mathematica, we can convert 0.1562 to a continued fraction using the
ContinuedFraction function:
ContinuedFraction[1562/10000]
The output of this is
{0,6,2,2,19,8}.
• Using SageMath, we can use the continued fraction function to convert
0.1562 into a continued fraction:
sage: continued_fraction(0.1562)
[0; 6, 2, 2, 19, 8]
The reason why we care about continued fractions is they allow us to find rational
approximations to numbers by truncating the continued fraction. These are called
convergents. For example, for 0.1562, the convergents are:
0th convergent = [0] = 0,
1st convergent = [0,6] = 0+ 1
6 = 1
6,
2nd convergent = [0,6,2] = 0+
1
6+ 1
2
= 2
13,
3rd convergent = [0,6,2,2] = 0+
1
6+
1
2+ 1
2
= 5
32,
7.9 Period of Modular Exponentiation
339
4rd convergent = [0,6,2,2,19] = 0+
1
6+
1
2+
1
2+ 1
19
= 97
621,
5th convergent = [0,6,2,2,19,8] = 0+
1
6+
1
2+
1
2+
1
19+ 1
8
= 781
5000.
In this example, the 5th convergent contains all the terms of the continued fraction,
and so the 5th convergent is exactly 0.1562 = 781/5000 = 1562/10000. We can
also calculate the convergents using a computer algebra system:
• In Mathematica, we can find a convergent using the FromContinuedFraction
function:
FromContinuedFraction[{0}]
FromContinuedFraction[{0, 6}]
FromContinuedFraction[{0, 6, 2}]
FromContinuedFraction[{0, 6, 2, 2}]
FromContinuedFraction[{0, 6, 2, 2, 19}]
FromContinuedFraction[{0, 6, 2, 2, 19, 8}]
This outputs the following numbers:
0, 1
6, 2
13, 5
32, 97
621, 781
5000.
• Using SageMath, we can find a convergent using the value() function within
a continued fraction object:
sage: continued_fraction([0]).value()
0
sage: continued_fraction([0, 6]).value()
1/6
sage: continued_fraction([0, 6, 2]).value()
2/13
sage: continued_fraction([0, 6, 2, 2]).value()
5/32
sage: continued_fraction([0, 6, 2, 2, 19]).value()
97/621
sage: continued_fraction([0, 6, 2, 2, 19, 8]).value()
781/5000
The higher the convergent, the better the approximation to 0.1562.
For our period finding problem, 0.1562 is a guess for s/r, where r is the period
of the modular exponential ax mod N, and s is an integer between 0 and r −1. Note
340
7 Quantum Algorithms
r must be less than N. Then, looking at the convergents, the best approximation to
s/r such that r < N = 7 is 1/6. Thus, using the convergents of continued fractions,
we were able to guess that s = 1 and r = 6. To check whether our guess is correct,
we can calculate 3r mod 7 and see if we get 1 mod 7:
36 mod 7 = 1 mod 7.
Thus, with this measurement result, we successfully found the period r = 6. Note it
is known that the continued fraction algorithm yields a guess for s and r in O(n3)
steps, if s and r are n-bit numbers.
From the previous table of significant measurement outcomes of the Quirk circuit
for phase estimation, some other likely estimates for s/r are 0, 0.3438, 0.5, 0.6562,
and 0.8438. For 0, we get s = 0 and no guess for r, so if we get this value, we need
to run the quantum circuit again in hopes for a better outcome. For the other values,
we can use the continued fraction algorithm and get the following guesses for s and
r:
Probability Binary Approx. of s/r Decimal Approx. of s/r Guess of s/r 3r mod 7
16.7963%
|00000⟩
0
N/A
N/A
11.4759%
|00101⟩
0.1562
1/6
1
11.4760%
|01011⟩
0.3438
1/3
6
16.7963%
|10000⟩
0.5
1/2
2
11.4759%
|10101⟩
0.6562
2/3
6
11.4760%
|11011⟩
0.8438
5/6
1
For example, for 0.3438, the continued fraction algorithm yields s = 1 and r = 3.
Checking if this guess for the period is correct, we calculate 3r mod 7 = 33 mod 7 =
6 mod 7 ̸= 1 mod 7, so 3 is not the period. Then, we run the quantum circuit again,
hoping to get a better guess for r. The guess for s and r, and the value of 3r mod 7
for each guess for r, is shown in the above table (see Exercise 7.42). The number
of times we may have to repeat the quantum circuit is small enough that it does not
affect the overall runtime of the algorithm, although a proof of this fact is beyond
the scope of this textbook (see Nielsen and Chuang for it).
Speaking of the overall runtime, let us find the circuit complexity assuming m =
O(n). The quantum algorithm takes one X gate to prepare the eigenvector register
in the state |00...01⟩, m Hadamard gates, and m controlled-Upower gates, and an
IQFT on m qubits. Each of the m controlled-Upower gates takes O(n2) gates for a
total of O(mn2) = O(n3) gates. The IQFT takes O(m2) = O(n2) gates. Finally, the
continued fraction algorithm takes O(n3) gates. Thus, the circuit complexity of the
quantum period algorithm is O(n3), which is a polynomial in n, so it is efficient.
Exercise 7.42. In the text, we considered the example of 2x mod 7, and we found several approxi-
mations to s/r. For each of the approximations 0.3438, 0.5, 0.6562, and 0.8438, do the following:
(a) Express the decimal as a continued fraction.
(b) Find the convergents of the continued fraction.
(c) What is the fractional guess for s/r, and hence, what are the guesses s and r?
(d) Calculate 2r mod 7.
7.10 Factoring
341
(e) Is this guess for r the period or not?
Exercise 7.43. Modify the Quirk circuit from the text to find the period of 2x mod 7. List the most
likely outcomes for the 5-bit approximation of s/r, and in each case, find what r would be and
calculate 2r mod 7 to see if it is the correct period. What is the period?
Exercise 7.44. Use Quirk to simulate the quantum circuit to find the period of 2x mod 15. Since
N = 15, you will need n = 4 qubits for the eigenvector register. You can use Quirk’s built-in modu-
lar multiplication gate and use it to make gates with higher powers. For the eigenvalue register, use
m = 8 qubits. List the most likely outcomes for the m-bit approximation of s/r, and in each case,
find what r would be and calculate 2r mod 15 to see if it is the correct period. What is the period?
7.10 Factoring
7.10.1 The Problem
Say we are given a number N that is the product of two prime numbers p and q. The
goal is to factor N, i.e., to find its factors p and q. In Section 6.6.2, we learned that
the believed difficulty of this factoring problem for classical computers is the basis
of RSA cryptography.
7.10.2 Classical Solution
The best known classical algorithm for factoring is the number field sieve. How it
works is beyond the scope of this textbook, but to factor an n-bit number, its run-
time is roughly en1/3, which is subexponential. It grows faster than polynomial, so
factoring is inefficient for classical computers, but it is also not exponential because
of the natural logarithms.
7.10.3 Quantum Solution: Shor’s Algorithm
An efficient quantum algorithm for factoring was invented by Peter Shor in 1994.
(This is the same Peter Shor who invented the Shor code from Section 4.7.4 in
1995.) This means quantum computers, if they can be built at scale, can break RSA
cryptography from Section 6.6.2. Historically, this greatly increased the amount of
money for research in quantum computing and is one of the reasons why quantum
computing has developed into the field it is today. The subexponential speedup over
the best known classical algorithm is evidence against the Strong Church-Turing
Thesis from Section 1.8.3, meaning a probabilistic Turing machine may not be able
to efficiently compute everything that is efficiently computable.
To factor N = pq, Shor’s algorithm consists of the following three steps:
342
7 Quantum Algorithms
1. Pick any number 1 < a < N. Calculate the gcd(a,N) to determine if we were
extraordinarily lucky and picked a multiple of p or q. If the gcd is not 1, then
the gcd is a nontrivial common factor of a and N, and so we have found one of
the factors of N. Let us call it p = gcd(a,N). Then, q = N/p, and we are done
factoring. If gcd(a,N) = 1, we continue to the next step.
2. Find the period r of ax mod N. Note this is believed to be hard for classical
computers, but it is efficient for quantum computers using the period finding
algorithm from the previous section. Make sure the period r is even; if it is odd,
go back to step 1 and pick a different a. Also, calculate ar/2 mod N and make
sure it does not equal N −1; if it equals N −1, go back to step 1 and pick a
different a. It is known that there is at least a 50% chance of picking a “good”
a that meets both criteria, so we will not have to try too many times. The proof
of this is beyond the scope of this textbook, but Theorem 5.3 of Nielsen and
Chuang has more details.
3. Since we calculated the period r in the previous step, we know that ar =
1 mod N. Subtracting 1 from both sides, this means
ar −1 = 0 mod N.
This says ar −1 divided by N has a remainder of 0, so ar −1 is a multiple of N.
Let us call the multiple k, so
ar −1 = kN.
Also substituting N = pq, we get
ar −1 = kpq.
Now, factoring the left-hand side, we get
(ar/2 −1)(ar/2 +1) = kpq.
From Step 2, we know that r is even. So, ar/2 is an integer, and ar/2 ±1 are also
integers. Now, for the product of ar/2 −1 and ar/2 +1 to equal kpq, at least one
of the terms ar/2 −1 or ar/2 +1 must contain p and/or q as a factor. That is, for
some integers c and d such that cd = k, we have three possibilities for ar/2 −1
and ar/2 +1:
1. (ar/2 −1)
|
{z
}
c
(ar/2 +1)
|
{z
}
dpq
= kpq,
2. (ar/2 −1)
|
{z
}
cp
(ar/2 +1)
|
{z
}
dq
= kpq,
3. (ar/2 −1)
|
{z
}
cpq
(ar/2 +1)
|
{z
}
d
= kpq.
7.10 Factoring
343
Let us show that the first and third cases are impossible by showing that ar/2 −
1 and ar/2 + 1 are not multiples of N. That is, we want to show that (ar/2 −
1) mod N ̸= 0 mod N and (ar/2 +1) mod N ̸= 0 mod N, so neither has N as a
factor.
Let us start with (ar/2 −1) mod N = 0 mod N and show that this equation is
not true. If we add 1 to both sides, we get ar/2 = 1 mod N. We know that r
is the period of ax mod N, however, which means r is the smallest value of x
such that ax = 1 mod N. Thus, it cannot be that ar/2 = 1 mod N, otherwise r/2
would be a smaller value of x such that ax = 1 mod N. Therefore, the equation
(ar/2 −1) mod N = 0 mod N is incorrect, and it must be that (ar/2 −1) mod N ̸=
0 mod N, so (ar/2 −1) does not have N as one of its factors.
Next, let us show that (ar/2 +1) mod N = 0 mod N is not true. If we subtract 1
from both sides, we get ar/2 mod N = −1 mod N. Recall that the modulus works
in a “cyclical” fashion. For example, with a 12-hour clock, 15 o’clock corre-
sponds to 3 o’clock. Similarly, −1 o’clock corresponds to 11 o’clock. Thus, our
modular equation becomes ar/2 mod N = N −1 mod N. This is not true, how-
ever, because in Step 2, we made sure that ar/2 mod N ̸= N −1 mod N. Thus,
ar/2 +1 also does not have N as one of its factors.
Thus, only the second case is possible:
(ar/2 −1)
|
{z
}
cp
(ar/2 +1)
|
{z
}
dq
= kpq.
This means ar/2 −1 and ar/2 +1 each share a nontrivial factor with N = pq, and
we can obtain them using the greatest common divisor:
p = gcd(ar/2 −1,N),
q = gcd(ar/2 +1,N).
Thus, we have factored N.
As an example, say we want to factor N = 15. We begin Shor’s algorithm by
picking a value for a that is greater than 1 but less than N = 15:
1. Say we pick a = 6. We calculate gcd(a,N) = gcd(6,15) = 3. This means that
3 is a factor of both a and N. Thus, we have found one of the factors of N. Let
us call it p = 3. The other factor is q = N/p = 15/3 = 5. So, we have factored
N = 15 into pq = 3·5, and we are done.
Let us work out what might happen if we did not have such a lucky pick for a.
1. Say we pick a = 2. We calculate gcd(a,N) = gcd(2,15) = 1, so we continue to
Step 2.
2. We find the period of ax mod N = 2x mod 15. This was Exercise 7.44, and we
used the quantum period finding algorithm to determine r = 4. This period is
even, and we confirm that ar/2 +1 = 5 mod 15 ̸= 14 mod 15.
3. Calculate the factors:
344
7 Quantum Algorithms
p = gcd(ar/2 −1,N) = gcd(22 −1,15) = gcd(3,15) = 3,
q = gcd(ar/2 +1,N) = gcd(22 +1,15) = gcd(5,15) = 5.
Thus, the factors of N = 15 are p = 3 and q = 5.
The bottleneck for Shor’s algorithm is Step 2, finding the period of the mod-
ular exponential. It is efficient on a quantum computer, but there is no known
polynomial-time algorithm for a classical computer.
Although quantum computers would break RSA cryptography, their creation
does not necessarily mean the end of digital privacy. Already, efforts are under-
way to choose a new public-key cryptography standard that is resistant to quantum
computers. Post-quantum cryptography refers to such classical cryptographic algo-
rithms that are resistant to attacks from future quantum computers. Besides this,
there is also quantum key distribution protocols, such as BB84 that was covered
in Section 6.6.3, that are secure from quantum computers. They require a quantum
network, however, to be used.
Exercise 7.45. Use Shor’s algorithm to factor N = 35.
(a) Pick a value of a such that gcd(a,N) = 1 so that we can continue with the remaining steps of
the algorithm.
(b) What is the period of ax mod N? For the sake of this problem, just find the period classically
since the numbers are small. Make sure the period r is even and ar/2 ̸= N −1 mod N. If not,
go back and pick a different value for a.
(c) Calculate the factors p = gcd(ar/2 −1,N) and q = gcd(ar/2 +1,N).
Exercise 7.46. Use Shor’s algorithm to factor N = 209. Say we pick a = 22.
(a) Show that gcd(a,N) ̸= 1.
(b) What are the factors of N?
7.11 Summary
For many algorithms in classical and quantum computing, it is easier to find the
query complexity of the algorithms, which is the number of calls to a function.
Quantum computers can provide provable exponential speedups in query complex-
ity, and we saw an example for the problem of finding a secret XOR mask. Quantum
computers also provide a quadratic speedup for the general problem of brute-force
searching. The most accurate way to quantify the complexity of an algorithm, how-
ever, is counting the number of elementary gates, but this circuit complexity can
differ depending on what gates are permitted, and even then it is difficult to know
if a circuit can be simplified further. Despite this, quantum computers are known to
provide speedups in circuit complexity for several problems, such as discrete Fourier
transforms, estimating the phases of eigenvalues of unitary matrices, finding the pe-
riod of modular exponents, and factoring. Factoring, in particular, is very relevant
to the real-world, as it underpins RSA cryptography, and the quantum speedup is
subexponential over the best known classical algorithms.
Chapter 8
Next Steps
Congratulations on finishing Introduction to Classical and Quantum Computing!
As you have seen, although this an introductory textbook with only minimal pre-
requisites, the material did not stay at an elementary or conceptual level. Instead,
we learned the mathematics needed to understand quantum computing more deeply.
You have learned a lot, and I am proud of you.
In this short chapter, we will explore some possible careers in quantum comput-
ing. They range from technical jobs where the ideas you learned in this textbook
will be used regularly, to supporting roles where a general familiarity with quantum
computing is helpful, but not required. For those who want to continue learning the
technical details, we discuss possible next steps.
8.1 Careers in Quantum Computing
Quantum computing, long confined to the halls of academic research, is now an
emerging industry. As such, there are many companies involved in quantum com-
puting, and they could be grouped into various types:
1. Traditional technology companies. Many well-established computer companies
have noted that quantum computing may be the future of computer technology,
and they want to be leaders in the field. As such, they are investing heavily in
building quantum hardware and/or developing quantum software expertise.
2. Technology startup companies. Since quantum computing is a relatively new
technological field, there is still plenty of room for new companies with new
ideas. As a result, startups have also entered the nascent quantum computing
industry. Some specialize in hardware, others specialize in software, while oth-
ers are attempting both.
3. Companies that use computing technology. Banks, car companies, airplane
manufacturers, and accounting firms are all examples of companies that have
been hiring experts in quantum computing. They are not interested in build-
ing quantum computers themselves, but they want to know how future quantum
345
346
8 Next Steps
computers can be used for each of their businesses. If they wait for fault-tolerant
quantum computers to be built before investigating their uses, they will be left
behind by competitors.
These companies are desperately trying to hire qualified individuals. Some of the
jobs are quantumly technical, such as building quantum computers and developing
quantum algorithms. Other jobs are classically technical. For example, web pro-
grammers and software engineers were needed to create the IBM Quantum Expe-
rience website, and these jobs require little or no prior experience with quantum
computing. As another example, electrical engineers with experience with radio-
frequency devices can easily pivot to helping to build superconducting qubits, where
radio frequency interactions are very important. Still, other jobs are non-technical.
Companies need accountants, marketers, experts in human resources, business ad-
ministrators, and more who may not need any knowledge of quantum computing at
all, although a general understanding may be useful. All this is to say that if you
want a job related to quantum computing but do not have the quantum skills, yet,
there are job opportunities that utilize non-quantum skills.
More universities are also hiring professors in quantum computing, recognizing
the growth of the field. In 2017, the Division of Quantum Information was formed
in the American Physical Society, placing it alongside well-established areas of
physics like astrophysics, condensed matter physics, and particles and fields.
While many students are aware of industrial and academic jobs, often, students
have little exposure to government careers. To ignore government jobs, however,
is to ignore a large sector of the quantum computing ecosystem. For example, the
U.S. Department of Defense is the largest employer of scientists and engineers in
the United States, and many of these are civilian jobs in research laboratories. In-
creasingly, government laboratories are hiring people to investigate how quantum
computers affect the missions of their organizations. Besides technical roles, experts
in quantum computing are also needed for program management, policy, and advis-
ing roles to help the government prioritize quantum computing research and work-
force development. Government jobs typically come with good non-salary benefits,
including retirement pensions and work-life balance. There are also many jobs at
national laboratories. Although national laboratories are funded by the government,
they are managed by contractors, so employees at national laboratories are typically
employees of the contractors and are not government employees. But, many of their
jobs are similar in that they can be mission-focused.
If you want a quantumly technical job, you will likely want to study quantum
computing, mathematics, and physics beyond this introductory textbook, and some
suggestions for possible next steps are next.
8.2 Technical Next Steps
As stated in the preface and throughout this textbook, Nielsen and Chuang’s text-
book, Quantum Computation and Quantum Information is the standard advanced
8.2 Technical Next Steps
347
textbook and will dive deeply into many of the results that were out of the scope of
this introductory textbook, such as proving the Solovay-Kitaev theorem about uni-
versal sets of quantum gates, and calculating the probability that the quantum phase
estimation algorithm will yield the wrong answer. There are plenty of resources
on the internet as well, including lecture notes from professors, video lectures, and
tutorials.
A major concept that one should learn is the mixed state. In this entire textbook,
the state of a qubit was |0⟩, |1⟩, or some superposition of |0⟩and |1⟩. For all of these,
the state can be known with certainty, even though the measurement outcome may
be probabilistic. These states were visualized as points on the Bloch sphere, and they
are called pure states. In contrast, if we are not sure if a qubit is in one pure state
or another pure state, then the state itself and not just its outcome is probabilistic.
These are called mixed states, and they can be visualized as a point inside the Bloch
sphere, so in this context, it is actually a Bloch ball, which also contains the inside
and not just the surface. While a pure state was written using vectors (with “kets”
being column vectors and “bras” being row vectors), a mixed state is written using
a matrix called a density matrix. Some of the topics that we covered in this textbook
should actually be done in terms of mixed states, namely quantum error correction,
the no-signaling principle, and aspects of entanglement. Again, Nielsen and Chuang
go into detail about all this.
If you are a student, you may wonder what courses or majors to consider. Physics,
computer science, mathematics, and engineering are all fine. If you are interested in
quantum hardware, experimental physics or electrical engineering are good choices.
If you are interested in the theoretical side of quantum computing and quantum al-
gorithms, then theoretical physics, computer science, and mathematics are good
choices. Personally, I work on quantum algorithms. My PhD was in theoretical
physics, but my PhD advisor was a mathematician. Then, I did two postdoctoral
research fellowships in computer science before landing a tenure-track job as a
physics professor. So, my own story is a good illustration of the interplay between
physics, mathematics, and computer science.
For students, a great way to learn quantum computing is to do research with a
professor. Look around at your university to see if any professors work on quantum
computing, and do not be afraid to look outside of your department. Remember my
story, that I was a physics student who was advised by a math professor. If there
are no professors at your university who work on quantum computing, see if any
professors are interested in learning about quantum computing, and if you can do an
independent study with them, where you read Nielsen and Chuang or some other ed-
ucational resource and teach the professor what you have been learning. This way, if
you apply for graduate school, there is a professor who can write you a letter of rec-
ommendation and explain that you have been self-learning quantum computing and
even teaching them. Another way to gain research experience is to apply for sum-
mer internships and research fellowships. Many quantum computing companies, as
well as government and national labs, host students for summer research. There is
also the Research Experiences for Undergraduates (REU) program, and some of the
universities may have quantum computing research projects for students.
348
8 Next Steps
Some people make broad statements like, “You don’t need to know so and so to
do quantum computing.” For example, a common critique among some computer
scientists is that you do not need to know Schr¨odinger’s equation, the fundamental
physics equation of quantum mechanics, to do quantum computing. In some sense,
that is true, as this is literally the first time I have mentioned Schr¨odinger’s equation
in this textbook. But, that is because this textbook builds up to quantum circuits
and algorithms. There are many other aspects of quantum computing where know-
ing Schr¨odinger’s equation is necessary, such as for physical quantum hardware,
for analog quantum algorithms like continuous-time quantum walks, and for many
optimization algorithms like those based on quantum annealing or the quantum ap-
proximate optimization algorithm (QAOA). You may want to learn Schr¨odinger’s
equation yourself, depending on what aspects of quantum computing you wish to
pursue. If you major in physics, you will certainly come across it in a Modern
Physics course and in a Quantum Mechanics course. I bring this up to say there
is no one right path to be come a quantum information scientist. If you want to take
a traditional physics approach through Schr¨odinger’s equation, that is fine. If you
want to take an alternative approach, that is also fine. Just because your path does
not look like someone else’s does not mean it is the wrong path for you.
8.3 Questions
I regret that I do not have the capacity to respond to individual questions, nor do I
have the expertise to answer most questions beyond my specific research area. So,
if you have any questions, I suggest submitting them to the Quantum Computing
Stack Exchange at
https://quantumcomputing.stackexchange.com
Many members of our community volunteer their expertise on the website to help
others, including those who are newer to the field.
8.4 Parting Words
As we end our journey together, I again want to celebrate your completion of this
textbook. You did it! I hope you will consider quantum computing as a potential
career. If you do become a quantum information scientist, please let me know. I
would be delighted to hear that I played a role in your journey. Also, I wrote this
textbook for my students, that it might help them in their learning. By completing
this textbook, you have, in some way, also become one of my students, and so this
textbook is also dedicated to you.
Answers to Exercises
Exercises of Chapter 1
1.1 (a) 24 = 16. (b) 25 = 32.
1.2 (a) 64 = 1296. (b) 65 = 7776.
1.3 1.
1.4 (a) 5 coins. (b) 2 dice.
1.5 (a) 23. (b) 202.
1.6 (a) 101010. (b) 111101111.
1.7 (a) 15228. (b) 11111111. (c) FA = 250, 10 = 16, E4 = 228.
1.8
Binary
Decimal
(Two’s Complement) (Base 10)
000
0
001
1
010
2
011
3
100
-4
101
-3
110
-2
111
-1
1.9 Answer varies. See Table 1.1.
1.10 Quantum.
349
350
Answers to Exercises
1.11 (a)
A B Output
0 0
1
0 1
1
1 0
1
1 1
0
.
(b) NAND.
1.12 (a)
A B Output
0 0
1
0 1
0
1 0
0
1 1
0
.
(b) NOR.
1.13 (a) On. (b). On. (c). On. (d) Off. (e) NAND.
1.14 (a) Off. (b). On. (c) On. (d) On. (e) OR.
1.15 (a) On. (b). Off. (c) Off. (d) Off. (e) NOR.
1.16 (a) Off. (b). On. (c) Off. (d) XOR.
1.17 (a) Answer varies. One example is the Intel 8086, introduced in 1978, which
had 29,000 transistors.
(b) Answer varies. One example is the Apple M1, introduced in 2020, which had
16,000,000,000 transistors.
1.18 (a)
A B Output
0 0
0
0
0 0
1
1
0 1
0
1
0 1
1
0
1 0
0
1
1 0
1
0
1 1
0
0
1 1
1
1
.
(b) 0. (c) 1.
1.19
A B C Output
0 0 0
1
0 0 1
1
0 1 0
0
0 1 1
1
1 0 0
0
1 0 1
1
1 1 0
0
1 1 1
1
.
Answers to Exercises
351
1.20 (a) 221 = 22 = 4. (b) 222 = 24 = 16. (c) 223 = 28 = 256. (d) 224 = 216 = 65536.
(e) 22n.
1.21 Answer varies. One answer is:
A
B
AB
AB
AB + AB
1.22 Answer varies. One answer is ABC +ABC +ABC +ABC:
A
A
B
B
C
C
ABC
ABC
ABC
ABC
.
1.23 Answer varies. One answer is ABC +ABC = ABCABC:
352
Answers to Exercises
A
A
B
B
C
C
ABC
ABC
1.24 In the text, the OR gate was implemented using three NOT gates and one AND
gate. Each NOT gate can be implemented using a single NAND gate, and the AND
gate can be implemented by one NAND gate and one NOT gate. The last two NOT
gates cancel out, however, yielding the following:
A
B
A + B
.
1.25 Start with {NOT,AND,OR}:
A
B
AB + AB
Then replace the gates with NANDs:
Answers to Exercises
353
A
B
AB + AB
1.26
A
B
AB
C
ABC
1.27
A B A B A+B A+B
0 0 1 1
1
0
0 1 1 0
1
0
1 0 0 1
1
0
1 1 0 0
0
1
1.28 (a)
A Output
0
1
1
0
(b)
A B Output
0 0
0
0 1
1
1 0
1
1 1
1
1.29 The outputs are all 1. Makes sense because B+B = 1, and A+1 = 1.
1.30 Changing the OR to an XOR does not change the logic, so the truth table stays
the same.
1.31 10000 or 16.
1.32 11100110.
1.33 10100 or 20.
1.34 A+B.
1.35 A+B+C.
1.36 ABC +ABC +ABC = B(A+C).
1.37 (a) ABC +ABC +ABC +ABC. (b) AB+AC.
1.38 (a) Irreversible. (b) Irreversible. (c) Irreversible. (d) Irreversible.
354
Answers to Exercises
1.39 (a) Irreversible. (b) Reversible.
1.40 (a)
A B C A′ B′ C′
0 0 0 0 0 0
0 0 1 0 0 1
0 1 0 0 1 0
0 1 1 0 1 1
1 0 0 1 0 0
1 0 1 1 1 0
1 1 0 1 0 1
1 1 1 1 1 1
.
(b) Reversible since the outputs are unique.
1.41 (a)
A B C A B AB⊕C
0 0 0 0 0
1
0 0 1 0 0
0
0 1 0 0 1
0
0 1 1 0 1
1
1 0 0 1 0
0
1 0 1 1 0
1
1 1 0 1 1
0
1 1 1 1 1
1
.
(b) De Morgan’s Law. (c) NOR. (d) OR. (e) Start with
a Toffoli gate and add a NOT gate before the first two
inputs and after the first two outputs.
1.42 (a) Reversible. (b) Irreversible. A reversible version is:
A
A
Gate
f(A)
B
f(A) ⊕B
1.43 The truth table is
A B A⊕B
0 0
0
0 1
1
1 0
1
1 1
0
.
It is irreversible. A reversible version is:
Answers to Exercises
355
A
A
B
B
A ⊕B
C
C ⊕A ⊕B
A B C A B A⊕B⊕C
0 0 0 0 0
0
0 0 1 0 0
1
0 1 0 0 1
1
0 1 1 0 1
0
1 0 0 1 0
1
1 0 1 1 0
0
1 1 0 1 1
0
1 1 1 1 1
1
1.44 (a) The truth table is
A B A⊕B B
0 0
0
0
0 1
1
1
1 0
1
0
1 1
0
1
.
It is reversible.
(b) The truth table is
A B A⊕B AB
0 0
0
0
0 1
1
0
1 0
1
0
1 1
0
1
.
It is irreversible. A reversible version is
A
A
B
B
Gate
A ⊕B
C
A ⊕B ⊕C
AB
D
AB ⊕D
1.45
356
Answers to Exercises
Cin
Cin
A
A
B
B
FA
S
Cout
D
S ⊕D
E
Cout ⊕E
1.46 (a) 213 = 8192. (b) spontaneously flip, Radioactive atoms, presence, absense,
single event upset. (c) miniaturized. (d) increased, greater, sky (e) cosmic rays, parti-
cles, black holes. (f) cascade, transistor. (g) error correction code. (h) month, cosmic
rays. (i) 10 to 30. (j) 161. (k) flash, every star.
1.47 (a) Even. (b) No. (c) Yes. The parity of the first seven bits doesn’t match the
parity bit. (d) No. If two bits flipped, the parity would be unchanged.
1.48 (a) Yes, the middle bit has flipped. (b) Yes, the left bit has flipped. (c) 0.1040.
Decrease, since 0.1040 < 0.2. (d) 0.7840. Increase, since 0.78404 > 0.7.
1.49 (a) Yes, b1 was flipped. (b) Yes, b2 and b1 were flipped. (c) 10p3(1 −p)2 +
5p4(1−p)+ p5. (d) p < 1/2. (e) 0.00856, decreases. (f) For 3-bit code, the proba-
bility of an uncorrectable error is 0.028, which is more likely than the 5-bit code’s
0.00856.
1.50 (a) f(100). (b) g(100). (c) They are equal when n = 477, after which g(n) is
greater than f(n). (d) true, true, false, false, false.
1.51 Possibilities are (a) iii or v, (b) i or v, (c) ii or v, (d) v, (e) i or iv. Thus, the only
correct answer is (a) iii, (b) i, (c) ii, (d) v, (e) iv.
1.52 (a) Efficient. (b) Efficient. (c) Efficient. (d) Efficient. (e) Efficient. (f) Efficient.
1.53 (a) Bounded-Error Probabilistic Polynomial-Time. (b) “As the class of feasible
problems for a computer with access to a genuine random-number source.”
1.54 Many possible answers, including factoring, graph isomorphism, n × n Su-
doku, traveling salesman, Hamiltonian path, and bin packing.
1.55
(a) Yang–Mills and Mass Gap, Riemann Hypothesis, P vs NP Problem,
Navier–Stokes Equation, Hodge Conjecture, Poincar´e Conjecture, and Birch and
Swinnerton-Dyer Conjecture. Only the Poincar´e Conjecture is solved. (b) Stephen
Cook and Leonid Levin in 1971.
1.56 Answers vary.
Answers to Exercises
357
1.57 Answers vary.
1.58 (a) ▷0 0 1. (b) ▷0 1 1. (c) ▷1 0 1. (d) ▷1 1 0. (e) NAND. (f) Calculates the
NAND of all the bits.
1.59 Answers may vary. Here is one:
Current State Current Tape Write to Tape Move Update State
qs
▷
▷
→
q1
q1
0
0
→
q1
q1
1
1
→
q2
q2
0
0
→
q2
q2
1
1
→
q1
q1
0
•
qh
q2
1
•
qh
1.60 (a) Run forever. (b) Halt. (c) By returning true, H is saying that Z halts. But
then Z responds to this by running forever. Z can’t both halt and run forever. That’s
a contradiction. (d) By returning false, H is saying that Z runs forever. But then Z
responds to this by halting. Z can’t both run forever and halt. That’s a contradiction.
1.61 (a) complete, consistent, decidable. (b) programs, themselves. (c) Can every
even number greater than 2 be written as the sum of two primes? (d) Runs forever.
(e) Halts. (f) Runs forever. Yes, it’s a contradiction. (g) Runs forever. Yes, it’s a
contradiction. (h) undecidable, can’t solve.
1.62 Answers vary.
1.63 (a) a million times. (b) all known, quantum supremacy.
1.64 (a) Simulating quantum physics. (b) Hardest. (c) Some.
Exercises of Chapter 2
2.1 (a) 1. (b) 1. (c) 0. (d) +. (e) 0. (f) −i. (g) 0. (h) 0.
2.2 (a) −i. (b) i. (c) −i. (d) −i. (e) −. (f) 0. (g) i. (h) 0 or 1 depending on die roll.
2.3 (a) The north pole. (b) The south pole. (c) The equator. (d) The northern hemi-
sphere. (e) The southern hemisphere.
2.4 (a) 1. (b) 2. (c)
358
Answers to Exercises
Re
Im
(d)
√
5eitan−1(2) =
√
5ei(1.107) =
√
5ei63.4◦. (e) 1−2i or
√
5e−i(1.107). (f)
√
5. (g) 5.
2.5 (a) −3. (b) −1. (c)
Re
Im
(d)
√
10ei(tan−1(1/3)+π) =
√
10ei(0.32+π) =
√
10ei3.46
or
√
10ei(18.4◦+180◦) =
√
10ei198.4◦. (e) −3+i or
√
10e−i3.46. (f)
√
10. (g) 10.
2.6 (a) 4/9. (b) 5/9.
2.7 (a) 1. (b) 0.
2.8 2/
√
5eiθ for any real θ.
2.9 (a) A = eiθ/
√
13 for any real θ. (b) 4/13. (c) 9/13.
2.10 (a) |0⟩with probability 1/4 or |1⟩with probability 3/4. (b) 1−
√
3
2
√
2
|+⟩+
1+
√
3
2
√
2
|−⟩. (c) |+⟩with probability (2 −
√
3)/4 ≈0.07 or |−⟩with probability
(2+
√
3)/4 ≈0.93.
2.11 (a)
√
3(1+i)
4
|a⟩−3+i
4 |b⟩. (b) |a⟩with probability 3/8 or |b⟩with probability
5/8.
2.12 (a) 1/2. (b) 1/2.
Answers to Exercises
359
2.13 (a) No, they differ by a global phase, which is irrelevant. They are the same
quantum state. (b) Yes. Measuring in the X-basis distinguishes them. (c) No, they
differ by a global phase, which is irrelevant. They are the same quantum state.
2.14 (a) (θ,φ) = (90◦,90◦) = (π/2,π/2). (b)
x
y
z
1
√
2 (|0⟩+ i|1⟩)
2.15 (a) (θ,φ) = (120◦,45◦) = (2π/3,π/4). (b)
x
y
z
2.16 (θa,φa) = (π/2,π/2) and (θb,φb) = (2π/3,π/2), so θb = π −θa and φb =
φa +π.
2.17 (a) (x,y,z) = (0,1,0). (b) (x,y,z) = (
√
3/2
√
2,
√
3/2
√
2,−1/2).
2.18 (a) polarization, decoherence. (b) electric fields. (c) laser beams. (d) molecule,
radio-frequency, one-qubit. (e) discrete energy levels, atomic nucleus. (f) Quantum
information, hyperfine. (g) spin, microwave, optical. (h) charge, flux, phase.
2.19 Answers vary.
2.20 Answers vary.
2.21 Answers vary. One is the Josephson junction is used to create a superconduct-
ing flux qubit, which uses currect as its qubit. Clockwise current corresponds to |0⟩,
and counterclockwise current corresponds to |1⟩.
2.22 (a) (α + β)|0⟩+ (α −β)|1⟩. (b) No, the total probability is 2, which is not
possible.
360
Answers to Exercises
2.23 (a)
 √
3
2 α +
√
3+i
4
β

|0⟩+
 √
3+i
4
α + −
√
3−3i
4
β

|1⟩. (b) No, the total probabil-
ity is 2, which is not possible.
2.24 (a) Yes, reversible. (b) No, not reversible.
2.25 (a) Yes, reversible. (b) No, not reversible.
2.26 ZX(α|0⟩+β|1⟩) = Z(β|0⟩+α|1⟩) = β|0⟩−α|1⟩
2.27 (a) XZXZ|0⟩= −|0⟩, XZXZ|1⟩= −|1⟩. (b) ZXZX|0⟩= −|0⟩, ZXZX|1⟩=
−|1⟩.
2.28 (a) α|0⟩+βeiθ|1⟩. (b) |α|2 +|βeiθ|2 = |α|2 +|β|2 = 1.
2.29 (a) α+β
√
2 |0⟩+ α+β
√
2 |1⟩. (b)
 α+β
√
2

2
+
 α+β
√
2

2
= |α|2 +|β|2 = 1.
2.30 Answers vary.
2.31 YH|0⟩= Y|+⟩= −i|−⟩.
2.32 HXH|0⟩= Z|0⟩= |0⟩and HXH|1⟩= Z|1⟩= −|1⟩.
2.33 (a)
1
2
√
2

(1−i+eiπ/4)|0⟩+(1+i)|1⟩

or
1
2
√
2
h
1+
√
2+(1−
√
2)i

|0⟩+(1+i)|1⟩
i
. (b) 3/4 and 1/4.
2.34 (a) (0,0,1). (b) eiγ 
cos
  π
8

I −isin
  π
8

Z

. (c) eiγe−iπ/8|0⟩. (d) eiγeiπ/8|1⟩. (e)
If γ = π/8, get U|0⟩= |0⟩and U|1⟩= eiπ/4|1⟩, which is the T gate.
2.35 (a)
x
y
z
(b)
x
y
z
(c) (1/
√
3,1/
√
3,1/
√
3). (d) −ieiγ 1
√
3 (X +Y +Z). (e) −ieiγ 1
√
3 [|0⟩+(1+i)|1⟩]. (f)
−ieiγ 1
√
3 [(1−i)|0⟩−|1⟩].
2.36 (a,b)
(c) 85.4% probability of getting |0⟩, and 14.6% probability of getting |1⟩.
Answers to Exercises
361
Exercises of Chapter 3
3.1
 
1
2
−
√
3
2
!
.
3.2 |0⟩with probability 3/4 and |1⟩with probability 1/4.
3.3 (a)
√
3
2 ⟨0|+ 1
2⟨1|. (b)
 √
3
2
1
2

. (c) 2
3⟨0|+ 1+2i
3 ⟨1|. (d)
  2
3
1+2i
3

.
3.4 (a) (3+2
√
15−i
√
3)/16. (b) (3+2
√
15+i
√
3)/16. (c) Complex conjugates.
3.5 (a) 13|A|2. (b) A = 1/
√
13.
3.6 (a) ⟨+|−⟩=
 1 0

0
1

= 0, so orthogonal.
(b) ⟨0|+⟩= 1/
√
2 ̸= 0, so not orthogonal.
(c) The inner product is 0, so orthogonal.
3.7 (a) x = (−3+i
√
3)/8. (b) x = eiθ√
15/4. (c) none.
3.8
⟨a|b⟩= cos(θa/2)cos(θb/2)+ei(θb−θa) sin(θa/2)sin(θb/2)
= cos(θa/2)cos(θb/2)−sin(θa/2)sin(θb/2)
= cos((θa +θb)/2) = cos(π/2) = 0.
3.9 (a) (
√
3 −i)/2
√
2. (b) (
√
3 + i)/2
√
2. (c) |i⟩with probability 1/2, |−i⟩with
probability 1/2. (d) (3−i)/4. (e)
√
3(1−i)/4. (f) |a⟩with probability 5/8, |b⟩with
probability 3/8.
3.10 (a) |0⟩with probability 3/4 and |1⟩with probability 1/4. (b) |+⟩with proba-
bility 1/8 and |−⟩with probability 7/8. (c) |i⟩with probability (4+
√
3)/8 ≈0.717,
|−i⟩with probability (4−
√
3)/8 = 0.283.
3.11 (a) 1−i
√
3 |+⟩−
i
√
3|−⟩. (b) 1−3i
2
√
3 |i⟩+ 1−i
2
√
3|−i⟩.
3.12 (a) U =
1
√
2

1 −i
−i 1

. (b)
1
√
2

α −iβ
−iα +β

. (c) Yes, the total probability is
 α−iβ
√
2

2
+
 −iα+β
√
2

2
= 1.
3.13 (a) U =
1
2
√
3

3+i
1−i
−(1+i) 3−i

. (b) See https://bit.ly/3qR5HnR. |0⟩with
probability 83.3%, |1⟩with probability 16.7%.
3.14
1
√
2

1 −1
1 1

.
3.15
362
Answers to Exercises
HTU|0⟩= 1
√
2

1 1
1 −1
1
0
0 eiπ/4
 1
2
√
2−i
1
−1
√
2+i

1
0

=
1
2
√
2
√
2−i−eiπ/4
√
2−i+eiπ/4

.
3.16 (a) XY|0⟩= i|0⟩, iZ|0⟩= i|0⟩. XY|1⟩= −i|1⟩, iZ|1⟩= −i|1⟩. (b)
XY =

1 0
0 1

0 −i
i 0

=

i 0
0 −i

, iZ = i

1 0
0 −1

=

i 0
0 −i

.
3.17 U†U =

1 i
−i 1

̸= I, so no.
3.18 U†U = I, so yes. U|0⟩= |i⟩, and U|1⟩= |−i⟩.
3.19 (a)
U−1 = U† =
 1+
√
3
2
√
2 −i 1−
√
3
2
√
6
−1+
√
3
2
√
6
−i 1−
√
3
2
√
6
1−
√
3
2
√
6 −i 1−
√
3
2
√
6
1+
√
3
2
√
2 −i −1+
√
3
2
√
6
!
(b) |ψ⟩=
√
3
2 |0⟩+ 1
2|1⟩.
3.20 (a) 1
2

1 −1
i −i

. (b) No, not unitary.
3.21 (a)
1
√
2

1 1
1 −1

. (b) Yes, it is unitary.
3.22
1
√
2

1
1
 1
√
2
 1 1

+ 1
√
2

1
−1
 1
√
2
 1 −1

=

1 0
0 1

.
3.23

1
0
 1 0

+ 1
√
2

1
1
 1
√
2
 1 1

= 1
2

3 1
1 1

.
Exercises of Chapter 4
4.1 (a) When the other player is on ONE. (b) To planet Phi Minus. (c) When the
blue player uses an H engine card.
4.2 (a) 5. (b) Conceptual. (c) 50-60%. (d) Entanglement.
4.3 (a) 0. (b) −1/2. (c) 0.
4.4 |1⟩⊗|1⟩⊗|0⟩=

0
1

⊗

0
1

⊗

1
0

.
Answers to Exercises
363
4.5 (a) |ψ⟩=




1/2
0
i/
√
2
(
√
3+i)/4



. (b) ⟨ψ| =
 1/2 0 −i/
√
2 (
√
3−i)/4

.
4.6




1
0
0
0




 1 0 0 0

+




0
1
0
0




 0 1 0 0

+




0
0
1
0




 0 0 1 0

+




0
0
0
1




 0 0 0 1

=




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1



.
4.7 |00⟩with probability 1/10, |01⟩with probability 1/2, |10⟩with probability
1/10, or |11⟩with probability 3/10.
4.8 A = 2/
√
17.
4.9
1
√
5|00⟩+ 2
√
5|01⟩with probability 5/16 or 2
√
2
√
11|10⟩+
q
3
11|11⟩with probability
11/16.
4.10 (|000⟩+
√
6|010⟩)/
√
7 with probability 7/36, (
√
2|001⟩+3|011⟩)/
√
11 with
probability 11/36, (|100⟩+ |110⟩)/
√
2 with probability 1/18, or (|101⟩+
√
3|111⟩)/2 with probability 4/9.
4.11 (a) Entangled state. (b) Product state. |1⟩⊗1
√
2 (|0⟩+i|1⟩) = |1⟩⊗|i⟩.
4.12 (a) Product state.
 √
3
2 |0⟩+ 1
2|1⟩

⊗
 √
3
2 |0⟩−1
2|1⟩

. (b) Entangled state.
4.13 (a) (X ⊗I)|Ψ +⟩= (I ⊗X)|Ψ +⟩= |Φ+⟩.
(b) (X ⊗I)|Φ+⟩= (I ⊗X)|Φ+⟩= |Ψ +⟩.
(c) (X ⊗I)|Ψ −⟩= −|Φ−⟩≡|Φ−⟩, (I ⊗X)|Ψ −⟩= |Φ−⟩.
(d) (X ⊗I)|Φ−⟩= −|Ψ −⟩≡|Φ−⟩, (I ⊗X)|Φ−⟩= |Ψ −⟩.
4.14 (a)
1
√
2




0 1 0
1
1 0 1
0
0 1 0 −1
1 0 −1 0



. (b) 2+
√
3
4
√
2 |00⟩+ 4+
√
2
8
|01⟩+ 2−
√
3
4
√
2 +
√
2−4
8
|11⟩.
4.15 (a)




0 0 1 0
0 0 0 1
0 1 0 0
1 0 0 0



. (b)




0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1



. (c)




1 0 0
0
0 1 0
0
0 0 0 −1
0 0 −1 0



. (d)




1 0 0 0
0 −1 0 0
0 0 0 −1
0 0 1 0



.
364
Answers to Exercises
4.16 (a)
A B C A′ B′ C′
0 0 0 0 0 0
0 0 1 0 0 1
0 1 0 0 1 1
0 1 1 0 1 0
1 0 0 1 0 1
1 0 1 1 0 0
1 1 0 1 1 0
1 1 1 1 1 1
.
(b) It is the same.
4.17 As matrices, both circuits are equal to




0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1



.
4.18 CNOT|+⟩|−⟩= |−⟩|−⟩, CNOT|−⟩|+⟩= |−⟩|+⟩, CNOT|−⟩|−⟩= |+⟩|−⟩.
4.19
1
√
2




1
0
0
1




1
√
2
 1 0 0 1

+ 1
√
2




1
0
0
−1




1
√
2
 1 0 0 −1

+ 1
√
2




0
1
1
0




1
√
2
 0 1 1 0

+ 1
√
2




0
1
−1
0




1
√
2
 0 1 −1 0

=




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1



.
4.20 CZ =




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 −1



.
4.21 (a) SWAP|ω0⟩= |ω3⟩. (b) (X ⊗I)|ω1⟩= |ω3⟩. (c) CNOT01|ω2⟩= |ω0⟩. (d)
CNOT|ω3⟩= |ω2⟩.
4.22 (a) MS =
1
√
2




1 0
0 i
0 1 −i 0
0 −i 1 0
i 0
0 1



. (b) MS8 = I.
Answers to Exercises
365
4.23












1 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 1
0 0 0 0 0 0 1 0












.
4.24 (a) |000⟩→|001⟩, |001⟩→|000⟩, and everything else stays the same.
(b)












0 1 0 0 0 0 0 0
1 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1












.
4.25 (a) Yes, since they are orthogonal. (b) Yes, since they are orthogonal. (c) No,
since they are not orthogonal.
4.26 See https://bit.ly/30LBtIi:
4.27 See https://bit.ly/30BByOi:
366
Answers to Exercises
4.28 (a) After all the carries, but before the CNOT, the bottom four qubits are
c′
3, a3, a3 ⊕b3, and s4. Taking the CNOT of c′
3 and a3 ⊕b3 changes the target to
s3 = a3 ⊕b3 ⊕c′
3.
(b) The number of C gates is n, and the number of C† gates is (n−1). Both C and C†
each have two Toffoli gates, so there is a total of 2n+2(n−1) = 4n−2 Toffoli gates.
The C and C† gates each have one CNOT gate, and the n−1 S gates each have two
CNOT gates, plus the extra CNOT, for a total of n+(n−1)+2(n−1)+1 = 4n−2
CNOT gates.
4.29 The final state should be |s⟩|b⟩= |1010⟩|0011⟩. The circuit can be viewed in
Quirk at https://bit.ly/3omghlU, and a picture is below:
4.30 (a) 14 Toffoli gates and 16 CNOT gates. (b) 30 Toffoli gates and 32 CNOT
gates.
4.31 3n+1.
4.32 (a) Tom Wong, the author of the textbook. (b) hands, quantum, some, no faster.
4.33 (a) Missing complex amplitudes.
(b) Cannot generate entanglement (no 2-qubit gates).
(c) Cannot generate entanglement. SWAP is 2-qubit, but cannot generate entangle-
ment.
4.34 Possible answers are replacing CNOT with Toffoli, H with Rπ/8, or S with T.
4.35
Answers to Exercises
367
|ψ00⟩= α|000⟩+β|100⟩
CNOT2,1
−−−−−→α|000⟩+β|110⟩
CNOT2,0
−−−−−→α|000⟩+β|111⟩.
4.36 (a) Nothing. (b) Apply X then Z to the rightmost qubit. (c) Apply X then Z to
the leftmost qubit. (d) Apply X then Z to the middle qubit.
4.37
|ψ00⟩= α|000⟩+β|100⟩
CNOT2,1
−−−−−→α|000⟩+β|110⟩
CNOT2,0
−−−−−→α|000⟩+β|111⟩
H⊗3
−−→α|+++⟩+β|−−−⟩.
4.38 (a) |0++⟩. (b) |1−+⟩. (c) |1+−⟩. (d) |0−−⟩. (e) It outputs 0 when there is
an even number of |−⟩’s, and outputs 1 when there is an odd number of |−⟩’s.
4.39 In the top row, the Hadamard gate can be moved to the end of the circuit. In
the next row, H2 = I, so we can remove the Hadamard gates in the middle. In the
next row, we can move the Hadamard gate to the beginning of the circuit.
4.40 (a) Probability 1−ε2, and the resulting state is α|+++⟩+β|−−−⟩, so there
is no error to correct.
(b) Probability ε2, and the resulting state is α|−++⟩−β|+−−⟩, so we apply
Z ⊗I ⊗I to get α|+++⟩−β|−−−⟩, then X ⊗I ⊗I to get α|+++⟩+β|−−−⟩.
(c) Probability zero.
(d) Probability zero.
4.41 (a)
|ψ00000000⟩= α|000000000⟩+β|100000000⟩
CNOT8;5,2
−−−−−−→α|000000000⟩+β|100100100⟩
(b)
H8,H5,H2
−−−−−→α|+⟩|00⟩|+⟩|00⟩|+⟩|00⟩+β|−⟩|00⟩|−⟩|00⟩|−⟩|00⟩
= α 1
√
2
(|0⟩+|1⟩)|00⟩1
√
2
(|0⟩+|1⟩)|00⟩1
√
2
(|0⟩+|1⟩)|00⟩
+β 1
√
2
(|000⟩−|100⟩) 1
√
2
(|000⟩−|100⟩) 1
√
2
(|000⟩−|100⟩)
= α
√
2
(|000⟩+|100⟩)(|000⟩+|100⟩)(|000⟩+|100⟩)
+ β
√
2
(|000⟩−|100⟩)(|000⟩−|100⟩)(|000⟩−|100⟩)
368
Answers to Exercises
(c)
CNOTs
−−−−→α
√
2
(|000⟩+|111⟩)(|000⟩+|111⟩)(|000⟩+|111⟩)
+ β
√
2
(|000⟩−|111⟩)(|000⟩−|111⟩)(|000⟩−|111⟩)
= α|0L⟩+β|1L⟩.
4.42 Qubits q6, q5, and q1 flipped. Correct by applying an X gate to each one.
4.43 See https://bit.ly/3wvdGrI:
4.44 Answers vary.
4.45 See https://bit.ly/309WJXu:
4.46 (a) Qubits |q7⟩, |q3⟩, and |q1⟩flipped. Fix by applying the X gate to each one.
(b) Triplet0 has a phase flip. Fix by applying a Z gate to any one of |q2⟩, |q1⟩, or
|q0⟩.
Exercises of Chapter 5
5.1 Answers vary.
5.2 All 1024 times, the output should be 111.
Answers to Exercises
369
5.3 (a) U2(0,π). (b) U1(−π/4). (c) U3(θ,−π/2,π/2).
(d)
gate cz a,b
{
h b;
cx a,b;
h b;
}
(e)
gate ccx a,b,c
{
h c;
cx b,c;
tdg c;
cx a,c;
t c;
cx b,c;
tdg c;
cx a,c;
t b;
t c;
h c;
cx a,b;
t a;
tdg b;
cx a,b;
}
5.4 (a)
OPENQASM 2.0;
include "qelib1.inc";
qreg q[3];
creg c[3];
h q[2];
cx q[2], q[1];
cx q[0], q[1];
h q[0];
cx q[1], q[2];
cz q[0], q[2];
measure q -> c;
(b)
(c) |000⟩, |001⟩, |010⟩, or |011⟩, each with probability 1/4.
5.5 Answers vary.
370
Answers to Exercises
Exercises of Chapter 6
6.1 Answers vary.
6.2
|0⟩with probability 1/2, and the state collapses to
√
3
2 |00⟩+ 1
2|01⟩=
|0⟩
 √
3
2 |0⟩+ 1
2|1⟩

. |1⟩with probability 1/2, and the state collapses to 1
2|10⟩+
√
3
2 |11⟩= |1⟩

1
2|0⟩+
√
3
2 |1⟩

. Partially entangled.
6.3 |0⟩with probability 1/2, and the state collapses to |01⟩. |1⟩with probability
1/2, and the state collapses to |10⟩. Maximally entangled.
6.4 (a) right now. (b) space, space.
6.5 (a)
HTHS
1
p
4+2
√
2
h
(1+
√
2)|0⟩+|1⟩
i
= 1+
√
2+i
p
4+2
√
2
|0⟩= eiθ|0⟩,
where θ = tan−1

1
1+
√
2

,
HTHS
1
p
4−2
√
2
h
(1−
√
2)|0⟩+|1⟩
i
= 1−
√
2+i
p
4−2
√
2
|1⟩= eiθ|1⟩,
where θ = π −tan−1 
1+
√
2

.
(b)
HT †HS
1
p
4+2
√
2
h
(−1−
√
2)|0⟩+|1⟩
i
= −1−
√
2+i
p
4+2
√
2
|0⟩= eiθ|0⟩,
where θ = π +tan−1 
1−
√
2

,
HT †HS
1
p
4−2
√
2
h
(−1+
√
2)|0⟩+|1⟩
i
= −1+
√
2+i
p
4−2
√
2
|1⟩= eiθ|1⟩,
where θ = tan−1 
1+
√
2

.
6.6 (a) 2−
√
2
8
. (b) 2+
√
2
8
. (c) 2+
√
2
8
. (d) 2−
√
2
8
. (e) −1
√
2.
6.7 (a) photons. (b) −2 ≤S ≤2. (c) S = 2.70 ± 0.05. (d) S = 2.697 ± 0.015. (e)
disagree.
6.8 (a) electron. (b) detection loophole. (c) 1.3 kilometers. (d) CHSH-Bell inequal-
ity, S ≤2. (e) 2.42±0.20. (f) 0.039.
6.9
1
√
2 (|++⟩+|−−⟩).
Answers to Exercises
371
6.10 (a) realism. (b) superposition. (c) incomplete, hidden variables. (d) locality,
Locality, speed of light. (e) entangled pair, instantly, local hidden variables (f) forces
(g) hidden variables, Nothing, no correlation (h) defined values, spooky action (i)
Bell inequalities, (j) violated, faster than (k) local hidden variables (l) realism, lo-
cality (m) consistent, information (n) Copenhagen
6.11 (b) |000⟩or |111⟩, each with probability 1/2.
(c) No. |000⟩and |111⟩are product states.
6.12 (b)
1
√
2 (|001⟩+|010⟩) with probability 2/3 or |100⟩with probability 1/3.
(c)
1
√
2 (|001⟩+|010⟩) = |0⟩|Ψ +⟩, so there is entanglement between the right two
qubits. |100⟩is not entangled.
6.13 ⟨Φ+|Φ+⟩= 1, ⟨Φ+|Ψ +⟩= 0, ⟨Φ+|Φ−⟩= 0, ⟨Φ+|Ψ −⟩= 0, ⟨Ψ +|Ψ +⟩= 1,
⟨Ψ +|Φ−⟩= 0, ⟨Ψ +|Ψ −⟩= 0, ⟨Φ−|Φ−⟩= 1, ⟨Φ−|Ψ −⟩= 0, ⟨Ψ −|Ψ −⟩= 1.
6.14 (a) 4. (b) 2. (c) 4.
6.15 (a) |11⟩. (b) Answers vary. (c) In the middle section, instead of applying X and
Z, Alice simply applies X.
6.16 (b)
|00⟩with probability 1/4, collapses to |00⟩(β|0⟩+α|1⟩),
|01⟩with probability 1/4, collapses to |01⟩(α|0⟩+β|1⟩),
|10⟩with probability 1/4, collapses to |10⟩(−β|0⟩+α|1⟩),
|11⟩with probability 1/4, collapses to |11⟩(α|0⟩−β|1⟩).
(c)
|00⟩(β|0⟩+α|1⟩), X gate
|01⟩(α|0⟩+β|1⟩), nothing
|10⟩(−β|0⟩+α|1⟩), X then Z gates
|11⟩(α|0⟩−β|1⟩). Z gate.
6.17 (b)
|000⟩with probability 1/4, collapses to |000⟩(α|0⟩+β|1⟩),
|010⟩with probability 1/4, collapses to |010⟩(β|0⟩+α|1⟩),
|100⟩with probability 1/4, collapses to |100⟩(α|0⟩−β|1⟩),
|110⟩with probability 1/4, collapses to |110⟩(−β|0⟩+α|1⟩).
(c)
|000⟩(α|0⟩+β|1⟩), nothing
|010⟩(β|0⟩+α|1⟩), X gate
372
Answers to Exercises
|100⟩(α|0⟩−β|1⟩), Z gate
|110⟩(−β|0⟩+α|1⟩), X then Z gates.
6.18 (b)
|00⟩with probability 1/4, collapses to |00⟩(α|00⟩+β|11⟩),
|01⟩with probability 1/4, collapses to |01⟩(β|00⟩+α|11⟩),
|10⟩with probability 1/4, collapses to |10⟩(α|00⟩−β|11⟩),
|11⟩with probability 1/4, collapses to |11⟩(−β|00⟩+α|11⟩).
(d)
|000⟩(α|0⟩+β|1⟩), nothing
|001⟩(α|0⟩−β|1⟩), Z gate
|010⟩(−β|0⟩+α|1⟩), X gate
|011⟩(−β|0⟩−α|1⟩), Z then X
|100⟩(α|0⟩+β|1⟩), nothing
|101⟩(α|0⟩−β|1⟩), Z gate
|110⟩(β|0⟩+α|1⟩), X then Z gates
|111⟩(β|0⟩−α|1⟩), Z then X then Z gates.
6.19 1000011 1101000 1100001 1110010 1101100 1101001 1100101 = Charlie.
6.20 (a) 3. (b) 1.
6.21 (a) 7493. (b) Any e satisfying gcd(e,7308) = 1. (c) d = e−1 mod 7308.
6.22 1679734.
6.23 1501096.
6.24
(a) 330 bits, 1991. (b) As of this writing, RSA 250, 829 bits, 2020. (c)
$200,000.
6.25 |−⟩,|+⟩,|0⟩,|1⟩,|0⟩,|+⟩,|+⟩,|−⟩,|−⟩.
6.26 0011.
6.27 log0.75(0.01) = 16.
Answers to Exercises
373
Exercises of Chapter 7
7.1 (a)
x y x y⊕f(x)
0 0 0
1
0 1 0
0
1 0 1
0
1 1 1
1
(b) Yes, since the outputs are unique. (c)
Uf =




0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1



.
(d) U†
f Uf = I.
7.2 (a) f(0) = 1:
(b) f(1) = 0:
7.3
 √
3
2 |0⟩+ 1
2|1⟩

|−⟩
=
√
3
2
√
2 (|0⟩|0⟩−|0⟩|1⟩) +
1
2
√
2 (|1⟩|0⟩−|1⟩|1⟩)
Uf
−→
√
3
2
√
2 (|0⟩|0⊕f(0)⟩−|0⟩|1⊕f(0)⟩)
+
1
2
√
2 (|1⟩|0⊕f(1)⟩−|1⟩|1⊕f(1)⟩)
=
√
3
2 (−1)f(0)|0⟩|−⟩+ 1
2(−1)f(1)|1⟩|−⟩=
 √
3
2 (−1)f(0)|0⟩+ 1
2(−1)f(1)|1⟩

|−⟩.
7.4 (a) |x⟩|+⟩. (b) |x⟩|+⟩. (c) They are equal to the initial state, so the oracle does
nothing to |x⟩|+⟩.
7.5 (a) The parity is 0 or even. See https://bit.ly/3s9rmsa:
(b) b0 = 1. (c) b1 = 1. (d) b0 ⊕b1 = 0. (e) 2.
7.6 (a) The parity is 1 or odd.
(b) Answers vary.
7.7 |0⟩with probability 1/2 or |1⟩with probability 1/2.
7.8 (a) 1 or odd. (b) Four queries. (c) 8.
374
Answers to Exercises
7.9 (a) 0 or even. (b) Five queries. (c) 9.
7.10 (a) 0.015625. (b) 0.0078125. (c) c = 11.
7.11
(H ⊗H ⊗H)|000⟩= |+++⟩= (|000⟩+ |001⟩+ |010⟩+ |011⟩+ |100⟩+
|101⟩+ |110⟩+ |111⟩)/23/2. Get one of the basis states |000⟩, |001⟩, |010⟩, |011⟩,
|100⟩, |101⟩, |110⟩, or |111⟩, each with probability 1/8.
7.12 (a) See https://bit.ly/3nwbFFS:
This yields |100⟩, so the function is balanced.
(b)
b2 b1 b0 f(b0,b1,b2)
0 0 0
0
0 0 1
0
0 1 0
0
0 1 1
0
1 0 0
1
1 0 1
1
1 1 0
1
1 1 1
1
(c) Balanced. (d) 5.
7.13 (a) See https://ibm.co/3yykVjK:
This yields |000⟩, so the function is constant. (b) Answers vary.
7.14 (a) See https://bit.ly/3m30FS6:
Answers to Exercises
375
s = 100101. (b) Set |y⟩= |0⟩. Then, |y⊕f(b5,...,b0)⟩= |f(b5,...,b0)⟩. Get s0 =
f(000001) = 1,s1 = f(000010) = 0,s2 = f(000100) = 1,s3 = f(001000) = 0,s4 =
f(010000) = 0,s5 = f(100000) = 1. (c) 6.
7.15 (a) See https://ibm.co/3m0WPZB:
s = 1101. (b) Answers vary.
7.16
x
(s+y)·x (−1)(s+y)·x
000
0
1
001
1
-1
010
0
1
011
1
-1
100
0
1
101
1
-1
110
0
1
111
1
-1
∑
x
(−1)(s+y)·x:
0
7.17 (a) The x,y pairs are 000 and 010, 001 and 011, 100 and 110, and 101 and 111.
(b) There are many different possible truth tables for f(x) that satisfies f(x) = f(y).
One is:
376
Answers to Exercises
x
y
000 100
001 101
010 100
011 101
100 011
101 000
110 011
111 000
7.18 s = 1100.
7.19 (a) 0.706316. (b) 0.891232. (c) 0.970374. (d) 0.994123.
7.20 s = 110.
7.21 (a) 128. (b) 8001/8192 = 0.977.
7.22 Geometrically, the initial angle is θ = sin−1(1/
√
N) = sin−1(1/2) = π/6.
Applying RsUf to this adds 2θ to this, resulting in 3θ = π/2, which is perfectly
aligned with |w⟩.
7.23 (a) 1/N. (b) 9/N −24/N2 +16/N3.
7.24 (a) 4. (b) 16. (c) 3. (d) See https://bit.ly/3E3WnzZ:
Answers to Exercises
377
w = 1011.
(e)
x
f(x)
0000
0
0001
0
0010
0
0011
0
0100
0
0101
0
0110
0
0111
0
1000
0
1001
0
1010
0
1011
1
1100
0
1101
0
1110
0
1111
0
7.25 (a) 5. (b) 32. (c) 4. (d,e,f) See https://bit.ly/3q4ExrK
w = 10110.
7.26 φ0 = 0.567,φ1 = 0.35+0.833i,φ2 = 0.415,φ3 = 0.35−0.833i.
7.27 (a) G4. (b) 440 Hz.
7.28 (a)
378
Answers to Exercises
0
100
200
300
400
500
600
700
800
900
1000
Frequency (Hz)
0
2
4
6
8
Amplitude
(b) 196 Hz, 247 Hz, 294 Hz. (c) G3, B3, D4.
7.29 (a) Mrs is row r of QFT† multiplied component-by-component with column
s of QFT, then added together. (ω p)∗= ω−p, so QFT† is the same matrix as QFT,
except with negative powers. Then, ignoring the overall factor of 1/
√
N, the rth row
of QFT† has terms ω−0r = 1, ω−1r, ω−2r, ..., ω−(N−1)r. Similarly, the sth column
of QFT has terms ω0s = 1, ω−1s, ω−2s, ..., ω−(N−1)s, again ignoring the overall
factor of 1/
√
N. Thus,
Mrs = 1
N

ω−0rω0s +ω−1rω1s +ω−2rω2s +···+ω−(N−1)rω(N−1)s
= 1
N
N−1
∑
k=0
ω−krωks = 1
N
N−1
∑
k=0
ωk(s−r).
(b) If r = s,
Mrs = 1
N
N−1
∑
k=0
ω0 = 1
N
N−1
∑
k=0
1 = 1
N (1+···+1)
|
{z
}
N times
= 1
N N = 1.
(c) If r ̸= s, then c = s −r is a nonzero integer, and M is a geometric series with
common ratio ωc, i.e.,
M =
N−1
∑
k=0
(ωc)k = 1
N

1+ωc +(ωc)2 +(ωc)3 +···+(ωc)N−1
.
Using the closed-form formula for a geometric series,
M = 1
N
1−ωNc
1−ωc =
1
√
N
1−(e2πi/N)Nc
1−ωc
=
1
√
N
1−e2πic
1−ωc =
1
√
N
1−1
1−ωc = 0.
7.30 See https://bit.ly/2ZgKfxr.
Answers to Exercises
379
7.31
The Hadamard gate on q2 can be swapped with the P(π/8) gate, so the circuit is
equivalent to the textbook’s.
7.32 See https://bit.ly/3nJIPoB:
7.33
# Number of qubits.
n = 4
# Create a quantum circuit.
qc = QuantumCircuit(n)
# Swap qubits.
for qubit in range(n//2):
qc.swap(qubit, n - qubit - 1)
# Iterate through each target qubit from 0 to (n-1).
for target in range(n):
# Iterate through the control qubits from 0 to (target-1).
for control in range(target):
# Calculate "r," the rotation by -2*pi/2**r.
r = target - control + 1
# Apply the controlled phase/rotation.
qc.cp(-2*np.pi/2**r, control, target)
# Apply the Hadamard gate.
380
Answers to Exercises
qc.h(target)
# Draw the circuit.
qc.draw()
7.34 (a) H

1+
√
2
1

=

1+
√
2
1

. (b) H

1+
√
2
1

= −

1+
√
2
1

.
7.35 U




2+i
√
2+1
1
1



= eiπ/4




2+i
√
2+1
1
1



.
7.36 (a,b) See https://tinyurl.com/emcnnxfk:
(c) 0.0001101. (d) 0.1133. (e) 0.7118. (f) e0.7118i = 0.7572 + 0.6532i. (g) We only
estimated j to eight binary places, and the actual value may need more bits.
7.37 3/8, 1/2, 1/8.
7.38 (a) gcd(4,5) = 1. (b) 1, 4, 1, 4. (c) 1, 4. (d) 2.
7.39 (a) gcd(4,13) = 1. (b) 1, 4, 3, 12, 9, 10, 1, 4, 3, 12, 9, 10. (c) 1, 4, 3, 12, 9, 10.
(d) 6.
7.40 49 mod 131.
7.41 33 mod 197.
Answers to Exercises
381
7.42 (a) 0.3438 = [0,2,1,9,1,18,1,1,1,2]. (b) Its convergents are 0, 1/2, 1/3, 10/29,
11/32, 208/605, 219/637, 427/1242, 646/1879, 1719/5000. (c) Best s/r is 1/3, so
s = 1 and r = 3. (d) 33 mod 7 = 6 mod 7. (e) Not the period.
(a) 0.5 = [0,1]. (b) Its convergents are 0, 1/2. (c) Best s/r is 1/2, so s = 1 and
r = 2. (d) 32 mod 7 = 2 mod 7. (e) Not the period.
(a) 0.6562 = [0,1,1,1,9,1,18,1,1,1,2]. (b) Its convergents are 0, 1, 1/2, 2/3,
19/29, 21/32, 397/605, 418/637, 815/1242, 1233/1879, 3281/5000. (c) Best s/r is
2/3, so s = 2 and r = 3. (d) 33 mod 7 = 6 mod 7. (e) Not the period.
(a) 0.8438 = [0,1,5,2,2,19,8]. (b) Its convergents are 0, 1, 5/6, 11/13, 27/32,
524/621, 4219/5000. (c) Best s/r is 5/6, so s = 5 and r = 6. (d) 36 mod 71 mod 7.
(e) The period.
7.43 See https://bit.ly/31E6M8h:
Probability Binary Approx. of s/r Decimal Approx. of s/r Guess of s/r 7r mod 13
33.3974%
|00000⟩
0
N/A
N/A
5.7377%
|01010⟩
0.3125
1/3
1
22.8399%
|01011⟩
0.3438
1/3
1
22.8399%
|10101⟩
0.6562
2/3
1
5.7377%
|10110⟩
0.6875
2/3
1
The period is r = 3.
7.44 See https://bit.ly/3y1rmLX:
382
Answers to Exercises
Probability Binary Approx. of s/r Decimal Approx. of s/r Guess of s/r 2r mod 15
25%
|00000000⟩
0
N/A
N/A
25%
|01000000⟩
0.25
1/4
1
25%
|10000000⟩
0.5
1/2
4
25%
|11000000⟩
0.75
3/4
1
The period is r = 4.
7.45 Answers vary.
7.46 (a) gcd(22,209) = 11. (b) p = 11, q = 209/11 = 19.
Index
Symbols
≡(equivalent to)
93
∃
59
∀
59
⊺
116
¬
12
∋
59
⊕
14
ψ
92
†
116
#P (complexity class)
69
A
adder
quantum
see quantum adder
ripple-carry
see ripple-carry adder
Advanced Encryption Standard
see AES
AES
263
Agrawal–Kayal–Saxena (AKS)
61
always 0 gate
12
always 1 gate
12
Amazon
236
amplitude
84
ancilla bits
171
AND gate
13, 45
answer qubit
275
anti-controlled-NOT gate
159
anti-Toffoli gate
48
ASCII
9
asymptotic notation
58
big-O notation
58
big-Omega notation
59
big-Theta notation
59
little-o notation
59
little-omega notation
59
atomic force microscope
5
azimuthal angle
94
B
balanced function
281
base 16 numbers
see hexadecimal
base 2 numbers
see binary numbers
basis
86
basis-changing gate
187
Bell basis
158, 256
Bell measurement
256
Bell states
158
Bell test
241
Bernstein-Vazirani algorithm
288
big endian
141
big-O notation
see asymptotic notation
big-Omega notation
see asymptotic notation
big-Theta notation
see asymptotic notation
bin packing problem
61
binary numbers
7
binary point
315
binary search
298
binary strings
see binary numbers
binomial coefficient
see combination
birthday problem
293
bit
6
coin
5
optical discs
5
switch
5
voltage
6
bit flip error
189
bit strings
see binary numbers
black box
see oracle
Bloch ball
347
Bloch sphere
74
Bohmian mechanics
see pilot wave theory
383
384
Index
boolean algebra
38
order of operations
38
boson sampling
69
BPP (complexity class)
283
BQP (complexity class)
69, 141
bra
see also bra-ket notation
bra-ket notation
74, 116
bra
116
bra-ket
118
ket
74
Braket
236
brute-force searching
298
buffer gate
12
C
Cartesian coordinates
95
Cartesian form
80
CCNOT gate
see Toffoli gate
Chinese remainder theorem
266
CHSH inequality
244
Church-Turing Thesis
67
ciphertext
262
circuit complexity
273
circuit diagram
11
circuit identity
108, 129, 156
Cirq
236
Clifford group
185
clone
165
CNOT gate
153
cochlea
307
codeword
54
coin
2
cold atoms
97
collapse
85
collision
292
Colossus
20
column vector
see vector
combination
56
complete orthonormal basis
135
completeness
135
completeness (complexity)
61
completeness relation
135
complex conjugate
82
complex number
80
complex plane
80
complexity class
61
component
80
computer algebra system
see also
Mathematica, see also SageMath, 56
concatenation
202
conjugate
see complex conjugate
conjugate transpose
116
constant function
281
continued fractions
337
control qubit
154
controlled-controlled-NOT gate
see Toffoli
gate
controlled-Hadamard gate
187
controlled-U
160
convergent
338
coprime
see relatively prime
csv file
312
CX gate
see CNOT gate
D
de Broglie-Bohm theory
see pilot wave
theory
De Morgan’s Laws
40
decode
4
decoherence
190
density matrix
254
Deutsch’s algorithm
278
Deutsch-Jozsa algorithm
283
die, dice
3
Dirac notation
see bra-ket notation
discrete Fourier transform
308
dot product
118, 285, 287
Draper’s adder
181
dual
117
E
eardrum
307
easy problems
60
efficient (complexity)
60, 274
eigenstate
322
eigenvalue
321
eigenvector
321
encode
4
encryption
262
entangled state
149
entanglement
139, 149
measure
239
monogamy
254
Entanglion
137, 149, 153, 154, 158, 162
EPR pairs
see Bell states
EPR paradox
240
EPR states
see Bell states
error syndrome
54, 191
error-correcting code
55
error-detecting code
54
Euclid’s algorithm
264
Euler’s formula
81
Eve
262
exclusive OR
see XOR gate
exponential time
60
Index
385
F
factoring
61, 68, 341
faster-than-light communication
see
no-signaling principle
fault tolerant
205
field-programmable gate array (FPGA)
29
Fourier transform
see discrete Fourier
transform
Fredkin gate
46
frequency spectrum
309
full adder
28, 32
function
48
G
Gale-Shapley algorithm
61
GHZ state
213, 254
global phase
91
Google
236
Gottesman-Knill theorem
185
graph isomorphism
61
Grover’s algorithm
299
H
Hadamard gate
105
half adder
28, 29
halting problem
67, 69
Hamiltonian path problem
61
hard problems
60
hardware description language
27, 29
head
63
hexadecimal
8
hidden variable
240
Holevo’s theorem
256
HTML
9
I
IBM Quantum Experience
209
standard header
221
identity gate
11, 102
inefficient (complexity)
60
information
4
information-theoretically secure
263
inner product
118
input qubit
275
inverse quantum Fourier transform
320
inverter gate
12
invertible matrix
see reversible matrix
ion
97
IQFT
see inverse quantum Fourier transform
irreversible gate
45
J
Josephson junction
97
K
ket
see bra-ket notation
key length
264
Kronecker product
142
L
least significant bit
7
linear algebra
115
linear combination
110
linear operator
99
linear programming
61
little endian
140
little-o notation
see asymptotic notation
little-omega notation
see asymptotic
notation
locality
240
logic gate
11
always 0 gate
12
always 1 gate
12
AND gate
13
identity gate
11
NAND gate
14
negative-AND gate
15
negative-OR gate
15
NOR gate
14
NOT gate
12
OR gate
13
XOR gate
13
logical bit
54
lower bound
59
M
magneto-optical trap
97
many-worlds interpretation
251
mask
291
Mathematica
56, 57, 122, 128, 130, 151,
244, 264–267, 293, 312, 338, 339
matrix
124
matrix multiplication
130
matrix-vector multiplication
124
maximally entangled
238
metric
187
Microsoft
236
mixed state
347
model of computation
63
modular exponentiation
327
modular multiplication
332
modulo
14
modulus
266, 327
386
Index
monogamous
see entanglement
most significant bit
7
Mølmer-Sørensen gate
163
N
NAND gate
14
negative battery terminal
16
negative OR gate
15
negative-AND gate
15
NISQ
see noisy intermediate-scale quantum
nitrogen-vacancy center
97
no-cloning theorem
166
no-signaling principle
251
noisy intermediate-scale quantum
209
NOR gate
14
norm
82
norm-square
82
normalization constant
85
normalized
85
NOT gate
12, 44, 102
NP (complexity class)
61
NP-complete
61
nuclear magnetic resonance
97
number field sieve
341
number theory
328
O
one-time pad
263
OpenQASM
219
optical lattice
97
OR gate
13
oracle
274
oracle separation
275
order
see period
order finding
328
orthogonal
119
ossicle
307
outer product
133
overlap
123
P
P (complexity class)
61, 141
parallel computing
184
parameterization
92
parity
22, 54, 67
parity bit
54
partially entangled
239
Pauli gates
X gate
102
Y gate
103
Z gate
103
period
328
period finding
328
phase estimation
322
phase flip error
190
phase gate
104
phase kickback
277
phase oracle
277
photon
97
pilot wave theory
251
plaintext
262
polar angle
94
polar form
80
polarization
97
polylog
187
positive battery terminal
16
post-quantum cryptography
344
postprocessing
325
postselection
204
primality testing
61
prime number
61, 264
principle of deferred measurement
194
product state
147
projection
123
public-key cryptography
263
pure state
347
PyQuil
236
Pythagorean identity
82
Pythagorean theorem
81
Python
228
Q
Q#
236
QASM
219
QFT
see quantum Fourier transform
QIS
228
Qiskit
228
quantum adder
183
quantum annealing
348
quantum approximate optimization algorithm
(QAOA)
348
quantum circuit diagram
111
quantum computational supremacy
69, 209
quantum dot
97
quantum Fourier transform
181, 314
quantum gate
98
quantum key distribution
269
quantum network
271
quantum parallelism
184
quantum state tomography
257
quantum supremacy
see quantum
computational supremacy
quantum teleportation
259
quantum walk
348
Index
387
qubit
74
physical
97
touchdown
73
Qubit Touchdown
73, 78, 85, 102
query
274
query complexity
274
Quil
236
Quirk
111, 126, 127, 129, 140, 151, 152,
155, 157, 161, 164, 171, 173, 180, 181,
183, 193, 195, 199, 203, 206, 259, 276,
280, 286, 289, 302, 306, 319, 321, 326,
335, 341
R
random circuit sampling
68
realism
240
rectangular form
see Cartesian form
register
30, 64
classical
30
quantum
172
relative phase
77, 91
relatively prime
264, 328
relay
20
repetition code
54
reversible gate
44
reversible matrix
132
RGB color model
9
Rigetti
236
ripple-carry adder
28, 35, 167
row vector
see vector
RSA
68
RSA cryptosystem
263
RSA Factoring Challenge
269
S
S gate
see phase gate
Sage
see SageMath
SageMath
56, 57, 122, 129, 130, 152, 244,
264–267, 293, 313, 338, 339
sampling
308
scalar
118
scalar product
see inner product
Schr¨odinger’s equation
348
searching problem
298
seed
215
semiconductor
97
Shor code
190, 201
Shor’s algorithm
68, 267, 341
Shor, Peter
201, 341
Simon’s algorithm
294
simply separable
see product state
single event upset
52
software development kit (SDK)
228
solid state
20
sound
306
spherical coordinates
94
spin
97
stable marriage problem
61
state
2
Strong Church-Turing Thesis
68, 341
subexponential time
60
Sudoku
61
superconductor
97
superluminal communication
see no-
signaling principle
superpolynomial
290
superpolynomial time
60
superposition
76
swap gate
161
T
T gate
105
taking the dual
117
tape
63
target qubit
154, see answer qubit
tensor product
140
three pole switch
19
time
60
Toffoli gate
46, 163
decomposition
164
topology
212
transistor
20
transpile
216
trapped ions
97
traveling salesman problem
61
triad
313
truth table
11
Tsirelson’s inequality
246
Turing machine
63
components
63
non-deterministic
61
probabilistic
68
two’s complement
9
U
uncomputation
171
uncorrectable error
55
undecidable problem
67
unitary matrix
99, 132
universal gate
25
universal gate set
23, 185
upper bound
58
388
Index
V
vacuum tube
20
vector
column vector
115
length
115
row vector
116
Verilog
27, 29
VHDL
29
W
W state
254
waveform
307
wire
11
wormhole
251
X
X gate
see Pauli gates
X-basis
86
XOR
154
XOR gate
13
XOR mask
291
Y
Y gate
see Pauli gates
Y-basis
86
Z
Z gate
see Pauli gates
Z-basis
86

============================================================
Quantum Computation and Quantum Information (Nielsen & Chuang)
============================================================

This page intentionally left blank
Quantum Computation and Quantum Information
10th Anniversary Edition
One of the most cited books in physics of all time, Quantum Computation and Quantum
Information remains the best textbook in this exciting ﬁeld of science. This 10th
Anniversary Edition includes a new Introduction and Afterword from the authors
setting the work in context.
This comprehensive textbook describes such remarkable effects as fast quantum
algorithms, quantum teleportation, quantum cryptography, and quantum
error-correction. Quantum mechanics and computer science are introduced, before
moving on to describe what a quantum computer is, how it can be used to solve problems
faster than “classical” computers, and its real-world implementation. It concludes with
an in-depth treatment of quantum information.
Containing a wealth of ﬁgures and exercises, this well-known textbook is ideal for
courses on the subject, and will interest beginning graduate students and researchers in
physics, computer science, mathematics, and electrical engineering.
MICHAEL NIELSEN was educated at the University of Queensland, and as a Fulbright
Scholar at the University of New Mexico. He worked at Los Alamos National
Laboratory, as the Richard Chace Tolman Fellow at Caltech, was Foundation Professor
of Quantum Information Science and a Federation Fellow at the University of
Queensland, and a Senior Faculty Member at the Perimeter Institute for Theoretical
Physics. He left Perimeter Institute to write a book about open science and now lives in
Toronto.
ISAAC CHUANG is a Professor at the Massachusetts Institute of Technology, jointly
appointed in Electrical Engineering & Computer Science, and in Physics. He leads the
quanta research group at the Center for Ultracold Atoms, in the MIT Research
Laboratory of Electronics, which seeks to understand and create information technology
and intelligence from the fundamental building blocks of physical systems, atoms, and
molecules.
In praise of the book 10 years after publication
Ten years after its initial publication, “Mike and Ike” (as it’s affectionately called) remains the quantum
computing textbook to which all others are compared. No other book in the ﬁeld matches its scope:
from experimental implementation to complexity classes, from the philosophical justiﬁcations for the
Church-Turing Thesis to the nitty-gritty of bra/ket manipulation. A dog-eared copy sits on my desk;
the section on trace distance and ﬁdelity alone has been worth many times the price of the book to me.
Scott Aaronson, Massachusetts Institute of Technology
Quantum information processing has become a huge interdisciplinary ﬁeld at the intersection of both,
theoretical and experimental quantum physics, computer science, mathematics, quantum engineering
and, more recently, even quantum metrology. The book by Michael Nielsen and Isaac Chuang was
seminal in many ways: it paved the way for a broader, yet deep understanding of the underlying
science, it introduced a common language now widely used by a growing community and it became
the standard book in the ﬁeld for a whole decade. In spite of the fast progress in the ﬁeld, even after
10 years the book provides the basic introduction into the ﬁeld for students and scholars alike and
the 10th anniversary edition will remain a bestseller for a long time to come. The foundations of
quantum computation and quantum information processing are excellently laid out in this book and
it also provides an overview over some experimental techniques that have become the testing ground
for quantum information processing during the last decade. In view of the rapid progress of the ﬁeld
the book will continue to be extremely valuable for all entering this highly interdisciplinary research
area and it will always provide the reference for those who grew up with it. This is an excellent book,
well written, highly commendable, and in fact imperative for everybody in the ﬁeld.
Rainer Blatt, Universtit¨at Innsbruck
My well-perused copy of Nielsen and Chuang is, as always, close at hand as I write this. It appears
that the material that Mike and Ike chose to cover, which was a lot, has turned out to be a large portion
of what will become the eternal verities of this still-young ﬁeld. When another researcher asks me to
give her a clear explanation of some important point of quantum information science, I breathe a sigh
of relief when I recall that it is in this book – my job is easy, I just send her there.
David DiVincenzo, IBM T. J. Watson Research Center
If there is anything you want to know, or remind yourself, about quantum information science, then
look no further than this comprehensive compendium by Ike and Mike. Whether you are an expert, a
student or a casual reader, tap into this treasure chest of useful and well presented information.
Artur Ekert, Mathematical Institute, University of Oxford
Nearly every child who has read Harry Potter believes that if you just say the right thing or do the
right thing, you can coerce matter to do something fantastic. But what adult would believe it? Until
quantum computation and quantum information came along in the early 1990s, nearly none. The
quantum computer is the Philosopher’s Stone of our century, and Nielsen and Chuang is our basic
book of incantations. Ten years have passed since its publication, and it is as basic to the ﬁeld as it
ever was. Matter will do wonderful things if asked to, but we must ﬁrst understand its language. No
book written since (there was no before) does the job of teaching the language of quantum theory’s
possibilities like Nielsen and Chuang’s.
Chris Fuchs, Perimeter Institute for Theoretical Physics
Nielsen and Chuang is the bible of the quantum information ﬁeld. It appeared 10 years ago, yet even
though the ﬁeld has changed enormously in these 10 years - the book still covers most of the important
concepts of the ﬁeld.
Lov Grover, Bell Labs
Quantum Computation and Quantum Information, commonly referred to as “Mike and Ike,” continues
to be a most valuable resource for background information on quantum information processing. As a
mathematically-impaired experimentalist, I particularly appreciate the fact that armed with a modest
background in quantum mechanics, it is possible to pick up at any point in the book and readily grasp
the basic ideas being discussed. To me, it is still “the” book on the subject.
David Wineland, National Institute of Standards and Technology, Boulder, Colorado
Endorsements for the original publication
Chuang and Nielsen have produced the ﬁrst comprehensive study of quantum computation. To
develop a robust understanding of this subject one must integrate many ideas whose origins are
variously within physics, computer science, or mathematics. Until this text, putting together the
essential material, much less mastering it, has been a challenge. Our Universe has intrinsic capa-
bilities and limitations on the processing of information. What these are will ultimately determine
the course of technology and shape our efforts to ﬁnd a fundamental physical theory. This book is
an excellent way for any scientist or graduate student – in any of the related ﬁelds – to enter the
discussion.
Michael Freedman, Fields Medalist, Microsoft
Nielsen and Chuang’s new text is remarkably thorough and up-to-date, covering many aspects
of this rapidly evolving ﬁeld from a physics perspective, complementing the computer science
perspective of Gruska’s 1999 text. The authors have succeeded in producing a self-contained book
accessible to anyone with a good undergraduate grounding in math, computer science or physical
sciences. An independent student could spend an enjoyable year reading this book and emerge ready
to tackle the current literature and do serious research. To streamline the exposition, footnotes have
been gathered into short but lively History and Further Reading sections at the end of each chapter.
Charles H Bennett, IBM
This is an excellent book. The ﬁeld is already too big to cover completely in one book, but Nielsen
and Chuang have made a good selection of topics, and explain the topics they have chosen very
well.
Peter Shor, Massachusetts Institute of Technology
Quantum Computation and Quantum Information
Michael A. Nielsen & Isaac L. Chuang
10th Anniversary Edition
CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore,
S˜ao Paulo, Delhi, Dubai, Tokyo, Mexico City
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9781107002173
C⃝M. Nielsen and I. Chuang 2010
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2000
Reprinted 2002, 2003, 2004, 2007, 2009
10th Anniversary edition published 2010
Printed in the United Kingdom at the University Press, Cambridge
A catalog record for this publication is available from the British Library
ISBN 978-1-107-00217-3 Hardback
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to in
this publication, and does not guarantee that any content on such websites is,
or will remain, accurate or appropriate.
To our parents,
and our teachers
Contents
Introduction to the Tenth Anniversary Edition
page xvii
Afterword to the Tenth Anniversary Edition
xix
Preface
xxi
Acknowledgements
xxvii
Nomenclature and notation
xxix
Part I Fundamental concepts
1
1 Introduction and overview
1
1.1 Global perspectives
1
1.1.1 History of quantum computation and quantum
information
2
1.1.2 Future directions
12
1.2 Quantum bits
13
1.2.1 Multiple qubits
16
1.3 Quantum computation
17
1.3.1 Single qubit gates
17
1.3.2 Multiple qubit gates
20
1.3.3 Measurements in bases other than the computational basis
22
1.3.4 Quantum circuits
22
1.3.5 Qubit copying circuit?
24
1.3.6 Example: Bell states
25
1.3.7 Example: quantum teleportation
26
1.4 Quantum algorithms
28
1.4.1 Classical computations on a quantum computer
29
1.4.2 Quantum parallelism
30
1.4.3 Deutsch’s algorithm
32
1.4.4 The Deutsch–Jozsa algorithm
34
1.4.5 Quantum algorithms summarized
36
1.5 Experimental quantum information processing
42
1.5.1 The Stern–Gerlach experiment
43
1.5.2 Prospects for practical quantum information processing
46
1.6 Quantum information
50
1.6.1 Quantum information theory: example problems
52
1.6.2 Quantum information in a wider context
58
x
Contents
2 Introduction to quantum mechanics
60
2.1 Linear algebra
61
2.1.1 Bases and linear independence
62
2.1.2 Linear operators and matrices
63
2.1.3 The Pauli matrices
65
2.1.4 Inner products
65
2.1.5 Eigenvectors and eigenvalues
68
2.1.6 Adjoints and Hermitian operators
69
2.1.7 Tensor products
71
2.1.8 Operator functions
75
2.1.9 The commutator and anti-commutator
76
2.1.10 The polar and singular value decompositions
78
2.2 The postulates of quantum mechanics
80
2.2.1 State space
80
2.2.2 Evolution
81
2.2.3 Quantum measurement
84
2.2.4 Distinguishing quantum states
86
2.2.5 Projective measurements
87
2.2.6 POVM measurements
90
2.2.7 Phase
93
2.2.8 Composite systems
93
2.2.9 Quantum mechanics: a global view
96
2.3 Application: superdense coding
97
2.4 The density operator
98
2.4.1 Ensembles of quantum states
99
2.4.2 General properties of the density operator
101
2.4.3 The reduced density operator
105
2.5 The Schmidt decomposition and puriﬁcations
109
2.6 EPR and the Bell inequality
111
3 Introduction to computer science
120
3.1 Models for computation
122
3.1.1 Turing machines
122
3.1.2 Circuits
129
3.2 The analysis of computational problems
135
3.2.1 How to quantify computational resources
136
3.2.2 Computational complexity
138
3.2.3 Decision problems and the complexity classes P and NP
141
3.2.4 A plethora of complexity classes
150
3.2.5 Energy and computation
153
3.3 Perspectives on computer science
161
Part II Quantum computation
171
4 Quantum circuits
171
4.1 Quantum algorithms
172
4.2 Single qubit operations
174
Contents
xi
4.3 Controlled operations
177
4.4 Measurement
185
4.5 Universal quantum gates
188
4.5.1 Two-level unitary gates are universal
189
4.5.2 Single qubit and CNOT gates are universal
191
4.5.3 A discrete set of universal operations
194
4.5.4 Approximating arbitrary unitary gates is generically hard
198
4.5.5 Quantum computational complexity
200
4.6 Summary of the quantum circuit model of computation
202
4.7 Simulation of quantum systems
204
4.7.1 Simulation in action
204
4.7.2 The quantum simulation algorithm
206
4.7.3 An illustrative example
209
4.7.4 Perspectives on quantum simulation
211
5 The quantum Fourier transform and its applications
216
5.1 The quantum Fourier transform
217
5.2 Phase estimation
221
5.2.1 Performance and requirements
223
5.3 Applications: order-ﬁnding and factoring
226
5.3.1 Application: order-ﬁnding
226
5.3.2 Application: factoring
232
5.4 General applications of the quantum Fourier
transform
234
5.4.1 Period-ﬁnding
236
5.4.2 Discrete logarithms
238
5.4.3 The hidden subgroup problem
240
5.4.4 Other quantum algorithms?
242
6 Quantum search algorithms
248
6.1 The quantum search algorithm
248
6.1.1 The oracle
248
6.1.2 The procedure
250
6.1.3 Geometric visualization
252
6.1.4 Performance
253
6.2 Quantum search as a quantum simulation
255
6.3 Quantum counting
261
6.4 Speeding up the solution of NP-complete problems
263
6.5 Quantum search of an unstructured database
265
6.6 Optimality of the search algorithm
269
6.7 Black box algorithm limits
271
7 Quantum computers: physical realization
277
7.1 Guiding principles
277
7.2 Conditions for quantum computation
279
7.2.1 Representation of quantum information
279
7.2.2 Performance of unitary transformations
281
xii
Contents
7.2.3 Preparation of ﬁducial initial states
281
7.2.4 Measurement of output result
282
7.3 Harmonic oscillator quantum computer
283
7.3.1 Physical apparatus
283
7.3.2 The Hamiltonian
284
7.3.3 Quantum computation
286
7.3.4 Drawbacks
286
7.4 Optical photon quantum computer
287
7.4.1 Physical apparatus
287
7.4.2 Quantum computation
290
7.4.3 Drawbacks
296
7.5 Optical cavity quantum electrodynamics
297
7.5.1 Physical apparatus
298
7.5.2 The Hamiltonian
300
7.5.3 Single-photon single-atom absorption and
refraction
303
7.5.4 Quantum computation
306
7.6 Ion traps
309
7.6.1 Physical apparatus
309
7.6.2 The Hamiltonian
317
7.6.3 Quantum computation
319
7.6.4 Experiment
321
7.7 Nuclear magnetic resonance
324
7.7.1 Physical apparatus
325
7.7.2 The Hamiltonian
326
7.7.3 Quantum computation
331
7.7.4 Experiment
336
7.8 Other implementation schemes
343
Part III Quantum information
353
8 Quantum noise and quantum operations
353
8.1 Classical noise and Markov processes
354
8.2 Quantum operations
356
8.2.1 Overview
356
8.2.2 Environments and quantum operations
357
8.2.3 Operator-sum representation
360
8.2.4 Axiomatic approach to quantum operations
366
8.3 Examples of quantum noise and quantum operations
373
8.3.1 Trace and partial trace
374
8.3.2 Geometric picture of single qubit quantum
operations
374
8.3.3 Bit ﬂip and phase ﬂip channels
376
8.3.4 Depolarizing channel
378
8.3.5 Amplitude damping
380
8.3.6 Phase damping
383
Contents
xiii
8.4 Applications of quantum operations
386
8.4.1 Master equations
386
8.4.2 Quantum process tomography
389
8.5 Limitations of the quantum operations formalism
394
9 Distance measures for quantum information
399
9.1 Distance measures for classical information
399
9.2 How close are two quantum states?
403
9.2.1 Trace distance
403
9.2.2 Fidelity
409
9.2.3 Relationships between distance measures
415
9.3 How well does a quantum channel preserve information?
416
10 Quantum error-correction
425
10.1 Introduction
426
10.1.1 The three qubit bit ﬂip code
427
10.1.2 Three qubit phase ﬂip code
430
10.2 The Shor code
432
10.3 Theory of quantum error-correction
435
10.3.1 Discretization of the errors
438
10.3.2 Independent error models
441
10.3.3 Degenerate codes
444
10.3.4 The quantum Hamming bound
444
10.4 Constructing quantum codes
445
10.4.1 Classical linear codes
445
10.4.2 Calderbank–Shor–Steane codes
450
10.5 Stabilizer codes
453
10.5.1 The stabilizer formalism
454
10.5.2 Unitary gates and the stabilizer formalism
459
10.5.3 Measurement in the stabilizer formalism
463
10.5.4 The Gottesman–Knill theorem
464
10.5.5 Stabilizer code constructions
464
10.5.6 Examples
467
10.5.7 Standard form for a stabilizer code
470
10.5.8 Quantum circuits for encoding, decoding, and
correction
472
10.6 Fault-tolerant quantum computation
474
10.6.1 Fault-tolerance: the big picture
475
10.6.2 Fault-tolerant quantum logic
482
10.6.3 Fault-tolerant measurement
489
10.6.4 Elements of resilient quantum computation
493
11 Entropy and information
500
11.1 Shannon entropy
500
11.2 Basic properties of entropy
502
11.2.1 The binary entropy
502
11.2.2 The relative entropy
504
xiv
Contents
11.2.3 Conditional entropy and mutual information
505
11.2.4 The data processing inequality
509
11.3 Von Neumann entropy
510
11.3.1 Quantum relative entropy
511
11.3.2 Basic properties of entropy
513
11.3.3 Measurements and entropy
514
11.3.4 Subadditivity
515
11.3.5 Concavity of the entropy
516
11.3.6 The entropy of a mixture of quantum states
518
11.4 Strong subadditivity
519
11.4.1 Proof of strong subadditivity
519
11.4.2 Strong subadditivity: elementary applications
522
12 Quantum information theory
528
12.1 Distinguishing quantum states and the accessible information
529
12.1.1 The Holevo bound
531
12.1.2 Example applications of the Holevo bound
534
12.2 Data compression
536
12.2.1 Shannon’s noiseless channel coding theorem
537
12.2.2 Schumacher’s quantum noiseless channel coding theorem
542
12.3 Classical information over noisy quantum channels
546
12.3.1 Communication over noisy classical channels
548
12.3.2 Communication over noisy quantum channels
554
12.4 Quantum information over noisy quantum channels
561
12.4.1 Entropy exchange and the quantum Fano inequality
561
12.4.2 The quantum data processing inequality
564
12.4.3 Quantum Singleton bound
568
12.4.4 Quantum error-correction, refrigeration and Maxwell’s demon
569
12.5 Entanglement as a physical resource
571
12.5.1 Transforming bi-partite pure state entanglement
573
12.5.2 Entanglement distillation and dilution
578
12.5.3 Entanglement distillation and quantum error-correction
580
12.6 Quantum cryptography
582
12.6.1 Private key cryptography
582
12.6.2 Privacy ampliﬁcation and information reconciliation
584
12.6.3 Quantum key distribution
586
12.6.4 Privacy and coherent information
592
12.6.5 The security of quantum key distribution
593
Appendices
608
Appendix 1:
Notes on basic probability theory
608
Appendix 2:
Group theory
610
A2.1 Basic deﬁnitions
610
A2.1.1 Generators
611
A2.1.2 Cyclic groups
611
A2.1.3 Cosets
612
Contents
xv
A2.2 Representations
612
A2.2.1 Equivalence and reducibility
612
A2.2.2 Orthogonality
613
A2.2.3 The regular representation
614
A2.3 Fourier transforms
615
Appendix 3:
The Solovay--Kitaev theorem
617
Appendix 4:
Number theory
625
A4.1 Fundamentals
625
A4.2 Modular arithmetic and Euclid’s algorithm
626
A4.3 Reduction of factoring to order-ﬁnding
633
A4.4 Continued fractions
635
Appendix 5:
Public key cryptography and the RSA cryptosystem
640
Appendix 6:
Proof of Lieb’s theorem
645
Bibliography
649
Index
665
Introduction to the Tenth Anniversary Edition
Quantum mechanics has the curious distinction of being simultaneously the most suc-
cessful and the most mysterious of our scientiﬁc theories. It was developed in ﬁts and
starts over a remarkable period from 1900 to the 1920s, maturing into its current form in
the late 1920s. In the decades following the 1920s, physicists had great success applying
quantum mechanics to understand the fundamental particles and forces of nature, cul-
minating in the development of the standard model of particle physics. Over the same
period, physicists had equally great success in applying quantum mechanics to understand
an astonishing range of phenomena in our world, from polymers to semiconductors, from
superﬂuids to superconductors. But, while these developments profoundly advanced our
understanding of the natural world, they did only a little to improve our understanding
of quantum mechanics.
This began to change in the 1970s and 1980s, when a few pioneers were inspired to
ask whether some of the fundamental questions of computer science and information
theory could be applied to the study of quantum systems. Instead of looking at quantum
systems purely as phenomena to be explained as they are found in nature, they looked at
them as systems that can be designed. This seems a small change in perspective, but the
implications are profound. No longer is the quantum world taken merely as presented,
but instead it can be created. The result was a new perspective that inspired both a
resurgence of interest in the fundamentals of quantum mechanics, and also many new
questions combining physics, computer science, and information theory. These include
questions such as: what are the fundamental physical limitations on the space and time
required to construct a quantum state? How much time and space are required for a given
dynamical operation? What makes quantum systems difﬁcult to understand and simulate
by conventional classical means?
Writing this book in the late 1990s, we were fortunate to be writing at a time when
these and other fundamental questions had just crystallized out. Ten years later it is
clear such questions offer a sustained force encouraging a broad research program at the
foundations of physics and computer science. Quantum information science is here to
stay. Although the theoretical foundations of the ﬁeld remain similar to what we discussed
10yearsago,detailedknowledgeinmanyareashasgreatlyprogressed.Originally,thisbook
served as a comprehensive overview of the ﬁeld, bringing readers near to the forefront
of research. Today, the book provides a basic foundation for understanding the ﬁeld,
appropriate either for someone who desires a broad perspective on quantum information
science, or an entryway for further investigation of the latest research literature. Of course,
xviii
Introduction to the Tenth Anniversary Edition
many fundamental challenges remain, and meeting those challenges promises to stimulate
exciting and unexpected links among many disparate parts of physics, computer science,
and information theory. We look forward to the decades ahead!
– Michael A. Nielsen and Isaac L. Chuang, March, 2010.
Afterword to the Tenth Anniversary Edition
An enormous amount has happened in quantum information science in the 10 years since
the ﬁrst edition of this book, and in this afterword we cannot summarize even a tiny
fraction of that work. But a few especially striking developments merit comment, and may
perhaps whet your appetite for more.
Perhaps the most impressive progress has been in the area of experimental implemen-
tation. While we are still many years from building large-scale quantum computers, much
progress has been made. Superconducting circuits have been used to implement simple
two-qubit quantum algorithms, and three-qubit systems are nearly within reach. Qubits
based on nuclear spins and single photons have been used, respectively, to demonstrate
proof-of-principle for simple forms of quantum error correction and quantum simulation.
But the most impressive progress of all has been made with trapped ion systems, which
have been used to implement many two- and three-qubit algorithms and algorithmic
building blocks, including the quantum search algorithm and the quantum Fourier trans-
form. Trapped ions have also been used to demonstrate basic quantum communication
primitives, including quantum error correction and quantum teleportation.
A second area of progress has been in understanding what physical resources are
requiredtoquantumcompute.Perhapsthemostintriguingbreakthroughherehasbeenthe
discovery that quantum computation can be done via measurement alone. For many years,
the conventional wisdom was that coherent superposition-preserving unitary dynamics
was an essential part of the power of quantum computers. This conventional wisdom
was blown away by the realization that quantum computation can be done without any
unitary dynamics at all. Instead, in some new models of quantum computation, quantum
measurementsalone can be used todoarbitraryquantum computations. The onlycoherent
resourceinthesemodelsisquantummemory,i.e.,theabilitytostorequantuminformation.
An especially interesting example of these models is the one-way quantum computer, or
cluster-state computer. To quantum compute in the cluster-state model requires only
that the experimenter have possession of a ﬁxed universal state known as the cluster state.
With a cluster state in hand, quantum computation can be implemented simply by doing
a sequence of single-qubit measurements, with the particular computation done being
determined by which qubits are measured, when they are measured, and how they are
measured. This is remarkable: you’re given a ﬁxed quantum state, and then quantum
compute by “looking” at the individual qubits in appropriate ways.
A third area of progress has been in classically simulating quantum systems. Feynman’s
pioneering 1982 paper on quantum computing was motivated in part by the observation
that quantum systems often seem hard to simulate on conventional classical computers.
Of course, at the time there was only a limited understanding of how difﬁcult it is
to simulate different quantum systems on ordinary classical computers. But in the 1990s
and, especially, in the 2000s, we have learned much about which quantum systems are easy
xx
Afterword to the Tenth Anniversary Edition
to simulate, and which are hard. Ingenious algorithms have been developed to classically
simulate many quantum systems that were formerly thought to be hard to simulate, in
particular, many quantum systems in one spatial dimension, and certain two-dimensional
quantum systems. These classical algorithms have been made possible by the development
of insightful classical descriptions that capture in a compact way much or all of the essential
physics of the system in question. At the same time, we have learned that some systems
that formerly seemed simple are surprisingly complex. For example, it has long been
known that quantum systems based on a certain type of optical component – what are
called linear optical systems – are easily simulated classically. So it was surprising when it
was discovered that adding two seemingly innocuous components – single-photon sources
and photodetectors – gave linear optics the full power of quantum computation. These
and similar investigations have deepened our understanding of which quantum systems
are easy to simulate, which quantum systems are hard to simulate, and why.
A fourth area of progress has been a greatly deepened understanding of quantum
communication channels. A beautiful and complete theory has been developed of how
entangled quantum states can assist classical communication over quantum channels. A
plethora of different quantum protocols for communication have been organized into
a comprehensive family (headed by “mother” and “father” protocols), unifying much
of our understanding of the different types of communication possible with quantum
information. A sign of the progress is the disproof of one of the key unsolved conjectures
reported in this book (p. 554), namely, that the communication capacity of a quantum
channel with product states is equal to the unconstrained capacity (i.e., the capacity with
any entangled state allowed as input). But, despite the progress, much remains beyond
our understanding. Only very recently, for example, it was discovered, to considerable
surprise, that two quantum channels, each with zero quantum capacity, can have a positive
quantum capacity when used together; the analogous result, with classical capacities over
classical channels, is known to be impossible.
One of the main motivations for work in quantum information science is the prospect of
fast quantum algorithms to solve important computational problems. Here, the progress
over the past decade has been mixed. Despite great ingenuity and effort, the chief algo-
rithmic insights stand as they were 10 years ago. There has been considerable technical
progress, but we do not yet understand what exactly it is that makes quantum comput-
ers powerful, or on what class of problems they can be expected to outperform classical
computers.
What is exciting, though, is that ideas from quantum computation have been used
to prove a variety of theorems about classical computation. These have included, for
example, results about the difﬁculty of ﬁnding certain hidden vectors in a discrete lattice
of points. The striking feature is that these proofs, utilizing ideas of quantum computation,
are sometimes considerably simpler and more elegant than prior, classical proofs. Thus,
an awareness has grown that quantum computation may be a more natural model of
computation than the classical model, and perhaps fundamental results may be more
easily revealed through the ideas of quantum computation.
Preface
This book provides an introduction to the main ideas and techniques of the ﬁeld of
quantum computation and quantum information. The rapid rate of progress in this ﬁeld
and its cross-disciplinary nature have made it difﬁcult for newcomers to obtain a broad
overview of the most important techniques and results of the ﬁeld.
Our purpose in this book is therefore twofold. First, we introduce the background
material in computer science, mathematics and physics necessary to understand quan-
tum computation and quantum information. This is done at a level comprehensible to
readers with a background at least the equal of a beginning graduate student in one or
more of these three disciplines; the most important requirements are a certain level of
mathematical maturity, and the desire to learn about quantum computation and quantum
information. The second purpose of the book is to develop in detail the central results of
quantum computation and quantum information. With thorough study the reader should
develop a working understanding of the fundamental tools and results of this exciting
ﬁeld, either as part of their general education, or as a prelude to independent research in
quantum computation and quantum information.
Structure of the book
The basic structure of the book is depicted in Figure 1. The book is divided into three
parts. The general strategy is to proceed from the concrete to the more abstract whenever
possible. Thus we study quantum computation before quantum information; speciﬁc
quantum error-correcting codes before the more general results of quantum information
theory; and throughout the book try to introduce examples before developing general
theory.
Part I provides a broad overview of the main ideas and results of the ﬁeld of quan-
tum computation and quantum information, and develops the background material in
computer science, mathematics and physics necessary to understand quantum compu-
tation and quantum information in depth. Chapter 1 is an introductory chapter which
outlines the historical development and fundamental concepts of the ﬁeld, highlighting
some important open problems along the way. The material has been structured so as
to be accessible even without a background in computer science or physics. The back-
ground material needed for a more detailed understanding is developed in Chapters 2
and 3, which treat in depth the fundamental notions of quantum mechanics and com-
puter science, respectively. You may elect to concentrate more or less heavily on different
chapters of Part I, depending upon your background, returning later as necessary to ﬁll
any gaps in your knowledge of the fundamentals of quantum mechanics and computer
science.
Part II describes quantum computation in detail. Chapter 4 describes the fundamen-
Preface
 	













 
 
 






 


	 





 
	






 !



"

	 



	





#


	 







"
Figure 1. Structure of the book.
tal elements needed to perform quantum computation, and presents many elementary
operations which may be used to develop more sophisticated applications of quantum
computation. Chapters 5 and 6 describe the quantum Fourier transform and the quantum
search algorithm, the two fundamental quantum algorithms presently known. Chapter 5
also explains how the quantum Fourier transform may be used to solve the factoring and
discrete logarithm problems, and the importance of these results to cryptography. Chap-
ter 7 describes general design principles and criteria for good physical implementations of
quantum computers, using as examples several realizations which have been successfully
demonstrated in the laboratory.
Part III is about quantum information: what it is, how information is represented and
communicated using quantum states, and how to describe and deal with the corruption of
quantum and classical information. Chapter 8 describes the properties of quantum noise
which are needed to understand real-world quantum information processing, and the
quantum operations formalism, a powerful mathematical tool for understanding quan-
tum noise. Chapter 9 describes distance measures for quantum information which allow
us to make quantitatively precise what it means to say that two items of quantum infor-
mation are similar. Chapter 10 explains quantum error-correcting codes, which may be
used to protect quantum computations against the effect of noise. An important result in
this chapter is the threshold theorem, which shows that for realistic noise models, noise
is in principle not a serious impediment to quantum computation. Chapter 11 introduces
the fundamental information-theoretic concept of entropy, explaining many properties of
entropy in both classical and quantum information theory. Finally, Chapter 12 discusses
the information carrying properties of quantum states and quantum communication chan-
xxii
Preface
nels, detailing many of the strange and interesting properties such systems can have for
the transmission of information both classical and quantum, and for the transmission of
secret information.
A large number of exercises and problems appear throughout the book. Exercises are
intended to solidify understanding of basic material and appear within the main body of
the text. With few exceptions these should be easily solved with a few minutes work.
Problems appear at the end of each chapter, and are intended to introduce you to new
and interesting material for which there was not enough space in the main text. Often the
problems are in multiple parts, intended to develop a particular line of thought in some
depth. A few of the problems were unsolved as the book went to press. When this is the
case it is noted in the statement of the problem. Each chapter concludes with a summary
of the main results of the chapter, and with a ‘History and further reading’ section that
charts the development of the main ideas in the chapter, giving citations and references
for the whole chapter, as well as providing recommendations for further reading.
The front matter of the book contains a detailed Table of Contents, which we encourage
you to browse. There is also a guide to nomenclature and notation to assist you as you
read.
The end matter of the book contains six appendices, a bibliography, and an index.
Appendix 1 reviews some basic deﬁnitions, notations, and results in elementary prob-
ability theory. This material is assumed to be familiar to readers, and is included for ease
of reference. Similarly, Apendix 2 reviews some elementary concepts from group theory,
and is included mainly for convenience. Appendix 3 contains a proof of the Solovay–
Kitaev theorem, an important result for quantum computation, which shows that a ﬁnite
set of quantum gates can be used to quickly approximate an arbitrary quantum gate.
Appendix 4 reviews the elementary material on number theory needed to understand
the quantum algorithms for factoring and discrete logarithm, and the RSA cryptosystem,
which is itself reviewed in Appendix 5. Appendix 6 contains a proof of Lieb’s theorem,
one of the most important results in quantum computation and quantum information,
and a precursor to important entropy inequalities such as the celebrated strong subad-
ditivity inequality. The proofs of the Solovay–Kitaev theorem and Lieb’s theorem are
lengthy enough that we felt they justiﬁed a treatment apart from the main text.
The bibliography contains a listing of all reference materials cited in the text of the
book. Our apologies to any researcher whose work we have inadvertently omitted from
citation.
The ﬁeld of quantum computation and quantum information has grown so rapidly in
recent years that we have not been able to cover all topics in as much depth as we would
have liked. Three topics deserve special mention. The ﬁrst is the subject of entanglement
measures. As we explain in the book, entanglement is a key element in effects such as
quantum teleportation, fast quantum algorithms, and quantum error-correction. It is,
in short, a resource of great utility in quantum computation and quantum information.
There is a thriving research community currently ﬂeshing out the notion of entanglement
as a new type of physical resource, ﬁnding principles which govern its manipulation and
utilization. We felt that these investigations, while enormously promising, are not yet
complete enough to warrant the more extensive coverage we have given to other subjects
in this book, and we restrict ourselves to a brief taste in Chapter 12. Similarly, the sub-
ject of distributed quantum computation (sometimes known as quantum communication
complexity) is an enormously promising subject under such active development that we
xxiii
x
Preface
have not given it a treatment for fear of being obsolete before publication of the book.
The implementation of quantum information processing machines has also developed
into a fascinating and rich area, and we limit ourselves to but a single chapter on this
subject. Clearly, much more can be said about physical implementations, but this would
begin to involve many more areas of physics, chemistry, and engineering, which we do
not have room for here.
How to use this book
This book may be used in a wide variety of ways. It can be used as the basis for a variety
of courses, from short lecture courses on a speciﬁc topic in quantum computation and
quantum information, through to full-year classes covering the entire ﬁeld. It can be
used for independent study by people who would like to learn just a little about quantum
computation and quantum information, or by people who would like to be brought up to
the research frontier. It is also intended to act as a reference work for current researchers
in the ﬁeld. We hope that it will be found especially valuable as an introduction for
researchers new to the ﬁeld.
Note to the independent reader
The book is designed to be accessible to the independent reader. A large number of exer-
cises are peppered throughout the text, which can be used as self-tests for understanding
of the material in the main text. The Table of Contents and end of chapter summaries
should enable you to quickly determine which chapters you wish to study in most depth.
The dependency diagram, Figure 1, will help you determine in what order material in
the book may be covered.
Note to the teacher
This book covers a diverse range of topics, and can therefore be used as the basis for a
wide variety of courses.
A one-semester course on quantum computation could be based upon a selection of
material from Chapters 1 through 3, depending on the background of the class, followed
by Chapter 4 on quantum circuits, Chapters 5 and 6 on quantum algorithms, and a
selection from Chapter 7 on physical implementations, and Chapters 8 through 10 to
understand quantum error-correction, with an especial focus on Chapter 10.
A one-semester course on quantum information could be based upon a selection of
material from Chapters 1 through 3, depending on the background of the class. Following
that, Chapters 8 through 10 on quantum error-correction, followed by Chapters 11 and 12
on quantum entropy and quantum information theory, respectively.
A full year class could cover all material in the book, with time for additional readings
selected from the ‘History and further reading’ section of several chapters. Quantum com-
putation and quantum information also lend themselves ideally to independent research
projects for students.
Aside from classes on quantum computation and quantum information, there is another
way we hope the book will be used, which is as the text for an introductory class in quan-
tum mechanics for physics students. Conventional introductions to quantum mechanics
rely heavily on the mathematical machinery of partial differential equations. We believe
this often obscures the fundamental ideas. Quantum computation and quantum informa-
xiv
Preface
tion offers an excellent conceptual laboratory for understanding the basic concepts and
unique aspects of quantum mechanics, without the use of heavy mathematical machinery.
Such a class would focus on the introduction to quantum mechanics in Chapter 2, basic
material on quantum circuits in Chapter 4, a selection of material on quantum algorithms
from Chapters 5 and 6, Chapter 7 on physical implementations of quantum computation,
and then almost any selection of material from Part III of the book, depending upon
taste.
Note to the student
We have written the book to be as self-contained as possible. The main exception is that
occasionally we have omitted arguments that one really needs to work through oneself
to believe; these are usually given as exercises. Let us suggest that you should at least
attempt all the exercises as you work through the book. With few exceptions the exercises
can be worked out in a few minutes. If you are having a lot of difﬁculty with many of
the exercises it may be a sign that you need to go back and pick up one or more key
concepts.
Further reading
As already noted, each chapter concludes with a ‘History and further reading’ section.
There are also a few broad-ranging references that might be of interest to readers.
Preskill’s[Pre98b] superb lecture notes approach quantum computation and quantum infor-
mation from a somewhat different point of view than this book. Good overview articles on
speciﬁc subjects include (in order of their appearance in this book): Aharonov’s review of
quantum computation[Aha99b], Kitaev’s review of algorithms and error-correction[Kit97b],
Mosca’s thesis on quantum algorithms[Mos99], Fuchs’ thesis[Fuc96] on distinguishability
and distance measures in quantum information, Gottesman’s thesis on quantum error-
correction[Got97], Preskill’s review of quantum error-correction[Pre97], Nielsen’s thesis on
quantum information theory[Nie98], and the reviews of quantum information theory by
Bennett and Shor[BS98] and by Bennett and DiVincenzo[BD00]. Other useful references
include Gruska’s book[Gru99], and the collection of review articles edited by Lo, Spiller,
and Popescu[LSP98].
Errors
Any lengthy document contains errors and omissions, and this book is surely no exception
to the rule. If you ﬁnd any errors or have other comments to make about the book,
please email them to: qci@squint.org. As errata are found, we will add them to a list
maintained at the book web site: http://www.squint.org/qci/.
xxv
Acknowledgements
A few people have decisively inﬂuenced how we think about quantum computation and
quantum information. For many enjoyable discussions which have helped us shape and
reﬁne our views, MAN thanks Carl Caves, Chris Fuchs, Gerard Milburn, John Preskill
and Ben Schumacher, and ILC thanks Tom Cover, Umesh Vazirani, Yoshi Yamamoto,
and Bernie Yurke.
An enormous number of people have helped in the construction of this book, both
directly and indirectly. A partial list includes Dorit Aharonov, Andris Ambainis, Nabil
Amer, Howard Barnum, Dave Beckman, Harry Buhrman, the Caltech Quantum Optics
Foosballers, Andrew Childs, Fred Chong, Richard Cleve, John Conway, John Cortese,
Michael DeShazo, Ronald de Wolf, David DiVincenzo, Steven van Enk, Henry Everitt,
Ron Fagin, Mike Freedman, Michael Gagen, Neil Gershenfeld, Daniel Gottesman, Jim
Harris, Alexander Holevo, Andrew Huibers, Julia Kempe, Alesha Kitaev, Manny Knill,
Shing Kong, Raymond Laﬂamme, Andrew Landahl, Ron Legere, Debbie Leung, Daniel
Lidar, Elliott Lieb, Theresa Lynn, Hideo Mabuchi, Yu Manin, Mike Mosca, Alex Pines,
Sridhar Rajagopalan, Bill Risk, Beth Ruskai, Sara Schneider, Robert Schrader, Peter
Shor, Sheri Stoll, Volker Strassen, Armin Uhlmann, Lieven Vandersypen, Anne Ver-
hulst, Debby Wallach, Mike Westmoreland, Dave Wineland, Howard Wiseman, John
Yard, Xinlan Zhou, and Wojtek Zurek.
Thanks to the folks at Cambridge University Press for their help turning this book
from an idea into reality. Our especial thanks go to our thoughtful and enthusiastic
editor Simon Capelin, who shepherded this project along for more than three years, and
to Margaret Patterson, for her timely and thorough copy-editing of the manuscript.
Parts of this book were completed while MAN was a Tolman Prize Fellow at the
California Institute of Technology, a member of the T-6 Theoretical Astrophysics Group
at the Los Alamos National Laboratory, and a member of the University of New Mexico
Center for Advanced Studies, and while ILC was a Research Staff Member at the IBM
Almaden Research Center, a consulting Assistant Professor of Electrical Engineering
at Stanford University, a visiting researcher at the University of California Berkeley
Department of Computer Science, a member of the Los Alamos National Laboratory T-6
Theoretical Astrophysics Group, and a visiting researcher at the University of California
Santa Barbara Institute for Theoretical Physics. We also appreciate the warmth and
hospitality of the Aspen Center for Physics, where the ﬁnal page proofs of this book were
ﬁnished.
MAN and ILC gratefully acknowledge support from DARPA under the NMRQC
research initiative and the QUIC Institute administered by the Army Research Ofﬁce.
We also thank the National Science Foundation, the National Security Agency, the Ofﬁce
of Naval Research, and IBM for their generous support.
Nomenclature and notation
There are several items of nomenclature and notation which have two or more meanings in
common use in the ﬁeld of quantum computation and quantum information. To prevent
confusion from arising, this section collects many of the more frequently used of these
items, together with the conventions that will be adhered to in this book.
Linear algebra and quantum mechanics
All vector spaces are assumed to be ﬁnite dimensional, unless otherwise noted. In many
instances this restriction is unnecessary, or can be removed with some additional technical
work, but making the restriction globally makes the presentation more easily comprehen-
sible, and doesn’t detract much from many of the intended applications of the results.
A positive operator A is one for which ⟨ψ|A|ψ⟩≥0 for all |ψ⟩. A positive deﬁnite
operator A is one for which ⟨ψ|A|ψ⟩> 0 for all |ψ⟩̸= 0. The support of an operator
is deﬁned to be the vector space orthogonal to its kernel. For a Hermitian operator, this
means the vector space spanned by eigenvectors of the operator with non-zero eigenvalues.
The notation U (and often but not always V ) will generically be used to denote a unitary
operator or matrix. H is usually used to denote a quantum logic gate, the Hadamard
gate, and sometimes to denote the Hamiltonian for a quantum system, with the meaning
clear from context.
Vectors will sometimes be written in column format, as for example,
 1
2

,
(0.1)
and sometimes for readability in the format (1, 2). The latter should be understood as
shorthand for a column vector. For two-level quantum systems used as qubits, we shall
usually identify the state |0⟩with the vector (1, 0), and similarly |1⟩with (0, 1). We also
deﬁne the Pauli sigma matrices in the conventional way – see ‘Frequently used quantum
gates and circuit symbols’, below. Most signiﬁcantly, the convention for the Pauli sigma
z matrix is that σz|0⟩= |0⟩and σz|1⟩= −|1⟩, which is reverse of what some physicists
(but usually not computer scientists or mathematicians) intuitively expect. The origin
of this dissonance is that the +1 eigenstate of σz is often identiﬁed by physicists with a
so-called ‘excited state’, and it seems natural to many to identify this with |1⟩, rather than
with |0⟩as is done in this book. Our choice is made in order to be consistent with the
usual indexing of matrix elements in linear algebra, which makes it natural to identify the
ﬁrst column of σz with the action of σz on |0⟩, and the second column with the action
on |1⟩. This choice is also in use throughout the quantum computation and quantum
information community. In addition to the conventional notations σx, σy and σz for the
Pauli sigma matrices, it will also be convenient to use the notations σ1, σ2, σ3 for these
Nomenclature and notation
three matrices, and to deﬁne σ0 as the 2×2 identity matrix. Most often, however, we use
the notations I, X, Y and Z for σ0, σ1, σ2 and σ3, respectively.
Information theory and probability
As beﬁts good information theorists, logarithms are always taken to base two, unless
otherwise noted. We use log(x) to denote logarithms to base 2, and ln(x) on those rare
occasions when we wish to take a natural logarithm. The term probability distribution
is used to refer to a ﬁnite set of real numbers, px, such that px ≥0 and 
x px = 1. The
relative entropy of a positive operator A with respect to a positive operator B is deﬁned
by S(A||B) ≡tr(A log A) −tr(A log B).
Miscellanea
⊕denotes modulo two addition. Throughout this book ‘z’ is pronounced ‘zed’.
Frequently used quantum gates and circuit symbols
Certain schematic symbols are often used to denote unitary transforms which are useful in
the design of quantum circuits. For the reader’s convenience, many of these are gathered
together below. The rows and columns of the unitary transforms are labeled from left to
right and top to bottom as 00 . . . 0, 00 . . . 1 to 11 . . . 1 with the bottom-most wire being
the least signiﬁcant bit. Note that eiπ/4 is the square root of i, so that the π/8 gate is the
square root of the phase gate, which itself is the square root of the Pauli-Z gate.
Hadamard
1
√
2
 1
1
1
−1

Pauli-X
 0
1
1
0

Pauli-Y
 0
−i
i
0

Pauli-Z
 1
0
0
−1

Phase
 1
0
0
i

π/8
 1
0
0
eiπ/4

xxx
Nomenclature and notation
controlled-
⎡
⎢⎣
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⎤
⎥⎦
swap
⎡
⎢⎣
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
⎤
⎥⎦
controlled-Z
•
Z
=
⎡
⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1
⎤
⎥⎦
controlled-phase
⎡
⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
i
⎤
⎥⎦
Toffoli
•
•
⊕
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
Fredkin (controlled-swap)
•
×
×
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
measurement

________

_ _ _ _ _ _ _ _

Projection onto |0⟩and |1⟩
qubit
wire carrying a single qubit
(time goes left to right)
classical bit
wire carrying a single classical bit
n qubits
wire carrying n qubits
xxxi
I Fundamental concepts
1 Introduction and overview
Science offers the boldest metaphysics of the age. It is a thoroughly human
construct, driven by the faith that if we dream, press to discover, explain, and
dream again, thereby plunging repeatedly into new terrain, the world will some-
how come clearer and we will grasp the true strangeness of the universe. And
the strangeness will all prove to be connected, and make sense.
– Edward O. Wilson
Information is physical.
– Rolf Landauer
What are the fundamental concepts of quantum computation and quantum information?
How did these concepts develop? To what uses may they be put? How will they be pre-
sented in this book? The purpose of this introductory chapter is to answer these questions
by developing in broad brushstrokes a picture of the ﬁeld of quantum computation and
quantum information. The intent is to communicate a basic understanding of the central
concepts of the ﬁeld, perspective on how they have been developed, and to help you
decide how to approach the rest of the book.
Our story begins in Section 1.1 with an account of the historical context in which
quantum computation and quantum information has developed. Each remaining section
in the chapter gives a brief introduction to one or more fundamental concepts from the
ﬁeld: quantum bits (Section 1.2), quantum computers, quantum gates and quantum cir-
cuits (Section 1.3), quantum algorithms (Section 1.4), experimental quantum information
processing (Section 1.5), and quantum information and communication (Section 1.6).
Along the way, illustrative and easily accessible developments such as quantum tele-
portation and some simple quantum algorithms are given, using the basic mathematics
taught in this chapter. The presentation is self-contained, and designed to be accessible
even without a background in computer science or physics. As we move along, we give
pointers to more in-depth discussions in later chapters, where references and suggestions
for further reading may also be found.
If as you read you’re ﬁnding the going rough, skip on to a spot where you feel more
comfortable. At points we haven’t been able to avoid using a little technical lingo which
won’t be completely explained until later in the book. Simply accept it for now, and come
back later when you understand all the terminology in more detail. The emphasis in this
ﬁrst chapter is on the big picture, with the details to be ﬁlled in later.
1.1
Global perspectives
Quantum computation and quantum information is the study of the information process-
ing tasks that can be accomplished using quantum mechanical systems. Sounds pretty
2
Introduction and overview
simple and obvious, doesn’t it? Like many simple but profound ideas it was a long time
before anybody thought of doing information processing using quantum mechanical sys-
tems. To see why this is the case, we must go back in time and look in turn at each
of the ﬁelds which have contributed fundamental ideas to quantum computation and
quantum information – quantum mechanics, computer science, information theory, and
cryptography. As we take our short historical tour of these ﬁelds, think of yourself ﬁrst
as a physicist, then as a computer scientist, then as an information theorist, and ﬁnally
as a cryptographer, in order to get some feel for the disparate perspectives which have
come together in quantum computation and quantum information.
1.1.1
History of quantum computation and quantum information
Our story begins at the turn of the twentieth century when an unheralded revolution was
underway in science. A series of crises had arisen in physics. The problem was that the
theories of physics at that time (now dubbed classical physics) were predicting absurdities
such as the existence of an ‘ultraviolet catastrophe’ involving inﬁnite energies, or electrons
spiraling inexorably into the atomic nucleus. At ﬁrst such problems were resolved with
the addition of ad hoc hypotheses to classical physics, but as a better understanding
of atoms and radiation was gained these attempted explanations became more and more
convoluted. The crisis came to a head in the early 1920s after a quarter century of turmoil,
and resulted in the creation of the modern theory of quantum mechanics. Quantum
mechanics has been an indispensable part of science ever since, and has been applied
with enormous success to everything under and inside the Sun, including the structure
of the atom, nuclear fusion in stars, superconductors, the structure of DNA, and the
elementary particles of Nature.
What is quantum mechanics? Quantum mechanics is a mathematical framework or set
of rules for the construction of physical theories. For example, there is a physical theory
known as quantum electrodynamics which describes with fantastic accuracy the interac-
tion of atoms and light. Quantum electrodynamics is built up within the framework of
quantum mechanics, but it contains speciﬁc rules not determined by quantum mechanics.
The relationship of quantum mechanics to speciﬁc physical theories like quantum elec-
trodynamics is rather like the relationship of a computer’s operating system to speciﬁc
applications software – the operating system sets certain basic parameters and modes of
operation, but leaves open how speciﬁc tasks are accomplished by the applications.
The rules of quantum mechanics are simple but even experts ﬁnd them counter-
intuitive, and the earliest antecedents of quantum computation and quantum information
may be found in the long-standing desire of physicists to better understand quantum
mechanics. The best known critic of quantum mechanics, Albert Einstein, went to his
grave unreconciled with the theory he helped invent. Generations of physicists since have
wrestled with quantum mechanics in an effort to make its predictions more palatable.
One of the goals of quantum computation and quantum information is to develop tools
which sharpen our intuition about quantum mechanics, and make its predictions more
transparent to human minds.
For example, in the early 1980s, interest arose in whether it might be possible to use
quantum effects to signal faster than light – a big no-no according to Einstein’s theory of
relativity. The resolution of this problem turns out to hinge on whether it is possible to
clone an unknown quantum state, that is, construct a copy of a quantum state. If cloning
were possible, then it would be possible to signal faster than light using quantum effects.
Global perspectives
3
However, cloning – so easy to accomplish with classical information (consider the words
in front of you, and where they came from!) – turns out not to be possible in general in
quantum mechanics. This no-cloning theorem, discovered in the early 1980s, is one of
the earliest results of quantum computation and quantum information. Many reﬁnements
of the no-cloning theorem have since been developed, and we now have conceptual tools
which allow us to understand how well a (necessarily imperfect) quantum cloning device
might work. These tools, in turn, have been applied to understand other aspects of
quantum mechanics.
A related historical strand contributing to the development of quantum computation
and quantum information is the interest, dating to the 1970s, of obtaining complete con-
trol over single quantum systems. Applications of quantum mechanics prior to the 1970s
typically involved a gross level of control over a bulk sample containing an enormous
number of quantum mechanical systems, none of them directly accessible. For example,
superconductivity has a superb quantum mechanical explanation. However, because a su-
perconductor involves a huge (compared to the atomic scale) sample of conducting metal,
we can only probe a few aspects of its quantum mechanical nature, with the individual
quantum systems constituting the superconductor remaining inaccessible. Systems such
as particle accelerators do allow limited access to individual quantum systems, but again
provide little control over the constituent systems.
Since the 1970s many techniques for controlling single quantum systems have been
developed. For example, methods have been developed for trapping a single atom in an
‘atom trap’, isolating it from the rest of the world and allowing us to probe many different
aspects of its behavior with incredible precision. The scanning tunneling microscope
has been used to move single atoms around, creating designer arrays of atoms at will.
Electronic devices whose operation involves the transfer of only single electrons have
been demonstrated.
Why all this effort to attain complete control over single quantum systems? Setting
aside the many technological reasons and concentrating on pure science, the principal
answer is that researchers have done this on a hunch. Often the most profound insights
in science come when we develop a method for probing a new regime of Nature. For
example, the invention of radio astronomy in the 1930s and 1940s led to a spectacular
sequence of discoveries, including the galactic core of the Milky Way galaxy, pulsars, and
quasars. Low temperature physics has achieved its amazing successes by ﬁnding ways to
lower the temperatures of different systems. In a similar way, by obtaining complete
control over single quantum systems, we are exploring untouched regimes of Nature in
the hope of discovering new and unexpected phenomena. We are just now taking our ﬁrst
steps along these lines, and already a few interesting surprises have been discovered in
this regime. What else shall we discover as we obtain more complete control over single
quantum systems, and extend it to more complex systems?
Quantum computation and quantum information ﬁt naturally into this program. They
provide a useful series of challenges at varied levels of difﬁculty for people devising
methods to better manipulate single quantum systems, and stimulate the development of
new experimental techniques and provide guidance as to the most interesting directions
in which to take experiment. Conversely, the ability to control single quantum systems
is essential if we are to harness the power of quantum mechanics for applications to
quantum computation and quantum information.
Despite this intense interest, efforts to build quantum information processing systems
4
Introduction and overview
have resulted in modest success to date. Small quantum computers, capable of doing
dozens of operations on a few quantum bits (or qubits) represent the state of the art in
quantum computation. Experimental prototypes for doing quantum cryptography – a
way of communicating in secret across long distances – have been demonstrated, and are
even at the level where they may be useful for some real-world applications. However, it
remains a great challenge to physicists and engineers of the future to develop techniques
for making large-scale quantum information processing a reality.
Let us turn our attention from quantum mechanics to another of the great intellectual
triumphs of the twentieth century, computer science. The origins of computer science
are lost in the depths of history. For example, cuneiform tablets indicate that by the time
of Hammurabi (circa 1750 B.C.) the Babylonians had developed some fairly sophisticated
algorithmic ideas, and it is likely that many of those ideas date to even earlier times.
The modern incarnation of computer science was announced by the great mathemati-
cian Alan Turing in a remarkable 1936 paper. Turing developed in detail an abstract
notion of what we would now call a programmable computer, a model for computation
now known as the Turing machine, in his honor. Turing showed that there is a Universal
Turing Machine that can be used to simulate any other Turing machine. Furthermore,
he claimed that the Universal Turing Machine completely captures what it means to per-
form a task by algorithmic means. That is, if an algorithm can be performed on any piece
of hardware (say, a modern personal computer), then there is an equivalent algorithm
for a Universal Turing Machine which performs exactly the same task as the algorithm
running on the personal computer. This assertion, known as the Church–Turing thesis
in honor of Turing and another pioneer of computer science, Alonzo Church, asserts the
equivalence between the physical concept of what class of algorithms can be performed
on some physical device with the rigorous mathematical concept of a Universal Turing
Machine. The broad acceptance of this thesis laid the foundation for the development of
a rich theory of computer science.
Not long after Turing’s paper, the ﬁrst computers constructed from electronic com-
ponents were developed. John von Neumann developed a simple theoretical model for
how to put together in a practical fashion all the components necessary for a computer
to be fully as capable as a Universal Turing Machine. Hardware development truly took
off, though, in 1947, when John Bardeen, Walter Brattain, and Will Shockley developed
the transistor. Computer hardware has grown in power at an amazing pace ever since, so
much so that the growth was codiﬁed by Gordon Moore in 1965 in what has come to be
known as Moore’s law, which states that computer power will double for constant cost
roughly once every two years.
Amazingly enough, Moore’s law has approximately held true in the decades since
the 1960s. Nevertheless, most observers expect that this dream run will end some time
during the ﬁrst two decades of the twenty-ﬁrst century. Conventional approaches to
the fabrication of computer technology are beginning to run up against fundamental
difﬁculties of size. Quantum effects are beginning to interfere in the functioning of
electronic devices as they are made smaller and smaller.
One possible solution to the problem posed by the eventual failure of Moore’s law
is to move to a different computing paradigm. One such paradigm is provided by the
theory of quantum computation, which is based on the idea of using quantum mechanics
to perform computations, instead of classical physics. It turns out that while an ordinary
computer can be used to simulate a quantum computer, it appears to be impossible to
Global perspectives
5
perform the simulation in an efﬁcient fashion. Thus quantum computers offer an essential
speed advantage over classical computers. This speed advantage is so signiﬁcant that many
researchers believe that no conceivable amount of progress in classical computation would
be able to overcome the gap between the power of a classical computer and the power of
a quantum computer.
What do we mean by ‘efﬁcient’ versus ‘inefﬁcient’ simulations of a quantum computer?
Many of the key notions needed to answer this question were actually invented before
the notion of a quantum computer had even arisen. In particular, the idea of efﬁcient
and inefﬁcient algorithms was made mathematically precise by the ﬁeld of computational
complexity. Roughly speaking, an efﬁcient algorithm is one which runs in time polynomial
in the size of the problem solved. In contrast, an inefﬁcient algorithm requires super-
polynomial (typically exponential) time. What was noticed in the late 1960s and early
1970s was that it seemed as though the Turing machine model of computation was at
least as powerful as any other model of computation, in the sense that a problem which
could be solved efﬁciently in some model of computation could also be solved efﬁciently
in the Turing machine model, by using the Turing machine to simulate the other model
of computation. This observation was codiﬁed into a strengthened version of the Church–
Turing thesis:
Any algorithmic process can be simulated efﬁciently using a Turing machine.
The key strengthening in the strong Church–Turing thesis is the word efﬁciently. If
the strong Church–Turing thesis is correct, then it implies that no matter what type of
machine we use to perform our algorithms, that machine can be simulated efﬁciently
using a standard Turing machine. This is an important strengthening, as it implies that
for the purposes of analyzing whether a given computational task can be accomplished
efﬁciently, we may restrict ourselves to the analysis of the Turing machine model of
computation.
One class of challenges to the strong Church–Turing thesis comes from the ﬁeld of
analog computation. In the years since Turing, many different teams of researchers have
noticed that certain types of analog computers can efﬁciently solve problems believed to
have no efﬁcient solution on a Turing machine. At ﬁrst glance these analog computers
appear to violate the strong form of the Church–Turing thesis. Unfortunately for analog
computation, it turns out that when realistic assumptions about the presence of noise in
analog computers are made, their power disappears in all known instances; they cannot
efﬁciently solve problems which are not efﬁciently solvable on a Turing machine. This
lesson – that the effects of realistic noise must be taken into account in evaluating the
efﬁciency of a computational model – was one of the great early challenges of quantum
computation and quantum information, a challenge successfully met by the development
of a theory of quantum error-correcting codes and fault-tolerant quantum computation.
Thus, unlike analog computation, quantum computation can in principle tolerate a ﬁnite
amount of noise and still retain its computational advantages.
The ﬁrst major challenge to the strong Church–Turing thesis arose in the mid 1970s,
when Robert Solovay and Volker Strassen showed that it is possible to test whether an in-
teger is prime or composite using a randomized algorithm. That is, the Solovay–Strassen
test for primality used randomness as an essential part of the algorithm. The algorithm
did not determine whether a given integer was prime or composite with certainty. Instead,
the algorithm could determine that a number was probably prime or else composite with
6
Introduction and overview
certainty. By repeating the Solovay–Strassen test a few times it is possible to determine
with near certainty whether a number is prime or composite. The Solovay-Strassen test
was of especial signiﬁcance at the time it was proposed as no deterministic test for pri-
mality was then known, nor is one known at the time of this writing. Thus, it seemed as
though computers with access to a random number generator would be able to efﬁciently
perform computational tasks with no efﬁcient solution on a conventional deterministic
Turing machine. This discovery inspired a search for other randomized algorithms which
has paid off handsomely, with the ﬁeld blossoming into a thriving area of research.
Randomized algorithms pose a challenge to the strong Church–Turing thesis, suggest-
ing that there are efﬁciently soluble problems which, nevertheless, cannot be efﬁciently
solved on a deterministic Turing machine. This challenge appears to be easily resolved
by a simple modiﬁcation of the strong Church–Turing thesis:
Any algorithmic process can be simulated efﬁciently using a
probabilistic Turing machine.
This ad hoc modiﬁcation of the strong Church–Turing thesis should leave you feeling
rather queasy. Might it not turn out at some later date that yet another model of computa-
tion allows one to efﬁciently solve problems that are not efﬁciently soluble within Turing’s
model of computation? Is there any way we can ﬁnd a single model of computation which
is guaranteed to be able to efﬁciently simulate any other model of computation?
Motivated by this question, in 1985 David Deutsch asked whether the laws of physics
could be use to derive an even stronger version of the Church–Turing thesis. Instead of
adopting ad hoc hypotheses, Deutsch looked to physical theory to provide a foundation
for the Church–Turing thesis that would be as secure as the status of that physical theory.
In particular, Deutsch attempted to deﬁne a computational device that would be capable
of efﬁciently simulating an arbitrary physical system. Because the laws of physics are
ultimately quantum mechanical, Deutsch was naturally led to consider computing devices
based upon the principles of quantum mechanics. These devices, quantum analogues of
the machines deﬁned forty-nine years earlier by Turing, led ultimately to the modern
conception of a quantum computer used in this book.
At the time of writing it is not clear whether Deutsch’s notion of a Universal Quan-
tum Computer is sufﬁcient to efﬁciently simulate an arbitrary physical system. Proving
or refuting this conjecture is one of the great open problems of the ﬁeld of quantum
computation and quantum information. It is possible, for example, that some effect of
quantum ﬁeld theory or an even more esoteric effect based in string theory, quantum
gravity or some other physical theory may take us beyond Deutsch’s Universal Quan-
tum Computer, giving us a still more powerful model for computation. At this stage, we
simply don’t know.
What Deutsch’s model of a quantum computer did enable was a challenge to the strong
form of the Church–Turing thesis. Deutsch asked whether it is possible for a quantum
computer to efﬁciently solve computational problems which have no efﬁcient solution on
a classical computer, even a probabilistic Turing machine. He then constructed a simple
example suggesting that, indeed, quantum computers might have computational powers
exceeding those of classical computers.
This remarkable ﬁrst step taken by Deutsch was improved in the subsequent decade
by many people, culminating in Peter Shor’s 1994 demonstration that two enormously
important problems – the problem of ﬁnding the prime factors of an integer, and the so-
Global perspectives
7
called ‘discrete logarithm’ problem – could be solved efﬁciently on a quantum computer.
This attracted widespread interest because these two problems were and still are widely
believed to have no efﬁcient solution on a classical computer. Shor’s results are a power-
ful indication that quantum computers are more powerful than Turing machines, even
probabilistic Turing machines. Further evidence for the power of quantum computers
came in 1995 when Lov Grover showed that another important problem – the problem of
conducting a search through some unstructured search space – could also be sped up on
a quantum computer. While Grover’s algorithm did not provide as spectacular a speed-
up as Shor’s algorithms, the widespread applicability of search-based methodologies has
excited considerable interest in Grover’s algorithm.
At about the same time as Shor’s and Grover’s algorithms were discovered, many
people were developing an idea Richard Feynman had suggested in 1982. Feynman had
pointed out that there seemed to be essential difﬁculties in simulating quantum mechan-
ical systems on classical computers, and suggested that building computers based on
the principles of quantum mechanics would allow us to avoid those difﬁculties. In the
1990s several teams of researchers began ﬂeshing this idea out, showing that it is indeed
possible to use quantum computers to efﬁciently simulate systems that have no known
efﬁcient simulation on a classical computer. It is likely that one of the major applications
of quantum computers in the future will be performing simulations of quantum mechan-
ical systems too difﬁcult to simulate on a classical computer, a problem with profound
scientiﬁc and technological implications.
What other problems can quantum computers solve more quickly than classical com-
puters? The short answer is that we don’t know. Coming up with good quantum algo-
rithms seems to be hard. A pessimist might think that’s because there’s nothing quantum
computers are good for other than the applications already discovered! We take a differ-
ent view. Algorithm design for quantum computers is hard because designers face two
difﬁcult problems not faced in the construction of algorithms for classical computers.
First, our human intuition is rooted in the classical world. If we use that intuition as an
aid to the construction of algorithms, then the algorithmic ideas we come up with will
be classical ideas. To design good quantum algorithms one must ‘turn off’ one’s classical
intuition for at least part of the design process, using truly quantum effects to achieve
the desired algorithmic end. Second, to be truly interesting it is not enough to design an
algorithm that is merely quantum mechanical. The algorithm must be better than any
existing classical algorithm! Thus, it is possible that one may ﬁnd an algorithm which
makes use of truly quantum aspects of quantum mechanics, that is nevertheless not of
widespread interest because classical algorithms with comparable performance charac-
teristics exist. The combination of these two problems makes the construction of new
quantum algorithms a challenging problem for the future.
Even more broadly, we can ask if there are any generalizations we can make about the
power of quantum computers versus classical computers. What is it that makes quantum
computers more powerful than classical computers – assuming that this is indeed the
case? What class of problems can be solved efﬁciently on a quantum computer, and how
does that class compare to the class of problems that can be solved efﬁciently on a classical
computer? One of the most exciting things about quantum computation and quantum
information is how little is known about the answers to these questions! It is a great
challenge for the future to understand these questions better.
Having come up to the frontier of quantum computation, let’s switch to the history
8
Introduction and overview
of another strand of thought contributing to quantum computation and quantum infor-
mation: information theory. At the same time computer science was exploding in the
1940s, another revolution was taking place in our understanding of communication. In
1948 Claude Shannon published a remarkable pair of papers laying the foundations for
the modern theory of information and communication.
Perhaps the key step taken by Shannon was to mathematically deﬁne the concept of
information. In many mathematical sciences there is considerable ﬂexibility in the choice
of fundamental deﬁnitions. Try thinking naively for a few minutes about the following
question: how would you go about mathematically deﬁning the notion of an information
source? Several different answers to this problem have found widespread use; however,
the deﬁnition Shannon came up with seems to be far and away the most fruitful in
terms of increased understanding, leading to a plethora of deep results and a theory
with a rich structure which seems to accurately reﬂect many (though not all) real-world
communications problems.
Shannon was interested in two key questions related to the communication of in-
formation over a communications channel. First, what resources are required to send
information over a communications channel? For example, telephone companies need
to know how much information they can reliably transmit over a given telephone cable.
Second, can information be transmitted in such a way that it is protected against noise
in the communications channel?
Shannon answered these two questions by proving the two fundamental theorems of
information theory. The ﬁrst, Shannon’s noiseless channel coding theorem, quantiﬁes
the physical resources required to store the output from an information source. Shan-
non’s second fundamental theorem, the noisy channel coding theorem, quantiﬁes how
much information it is possible to reliably transmit through a noisy communications
channel. To achieve reliable transmission in the presence of noise, Shannon showed that
error-correcting codes could be used to protect the information being sent. Shannon’s
noisy channel coding theorem gives an upper limit on the protection afforded by error-
correcting codes. Unfortunately, Shannon’s theorem does not explicitly give a practically
useful set of error-correcting codes to achieve that limit. From the time of Shannon’s pa-
pers until today, researchers have constructed more and better classes of error-correcting
codes in their attempts to come closer to the limit set by Shannon’s theorem. A sophisti-
cated theory of error-correcting codes now exists offering the user a plethora of choices
in their quest to design a good error-correcting code. Such codes are used in a multitude
of places including, for example, compact disc players, computer modems, and satellite
communications systems.
Quantum information theory has followed with similar developments. In 1995, Ben
Schumacher provided an analogue to Shannon’s noiseless coding theorem, and in the
process deﬁned the ‘quantum bit’ or ‘qubit’ as a tangible physical resource. However,
no analogue to Shannon’s noisy channel coding theorem is yet known for quantum in-
formation. Nevertheless, in analogy to their classical counterparts, a theory of quantum
error-correction has been developed which, as already mentioned, allows quantum com-
puters to compute effectively in the presence of noise, and also allows communication
over noisy quantum channels to take place reliably.
Indeed, classical ideas of error-correction have proved to be enormously important
in developing and understanding quantum error-correcting codes. In 1996, two groups
working independently, Robert Calderbank and Peter Shor, and Andrew Steane, discov-
Global perspectives
9
ered an important class of quantum codes now known as CSS codes after their initials.
This work has since been subsumed by the stabilizer codes, independently discovered by
Robert Calderbank, Eric Rains, Peter Shor and Neil Sloane, and by Daniel Gottesman.
By building upon the basic ideas of classical linear coding theory, these discoveries greatly
facilitated a rapid understanding of quantum error-correcting codes and their application
to quantum computation and quantum information.
The theory of quantum error-correcting codes was developed to protect quantum states
against noise. What about transmitting ordinary classical information using a quantum
channel? How efﬁciently can this be done? A few surprises have been discovered in this
arena. In 1992 Charles Bennett and Stephen Wiesner explained how to transmit two
classical bits of information, while only transmitting one quantum bit from sender to
receiver, a result dubbed superdense coding.
Even more interesting are the results in distributed quantum computation. Imagine
you have two computers networked, trying to solve a particular problem. How much
communication is required to solve the problem? Recently it has been shown that quan-
tum computers can require exponentially less communication to solve certain problems
than would be required if the networked computers were classical! Unfortunately, as yet
these problems are not especially important in a practical setting, and suffer from some
undesirable technical restrictions. A major challenge for the future of quantum compu-
tation and quantum information is to ﬁnd problems of real-world importance for which
distributed quantum computation offers a substantial advantage over distributed classical
computation.
Let’s return to information theory proper. The study of information theory begins with
the properties of a single communications channel. In applications we often do not deal
with a single communications channel, but rather with networks of many channels. The
subject of networked information theory deals with the information carrying properties
of such networks of communications channels, and has been developed into a rich and
intricate subject.
By contrast, the study of networked quantum information theory is very much in its
infancy. Even for very basic questions we know little about the information carrying abil-
ities of networks of quantum channels. Several rather striking preliminary results have
been found in the past few years; however, no unifying theory of networked information
theory exists for quantum channels. One example of networked quantum information
theory should sufﬁce to convince you of the value such a general theory would have.
Imagine that we are attempting to send quantum information from Alice to Bob through
a noisy quantum channel. If that channel has zero capacity for quantum information,
then it is impossible to reliably send any information from Alice to Bob. Imagine instead
that we consider two copies of the channel, operating in synchrony. Intuitively it is clear
(and can be rigorously justiﬁed) that such a channel also has zero capacity to send quan-
tum information. However, if we instead reverse the direction of one of the channels, as
illustrated in Figure 1.1, it turns out that sometimes we can obtain a non-zero capacity
for the transmission of information from Alice to Bob! Counter-intuitive properties like
this illustrate the strange nature of quantum information. Better understanding the in-
formation carrying properties of networks of quantum channels is a major open problem
of quantum computation and quantum information.
Let’s switch ﬁelds one last time, moving to the venerable old art and science of cryp-
tography. Broadly speaking, cryptography is the problem of doing communication or
10
Introduction and overview
Figure 1.1. Classically, if we have two very noisy channels of zero capacity running side by side, then the combined
channel has zero capacity to send information. Not surprisingly, if we reverse the direction of one of the channels,
we still have zero capacity to send information. Quantum mechanically, reversing one of the zero capacity channels
can actually allow us to send information!
computation involving two or more parties who may not trust one another. The best
known cryptographic problem is the transmission of secret messages. Suppose two parties
wish to communicate in secret. For example, you may wish to give your credit card num-
ber to a merchant in exchange for goods, hopefully without any malevolent third party
intercepting your credit card number. The way this is done is to use a cryptographic
protocol. We’ll describe in detail how cryptographic protocols work later in the book, but
for now it will sufﬁce to make a few simple distinctions. The most important distinction
is between private key cryptosystems and public key cryptosystems.
The way a private key cryptosystem works is that two parties, ‘Alice’ and ‘Bob’, wish
to communicate by sharing a private key, which only they know. The exact form of the
key doesn’t matter at this point – think of a string of zeroes and ones. The point is that
this key is used by Alice to encrypt the information she wishes to send to Bob. After
Alice encrypts she sends the encrypted information to Bob, who must now recover the
original information. Exactly how Alice encrypts the message depends upon the private
key, so that to recover the original message Bob needs to know the private key, in order
to undo the transformation Alice applied.
Unfortunately, private key cryptosystems have some severe problems in many contexts.
The most basic problem is how to distribute the keys? In many ways, the key distribution
problem is just as difﬁcult as the original problem of communicating in private – a
malevolent third party may be eavesdropping on the key distribution, and then use the
intercepted key to decrypt some of the message transmission.
One of the earliest discoveries in quantum computation and quantum information was
that quantum mechanics can be used to do key distribution in such a way that Alice and
Bob’s security can not be compromised. This procedure is known as quantum cryptog-
raphy or quantum key distribution. The basic idea is to exploit the quantum mechanical
principle that observation in general disturbs the system being observed. Thus, if there is
an eavesdropper listening in as Alice and Bob attempt to transmit their key, the presence
of the eavesdropper will be visible as a disturbance of the communications channel Alice
and Bob are using to establish the key. Alice and Bob can then throw out the key bits
established while the eavesdropper was listening in, and start over. The ﬁrst quantum
cryptographic ideas were proposed by Stephen Wiesner in the late 1960s, but unfortu-
Global perspectives
11
nately were not accepted for publication! In 1984 Charles Bennett and Gilles Brassard,
building on Wiesner’s earlier work, proposed a protocol using quantum mechanics to
distribute keys between Alice and Bob, without any possibility of a compromise. Since
then numerous quantum cryptographic protocols have been proposed, and experimental
prototypes developed. At the time of this writing, the experimental prototypes are nearing
the stage where they may be useful in limited-scale real-world applications.
The second major type of cryptosystem is the public key cryptosystem. Public key
cryptosystems don’t rely on Alice and Bob sharing a secret key in advance. Instead, Bob
simply publishes a ‘public key’, which is made available to the general public. Alice
can make use of this public key to encrypt a message which she sends to Bob. What
is interesting is that a third party cannot use Bob’s public key to decrypt the message!
Strictly speaking, we shouldn’t say cannot. Rather, the encryption transformation is
chosen in a very clever and non-trivial way so that it is extremely difﬁcult (though not
impossible) to invert, given only knowledge of the public key. To make inversion easy, Bob
has a secret key matched to his public key, which together enable him to easily perform
the decryption. This secret key is not known to anybody other than Bob, who can therefore
be conﬁdent that only he can read the contents of Alice’s transmission, to the extent that
it is unlikely that anybody else has the computational power to invert the encryption,
given only the public key. Public key cryptosystems solve the key distribution problem
by making it unnecessary for Alice and Bob to share a private key before communicating.
Rather remarkably, public key cryptography did not achieve widespread use until the
mid-1970s, when it was proposed independently by Whitﬁeld Difﬁe and Martin Hellman,
and by Ralph Merkle, revolutionizing the ﬁeld of cryptography. A little later, Ronald
Rivest, Adi Shamir, and Leonard Adleman developed the RSA cryptosystem, which
at the time of writing is the most widely deployed public key cryptosystem, believed to
offer a ﬁne balance of security and practical usability. In 1997 it was disclosed that these
ideas – public key cryptography, the Difﬁe–Hellman and RSA cryptosystems – were
actually invented in the late 1960s and early 1970s by researchers working at the British
intelligence agency GCHQ.
The key to the security of public key cryptosystems is that it should be difﬁcult to
invert the encryption stage if only the public key is available. For example, it turns out
that inverting the encryption stage of RSA is a problem closely related to factoring.
Much of the presumed security of RSA comes from the belief that factoring is a problem
hard to solve on a classical computer. However, Shor’s fast algorithm for factoring on
a quantum computer could be used to break RSA! Similarly, there are other public key
cryptosystems which can be broken if a fast algorithm for solving the discrete logarithm
problem – like Shor’s quantum algorithm for discrete logarithm – were known. This
practical application of quantum computers to the breaking of cryptographic codes has
excited much of the interest in quantum computation and quantum information.
We have been looking at the historical antecedents for quantum computation and
quantum information. Of course, as the ﬁeld has grown and matured, it has sprouted
its own subﬁelds of research, whose antecedents lie mainly within quantum computation
and quantum information.
Perhaps the most striking of these is the study of quantum entanglement. Entangle-
ment is a uniquely quantum mechanical resource that plays a key role in many of the
most interesting applications of quantum computation and quantum information; en-
tanglement is iron to the classical world’s bronze age. In recent years there has been a
12
Introduction and overview
tremendous effort trying to better understand the properties of entanglement considered
as a fundamental resource of Nature, of comparable importance to energy, information,
entropy, or any other fundamental resource. Although there is as yet no complete theory
of entanglement, some progress has been made in understanding this strange property of
quantum mechanics. It is hoped by many researchers that further study of the properties
of entanglement will yield insights that facilitate the development of new applications in
quantum computation and quantum information.
1.1.2
Future directions
We’ve looked at some of the history and present status of quantum computation and
quantum information. What of the future? What can quantum computation and quan-
tum information offer to science, to technology, and to humanity? What beneﬁts does
quantum computation and quantum information confer upon its parent ﬁelds of computer
science, information theory, and physics? What are the key open problems of quantum
computation and quantum information? We will make a few very brief remarks about
these overarching questions before moving onto more detailed investigations.
Quantum computation and quantum information has taught us to think physically
about computation, and we have discovered that this approach yields many new and
exciting capabilities for information processing and communication. Computer scientists
and information theorists have been gifted with a new and rich paradigm for explo-
ration. Indeed, in the broadest terms we have learned that any physical theory, not just
quantum mechanics, may be used as the basis for a theory of information processing
and communication. The fruits of these explorations may one day result in information
processing devices with capabilities far beyond today’s computing and communications
systems, with concomitant beneﬁts and drawbacks for society as a whole.
Quantum computation and quantum information certainly offer challenges aplenty
to physicists, but it is perhaps a little subtle what quantum computation and quantum
information offers to physics in the long term. We believe that just as we have learned to
think physically about computation, we can also learn to think computationally about
physics. Whereas physics has traditionally been a discipline focused on understanding
‘elementary’ objects and simple systems, many interesting aspects of Nature arise only
when things become larger and more complicated. Chemistry and engineering deal with
such complexity to some extent, but most often in a rather ad hoc fashion. One of
the messages of quantum computation and information is that new tools are available
for traversing the gulf between the small and the relatively complex: computation and
algorithms provide systematic means for constructing and understanding such systems.
Applying ideas from these ﬁelds is already beginning to yield new insights into physics.
It is our hope that this perspective will blossom in years to come into a fruitful way of
understanding all aspects of physics.
We’ve brieﬂy examined some of the key motivations and ideas underlying quantum
computation and quantum information. Over the remaining sections of this chapter we
give a more technical but still accessible introduction to these motivations and ideas, with
the hope of giving you a bird’s-eye view of the ﬁeld as it is presently poised.
Quantum bits
13
1.2
Quantum bits
The bit is the fundamental concept of classical computation and classical information.
Quantum computation and quantum information are built upon an analogous concept,
the quantum bit, or qubit for short. In this section we introduce the properties of single
and multiple qubits, comparing and contrasting their properties to those of classical bits.
What is a qubit? We’re going to describe qubits as mathematical objects with certain
speciﬁc properties. ‘But hang on’, you say, ‘I thought qubits were physical objects.’ It’s
true that qubits, like bits, are realized as actual physical systems, and in Section 1.5 and
Chapter 7 we describe in detail how this connection between the abstract mathematical
point of view and real systems is made. However, for the most part we treat qubits as
abstract mathematical objects. The beauty of treating qubits as abstract entities is that it
gives us the freedom to construct a general theory of quantum computation and quantum
information which does not depend upon a speciﬁc system for its realization.
What then is a qubit? Just as a classical bit has a state – either 0 or 1 – a qubit also
has a state. Two possible states for a qubit are the states |0⟩and |1⟩, which as you might
guess correspond to the states 0 and 1 for a classical bit. Notation like ‘| ⟩’ is called the
Dirac notation, and we’ll be seeing it often, as it’s the standard notation for states in
quantum mechanics. The difference between bits and qubits is that a qubit can be in a
state other than |0⟩or |1⟩. It is also possible to form linear combinations of states, often
called superpositions:
|ψ⟩= α |0⟩+ β |1⟩.
(1.1)
The numbers α and β are complex numbers, although for many purposes not much is
lost by thinking of them as real numbers. Put another way, the state of a qubit is a vector
in a two-dimensional complex vector space. The special states |0⟩and |1⟩are known as
computational basis states, and form an orthonormal basis for this vector space.
We can examine a bit to determine whether it is in the state 0 or 1. For example,
computers do this all the time when they retrieve the contents of their memory. Rather
remarkably, we cannot examine a qubit to determine its quantum state, that is, the
values of α and β. Instead, quantum mechanics tells us that we can only acquire much
more restricted information about the quantum state. When we measure a qubit we get
either the result 0, with probability |α|2, or the result 1, with probability |β|2. Naturally,
|α|2 + |β|2 = 1, since the probabilities must sum to one. Geometrically, we can interpret
this as the condition that the qubit’s state be normalized to length 1. Thus, in general a
qubit’s state is a unit vector in a two-dimensional complex vector space.
This dichotomy between the unobservable state of a qubit and the observations we
can make lies at the heart of quantum computation and quantum information. In most
of our abstract models of the world, there is a direct correspondence between elements
of the abstraction and the real world, just as an architect’s plans for a building are in
correspondence with the ﬁnal building. The lack of this direct correspondence in quantum
mechanics makes it difﬁcult to intuit the behavior of quantum systems; however, there
is an indirect correspondence, for qubit states can be manipulated and transformed in
ways which lead to measurement outcomes which depend distinctly on the different
properties of the state. Thus, these quantum states have real, experimentally veriﬁable
consequences, which we shall see are essential to the power of quantum computation and
quantum information.
14
Introduction and overview
The ability of a qubit to be in a superposition state runs counter to our ‘common sense’
understanding of the physical world around us. A classical bit is like a coin: either heads
or tails up. For imperfect coins, there may be intermediate states like having it balanced
on an edge, but those can be disregarded in the ideal case. By contrast, a qubit can exist
in a continuum of states between |0⟩and |1⟩– until it is observed. Let us emphasize
again that when a qubit is measured, it only ever gives ‘0’ or ‘1’ as the measurement
result – probabilistically. For example, a qubit can be in the state
1
√
2
|0⟩+ 1
√
2
|1⟩,
(1.2)
which, when measured, gives the result 0 ﬁfty percent (|1/
√
2|2) of the time, and the
result 1 ﬁfty percent of the time. We will return often to this state, which is sometimes
denoted |+⟩.
Despite this strangeness, qubits are decidedly real, their existence and behavior ex-
tensively validated by experiments (discussed in Section 1.5 and Chapter 7), and many
different physical systems can be used to realize qubits. To get a concrete feel for how a
qubit can be realized it may be helpful to list some of the ways this realization may occur:
as the two different polarizations of a photon; as the alignment of a nuclear spin in a
uniform magnetic ﬁeld; as two states of an electron orbiting a single atom such as shown
in Figure 1.2. In the atom model, the electron can exist in either the so-called ‘ground’
or ‘excited’ states, which we’ll call |0⟩and |1⟩, respectively. By shining light on the atom,
with appropriate energy and for an appropriate length of time, it is possible to move
the electron from the |0⟩state to the |1⟩state and vice versa. But more interestingly, by
reducing the time we shine the light, an electron initially in the state |0⟩can be moved
‘halfway’ between |0⟩and |1⟩, into the |+⟩state.
Figure 1.2. Qubit represented by two electronic levels in an atom.
Naturally, a great deal of attention has been given to the ‘meaning’ or ‘interpretation’
that might be attached to superposition states, and of the inherently probabilistic nature of
observations on quantum systems. However, by and large, we shall not concern ourselves
with such discussions in this book. Instead, our intent will be to develop mathematical
and conceptual pictures which are predictive.
One picture useful in thinking about qubits is the following geometric representation.
Quantum bits
15
Because |α|2 + |β|2 = 1, we may rewrite Equation (1.1) as
|ψ⟩= eiγ

cos θ
2|0⟩+ eiϕ sin θ
2|1⟩

,
(1.3)
where θ, ϕ and γ are real numbers. In Chapter 2 we will see that we can ignore the factor
of eiγ out the front, because it has no observable effects, and for that reason we can
effectively write
|ψ⟩= cos θ
2|0⟩+ eiϕ sin θ
2|1⟩.
(1.4)
The numbers θ and ϕ deﬁne a point on the unit three-dimensional sphere, as shown in
Figure 1.3. This sphere is often called the Bloch sphere; it provides a useful means of
visualizing the state of a single qubit, and often serves as an excellent testbed for ideas
about quantum computation and quantum information. Many of the operations on single
qubits which we describe later in this chapter are neatly described within the Bloch sphere
picture. However, it must be kept in mind that this intuition is limited because there is
no simple generalization of the Bloch sphere known for multiple qubits.


ϕ
|ψ⟩
θ
x
y
z
Figure 1.3. Bloch sphere representation of a qubit.
How much information is represented by a qubit? Paradoxically, there are an inﬁnite
number of points on the unit sphere, so that in principle one could store an entire text
of Shakespeare in the inﬁnite binary expansion of θ. However, this conclusion turns
out to be misleading, because of the behavior of a qubit when observed. Recall that
measurement of a qubit will give only either 0 or 1. Furthermore, measurement changes
the state of a qubit, collapsing it from its superposition of |0⟩and |1⟩to the speciﬁc state
consistent with the measurement result. For example, if measurement of |+⟩gives 0,
then the post-measurement state of the qubit will be |0⟩. Why does this type of collapse
occur? Nobody knows. As discussed in Chapter 2, this behavior is simply one of the
fundamental postulates of quantum mechanics. What is relevant for our purposes is that
from a single measurement one obtains only a single bit of information about the state of
the qubit, thus resolving the apparent paradox. It turns out that only if inﬁnitely many
16
Introduction and overview
identically prepared qubits were measured would one be able to determine α and β for
a qubit in the state given in Equation (1.1).
But an even more interesting question to ask might be: how much information is
represented by a qubit if we do not measure it? This is a trick question, because how
can one quantify information if it cannot be measured? Nevertheless, there is something
conceptually important here, because when Nature evolves a closed quantum system of
qubits, not performing any ‘measurements’, she apparently does keep track of all the
continuous variables describing the state, like α and β. In a sense, in the state of a qubit,
Nature conceals a great deal of ‘hidden information’. And even more interestingly, we will
see shortly that the potential amount of this extra ‘information’ grows exponentially with
the number of qubits. Understanding this hidden quantum information is a question
that we grapple with for much of this book, and which lies at the heart of what makes
quantum mechanics a powerful tool for information processing.
1.2.1
Multiple qubits
Hilbert space is a big place.
– Carlton Caves
Suppose we have two qubits. If these were two classical bits, then there would be four
possible states, 00, 01, 10, and 11. Correspondingly, a two qubit system has four com-
putational basis states denoted |00⟩, |01⟩, |10⟩, |11⟩. A pair of qubits can also exist in
superpositions of these four states, so the quantum state of two qubits involves associating
a complex coefﬁcient – sometimes called an amplitude – with each computational basis
state, such that the state vector describing the two qubits is
|ψ⟩= α00|00⟩+ α01|01⟩+ α10|10⟩+ α11|11⟩.
(1.5)
Similar to the case for a single qubit, the measurement result x (= 00, 01, 10 or 11) occurs
with probability |αx|2, with the state of the qubits after the measurement being |x⟩. The
condition that probabilities sum to one is therefore expressed by the normalization
condition that 
x∈{0,1}2 |αx|2 = 1, where the notation ‘{0, 1}2’ means ‘the set of strings
of length two with each letter being either zero or one’. For a two qubit system, we could
measure just a subset of the qubits, say the ﬁrst qubit, and you can probably guess how
this works: measuring the ﬁrst qubit alone gives 0 with probability |α00|2 +|α01|2, leaving
the post-measurement state
|ψ′⟩= α00|00⟩+ α01|01⟩

|α00|2 + |α01|2 .
(1.6)
Note how the post-measurement state is re-normalized by the factor

|α00|2 + |α01|2
so that it still satisﬁes the normalization condition, just as we expect for a legitimate
quantum state.
An important two qubit state is the Bell state or EPR pair,
|00⟩+ |11⟩
√
2
.
(1.7)
This innocuous-looking state is responsible for many surprises in quantum computation
Quantum computation
17
and quantum information. It is the key ingredient in quantum teleportation and super-
dense coding, which we’ll come to in Section 1.3.7 and Section 2.3, respectively, and
the prototype for many other interesting quantum states. The Bell state has the property
that upon measuring the ﬁrst qubit, one obtains two possible results: 0 with probability
1/2, leaving the post-measurement state |ϕ′⟩= |00⟩, and 1 with probability 1/2, leaving
|ϕ′⟩= |11⟩. As a result, a measurement of the second qubit always gives the same result
as the measurement of the ﬁrst qubit. That is, the measurement outcomes are correlated.
Indeed, it turns out that other types of measurements can be performed on the Bell
state, by ﬁrst applying some operations to the ﬁrst or second qubit, and that interesting
correlations still exist between the result of a measurement on the ﬁrst and second qubit.
These correlations have been the subject of intense interest ever since a famous paper
by Einstein, Podolsky and Rosen, in which they ﬁrst pointed out the strange properties
of states like the Bell state. EPR’s insights were taken up and greatly improved by John
Bell, who proved an amazing result: the measurement correlations in the Bell state are
stronger than could ever exist between classical systems. These results, described in de-
tail in Section 2.6, were the ﬁrst intimation that quantum mechanics allows information
processing beyond what is possible in the classical world.
More generally, we may consider a system of n qubits. The computational basis states
of this system are of the form |x1x2 . . . xn⟩, and so a quantum state of such a system
is speciﬁed by 2n amplitudes. For n = 500 this number is larger than the estimated
number of atoms in the Universe! Trying to store all these complex numbers would not
be possible on any conceivable classical computer. Hilbert space is indeed a big place.
In principle, however, Nature manipulates such enormous quantities of data, even for
systems containing only a few hundred atoms. It is as if Nature were keeping 2500 hidden
pieces of scratch paper on the side, on which she performs her calculations as the system
evolves. This enormous potential computational power is something we would very much
like to take advantage of. But how can we think of quantum mechanics as computation?
1.3
Quantum computation
Changes occurring to a quantum state can be described using the language of quantum
computation. Analogous to the way a classical computer is built from an electrical circuit
containing wires and logic gates, a quantum computer is built from a quantum circuit
containing wires and elementary quantum gates to carry around and manipulate the
quantum information. In this section we describe some simple quantum gates, and present
several example circuits illustrating their application, including a circuit which teleports
qubits!
1.3.1
Single qubit gates
Classical computer circuits consist of wires and logic gates. The wires are used to carry
information around the circuit, while the logic gates perform manipulations of the infor-
mation, converting it from one form to another. Consider, for example, classical single bit
logic gates. The only non-trivial member of this class is the
gate, whose operation
is deﬁned by its truth table, in which 0 →1 and 1 →0, that is, the 0 and 1 states are
interchanged.
Can an analogous quantum
gate for qubits be deﬁned? Imagine that we had
some process which took the state |0⟩to the state |1⟩, and vice versa. Such a process
18
Introduction and overview
would obviously be a good candidate for a quantum analogue to the
gate. However,
specifying the action of the gate on the states |0⟩and |1⟩does not tell us what happens to
superpositions of the states |0⟩and |1⟩, without further knowledge about the properties
of quantum gates. In fact, the quantum
gate acts linearly, that is, it takes the state
α|0⟩+ β|1⟩
(1.8)
to the corresponding state in which the role of |0⟩and |1⟩have been interchanged,
α|1⟩+ β|0⟩.
(1.9)
Why the quantum
gate acts linearly and not in some nonlinear fashion is a very
interesting question, and the answer is not at all obvious. It turns out that this linear
behavior is a general property of quantum mechanics, and very well motivated empirically;
moreover, nonlinear behavior can lead to apparent paradoxes such as time travel, faster-
than-light communication, and violations of the second laws of thermodynamics. We’ll
explore this point in more depth in later chapters, but for now we’ll just take it as given.
There is a convenient way of representing the quantum
gate in matrix form,
which follows directly from the linearity of quantum gates. Suppose we deﬁne a matrix
X to represent the quantum
gate as follows:
X ≡
 0
1
1
0

.
(1.10)
(The notation X for the quantum
is used for historical reasons.) If the quantum
state α|0⟩+ β|1⟩is written in a vector notation as
 α
β

,
(1.11)
with the top entry corresponding to the amplitude for |0⟩and the bottom entry the
amplitude for |1⟩, then the corresponding output from the quantum
gate is
X
 α
β

=
 β
α

.
(1.12)
Notice that the action of the
gate is to take the state |0⟩and replace it by the state
corresponding to the ﬁrst column of the matrix X. Similarly, the state |1⟩is replaced by
the state corresponding to the second column of the matrix X.
So quantum gates on a single qubit can be described by two by two matrices. Are there
any constraints on what matrices may be used as quantum gates? It turns out that there
are. Recall that the normalization condition requires |α|2 + |β|2 = 1 for a quantum state
α|0⟩+ β|1⟩. This must also be true of the quantum state |ψ′⟩= α′|0⟩+ β′|1⟩after the
gate has acted. It turns out that the appropriate condition on the matrix representing the
gate is that the matrix U describing the single qubit gate be unitary, that is U †U = I,
where U † is the adjoint of U (obtained by transposing and then complex conjugating
U), and I is the two by two identity matrix. For example, for the
gate it is easy to
verify that X†X = I.
Amazingly, this unitarity constraint is the only constraint on quantum gates. Any
unitary matrix speciﬁes a valid quantum gate! The interesting implication is that in
contrast to the classical case, where only one non-trivial single bit gate exists – the
Quantum computation
19






x
x
x
y
y
y
z
z
z
Figure 1.4. Visualization of the Hadamard gate on the Bloch sphere, acting on the input state (|0⟩+ |1⟩)/
√
2.
gate – there are many non-trivial single qubit gates. Two important ones which we shall
use later are the Z gate:
Z ≡
 1
0
0
−1

,
(1.13)
which leaves |0⟩unchanged, and ﬂips the sign of |1⟩to give −|1⟩, and the Hadamard
gate,
H ≡
1
√
2
 1
1
1
−1

.
(1.14)
This gate is sometimes described as being like a ‘square-root of
’ gate, in that it turns
a |0⟩into (|0⟩+ |1⟩)/
√
2 (ﬁrst column of H), ‘halfway’ between |0⟩and |1⟩, and turns
|1⟩into (|0⟩−|1⟩)/
√
2 (second column of H), which is also ‘halfway’ between |0⟩and
|1⟩. Note, however, that H2 is not a
gate, as simple algebra shows that H2 = I, and
thus applying H twice to a state does nothing to it.
The Hadamard gate is one of the most useful quantum gates, and it is worth trying to
visualize its operation by considering the Bloch sphere picture. In this picture, it turns
out that single qubit gates correspond to rotations and reﬂections of the sphere. The
Hadamard operation is just a rotation of the sphere about the ˆy axis by 90◦, followed by
a rotation about the ˆx axis by 180◦, as illustrated in Figure 1.4. Some important single
qubit gates are shown in Figure 1.5, and contrasted with the classical case.


Figure 1.5. Single bit (left) and qubit (right) logic gates.
There are inﬁnitely many two by two unitary matrices, and thus inﬁnitely many single
20
Introduction and overview
qubit gates. However, it turns out that the properties of the complete set can be under-
stood from the properties of a much smaller set. For example, as explained in Box 1.1,
an arbitrary single qubit unitary gate can be decomposed as a product of rotations
 cos γ
2
−sin γ
2
sin γ
2
cos γ
2

,
(1.15)
and a gate which we’ll later understand as being a rotation about the ˆz axis,
 e−iβ/2
0
0
eiβ/2

,
(1.16)
together with a (global) phase shift – a constant multiplier of the form eiα. These gates
can be broken down further – we don’t need to be able to do these gates for arbitrary
α, β and γ, but can build arbitrarily good approximations to such gates using only certain
special ﬁxed values of α, β and γ. In this way it is possible to build up an arbitrary single
qubit gate using a ﬁnite set of quantum gates. More generally, an arbitrary quantum
computation on any number of qubits can be generated by a ﬁnite set of gates that is said
to be universal for quantum computation. To obtain such a universal set we ﬁrst need
to introduce some quantum gates involving multiple qubits.
Box 1.1: Decomposing single qubit operations
In Section 4.2 starting on page 174 we prove that an arbitrary 2×2 unitary matrix
may be decomposed as
U = eiα
 e−iβ/2
0
0
eiβ/2
  cos γ
2
−sin γ
2
sin γ
2
cos γ
2

,
 e−iδ/2
0
0
eiδ/2

, (1.17)
where α, β, γ, and δ are real-valued. Notice that the second matrix is just an
ordinary rotation. It turns out that the ﬁrst and last matrices can also be understood
as rotations in a different plane. This decomposition can be used to give an exact
prescription for performing an arbitrary single qubit quantum logic gate.
1.3.2
Multiple qubit gates
Now let us generalize from one to multiple qubits. Figure 1.6 shows ﬁve notable multiple
bit classical gates, the
,
,
(exclusive-
),
and
gates. An important
theoretical result is that any function on bits can be computed from the composition of
gates alone, which is thus known as a universal gate. By contrast, the
alone or
even together with
is not universal. One way of seeing this is to note that applying
an
gate does not change the total parity of the bits. As a result, any circuit involving
only
and
gates will, if two inputs x and y have the same parity, give outputs
with the same parity, restricting the class of functions which may be computed, and thus
precluding universality.
The prototypical multi-qubit quantum logic gate is the controlled-
or
gate.
This gate has two input qubits, known as the control qubit and the target qubit, respec-
tively. The circuit representation for the
is shown in the top right of Figure 1.6;
the top line represents the control qubit, while the bottom line represents the target
Quantum computation
21



  


  


  


  



  

 




	


Figure 1.6. On the left are some standard single and multiple bit gates, while on the right is the prototypical
multiple qubit gate, the controlled-
. The matrix representation of the controlled-
, UCN, is written with
respect to the amplitudes for |00⟩, |01⟩, |10⟩, and |11⟩, in that order.
qubit. The action of the gate may be described as follows. If the control qubit is set to
0, then the target qubit is left alone. If the control qubit is set to 1, then the target qubit
is ﬂipped. In equations:
|00⟩→|00⟩; |01⟩→|01⟩; |10⟩→|11⟩; |11⟩→|10⟩.
(1.18)
Another way of describing the
is as a generalization of the classical
gate, since
the action of the gate may be summarized as |A, B⟩→|A, B ⊕A⟩, where ⊕is addition
modulo two, which is exactly what the
gate does. That is, the control qubit and the
target qubit are
ed and stored in the target qubit.
Yet another way of describing the action of the
is to give a matrix represen-
tation, as shown in the bottom right of Figure 1.6. You can easily verify that the ﬁrst
column of UCN describes the transformation that occurs to |00⟩, and similarly for the
other computational basis states, |01⟩, |10⟩, and |11⟩. As for the single qubit case, the
requirement that probability be conserved is expressed in the fact that UCN is a unitary
matrix, that is, U †
CNUCN = I.
We noticed that the
can be regarded as a type of generalized-
gate. Can
other classical gates such as the
or the regular
gate be understood as unitary
gates in a sense similar to the way the quantum
gate represents the classical
gate? It turns out that this is not possible. The reason is because the
and
gates
are essentially irreversible or non-invertible. For example, given the output A ⊕B from
an
gate, it is not possible to determine what the inputs A and B were; there is an
irretrievable loss of information associated with the irreversible action of the
gate.
On the other hand, unitary quantum gates are always invertible, since the inverse of a
unitary matrix is also a unitary matrix, and thus a quantum gate can always be inverted
by another quantum gate. Understanding how to do classical logic in this reversible or
invertible sense will be a crucial step in understanding how to harness the power of
22
Introduction and overview
quantum mechanics for computation. We’ll explain the basic idea of how to do reversible
computation in Section 1.4.1.
Of course, there are many interesting quantum gates other than the controlled-
.
However, in a sense the controlled-
and single qubit gates are the prototypes for all
other gates because of the following remarkable universality result: Any multiple qubit
logic gate may be composed from
and single qubit gates. The proof is given in
Section 4.5, and is the quantum parallel of the universality of the
gate.
1.3.3
Measurements in bases other than the computational basis
We’ve described quantum measurements of a single qubit in the state α|0⟩+ β|1⟩as
yielding the result 0 or 1 and leaving the qubit in the corresponding state |0⟩or |1⟩,
with respective probabilities |α|2 and |β|2. In fact, quantum mechanics allows somewhat
more versatility in the class of measurements that may be performed, although certainly
nowhere near enough to recover α and β from a single measurement!
Note that the states |0⟩and |1⟩represent just one of many possible choices of basis
states for a qubit. Another possible choice is the set |+⟩≡(|0⟩+ |1⟩)/
√
2 and |−⟩≡
(|0⟩−|1⟩)/
√
2. An arbitrary state |ψ⟩= α|0⟩+ β|1⟩can be re-expressed in terms of the
states |+⟩and |−⟩:
|ψ⟩= α|0⟩+ β|1⟩= α|+⟩+ |−⟩
√
2
+ β |+⟩−|−⟩
√
2
= α + β
√
2
|+⟩+ α −β
√
2
|−⟩. (1.19)
It turns out that it is possible to treat the |+⟩and |−⟩states as though they were the com-
putational basis states, and measure with respect to this new basis. Naturally, measuring
with respect to the |+⟩, |−⟩basis results in the result ‘+’ with probability |α+β|2/2 and
the result ‘−’ with probability |α −β|2/2, with corresponding post-measurement states
|+⟩and |−⟩, respectively.
More generally, given any basis states |a⟩and |b⟩for a qubit, it is possible to express an
arbitrary state as a linear combination α|a⟩+β|b⟩of those states. Furthermore, provided
the states are orthonormal, it is possible to perform a measurement with respect to
the |a⟩, |b⟩basis, giving the result a with probability |α|2 and b with probability |β|2.
The orthonormality constraint is necessary in order that |α|2 + |β|2 = 1 as we expect for
probabilities. In an analogous way it is possible in principle to measure a quantum system
of many qubits with respect to an arbitrary orthonormal basis. However, just because it
is possible in principle does not mean that such a measurement can be done easily, and
we return later to the question of how efﬁciently a measurement in an arbitrary basis can
be performed.
There are many reasons for using this extended formalism for quantum measure-
ments, but ultimately the best one is this: the formalism allows us to describe observed
experimental results, as we will see in our discussion of the Stern–Gerlach experiment
in Section 1.5.1. An even more sophisticated and convenient (but essentially equivalent)
formalism for describing quantum measurements is described in the next chapter, in
Section 2.2.3.
1.3.4
Quantum circuits
We’ve already met a few simple quantum circuits. Let’s look in a little more detail at
the elements of a quantum circuit. A simple quantum circuit containing three quantum
gates is shown in Figure 1.7. The circuit is to be read from left-to-right. Each line
Quantum computation
23
in the circuit represents a wire in the quantum circuit. This wire does not necessarily
correspond to a physical wire; it may correspond instead to the passage of time, or perhaps
to a physical particle such as a photon – a particle of light – moving from one location
to another through space. It is conventional to assume that the state input to the circuit
is a computational basis state, usually the state consisting of all |0⟩s. This rule is broken
frequently in the literature on quantum computation and quantum information, but it is
considered polite to inform the reader when this is the case.
The circuit in Figure 1.7 accomplishes a simple but useful task – it swaps the states
of the two qubits. To see that this circuit accomplishes the swap operation, note that the
sequence of gates has the following sequence of effects on a computational basis state
|a, b⟩,
|a, b⟩−→|a, a ⊕b⟩
−→|a ⊕(a ⊕b), a ⊕b⟩= |b, a ⊕b⟩
−→|b, (a ⊕b) ⊕b⟩= |b, a⟩,
(1.20)
where all additions are done modulo 2. The effect of the circuit, therefore, is to inter-
change the state of the two qubits.
Figure 1.7. Circuit swapping two qubits, and an equivalent schematic symbol notation for this common and useful
circuit.
There are a few features allowed in classical circuits that are not usually present in
quantum circuits. First of all, we don’t allow ‘loops’, that is, feedback from one part of the
quantum circuit to another; we say the circuit is acyclic. Second, classical circuits allow
wires to be ‘joined’ together, an operation known as
, with the resulting single wire
containing the bitwise
of the inputs. Obviously this operation is not reversible and
therefore not unitary, so we don’t allow
in our quantum circuits. Third, the inverse
operation,
, whereby several copies of a bit are produced is also not allowed in
quantum circuits. In fact, it turns out that quantum mechanics forbids the copying of a
qubit, making the
operation impossible! We’ll see an example of this in the next
section when we attempt to design a circuit to copy a qubit.
As we proceed we’ll introduce new quantum gates as needed. It’s convenient to in-
troduce another convention about quantum circuits at this point. This convention is
illustrated in Figure 1.8. Suppose U is any unitary matrix acting on some number n of
qubits, so U can be regarded as a quantum gate on those qubits. Then we can deﬁne a
controlled-U gate which is a natural extension of the controlled-
gate. Such a gate
has a single control qubit, indicated by the line with the black dot, and n target qubits,
indicated by the boxed U. If the control qubit is set to 0 then nothing happens to the
target qubits. If the control qubit is set to 1 then the gate U is applied to the target qubits.
The prototypical example of the controlled-U gate is the controlled-
gate, which is
a controlled-U gate with U = X, as illustrated in Figure 1.9.
Another important operation is measurement, which we represent by a ‘meter’ symbol,
24
Introduction and overview
Figure 1.8. Controlled-U gate.
Figure 1.9. Two different representations for the controlled-
.
as shown in Figure 1.10. As previously described, this operation converts a single qubit
state |ψ⟩= α|0⟩+β|1⟩into a probabilistic classical bit M (distinguished from a qubit by
drawing it as a double-line wire), which is 0 with probability |α|2, or 1 with probability
|β|2.

________

_ _ _ _ _ _ _ _

Figure 1.10. Quantum circuit symbol for measurement.
We shall ﬁnd quantum circuits useful as models of all quantum processes, including
but not limited to computation, communication, and even quantum noise. Several simple
examples illustrate this below.
1.3.5
Qubit copying circuit?
The
gate is useful for demonstrating one particularly fundamental property of
quantum information. Consider the task of copying a classical bit. This may be done
using a classical
gate, which takes in the bit to copy (in some unknown state x)
and a ‘scratchpad’ bit initialized to zero, as illustrated in Figure 1.11. The output is two
bits, both of which are in the same state x.
Suppose we try to copy a qubit in the unknown state |ψ⟩= a |0⟩+ b |1⟩in the same
manner by using a
gate. The input state of the two qubits may be written as

a |0⟩+ b |1⟩
	
|0⟩= a |00⟩+ b |10⟩,
(1.21)
The function of
is to negate the second qubit when the ﬁrst qubit is 1, and thus
the output is simply a |00⟩+ b |11⟩. Have we successfully copied |ψ⟩? That is, have we
created the state |ψ⟩|ψ⟩? In the case where |ψ⟩= |0⟩or |ψ⟩= |1⟩that is indeed what this
circuit does; it is possible to use quantum circuits to copy classical information encoded
as a |0⟩or a |1⟩. However, for a general state |ψ⟩we see that
|ψ⟩|ψ⟩= a2|00⟩+ ab|01⟩+ ab|10⟩+ b2|11⟩.
(1.22)
Quantum computation
25









Figure 1.11. Classical and quantum circuits to ‘copy’ an unknown bit or qubit.
Comparing with a|00⟩+b|11⟩, we see that unless ab = 0 the ‘copying circuit’ above does
not copy the quantum state input. In fact, it turns out to be impossible to make a copy
of an unknown quantum state. This property, that qubits cannot be copied, is known
as the no-cloning theorem, and it is one of the chief differences between quantum and
classical information. The no-cloning theorem is discussed at more length in Box 12.1
on page 532; the proof is very simple, and we encourage you to skip ahead and read the
proof now.
There is another way of looking at the failure of the circuit in Figure 1.11, based on
the intuition that a qubit somehow contains ‘hidden’ information not directly accessible
to measurement. Consider what happens when we measure one of the qubits of the state
a|00⟩+ b|11⟩. As previously described, we obtain either 0 or 1 with probabilities |a|2
and |b|2. However, once one qubit is measured, the state of the other one is completely
determined, and no additional information can be gained about a and b. In this sense, the
extra hidden information carried in the original qubit |ψ⟩was lost in the ﬁrst measure-
ment, and cannot be regained. If, however, the qubit had been copied, then the state of
the other qubit should still contain some of that hidden information. Therefore, a copy
cannot have been created.
1.3.6
Example: Bell states
Let’s consider a slightly more complicated circuit, shown in Figure 1.12, which has a
Hadamard gate followed by a
, and transforms the four computational basis states
according to the table given. As an explicit example, the Hadamard gate takes the input
|00⟩to (|0⟩+ |1⟩)|0⟩/
√
2, and then the
gives the output state (|00⟩+ |11⟩)/
√
2.
Note how this works: ﬁrst, the Hadamard transform puts the top qubit in a superposition;
this then acts as a control input to the
, and the target gets inverted only when the
control is 1. The output states
|β00⟩= |00⟩+ |11⟩
√
2
;
(1.23)
|β01⟩= |01⟩+ |10⟩
√
2
;
(1.24)
|β10⟩= |00⟩−|11⟩
√
2
; and
(1.25)
|β11⟩= |01⟩−|10⟩
√
2
,
(1.26)
are known as the Bell states, or sometimes the EPR states or EPR pairs, after some of
the people – Bell, and Einstein, Podolsky, and Rosen – who ﬁrst pointed out the strange
properties of states like these. The mnemonic notation |β00⟩, |β01⟩, |β10⟩, |β11⟩may be
26
Introduction and overview
understood via the equations
|βxy⟩≡|0, y⟩+ (−1)x|1, ¯y⟩
√
2
,
(1.27)
where ¯y is the negation of y.
In
Out
|00⟩
(|00⟩+ |11⟩)/
√
2 ≡|β00⟩
|01⟩
(|01⟩+ |10⟩)/
√
2 ≡|β01⟩
|10⟩
(|00⟩−|11⟩)/
√
2 ≡|β10⟩
|11⟩
(|01⟩−|10⟩)/
√
2 ≡|β11⟩
Figure 1.12. Quantum circuit to create Bell states, and its input–ouput quantum ‘truth table’.
1.3.7
Example: quantum teleportation
We will now apply the techniques of the last few pages to understand something non-
trivial, surprising, and a lot of fun – quantum teleportation! Quantum teleportation is a
technique for moving quantum states around, even in the absence of a quantum commu-
nications channel linking the sender of the quantum state to the recipient.
Here’s how quantum teleportation works. Alice and Bob met long ago but now live
far apart. While together they generated an EPR pair, each taking one qubit of the EPR
pair when they separated. Many years later, Bob is in hiding, and Alice’s mission, should
she choose to accept it, is to deliver a qubit |ψ⟩to Bob. She does not know the state of
the qubit, and moreover can only send classical information to Bob. Should Alice accept
the mission?
Intuitively, things look pretty bad for Alice. She doesn’t know the state |ψ⟩of the
qubit she has to send to Bob, and the laws of quantum mechanics prevent her from
determining the state when she only has a single copy of |ψ⟩in her possession. What’s
worse, even if she did know the state |ψ⟩, describing it precisely takes an inﬁnite amount
of classical information since |ψ⟩takes values in a continuous space. So even if she did
know |ψ⟩, it would take forever for Alice to describe the state to Bob. It’s not looking
good for Alice. Fortunately for Alice, quantum teleportation is a way of utilizing the
entangled EPR pair in order to send |ψ⟩to Bob, with only a small overhead of classical
communication.
In outline, the steps of the solution are as follows: Alice interacts the qubit |ψ⟩with
her half of the EPR pair, and then measures the two qubits in her possession, obtaining
one of four possible classical results, 00, 01, 10, and 11. She sends this information to
Bob. Depending on Alice’s classical message, Bob performs one of four operations on his
half of the EPR pair. Amazingly, by doing this he can recover the original state |ψ⟩!
The quantum circuit shown in Figure 1.13 gives a more precise description of quantum
teleportation. The state to be teleported is |ψ⟩= α|0⟩+β|1⟩, where α and β are unknown
amplitudes. The state input into the circuit |ψ0⟩is
|ψ0⟩= |ψ⟩|β00⟩
(1.28)
Quantum computation
27

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Figure 1.13. Quantum circuit for teleporting a qubit. The two top lines represent Alice’s system, while the bottom
line is Bob’s system. The meters represent measurement, and the double lines coming out of them carry classical
bits (recall that single lines denote qubits).
=
1
√
2

α|0⟩(|00⟩+ |11⟩) + β|1⟩(|00⟩+ |11⟩)
	
,
(1.29)
where we use the convention that the ﬁrst two qubits (on the left) belong to Alice, and
the third qubit to Bob. As we explained previously, Alice’s second qubit and Bob’s qubit
start out in an EPR state. Alice sends her qubits through a
gate, obtaining
|ψ1⟩=
1
√
2

α|0⟩(|00⟩+ |11⟩) + β|1⟩(|10⟩+ |01⟩)
	
.
(1.30)
She then sends the ﬁrst qubit through a Hadamard gate, obtaining
|ψ2⟩= 1
2

α(|0⟩+ |1⟩)(|00⟩+ |11⟩) + β(|0⟩−|1⟩)(|10⟩+ |01⟩)
	
.
(1.31)
This state may be re-written in the following way, simply by regrouping terms:
|ψ2⟩= 1
2

|00⟩

α|0⟩+ β|1⟩
 + |01⟩

α|1⟩+ β|0⟩

+ |10⟩

α|0⟩−β|1⟩
 + |11⟩

α|1⟩−β|0⟩
	
.
(1.32)
This expression naturally breaks down into four terms. The ﬁrst term has Alice’s qubits
in the state |00⟩, and Bob’s qubit in the state α|0⟩+ β|1⟩– which is the original state
|ψ⟩. If Alice performs a measurement and obtains the result 00 then Bob’s system will
be in the state |ψ⟩. Similarly, from the previous expression we can read off Bob’s post-
measurement state, given the result of Alice’s measurement:
00 −→|ψ3(00)⟩≡

α|0⟩+ β|1⟩
	
(1.33)
01 −→|ψ3(01)⟩≡

α|1⟩+ β|0⟩
	
(1.34)
10 −→|ψ3(10)⟩≡

α|0⟩−β|1⟩
	
(1.35)
11 −→|ψ3(11)⟩≡

α|1⟩−β|0⟩
	
.
(1.36)
Depending on Alice’s measurement outcome, Bob’s qubit will end up in one of these
four possible states. Of course, to know which state it is in, Bob must be told the result of
Alice’s measurement – we will show later that it is this fact which prevents teleportation
28
Introduction and overview
from being used to transmit information faster than light. Once Bob has learned the mea-
surement outcome, Bob can ‘ﬁx up’ his state, recovering |ψ⟩, by applying the appropriate
quantum gate. For example, in the case where the measurement yields 00, Bob doesn’t
need to do anything. If the measurement is 01 then Bob can ﬁx up his state by applying
the X gate. If the measurement is 10 then Bob can ﬁx up his state by applying the Z
gate. If the measurement is 11 then Bob can ﬁx up his state by applying ﬁrst an X and
then a Z gate. Summing up, Bob needs to apply the transformation ZM1XM2 (note how
time goes from left to right in circuit diagrams, but in matrix products terms on the right
happen ﬁrst) to his qubit, and he will recover the state |ψ⟩.
There are many interesting features of teleportation, some of which we shall return
to later in the book. For now we content ourselves with commenting on a couple of
aspects. First, doesn’t teleportation allow one to transmit quantum states faster than
light? This would be rather peculiar, because the theory of relativity implies that faster
than light information transfer could be used to send information backwards in time.
Fortunately, quantum teleportation does not enable faster than light communication,
because to complete the teleportation Alice must transmit her measurement result to
Bob over a classical communications channel. We will show in Section 2.4.3 that without
this classical communication, teleportation does not convey any information at all. The
classical channel is limited by the speed of light, so it follows that quantum teleportation
cannot be accomplished faster than the speed of light, resolving the apparent paradox.
A second puzzle about teleportation is that it appears to create a copy of the quan-
tum state being teleported, in apparent violation of the no-cloning theorem discussed in
Section 1.3.5. This violation is only illusory since after the teleportation process only the
target qubit is left in the state |ψ⟩, and the original data qubit ends up in one of the
computational basis states |0⟩or |1⟩, depending upon the measurement result on the ﬁrst
qubit.
What can we learn from quantum teleportation? Quite a lot! It’s much more than
just a neat trick one can do with quantum states. Quantum teleportation emphasizes the
interchangeability of different resources in quantum mechanics, showing that one shared
EPR pair together with two classical bits of communication is a resource at least the
equal of one qubit of communication. Quantum computation and quantum information
has revealed a plethora of methods for interchanging resources, many built upon quantum
teleportation. In particular, in Chapter 10 we explain how teleportation can be used to
build quantum gates which are resistant to the effects of noise, and in Chapter 12 we show
that teleportation is intimately connected with the properties of quantum error-correcting
codes. Despite these connections with other subjects, it is fair to say that we are only
beginning to understand why it is that quantum teleportation is possible in quantum
mechanics; in later chapters we endeavor to explain some of the insights that make such
an understanding possible.
1.4
Quantum algorithms
What class of computations can be performed using quantum circuits? How does that class
compare with the computations which can be performed using classical logical circuits?
Can we ﬁnd a task which a quantum computer may perform better than a classical
computer? In this section we investigate these questions, explaining how to perform
classical computations on quantum computers, giving some examples of problems for
Quantum algorithms
29
which quantum computers offer an advantage over classical computers, and summarizing
the known quantum algorithms.
1.4.1
Classical computations on a quantum computer
Can we simulate a classical logic circuit using a quantum circuit? Not surprisingly, the
answer to this question turns out to be yes. It would be very surprising if this were not
the case, as physicists believe that all aspects of the world around us, including classical
logic circuits, can ultimately be explained using quantum mechanics. As pointed out
earlier, the reason quantum circuits cannot be used to directly simulate classical circuits
is because unitary quantum logic gates are inherently reversible, whereas many classical
logic gates such as the
gate are inherently irreversible.
Any classical circuit can be replaced by an equivalent circuit containing only reversible
elements, by making use of a reversible gate known as the Toffoli gate. The Toffoli gate
has three input bits and three output bits, as illustrated in Figure 1.14. Two of the bits are
control bits that are unaffected by the action of the Toffoli gate. The third bit is a target
bit that is ﬂipped if both control bits are set to 1, and otherwise is left alone. Note that
applying the Toffoli gate twice to a set of bits has the effect (a, b, c) →(a, b, c ⊕ab) →
(a, b, c), and thus the Toffoli gate is a reversible gate, since it has an inverse – itself.
Inputs
Outputs
a
b
c
a′
b′
c′
0
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
1
1
1
0
0
1
0
0
1
0
1
1
0
1
1
1
0
1
1
1
1
1
1
1
1
0
Figure 1.14. Truth table for the Toffoli gate, and its circuit representation.
The Toffoli gate can be used to simulate
gates, as shown in Figure 1.15, and
can also be used to do
, as shown in Figure 1.16. With these two operations it
becomes possible to simulate all other elements in a classical circuit, and thus an arbitrary
classical circuit can be simulated by an equivalent reversible circuit.
The Toffoli gate has been described as a classical gate, but it can also be implemented
as a quantum logic gate. By deﬁnition, the quantum logic implementation of the Toffoli
gate simply permutes computational basis states in the same way as the classical Toffoli
gate. For example, the quantum Toffoli gate acting on the state |110⟩ﬂips the third qubit
because the ﬁrst two are set, resulting in the state |111⟩. It is tedious but not difﬁcult
to write this transformation out as an 8 by 8 matrix, U, and verify explicitly that U is
a unitary matrix, and thus the Toffoli gate is a legitimate quantum gate. The quantum
Toffoli gate can be used to simulate irreversible classical logic gates, just as the classical
30
Introduction and overview
Figure 1.15. Classical circuit implementing a
gate using a Toffoli gate. The top two bits represent the input
to the
, while the third bit is prepared in the standard state 1, sometimes known as an ancilla state. The
output from the
is on the third bit.
Figure 1.16.
with the Toffoli gate, with the second bit being the input to the
(and the other two
bits standard ancilla states), and the output from
appearing on the second and third bits.
Toffoli gate was, and ensures that quantum computers are capable of performing any
computation which a classical (deterministic) computer may do.
What if the classical computer is non-deterministic, that is, has the ability to generate
random bits to be used in the computation? Not surprisingly, it is easy for a quantum
computer to simulate this. To perform such a simulation it turns out to be sufﬁcient to
produce random fair coin tosses, which can be done by preparing a qubit in the state
|0⟩, sending it through a Hadamard gate to produce (|0⟩+ |1⟩)/
√
2, and then measuring
the state. The result will be |0⟩or |1⟩with 50/50 probability. This provides a quantum
computer with the ability to efﬁciently simulate a non-deterministic classical computer.
Of course, if the ability to simulate classical computers were the only feature of quan-
tum computers there would be little point in going to all the trouble of exploiting quantum
effects! The advantage of quantum computing is that much more powerful functions may
be computed using qubits and quantum gates. In the next few sections we explain how
to do this, culminating in the Deutsch–Jozsa algorithm, our ﬁrst example of a quantum
algorithm able to solve a problem faster than any classical algorithm.
1.4.2
Quantum parallelism
Quantum parallelism is a fundamental feature of many quantum algorithms. Heuristi-
cally, and at the risk of over-simplifying, quantum parallelism allows quantum computers
to evaluate a function f(x) for many different values of x simultaneously. In this section
we explain how quantum parallelism works, and some of its limitations.
Suppose f(x) : {0, 1} →{0, 1} is a function with a one-bit domain and range. A
Quantum algorithms
31
convenient way of computing this function on a quantum computer is to consider a two
qubit quantum computer which starts in the state |x, y⟩. With an appropriate sequence
of logic gates it is possible to transform this state into |x, y ⊕f(x)⟩, where ⊕indicates
addition modulo 2; the ﬁrst register is called the ‘data’ register, and the second register the
‘target’ register. We give the transformation deﬁned by the map |x, y⟩→|x, y ⊕f(x)⟩a
name, Uf, and note that it is easily shown to be unitary. If y = 0, then the ﬁnal state of the
second qubit is just the value f(x). (In Section 3.2.5 we show that given a classical circuit
for computing f there is a quantum circuit of comparable efﬁciency which computes the
transformation Uf on a quantum computer. For our purposes it can be considered to be
a black box.)
Figure 1.17. Quantum circuit for evaluating f(0) and f(1) simultaneously. Uf is the quantum circuit which takes
inputs like |x, y⟩to |x, y ⊕f(x)⟩.
Consider the circuit shown in Figure 1.17, which applies Uf to an input not in the
computational basis. Instead, the data register is prepared in the superposition (|0⟩+
|1⟩)/
√
2, which can be created with a Hadamard gate acting on |0⟩. Then we apply Uf,
resulting in the state:
|0, f(0)⟩+ |1, f(1)⟩
√
2
.
(1.37)
This is a remarkable state! The different terms contain information about both f(0) and
f(1); it is almost as if we have evaluated f(x) for two values of x simultaneously, a feature
known as ‘quantum parallelism’. Unlike classical parallelism, where multiple circuits each
built to compute f(x) are executed simultaneously, here a single f(x) circuit is employed
to evaluate the function for multiple values of x simultaneously, by exploiting the ability
of a quantum computer to be in superpositions of different states.
This procedure can easily be generalized to functions on an arbitrary number of bits, by
using a general operation known as the Hadamard transform, or sometimes the Walsh–
Hadamard transform. This operation is just n Hadamard gates acting in parallel on n
qubits. For example, shown in Figure 1.18 is the case n = 2 with qubits initially prepared
as |0⟩, which gives
|0⟩+ |1⟩
√
2
 |0⟩+ |1⟩
√
2

= |00⟩+ |01⟩+ |10⟩+ |11⟩
2
(1.38)
as output. We write H⊗2 to denote the parallel action of two Hadamard gates, and read
‘⊗’ as ‘tensor’. More generally, the result of performing the Hadamard transform on n
32
Introduction and overview
qubits initially in the all |0⟩state is
1
√
2n

x
|x⟩,
(1.39)
where the sum is over all possible values of x, and we write H⊗n to denote this action.
That is, the Hadamard transform produces an equal superposition of all computational
basis states. Moreover, it does this extremely efﬁciently, producing a superposition of 2n
states using just n gates.
Figure 1.18. The Hadamard transform H⊗2 on two qubits.
Quantum parallel evaluation of a function with an n bit input x and 1 bit output, f(x),
can thus be performed in the following manner. Prepare the n + 1 qubit state |0⟩⊗n|0⟩,
then apply the Hadamard transform to the ﬁrst n qubits, followed by the quantum circuit
implementing Uf. This produces the state
1
√
2n

x
|x⟩|f(x)⟩.
(1.40)
In some sense, quantum parallelism enables all possible values of the function f to be
evaluated simultaneously, even though we apparently only evaluated f once. However,
this parallelism is not immediately useful. In our single qubit example, measurement of the
state gives only either |0, f(0)⟩or |1, f(1)⟩! Similarly, in the general case, measurement of
the state 
x |x, f(x)⟩would give only f(x) for a single value of x. Of course, a classical
computer can do this easily! Quantum computation requires something more than just
quantum parallelism to be useful; it requires the ability to extract information about more
than one value of f(x) from superposition states like 
x |x, f(x)⟩. Over the next two
sections we investigate examples of how this may be done.
1.4.3
Deutsch’s algorithm
A simple modiﬁcation of the circuit in Figure 1.17 demonstrates how quantum circuits
can outperform classical ones by implementing Deutsch’s algorithm (we actually present
a simpliﬁed and improved version of the original algorithm; see ‘History and further
reading’ at the end of the chapter). Deutsch’s algorithm combines quantum parallelism
with a property of quantum mechanics known as interference. As before, let us use the
Hadamard gate to prepare the ﬁrst qubit as the superposition (|0⟩+ |1⟩)/
√
2, but now
let us prepare the second qubit y as the superposition (|0⟩−|1⟩)/
√
2, using a Hadamard
gate applied to the state |1⟩. Let us follow the states along to see what happens in this
circuit, shown in Figure 1.19.
The input state
|ψ0⟩= |01⟩
(1.41)
Quantum algorithms
33
Figure 1.19. Quantum circuit implementing Deutsch’s algorithm.
is sent through two Hadamard gates to give
|ψ1⟩=
|0⟩+ |1⟩
√
2
 |0⟩−|1⟩
√
2

.
(1.42)
A little thought shows that if we apply Uf to the state |x⟩(|0⟩−|1⟩)/
√
2 then we obtain
the state (−1)f(x)|x⟩(|0⟩−|1⟩)/
√
2. Applying Uf to |ψ1⟩therefore leaves us with one of
two possibilities:
|ψ2⟩=
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
±
|0⟩+ |1⟩
√
2
 |0⟩−|1⟩
√
2

if f(0) = f(1)
±
|0⟩−|1⟩
√
2
 |0⟩−|1⟩
√
2

if f(0) ̸= f(1).
(1.43)
The ﬁnal Hadamard gate on the ﬁrst qubit thus gives us
|ψ3⟩=
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
±|0⟩
|0⟩−|1⟩
√
2

if f(0) = f(1)
±|1⟩
|0⟩−|1⟩
√
2

if f(0) ̸= f(1).
(1.44)
Realizing that f(0) ⊕f(1) is 0 if f(0) = f(1) and 1 otherwise, we can rewrite this result
concisely as
|ψ3⟩= ±|f(0) ⊕f(1)⟩
|0⟩−|1⟩
√
2

,
(1.45)
so by measuring the ﬁrst qubit we may determine f(0) ⊕f(1). This is very interesting
indeed: the quantum circuit has given us the ability to determine a global property of
f(x), namely f(0)⊕f(1), using only one evaluation of f(x)! This is faster than is possible
with a classical apparatus, which would require at least two evaluations.
This example highlights the difference between quantum parallelism and classical
randomized algorithms. Naively, one might think that the state |0⟩|f(0)⟩+ |1⟩|f(1)⟩
corresponds rather closely to a probabilistic classical computer that evaluates f(0) with
probability one-half, or f(1) with probability one-half. The difference is that in a classical
computer these two alternatives forever exclude one another; in a quantum computer it is
34
Introduction and overview
possible for the two alternatives to interfere with one another to yield some global property
of the function f, by using something like the Hadamard gate to recombine the different
alternatives, as was done in Deutsch’s algorithm. The essence of the design of many
quantum algorithms is that a clever choice of function and ﬁnal transformation allows
efﬁcient determination of useful global information about the function – information
which cannot be attained quickly on a classical computer.
1.4.4
The Deutsch–Jozsa algorithm
Deutsch’s algorithm is a simple case of a more general quantum algorithm, which we shall
refer to as the Deutsch–Jozsa algorithm. The application, known as Deutsch’s problem,
may be described as the following game. Alice, in Amsterdam, selects a number x from
0 to 2n −1, and mails it in a letter to Bob, in Boston. Bob calculates some function
f(x) and replies with the result, which is either 0 or 1. Now, Bob has promised to use
a function f which is of one of two kinds; either f(x) is constant for all values of x,
or else f(x) is balanced, that is, equal to 1 for exactly half of all the possible x, and 0
for the other half. Alice’s goal is to determine with certainty whether Bob has chosen a
constant or a balanced function, corresponding with him as little as possible. How fast
can she succeed?
In the classical case, Alice may only send Bob one value of x in each letter. At worst,
Alice will need to query Bob at least 2n/2+1 times, since she may receive 2n/2 0s before
ﬁnally getting a 1, telling her that Bob’s function is balanced. The best deterministic
classical algorithm she can use therefore requires 2n/2 + 1 queries. Note that in each
letter, Alice sends Bob n bits of information. Furthermore, in this example, physical
distance is being used to artiﬁcially elevate the cost of calculating f(x), but this is not
needed in the general problem, where f(x) may be inherently difﬁcult to calculate.
If Bob and Alice were able to exchange qubits, instead of just classical bits, and if Bob
agreed to calculate f(x) using a unitary transform Uf, then Alice could achieve her goal
in just one correspondence with Bob, using the following algorithm.
Analogously to Deutsch’s algorithm, Alice has an n qubit register to store her query
in, and a single qubit register which she will give to Bob, to store the answer in. She
begins by preparing both her query and answer registers in a superposition state. Bob
will evaluate f(x) using quantum parallelism and leave the result in the answer register.
Alice then interferes states in the superposition using a Hadamard transform on the query
register, and ﬁnishes by performing a suitable measurement to determine whether f was
constant or balanced.
The speciﬁc steps of the algorithm are depicted in Figure 1.20. Let us follow the states
through this circuit. The input state
|ψ0⟩= |0⟩⊗n|1⟩
(1.46)
is similar to that of Equation (1.41), but here the query register describes the state of n
qubits all prepared in the |0⟩state. After the Hadamard transform on the query register
and the Hadamard gate on the answer register we have
|ψ1⟩=

x∈{0,1}n
|x⟩
√
2n
|0⟩−|1⟩
√
2

.
(1.47)
The query register is now a superposition of all values, and the answer register is in an
Quantum algorithms
35
Figure 1.20. Quantum circuit implementing the general Deutsch–Jozsa algorithm. The wire with a ‘/’ through it
represents a set of n qubits, similar to the common engineering notation.
evenly weighted superposition of 0 and 1. Next, the function f is evaluated (by Bob)
using Uf : |x, y⟩→|x, y ⊕f(x)⟩, giving
|ψ2⟩=

x
(−1)f(x)|x⟩
√
2n
|0⟩−|1⟩
√
2

.
(1.48)
Alice now has a set of qubits in which the result of Bob’s function evaluation is stored
in the amplitude of the qubit superposition state. She now interferes terms in the super-
position using a Hadamard transform on the query register. To determine the result of
the Hadamard transform it helps to ﬁrst calculate the effect of the Hadamard transform
on a state |x⟩. By checking the cases x = 0 and x = 1 separately we see that for a single
qubit H|x⟩= 
z(−1)xz|z⟩/
√
2. Thus
H⊗n|x1, . . . , xn⟩=

z1,...,zn(−1)x1z1+·· +xnzn|z1, . . . , zn⟩
√
2n
.
(1.49)
This can be summarized more succinctly in the very useful equation
H⊗n|x⟩=

z(−1)x·z|z⟩
√
2n
,
(1.50)
where x · z is the bitwise inner product of x and z, modulo 2. Using this equation
and (1.48) we can now evaluate |ψ3⟩,
|ψ3⟩=

z

x
(−1)x·z+f(x)|z⟩
2n
|0⟩−|1⟩
√
2

.
(1.51)
Alice now observes the query register. Note that the amplitude for the state |0⟩⊗n is

x(−1)f(x)/2n. Let’s look at the two possible cases – f constant and f balanced – to
discern what happens. In the case where f is constant the amplitude for |0⟩⊗n is +1 or
−1, depending on the constant value f(x) takes. Because |ψ3⟩is of unit length it follows
that all the other amplitudes must be zero, and an observation will yield 0s for all qubits
in the query register. If f is balanced then the positive and negative contributions to the
amplitude for |0⟩⊗n cancel, leaving an amplitude of zero, and a measurement must yield
a result other than 0 on at least one qubit in the query register. Summarizing, if Alice
36
Introduction and overview
measures all 0s then the function is constant; otherwise the function is balanced. The
Deutsch–Jozsa algorithm is summarized below.
Algorithm:
Deutsch–Jozsa
Inputs: (1) A black box Uf which performs the transformation
|x⟩|y⟩→|x⟩|y ⊕f(x)⟩, for x ∈{0, . . ., 2n −1} and f(x) ∈{0, 1}. It is
promised that f(x) is either constant for all values of x, or else f(x) is balanced,
that is, equal to 1 for exactly half of all the possible x, and 0 for the other half.
Outputs: 0 if and only if f is constant.
Runtime: One evaluation of Uf. Always succeeds.
Procedure:
1.
|0⟩⊗n|1⟩
initialize state
2.
→
1
√
2n
2n−1

x=0
|x⟩
|0⟩−|1⟩
√
2

create superposition using
Hadamard gates
3.
→

x
(−1)f(x)|x⟩
|0⟩−|1⟩
√
2

calculate function f using Uf
4.
→

z

x
(−1)x·z+f(x)|z⟩
√
2n
|0⟩−|1⟩
√
2

perform Hadamard transform
5.
→z
measure to obtain ﬁnal output z
We’ve shown that a quantum computer can solve Deutsch’s problem with one evalu-
ation of the function f compared to the classical requirement for 2n/2 + 1 evaluations.
This appears impressive, but there are several important caveats. First, Deutsch’s prob-
lem is not an especially important problem; it has no known applications. Second, the
comparison between classical and quantum algorithms is in some ways an apples and
oranges comparison, as the method for evaluating the function is quite different in the
two cases. Third, if Alice is allowed to use a probabilistic classical computer, then by
asking Bob to evaluate f(x) for a few randomly chosen x she can very quickly determine
with high probability whether f is constant or balanced. This probabilistic scenario is
perhaps more realistic than the deterministic scenario we have been considering. Despite
these caveats, the Deutsch–Jozsa algorithm contains the seeds for more impressive quan-
tum algorithms, and it is enlightening to attempt to understand the principles behind its
operation.
Exercise 1.1: (Probabilistic classical algorithm)
Suppose that the problem is not
to distinguish between the constant and balanced functions with certainty, but
rather, with some probability of error ϵ < 1/2. What is the performance of the
best classical algorithm for this problem?
1.4.5
Quantum algorithms summarized
The Deutsch–Jozsa algorithm suggests that quantum computers may be capable of solving
some computational problems much more efﬁciently than classical computers. Unfortu-
nately, the problem it solves is of little practical interest. Are there more interesting
Quantum algorithms
37
problems whose solution may be obtained more efﬁciently using quantum algorithms?
What are the principles underlying such algorithms? What are the ultimate limits of a
quantum computer’s computational power?
Broadly speaking, there are three classes of quantum algorithms which provide an
advantage over known classical algorithms. First, there is the class of algorithms based
upon quantum versions of the Fourier transform, a tool which is also widely used in
classical algorithms. The Deutsch–Jozsa algorithm is an example of this type of algo-
rithm, as are Shor’s algorithms for factoring and discrete logarithm. The second class
of algorithms is quantum search algorithms. The third class of algorithms is quantum
simulation, whereby a quantum computer is used to simulate a quantum system. We now
brieﬂy describe each of these classes of algorithms, and then summarize what is known
or suspected about the computational power of quantum computers.
Quantum algorithms based upon the Fourier transform
The discrete Fourier transform is usually described as transforming a set x0, . . . , xN−1
of N complex numbers into a set of complex numbers y0, . . . , yN−1 deﬁned by
yk ≡
1
√
N
N−1

j=0
e2πijk/Nxj .
(1.52)
Of course, this transformation has an enormous number of applications in many branches
of science; the Fourier transformed version of a problem is often easier than the original
problem, enabling a solution.
The Fourier transform has proved so useful that a beautiful generalized theory of
Fourier transforms has been developed which goes beyond the deﬁnition (1.52). This
general theory involves some technical ideas from the character theory of ﬁnite groups,
and we will not attempt to describe it here. What is important is that the Hadamard
transform used in the Deutsch–Jozsa algorithm is an example of this generalized class
of Fourier transforms. Moreover, many of the other important quantum algorithms also
involve some type of Fourier transform.
The most important quantum algorithms known, Shor’s fast algorithms for factoring
and discrete logarithm, are two examples of algorithms based upon the Fourier trans-
form deﬁned in Equation (1.52). The Equation (1.52) does not appear terribly quantum
mechanical in the form we have written it. Imagine, however, that we deﬁne a linear
transformation U on n qubits by its action on computational basis states |j⟩, where
0 ≤j ≤2n −1,
|j⟩−→
1
√
2n
2n−1

k=0
e2πijk/2n|k⟩.
(1.53)
It can be checked that this transformation is unitary, and in fact can be realized as a
quantum circuit. Moreover, if we write out its action on superpositions,
2n−1

j=0
xj|j⟩−→
1
√
2n
2n−1

k=0
⎡
⎣
2n−1

j=0
e2πijk/2nxj
⎤
⎦|k⟩=
2n−1

k=0
yk|k⟩,
(1.54)
we see that it corresponds to a vector notation for the Fourier transform (1.52) for the
case N = 2n.
38
Introduction and overview
How quickly can we perform the Fourier transform? Classically, the fast Fourier trans-
form takes roughly N log(N) = n2n steps to Fourier transform N = 2n numbers. On a
quantum computer, the Fourier transform can be accomplished using about log2(N) = n2
steps, an exponential saving! The quantum circuit to do this is explained in Chapter 5.
This result seems to indicate that quantum computers can be used to very quickly
compute the Fourier transform of a vector of 2n complex numbers, which would be
fantastically useful in a wide range of applications. However, that is not exactly the case;
the Fourier transform is being performed on the information ‘hidden’ in the amplitudes
of the quantum state. This information is not directly accessible to measurement. The
catch, of course, is that if the output state is measured, it will collapse each qubit into
the state |0⟩or |1⟩, preventing us from learning the transform result yk directly. This
example speaks to the heart of the conundrum of devising a quantum algorithm. On the
one hand, we can perform certain calculations on the 2n amplitudes associated with n
qubits far more efﬁciently than would be possible on a classical computer. But on the
other hand, the results of such a calculation are not available to us if we go about it in
a straightforward manner. More cleverness is required in order to harness the power of
quantum computation.
Fortunately, it does turn out to be possible to utilize the quantum Fourier transform
to efﬁciently solve several problems that are believed to have no efﬁcient solution on a
classical computer. These problems include Deutsch’s problem, and Shor’s algorithms for
discrete logarithm and factoring. This line of thought culminated in Kitaev’s discovery
of a method to solve the Abelian stabilizer problem, and the generalization to the hidden
subgroup problem,
Let f be a function from a ﬁnitely generated group G to a ﬁnite set X such that
f is constant on the cosets of a subgroup K, and distinct on each coset. Given a
quantum black box for performing the unitary transform U|g⟩|h⟩= |g⟩|h⊕f(g)⟩,
for g ∈G, h ∈X, and ⊕an appropriately chosen binary operation on X, ﬁnd a
generating set for K.
The Deutsch–Jozsa algorithm, Shor’s algorithms, and related ‘exponentially fast’ quan-
tum algorithms can all be viewed as special cases of this algorithm. The quantum Fourier
transform and its applications are described in Chapter 5.
Quantum search algorithms
A completely different class of algorithms is represented by the quantum search algorithm,
whose basic principles were discovered by Grover. The quantum search algorithm solves
the following problem: Given a search space of size N, and no prior knowledge about the
structure of the information in it, we want to ﬁnd an element of that search space satisfying
a known property. How long does it take to ﬁnd an element satisfying that property?
Classically, this problem requires approximately N operations, but the quantum search
algorithm allows it to be solved using approximately
√
N operations.
The quantum search algorithm offers only a quadratic speedup, as opposed to the more
impressive exponential speedup offered by algorithms based on the quantum Fourier
transform. However, the quantum search algorithm is still of great interest, since search-
ing heuristics have a wider range of application than the problems solved using the quan-
tum Fourier transform, and adaptations of the quantum search algorithm may have utility
Quantum algorithms
39
for a very wide range of problems. The quantum search algorithm and its applications
are described in Chapter 6.
Quantum simulation
Simulating naturally occurring quantum mechanical systems is an obvious candidate for
a task at which quantum computers may excel, yet which is believed to be difﬁcult
on a classical computer. Classical computers have difﬁculty simulating general quantum
systems for much the same reasons they have difﬁculty simulating quantum computers –
the number of complex numbers needed to describe a quantum system generally grows
exponentially with the size of the system, rather than linearly, as occurs in classical
systems. In general, storing the quantum state of a system with n distinct components
takes something like cn bits of memory on a classical computer, where c is a constant
which depends upon details of the system being simulated, and the desired accuracy of
the simulation.
By contrast, a quantum computer can perform the simulation using kn qubits, where
k is again a constant which depends upon the details of the system being simulated. This
allows quantum computers to efﬁciently perform simulations of quantum mechanical
systems that are believed not to be efﬁciently simulatable on a classical computer. A
signiﬁcant caveat is that even though a quantum computer can simulate many quantum
systems far more efﬁciently than a classical computer, this does not mean that the fast
simulation will allow the desired information about the quantum system to be obtained.
When measured, a kn qubit simulation will collapse into a deﬁnite state, giving only kn
bits of information; the cn bits of ‘hidden information’ in the wavefunction is not entirely
accessible. Thus, a crucial step in making quantum simulations useful is development of
systematic means by which desired answers can be efﬁciently extracted; how to do this
is only partially understood.
Despite this caveat, quantum simulation is likely to be an important application of
quantum computers. The simulation of quantum systems is an important problem in
many ﬁelds, notably quantum chemistry, where the computational constraints imposed
by classical computers make it difﬁcult to accurately simulate the behavior of even mod-
erately sized molecules, much less the very large molecules that occur in many important
biological systems. Obtaining faster and more accurate simulations of such systems may
therefore have the welcome effect of enabling advances in other ﬁelds in which quantum
phenomena are important.
In the future we may discover a physical phenomenon in Nature which cannot be
efﬁciently simulated on a quantum computer. Far from being bad news, this would be
wonderful! At the least, it will stimulate us to extend our models of computation to
encompass the new phenomenon, and increase the power of our computational models
beyond the existing quantum computing model. It also seems likely that very interesting
new physical effects will be associated with any such phenomenon!
Another application for quantum simulation is as a general method to obtain insight
into other quantum algorithms; for example, in Section 6.2 we explain how the quantum
search algorithm can be viewed as the solution to a problem of quantum simulation. By
approaching the problem in this fashion it becomes much easier to understand the origin
of the quantum search algorithm.
Finally, quantum simulation also gives rise to an interesting and optimistic ‘quantum
corollary’ to Moore’s law. Recall that Moore’s law states that the power of classical
40
Introduction and overview
computers will double once every two years or so, for constant cost. However, suppose
we are simulating a quantum system on a classical computer, and want to add a single
qubit (or a larger system) to the system being simulated. This doubles or more the
memory requirements needed for a classical computer to store a description of the state
of the quantum system, with a similar or greater cost in the time needed to simulate the
dynamics. The quantum corollary to Moore’s law follows from this observation, stating
that quantum computers are keeping pace with classical computers provided a single
qubit is added to the quantum computer every two years. This corollary should not be
taken too seriously, as the exact nature of the gain, if any, of quantum computation over
classical is not yet clear. Nevertheless, this heuristic statement helps convey why we
should be interested in quantum computers, and hopeful that they will one day be able
to outperform the most powerful classical computers, at least for some applications.
The power of quantum computation
How powerful are quantum computers? What gives them their power? Nobody yet knows
the answers to these questions, despite the suspicions fostered by examples such as fac-
toring, which strongly suggest that quantum computers are more powerful than classical
computers. It is still possible that quantum computers are no more powerful than classical
computers, in the sense that any problem which can be efﬁciently solved on a quantum
computer can also be efﬁciently solved on a classical computer. On the other hand, it
may eventually be proved that quantum computers are much more powerful than classi-
cal computers. We now take a brief look at what is known about the power of quantum
computation.
Computational complexity theory is the subject of classifying the difﬁculty of vari-
ous computational problems, both classical and quantum, and to understand the power of
quantum computers we will ﬁrst examine some general ideas from computational com-
plexity. The most basic idea is that of a complexity class. A complexity class can be
thought of as a collection of computational problems, all of which share some common
feature with respect to the computational resources needed to solve those problems.
Two of the most important complexity classes go by the names P and NP. Roughly
speaking, P is the class of computational problems that can be solved quickly on a classical
computer. NP is the class of problems which have solutions which can be quickly checked
on a classical computer. To understand the distinction between P and NP, consider the
problem of ﬁnding the prime factors of an integer, n. So far as is known there is no fast
way of solving this problem on a classical computer, which suggests that the problem is
not in P. On the other hand, if somebody tells you that some number p is a factor of
n, then we can quickly check that this is correct by dividing p into n, so factoring is a
problem in NP.
It is clear that P is a subset of NP, since the ability to solve a problem implies the ability
to check potential solutions. What is not so clear is whether or not there are problems
in NP that are not in P. Perhaps the most important unsolved problem in theoretical
computer science is to determine whether these two classes are different:
P
?
̸= NP .
(1.55)
Most researchers believe that NP contains problems that are not in P. In particular,
there is an important subclass of the NP problems, the NP-complete problems, that are
Quantum algorithms
41
of especial importance for two reasons. First, there are thousands of problems, many
highly important, that are known to be NP-complete. Second, any given NP-complete
problem is in some sense ‘at least as hard’ as all other problems in NP. More precisely,
an algorithm to solve a speciﬁc NP-complete problem can be adapted to solve any other
problem in NP, with a small overhead. In particular, if P ̸= NP, then it will follow that
no NP-complete problem can be efﬁciently solved on a classical computer.
It is not known whether quantum computers can be used to quickly solve all the
problems in NP, despite the fact that they can be used to solve some problems – like
factoring – which are believed by many people to be in NP but not in P. (Note that
factoring is not known to be NP-complete, otherwise we would already know how to
efﬁciently solve all problems in NP using quantum computers.) It would certainly be
very exciting if it were possible to solve all the problems in NP efﬁciently on a quantum
computer. There is a very interesting negative result known in this direction which
rules out using a simple variant of quantum parallelism to solve all the problems in
NP. Speciﬁcally, one approach to the problem of solving problems in NP on a quantum
computer is to try to use some form of quantum parallelism to search in parallel through
all the possible solutions to the problem. In Section 6.6 we will show that no approach
based upon such a search-based methodology can yield an efﬁcient solution to all the
problems in NP. While it is disappointing that this approach fails, it does not rule out
that some deeper structure exists in the problems in NP that will allow them all to be
solved quickly using a quantum computer.
P and NP are just two of a plethora of complexity classes that have been deﬁned.
Another important complexity class is PSPACE. Roughly speaking, PSPACE consists
of those problems which can be solved using resources which are few in spatial size (that
is, the computer is ‘small’), but not necessarily in time (‘long’ computations are ﬁne).
PSPACE is believed to be strictly larger than both P and NP although, again, this has
never been proved. Finally, the complexity class BPP is the class of problems that can be
solved using randomized algorithms in polynomial time, if a bounded probability of error
(say 1/4) is allowed in the solution to the problem. BPP is widely regarded as being, even
more so than P, the class of problems which should be considered efﬁciently soluble on
a classical computer. We have elected to concentrate here on P rather than BPP because
P has been studied in more depth, however many similar ideas and conclusions arise in
connection with BPP.
What of quantum complexity classes? We can deﬁne BQP to be the class of all com-
putational problems which can be solved efﬁciently on a quantum computer, where a
bounded probability of error is allowed. (Strictly speaking this makes BQP more analo-
gous to the classical complexity class BPP than to P, however we will ignore this subtlety
for the purposes of the present discussion, and treat it as the analogue of P.) Exactly
where BQP ﬁts with respect to P, NP and PSPACE is as yet unknown. What is known
is that quantum computers can solve all the problems in P efﬁciently, but that there
are no problems outside of PSPACE which they can solve efﬁciently. Therefore, BQP
lies somewhere between P and PSPACE, as illustrated in Figure 1.21. An important
implication is that if it is proved that quantum computers are strictly more powerful than
classical computers, then it will follow that P is not equal to PSPACE. Proving this latter
result has been attempted without success by many computer scientists, suggesting that
it may be non-trivial to prove that quantum computers are more powerful than classical
computers, despite much evidence in favor of this proposition.
42
Introduction and overview







Figure 1.21. The relationship between classical and quantum complexity classes. Quantum computers can quickly
solve any problem in P, and it is known that they can’t solve problems outside of PSPACE quickly. Where
quantum computers ﬁt between P and PSPACE is not known, in part because we don’t even know whether
PSPACE is bigger than P!
We won’t speculate further on the ultimate power of quantum computation now,
preferring to wait until after we have better understood the principles on which fast
quantum algorithms are based, a topic which occupies us for most of Part II of this
book. What is already clear is that the theory of quantum computation poses interesting
and signiﬁcant challenges to the traditional notions of computation. What makes this an
important challenge is that the theoretical model of quantum computation is believed
to be experimentally realizable, because – to the best of our knowledge – this theory is
consistent with the way Nature works. If this were not so then quantum computation
would be just another mathematical curiosity.
1.5
Experimental quantum information processing
Quantum computation and quantum information is a wonderful theoretical discovery,
but its central concepts, such as superpositions and entanglement, run counter to the
intuition we garner from the everyday world around us. What evidence do we have that
these ideas truly describe how Nature operates? Will the realization of large-scale quantum
Experimental quantum information processing
43
computers be experimentally feasible? Or might there be some principle of physics which
fundamentally prohibits their eventual scaling? In the next two sections we address these
questions. We begin with a review of the famous ‘Stern–Gerlach’ experiment, which
provides evidence for the existence of qubits in Nature. We then widen our scope,
addressing the broader problem of how to build practical quantum information processing
systems.
1.5.1
The Stern–Gerlach experiment
The qubit is a fundamental element for quantum computation and quantum information.
How do we know that systems with the properties of qubits exist in Nature? At the time
of writing there is an enormous amount of evidence that this is so, but in the early days
of quantum mechanics the qubit structure was not at all obvious, and people struggled
with phenomena that we may now understand in terms of qubits, that is, in terms of two
level quantum systems.
A decisive (and very famous) early experiment indicating the qubit structure was
conceived by Stern in 1921 and performed with Gerlach in 1922 in Frankfurt. In the
original Stern–Gerlach experiment, hot atoms were ‘beamed’ from an oven through a
magnetic ﬁeld which caused the atoms to be deﬂected, and then the position of each atom
was recorded, as illustrated in Figure 1.22. The original experiment was done with silver
atoms, which have a complicated structure that obscures the effects we are discussing.
What we describe below actually follows a 1927 experiment done using hydrogen atoms.
The same basic effect is observed, but with hydrogen atoms the discussion is easier
to follow. Keep in mind, though, that this privilege wasn’t available to people in the
early 1920s, and they had to be very ingenious to think up explanations for the more
complicated effects they observed.
Hydrogen atoms contain a proton and an orbiting electron. You can think of this elec-
tron as a little ‘electric current’ around the proton. This electric current causes the atom
to have a magnetic ﬁeld; each atom has what physicists call a ‘magnetic dipole moment’.
As a result each atom behaves like a little bar magnet with an axis corresponding to the
axis the electron is spinning around. Throwing little bar magnets through a magnetic ﬁeld
causes the magnets to be deﬂected by the ﬁeld, and we expect to see a similar deﬂection
of atoms in the Stern–Gerlach experiment.
How the atom is deﬂected depends upon both the atom’s magnetic dipole moment –
the axis the electron is spinning around – and the magnetic ﬁeld generated by the Stern–
Gerlach device. We won’t go through the details, but sufﬁce to say that by constructing
the Stern–Gerlach device appropriately, we can cause the atom to be deﬂected by an
amount that depends upon the ˆz component of the atom’s magnetic dipole moment,
where ˆz is some ﬁxed external axis.
Two major surprises emerge when this experiment is performed. First, since the
hot atoms exiting the oven would naturally be expected to have their dipoles oriented
randomly in every direction, it would follow that there would be a continuous distribution
of atoms seen at all angles exiting from the Stern–Gerlach device. Instead, what is seen
is atoms emerging from a discrete set of angles. Physicists were able to explain this by
assuming that the magnetic dipole moment of the atoms is quantized, that is, comes in
discrete multiples of some fundamental amount.
This observation of quantization in the Stern–Gerlach experiment was surprising to
physicists of the 1920s, but not completely astonishing because evidence for quantization
44
Introduction and overview
effects in other systems was becoming widespread at that time. What was truly surpris-
ing was the number of peaks seen in the experiment. The hydrogen atoms being used
were such that they should have had zero magnetic dipole moment. Classically, this is
surprising in itself, since it corresponds to no orbital motion of the electron, but based
on what was known of quantum mechanics at that time this was an acceptable notion.
Since the hydrogen atoms would therefore have zero magnetic moment, it was expected
that only one beam of atoms would be seen, and this beam would not be deﬂected by
the magnetic ﬁeld. Instead, two beams were seen, one deﬂected up by the magnetic ﬁeld,
and the other deﬂected down!
This puzzling doubling was explained after considerable effort by positing that the
electron in the hydrogen atom has associated with it a quantity called spin. This spin
is not in any way associated to the usual rotational motion of the electron around the
proton; it is an entirely new quantity to be associated with an electron. The great physicist
Heisenberg labeled the idea ‘brave’ at the time it was suggested, and it is a brave idea, since
it introduces an essentially new physical quantity into Nature. The spin of the electron
is posited to make an extra contribution to the magnetic dipole moment of a hydrogen
atom, in addition to the contribution due to the rotational motion of the electron.



Figure 1.22. Abstract schematic of the Stern–Gerlach experiment. Hot hydrogen atoms are beamed from an oven
through a magnetic ﬁeld, causing a deﬂection either up (| + Z⟩) or down (| −Z⟩).
What is the proper description of the spin of the electron? As a ﬁrst guess, we might
hypothesize that the spin is speciﬁed by a single bit, telling the hydrogen atom to go up or
down. Additional experimental results provide further useful information to determine if
this guess needs reﬁnement or replacement. Let’s represent the original Stern–Gerlach
apparatus as shown in Figure 1.22. Its outputs are two beams of atoms, which we shall
call |+Z⟩and |−Z⟩. (We’re using suggestive notation which looks quantum mechanical,
but of course you’re free to use whatever notation you prefer.) Now suppose we cascade
two Stern–Gerlach apparatus together, as shown in Figure 1.23. We arrange it so that the
second apparatus is tipped sideways, so the magnetic ﬁeld deﬂects atoms along the ˆx axis.
In our thought-experiment we’ll block off the |−Z⟩output from the ﬁrst Stern–Gerlach
apparatus, while the | + Z⟩output is sent through a second apparatus oriented along the
ˆx axis. A detector is placed at the ﬁnal output to measure the distribution of atoms along
the ˆx axis.
A classical magnetic dipole pointed in the +ˆz direction has no net magnetic moment
in the ˆx direction, so we might expect that the ﬁnal output would have one central peak.
However, experimentally it is observed that there are two peaks of equal intensity! So
perhaps these atoms are peculiar, and have deﬁnite magnetic moments along each axis,
independently. That is, maybe each atom passing through the second apparatus can be
Experimental quantum information processing
45





Figure 1.23. Cascaded Stern–Gerlach measurements.
described as being in a state we might write as | + Z⟩| + X⟩or | + Z⟩| −X⟩, to indicate
the two values for spin that might be observed.







Figure 1.24. Three stage cascaded Stern–Gerlach measurements.
Another experiment, shown in Figure 1.24, can test this hypothesis by sending one
beam of the previous output through a second ˆz oriented Stern–Gerlach apparatus. If
the atoms had retained their | + Z⟩orientation, then the output would be expected to
have only one peak, at the | + Z⟩output. However, again two beams are observed at
the ﬁnal output, of equal intensity. Thus, the conclusion would seem to be that contrary
to classical expectations, a | + Z⟩state consists of equal portions of | + X⟩and | −X⟩
states, and a | + X⟩state consists of equal portions of | + Z⟩and | −Z⟩states. Similar
conclusions can be reached if the Stern–Gerlach apparatus is aligned along some other
axis, like the ˆy axis.
The qubit model provides a simple explanation of this experimentally observed be-
havior. Let |0⟩and |1⟩be the states of a qubit, and make the assignments
| + Z⟩←|0⟩
(1.56)
| −Z⟩←|1⟩
(1.57)
| + X⟩←(|0⟩+ |1⟩)/
√
2.
(1.58)
| −X⟩←(|0⟩−|1⟩)/
√
2
(1.59)
Then the results of the cascaded Stern–Gerlach experiment can be explained by assuming
that the ˆz Stern–Gerlach apparatus measures the spin (that is, the qubit) in the computa-
tional basis |0⟩, |1⟩, and the ˆx Stern–Gerlach apparatus measures the spin with respect to
the basis (|0⟩+|1⟩)/
√
2, (|0⟩−|1⟩)/
√
2. For example, in the cascaded ˆz- ˆx-ˆz experiment,
if we assume that the spins are in the state | + Z⟩= |0⟩= (| + X⟩+ | −X⟩)/
√
2 after
exiting the ﬁrst Stern–Gerlach experiment, then the probability for obtaining | + X⟩
out of the second apparatus is 1/2, and the probability for | −X⟩is 1/2. Similarly, the
probability for obtaining | + Z⟩out of the third apparatus is 1/2. A qubit model thus
properly predicts results from this type of cascaded Stern–Gerlach experiment.
46
Introduction and overview
This example demonstrates how qubits could be a believable way of modeling systems
in Nature. Of course it doesn’t establish beyond all doubt that the qubit model is the
correct way of understanding electron spin – far more experimental corroboration is
required. Nevertheless, because of many experiments like these, we now believe that
electron spin is best described by the qubit model. What is more, we believe that the
qubit model (and generalizations of it to higher dimensions; quantum mechanics, in other
words) is capable of describing every physical system. We now turn to the question of
what systems are especially well adapted to quantum information processing.
1.5.2
Prospects for practical quantum information processing
Building quantum information processing devices is a great challenge for scientists and
engineers of the third millennium. Will we rise to meet this challenge? Is it possible at
all? Is it worth attempting? If so, how might the feat be accomplished? These are difﬁcult
and important questions, to which we essay brief answers in this section, to be expanded
upon throughout the book.
The most fundamental question is whether there is any point of principle that prohibits
us from doing one or more forms of quantum information processing? Two possible
obstructions suggest themselves: that noise may place a fundamental barrier to useful
quantum information processing; or that quantum mechanics may fail to be correct.
Noise is without a doubt a signiﬁcant obstruction to the development of practical
quantum information processing devices. Is it a fundamentally irremovable obstruction
that will forever prevent the development of large-scale quantum information process-
ing devices? The theory of quantum error-correcting codes strongly suggests that while
quantum noise is a practical problem that needs to be addressed, it does not present a
fundamental problem of principle. In particular, there is a threshold theorem for quan-
tum computation, which states, roughly speaking, that provided the level of noise in a
quantum computer can be reduced below a certain constant ‘threshold’ value, quantum
error-correcting codes can be used to push it down even further, essentially ad inﬁni-
tum, for a small overhead in the complexity of the computation. The threshold theorem
makes some broad assumptions about the nature and magnitude of the noise occurring in
a quantum computer, and the architecture available for performing quantum computa-
tion; however, provided those assumptions are satisﬁed, the effects of noise can be made
essentially negligible for quantum information processing. Chapters 8, 10 and 12 discuss
quantum noise, quantum error-correction and the threshold theorem in detail.
A second possibility that may preclude quantum information processing is if quan-
tum mechanics is incorrect. Indeed, probing the validity of quantum mechanics (both
relativistic and non-relativistic) is one reason for being interested in building quantum
information processing devices. Never before have we explored a regime of Nature in
which complete control has been obtained over large-scale quantum systems, and perhaps
Nature may reveal some new surprises in this regime which are not adequately explained
by quantum mechanics. If this occurs, it will be a momentous discovery in the history of
science, and can be expected to have considerable consequences in other areas of science
and technology, as did the discovery of quantum mechanics. Such a discovery might also
impact quantum computation and quantum information; however, whether the impact
would enhance, detract or not affect the power of quantum information processing can-
not be predicted in advance. Until and unless such effects are found we have no way of
knowing how they might affect information processing, so for the remainder of this book
Experimental quantum information processing
47
we go with all the evidence to date and assume that quantum mechanics is a complete
and correct description of the world.
Given that there is no fundamental obstacle to building quantum information process-
ing devices, why should we invest enormous amounts of time and money in the attempt
to do so? We have already discussed several reasons for wanting to do so: practical appli-
cations such as quantum cryptography and the factoring of large composite numbers; and
the desire to obtain fundamental insights into Nature and into information processing.
These are good reasons, and justify a considerable investment of time and money in
the effort to build quantum information processing devices. However, it is fair to say that
a clearer picture of the relative power of quantum and classical information processing is
needed in order to assess their relative merits. To obtain such a picture requires further
theoretical work on the foundations of quantum computation and quantum information.
Of particular interest is a decisive answer to the question ‘Are quantum computers more
powerful than classical computers?’ Even if the answer to such a question eludes us for
the time being, it would be useful to have a clear path of interesting applications at
varying levels of complexity to aid researchers aiming to experimentally realize quantum
information processing. Historically, the advance of technology is often hastened by the
use of short- to medium-term incentives as a stepping-stone to long-term goals. Consider
that microprocessors were initially used as controllers for elevators and other simple
devices, before graduating to be the fundamental component in personal computers (and
then on to who-knows-what). Below we sketch out a path of short- to medium-term goals
for people interested in achieving the long-term goal of large-scale quantum information
processing.
Surprisingly many small-scale applications of quantum computation and quantum in-
formation are known. Not all are as ﬂashy as cousins like the quantum factoring algorithm,
but the relative ease of implementing small-scale applications makes them extremely im-
portant as medium-term goals in themselves.
Quantum state tomography and quantum process tomography are two elementary
processes whose perfection is of great importance to quantum computation and quantum
information, as well as being of independent interest in their own right. Quantum state
tomography is a method for determining the quantum state of a system. To do this, it
has to overcome the ‘hidden’ nature of the quantum state – remember, the state can’t be
directly determined by a measurement – by performing repeated preparations of the same
quantum state, which is then measured in different ways in order to build up a complete
description of the quantum state. Quantum process tomography is a more ambitious (but
closely related) procedure to completely characterize the dynamics of a quantum system.
Quantum process tomography can, for example, be used to characterize the performance
of an alleged quantum gate or quantum communications channel, or to determine the
types and magnitudes of different noise processes in a system. Beside obvious applica-
tions to quantum computation and quantum information, quantum process tomography
can be expected to have signiﬁcant applications as a diagnostic tool to aid in the eval-
uation and improvement of primitive operations in any ﬁeld of science and technology
where quantum effects are important. Quantum state tomography and quantum process
tomography are described in more detail in Chapter 8.
Various small-scale communications primitives are also of great interest. We have al-
ready mentioned quantum cryptography and quantum teleportation. The former is likely
to be useful in practical applications involving the distribution of a small amount of key
48
Introduction and overview
material that needs to be highly secure. The uses of quantum teleportation are perhaps
more open to question. We will see in Chapter 12 that teleportation may be an extremely
useful primitive for transmitting quantum states between distant nodes in a network, in
the presence of noise. The idea is to focus one’s efforts on distributing EPR pairs between
the nodes that wish to communicate. The EPR pairs may be corrupted during commu-
nication, but special ‘entanglement distillation’ protocols can then be used to ‘clean up’
the EPR pairs, enabling them to be used to teleport quantum states from one location
to another. In fact, procotols based upon entanglement distillation and teleportation of-
fer performance superior to more conventional quantum error-correction techniques in
enabling noise free communication of qubits.
What of the medium-scale? A promising medium-scale application of quantum in-
formation processing is to the simulation of quantum systems. To simulate a quantum
system containing even a few dozen ‘qubits’ (or the equivalent in terms of some other
basic system) strains the resources of even the largest supercomputers. A simple calcu-
lation is instructive. Suppose we have a system containing 50 qubits. To describe the
state of such a system requires 250 ≈1015 complex amplitudes. If the amplitudes are
stored to 128 bits of precision, then it requires 256 bits or 32 bytes in order to store each
amplitude, for a total of 32 × 1015 bytes of information, or about 32 thousand terabytes
of information, well beyond the capacity of existing computers, and corresponding to
about the storage capacity that might be expected to appear in supercomputers during
the second decade of the twenty-ﬁrst century, presuming that Moore’s law continues on
schedule. 90 qubits at the same level of precision requires 32 × 1027 bytes, which, even
if implemented using single atoms to represent bits, would require kilograms (or more)
of matter.
How useful will quantum simulations be? It seems likely that conventional methods will
still be used to determine elementary properties of materials, such as bond strengths and
basic spectroscopic properties. However, once the basic properties are well understood,
it seems likely that quantum simulation will be of great utility as a laboratory for the
design and testing of properties of novel molecules. In a conventional laboratory setup,
many different types of ‘hardware’ – chemicals, detectors, and so on – may be required
to test a wide variety of possible designs for a molecule. On a quantum computer, these
different types of hardware can all be simulated in software, which is likely to be much
less expensive and much faster. Of course, ﬁnal design and testing must be performed
with real physical systems; however, quantum computers may enable a much larger range
of potential designs to be explored and evaluated en route to a better ﬁnal design. It is
interesting to note that such ab initio calculations to aid in the design of new molecules
have been attempted on classical computers; however, they have met with limited success
due to the enormous computational resources needed to simulate quantum mechanics on a
classical computer. Quantum computers should be able to do much better in the relatively
near future.
What of large-scale applications? Aside from scaling up applications like quantum
simulation and quantum cryptography, relatively few large-scale applications are known:
the factoring of large numbers, taking discrete logarithms, and quantum searching. In-
terest in the ﬁrst two of these derives mainly from the negative effect they would have
of limiting the viability of existing public key cryptographic systems. (They might also
be of substantial practical interest to mathematicians interested in these problems sim-
ply for their own sake.) So it does not seem likely that factoring and discrete logarithm
Experimental quantum information processing
49
will be all that important as applications for the long run. Quantum searching may be
of tremendous use because of the wide utility of the search heuristic, and we discuss
some possible applications in Chapter 6. What would really be superb are many more
large-scale applications of quantum information processing. This is a great goal for the
future!
Given a path of potential applications for quantum information processing, how can it
be achieved in real physical systems? At the small scale of a few qubits there are already
several working proposals for quantum information processing devices. Perhaps the easiest
to realize are based upon optical techniques, that is, electromagnetic radiation. Simple
devices like mirrors and beamsplitters can be used to do elementary manipulations of
photons. Interestingly, a major difﬁculty has been producing single photons on demand;
experimentalists have instead opted to use schemes which produce single photons ‘every
now and then’, at random, and wait for such an event to occur. Quantum cryptography,
superdense coding, and quantum teleportation have all been realized using such optical
techniques. A major advantage of the optical techniques is that photons tend to be highly
stable carriers of quantum mechanical information. A major disadvantage is that photons
don’t directly interact with one another. Instead, the interaction has to be mediated by
something else, like an atom, which introduces additional noise and complications into
the experiment. An effective interaction between two photons is set up, which essentially
works in two steps: photon number one interacts with the atom, which in turn interacts
with the second photon, causing an overall interaction between the two photons.
An alternative scheme is based upon methods for trapping different types of atom: there
is the ion trap, in which a small number of charged atoms are trapped in a conﬁned space;
and neutral atom traps, for trapping uncharged atoms in a conﬁned space. Quantum
information processing schemes based upon atom traps use the atoms to store qubits.
Electromagnetic radiation also shows up in these schemes, but in a rather different way
than in what we referred to as the ‘optical’ approach to quantum information processing.
In these schemes, photons are used to manipulate the information stored in the atoms
themselves, rather than as the place the information is stored. Single qubit quantum
gates can be performed by applying appropriate pulses of electromagnetic radiation to
individual atoms. Neighboring atoms can interact with one another via (for example)
dipole forces that enable quantum gates to be accomplished. Moreover, the exact nature of
the interaction between neighboring atoms can be modiﬁed by applying appropriate pulses
of electromagnetic radiation to the atoms, giving the experimentalist control over what
gates are performed in the system. Finally, quantum measurement can be accomplished in
these systems using the long established quantum jumps technique, which implements
with superb accuracy the measurements in the computational basis used for quantum
computation.
Another class of quantum information processing schemes is based upon Nuclear
Magnetic Resonance, often known by its initials, NMR. These schemes store quantum
information in the nuclear spin of atoms in a molecule, and manipulate that information
using electromagnetic radiation. Such schemes pose special difﬁculties, because in NMR
it is not possible to directly access individual nuclei. Instead, a huge number (typically
around 1015) of essentially identical molecules are stored in solution. Electromagnetic
pulses are applied to the sample, causing each molecule to respond in roughly the same
way. You should think of each molecule as being an independent computer, and the
sample as containing a huge number of computers all running in parallel (classically).
50
Introduction and overview
NMR quantum information processing faces three special difﬁculties that make it rather
different from other quantum information processing schemes. First, the molecules are
typically prepared by letting them equilibrate at room temperature, which is so much
higher than typical spin ﬂip energies that the spins become nearly completely randomly
oriented. This fact makes the initial state rather more ‘noisy’ than is desirable for quantum
information processing. How this noise may be overcome is an interesting story that we
tell in Chapter 7. A second problem is that the class of measurements that may be
performed in NMR falls well short of the most general measurements we would like to
perform in quantum information processing. Nevertheless, for many instances of quantum
information processing the class of measurements allowed in NMR is sufﬁcient. Third,
because molecules cannot be individually addressed in NMR you might ask how it is that
individual qubits can be manipulated in an appropriate way. Fortunately, different nuclei
in the molecule can have different properties that allow them to be individually addressed
– or at least addressed at a sufﬁciently ﬁne-grained scale to allow the operations essential
for quantum computation.
Many of the elements required to perform large-scale quantum information processing
can be found in existing proposals: superb state preparation and quantum measurements
can be performed on a small number of qubits in the ion trap; superb dynamics can be
performed in small molecules using NMR; fabrication technology in solid state systems
allows designs to be scaled up tremendously. A single system having all these elements
would be a long way down the road to a dream quantum computer. Unfortunately, all
these systems are very different, and we are many, many years from having large-scale
quantum computers. However, we believe that the existence of all these properties in
existing (albeit different) systems does bode well for the long-term existence of large-
scale quantum information processors. Furthermore, it suggests that there is a great deal
of merit to pursuing hybrid designs which attempt to marry the best features of two or
more existing technologies. For example, there is much work being done on trapping
atoms inside electromagnetic cavities. This enables ﬂexible manipulation of the atom
inside the cavity via optical techniques, and makes possible real-time feedback control of
single atoms in ways unavailable in conventional atom traps.
To conclude, note that it is important not to assess quantum information processing
as though it were just another technology for information processing. For example, it
is tempting to dismiss quantum computation as yet another technological fad in the
evolution of the computer that will pass in time, much as other fads have passed – for
example, the ‘bubble memories’ widely touted as the next big thing in memory during the
early 1980s. This is a mistake, since quantum computation is an abstract paradigm for
information processing that may have many different implementations in technology. One
can compare two different proposals for quantum computing as regards their technological
merits – it makes sense to compare a ‘good’ proposal to a ‘bad’ proposal – however even
a very poor proposal for a quantum computer is of a different qualitative nature from a
superb design for a classical computer.
1.6
Quantum information
The term ‘quantum information’ is used in two distinct ways in the ﬁeld of quantum
computation and quantum information. The ﬁrst usage is as a broad catch-all for all
manner of operations that might be interpreted as related to information processing
Quantum information
51
using quantum mechanics. This use encompasses subjects such as quantum computation,
quantum teleportation, the no-cloning theorem, and virtually all other topics in this book.
The second use of ‘quantum information’ is much more specialized: it refers to the
study of elementary quantum information processing tasks. It does not typically include,
for example, quantum algorithm design, since the details of speciﬁc quantum algorithms
are beyond the scope of ‘elementary’. To avoid confusion we will use the term ‘quantum
information theory’ to refer to this more specialized ﬁeld, in parallel with the widely
used term ‘(classical) information theory’ to describe the corresponding classical ﬁeld.
Of course, the term ‘quantum information theory’ has a drawback of its own – it might
be seen as implying that theoretical considerations are all that matter! Of course, this
is not the case, and experimental demonstration of the elementary processes studied by
quantum information theory is of great interest.
The purpose of this section is to introduce the basic ideas of quantum information
theory. Even with the restriction to elementary quantum information processing tasks,
quantum information theory may look like a disordered zoo to the beginner, with many
apparently unrelated subjects falling under the ‘quantum information theory’ rubric. In
part, that’s because the subject is still under development, and it’s not yet clear how all
the pieces ﬁt together. However, we can identify a few fundamental goals uniting work
on quantum information theory:
(1) Identify elementary classes of static resources in quantum mechanics. An
example is the qubit. Another example is the bit; classical physics arises as a special
case of quantum physics, so it should not be surprising that elementary static
resources appearing in classical information theory should also be of great relevance
in quantum information theory. Yet another example of an elementary class of
static resources is a Bell state shared between two distant parties.
(2) Identify elementary classes of dynamical processes in quantum mechanics.
A simple example is memory, the ability to store a quantum state over some period
of time. Less trivial processes are quantum information transmission between two
parties, Alice and Bob; copying (or trying to copy) a quantum state, and the process
of protecting quantum information processing against the effects of noise.
(3) Quantify resource tradeoffs incurred performing elementary dynamical
processes. For example, what are the minimal resources required to reliably
transfer quantum information between two parties using a noisy communications
channel?
Similar goals deﬁne classical information theory; however, quantum information theory
is broader in scope than classical information theory, for quantum information theory
includes all the static and dynamic elements of classical information theory, as well as
additional static and dynamic elements.
The remainder of this section describes some examples of questions studied by quan-
tum information theory, in each case emphasizing the fundamental static and dynamic
elements under consideration, and the resource tradeoffs being considered. We begin with
an example that will appear quite familiar to classical information theorists: the problem
of sending classical information through a quantum channel. We then begin to branch out
and explore some of the new static and dynamic processes present in quantum mechan-
ics, such as quantum error-correction, the problem of distinguishing quantum states, and
entanglement transformation. The chapter concludes with some reﬂections on how the
52
Introduction and overview
tools of quantum information theory can be applied elsewhere in quantum computation
and quantum information.
1.6.1
Quantum information theory: example problems
Classical information through quantum channels
The fundamental results of classical information theory are Shannon’s noiseless channel
coding theorem and Shannon’s noisy channel coding theorem. The noiseless channel
coding theorem quantiﬁes how many bits are required to store information being emitted
by a source of information, while the noisy channel coding theorem quantiﬁes how much
information can be reliably transmitted through a noisy communications channel.
What do we mean by an information source? Deﬁning this notion is a fundamental
problem of classical and quantum information theory, one we’ll re-examine several times.
For now, let’s go with a provisional deﬁnition: a classical information source is described
by a set of probabilities pj, j = 1, 2, . . ., d. Each use of the source results in the ‘letter’
j being emitted, chosen at random with probability pj, independently for each use of
the source. For instance, if the source were of English text, then the numbers j might
correspond to letters of the alphabet and punctuation, with the probabilities pj giving
the relative frequencies with which the different letters appear in regular English text.
Although it is not true that the letters in English appear in an independent fashion, for
our purposes it will be a good enough approximation.
Regular English text includes a considerable amount of redundancy, and it is possible to
exploit that redundancy to compress the text. For example, the letter ‘e’ occurs much more
frequently in regular English text than does the letter ‘z’. A good scheme for compressing
English text will therefore represent the letter ‘e’ using fewer bits of information than
it uses to represent ‘z’. Shannon’s noiseless channel coding theorem quantiﬁes exactly
how well such a compression scheme can be made to work. More precisely, the noiseless
channel coding theorem tells us that a classical source described by probabilities pj can be
compressed so that on average each use of the source can be represented using H(pj) bits
of information, where H(pj) ≡−
j pj log(pj) is a function of the source probability
distribution known as the Shannon entropy. Moreover, the noiseless channel coding
theorem tells us that to attempt to represent the source using fewer bits than this will
result in a high probability of error when the information is decompressed. (Shannon’s
noiseless channel coding theorem is discussed in much greater detail in Chapter 12.)
Shannon’s noiseless coding theorem provides a good example where the goals of infor-
mation theory listed earlier are all met. Two static resources are identiﬁed (goal number 1):
the bit and the information source. A two-stage dynamic process is identiﬁed (goal 2),
compressing an information source, and then decompressing to recover the information
source. Finally a quantitative criterion for determining the resources consumed (goal 3)
by an optimal data compression scheme is found.
Shannon’s second major result, the noisy channel coding theorem, quantiﬁes the
amount of information that can be reliably transmitted through a noisy channel. In par-
ticular, suppose we wish to transfer the information being produced by some information
source to another location through a noisy channel. That location may be at another point
in space, or at another point in time – the latter is the problem of storing information
in the presence of noise. The idea in both instances is to encode the information being
produced using error-correcting codes, so that any noise introduced by the channel can
be corrected at the other end of the channel. The way error-correcting codes achieve this
Quantum information
53
is by introducing enough redundancy into the information sent through the channel so
that even after some of the information has been corrupted it is still possible to recover
the original message. For example, suppose the noisy channel is for the transmission of
single bits, and the noise in the channel is such that to achieve reliable transmission each
bit produced by the source must be encoded using two bits before being sent through
the channel. We say that such a channel has a capacity of half a bit, since each use of
the channel can be used to reliably convey roughly half a bit of information. Shannon’s
noisy channel coding theorem provides a general procedure for calculating the capacity
of an arbitrary noisy channel.
Shannon’s noisy channel coding theorem also achieves the three goals of information
theory we stated earlier. Two types of static resources are involved (goal 1), the informa-
tion source, and the bits being sent through the channel. Three dynamical processes are
involved (goal 2). The primary process is the noise in the channel. To combat this noise
we perform the dual processes of encoding and decoding the state in an error-correcting
code. For a ﬁxed noise model, Shannon’s theorem tells us how much redundancy must
be introduced by an optimal error-correction scheme if reliable information transmission
is to be achieved (goal 3).
For both the noiseless and noisy channel coding theorems Shannon restricted himself
to storing the output from an information source in classical systems – bits and the like. A
natural question for quantum information theory is what happens if the storage medium is
changed so that classical information is transmitted using quantum states as the medium.
For example, it may be that Alice wishes to compress some classical information produced
by an information source, transmitting the compressed information to Bob, who then
decompresses it. If the medium used to store the compressed information is a quantum
state, then Shannon’s noiseless channel coding theorem cannot be used to determine
the optimal compression and decompression scheme. One might wonder, for example,
if using qubits allows a better compression rate than is possible classically. We’ll study
this question in Chapter 12, and prove that, in fact, qubits do not allow any signiﬁcant
saving in the amount of communication required to transmit information over a noiseless
channel.
Naturally, the next step is to investigate the problem of transmitting classical informa-
tion through a noisy quantum channel. Ideally, what we’d like is a result that quantiﬁes
the capacity of such a channel for the transmission of information. Evaluating the capac-
ity is a very tricky job for several reasons. Quantum mechanics gives us a huge variety of
noise models, since it takes place in a continuous space, and it is not at all obvious how
to adapt classical error-correction techniques to combat the noise. Might it be advanta-
geous, for example, to encode the classical information using entangled states, which are
then transmitted one piece at a time through the noisy channel? Or perhaps it will be
advantageous to decode using entangled measurements? In Chapter 12 we’ll prove the
HSW (Holevo–Schumacher–Westmoreland) theorem, which provides a lower bound
on the capacity of such a channel. Indeed, it is widely believed that the HSW theorem
provides an exact evaluation of the capacity, although a complete proof of this is not yet
known! What remains at issue is whether or not encoding using entangled states can be
used to raise the capacity beyond the lower bound provided by the HSW theorem. All
evidence to date suggests that this doesn’t help raise the capacity, but it is still a fasci-
nating open problem of quantum information theory to determine the truth or falsity of
this conjecture.
54
Introduction and overview
Quantum information through quantum channels
Classical information is, of course, not the only static resource available in quantum
mechanics. Quantum states themselves are a natural static resource, even more natural
than classical information. Let’s look at a different quantum analogue of Shannon’s coding
theorems, this time involving the compression and decompression of quantum states.
To begin, we need to deﬁne some quantum notion of an information source, analogous
to the classical deﬁnition of an information source. As in the classical case, there are several
different ways of doing this, but for the sake of deﬁniteness let’s make the provisional
deﬁnition that a quantum source is described by a set of probabilities pj and corresponding
quantum states |ψj⟩. Each use of the source produces a state |ψj⟩with probability pj,
with different uses of the source being independent of one another.
Is it possible to compress the output from such a quantum mechanical source? Consider
the case of a qubit source which outputs the state |0⟩with probability p and the state |1⟩
with probability 1 −p. This is essentially the same as a classical source emitting single
bits, either 0 with probability p, or 1 with probability 1 −p, so it is not surprising that
similar techniques can be used to compress the source so that only H(p, 1 −p) qubits
are required to store the compressed source, where H(·) is again the Shannon entropy
function.
What if the source had instead been producing the state |0⟩with probability p, and
the state (|0⟩+ |1⟩)/
√
2 with probability 1 −p? The standard techniques of classical
data compression no longer apply, since in general it is not possible for us to distinguish
the states |0⟩and (|0⟩+ |1⟩)/
√
2. Might it still be possible to perform some type of
compression operation?
It turns out that a type of compression is still possible, even in this instance. What is
interesting is that the compression may no longer be error-free, in the sense that the quan-
tum states being produced by the source may be slightly distorted by the compression–
decompression procedure. Nevertheless, we require that this distortion ought to become
very small and ultimately negligible in the limit of large blocks of source output being
compressed. To quantify the distortion we introduce a ﬁdelity measure for the com-
pression scheme, which measures the average distortion introduced by the compression
scheme. The idea of quantum data compression is that the compressed data should be
recovered with very good ﬁdelity. Think of the ﬁdelity as being analogous to the proba-
bility of doing the decompression correctly – in the limit of large block lengths, it should
tend towards the no error limit of 1.
Schumacher’s noiseless channel coding theorem quantiﬁes the resources required to do
quantum data compression, with the restriction that it be possible to recover the source
with ﬁdelity close to 1. In the case of a source producing orthogonal quantum states
|ψj⟩with probabilities pj Schumacher’s theorem reduces to telling us that the source
may be compressed down to but not beyond the classical limit H(pj). However, in the
more general case of non-orthogonal states being produced by the source, Schumacher’s
theorem tells us how much a quantum source may be compressed, and the answer is
not the Shannon entropy H(pj)! Instead, a new entropic quantity, the von Neumann
entropy, turns out to be the correct answer. In general, the von Neumann entropy agrees
with the Shannon entropy if and only if the states |ψj⟩are orthogonal. Otherwise, the von
Neumann entropy for the source pj, |ψj⟩is in general strictly smaller than the Shannon
entropy H(pj). Thus, for example, a source producing the state |0⟩with probability p
Quantum information
55
and (|0⟩+ |1⟩)/
√
2 with probability 1 −p can be reliably compressed using fewer than
H(p, 1 −p) qubits per use of the source!
The basic intuition for this decrease in resources required can be understood quite
easily. Suppose the source emitting states |0⟩with probability p and (|0⟩+ |1⟩)/
√
2 with
probability 1 −p is used a large number n times. Then by the law of large numbers,
with high probability the source emits about np copies of |0⟩and n(1 −p) copies of
(|0⟩+ |1⟩)/
√
2. That is, it has the form
|0⟩⊗np
|0⟩+ |1⟩
√
2
⊗n(1−p)
,
(1.60)
up to re-ordering of the systems involved. Suppose we expand the product of |0⟩+ |1⟩
terms on the right hand side. Since n(1 −p) is large, we can again use the law of large
numbers to deduce that the terms in the product will be roughly one-half |0⟩s and one-
half |1⟩s. That is, the |0⟩+ |1⟩product can be well approximated by a superposition of
states of the form
|0⟩⊗n(1−p)/2|1⟩⊗n(1−p)/2.
(1.61)
Thus the state emitted by the source can be approximated as a superposition of terms of
the form
|0⟩⊗n(1+p)/2|1⟩⊗n(1−p)/2.
(1.62)
How many states of this form are there? Roughly n choose n(1 + p)/2, which by Stir-
ling’s approximation is equal to N ≡2nH[(1+p)/2,(1−p)/2]. A simple compression method
then is to label all states of the form (1.62) |c1⟩through |cN⟩. It is possible to per-
form a unitary transform on the n qubits emitted from the source that takes |cj⟩to
|j⟩|0⟩⊗n−nH[(1+p)/2,(1−p)/2], since j is an nH[(1 + p)/2, (1 −p)/2] bit number. The com-
pression operation is to perform this unitary transformation, and then drop the ﬁnal
n−nH[(1+p)/2, (1−p)/2] qubits, leaving a compressed state of nH[(1+p)/2, (1−p)/2]
qubits. To decompress we append the state |0⟩⊗n−nH[(1+p)/2,(1−p)/2] to the compressed
state, and perform the inverse unitary transformation.
This procedure for quantum data compression and decompression results in a storage
requirement of H[(1 + p)/2, (1 −p)/2] qubits per use of the source, which whenever
p ≥1/3 is an improvement over the H(p, 1 −p) qubits we might naively have expected
from Shannon’s noiseless channel coding theorem. In fact, Schumacher’s noiseless chan-
nel coding theorem allows us to do somewhat better even than this, as we will see in
Chapter 12; however, the essential reason in that construction is the same as the reason
we were able to compress here: we exploited the fact that |0⟩and (|0⟩+ |1⟩)/
√
2 are not
orthogonal. Intuitively, the states contain some redundancy since both have a component
in the |0⟩direction, which results in more physical similarity than would be obtained
from orthogonal states. It is this redundancy that we have exploited in the coding scheme
just described, and which is used in the full proof of Schumacher’s noiseless channel
coding theorem. Note that the restriction p ≥1/3 arises because when p < 1/3 this
particular scheme doesn’t exploit the redundancy in the states: we end up effectively
increasing the redundancy present in the problem! Of course, this is an artifact of the
particular scheme we have chosen, and the general solution exploits the redundancy in a
much more sensible way to achieve data compression.
Schumacher’s noiseless channel coding theorem is an analogue of Shannon’s noiseless
56
Introduction and overview
channel coding theorem for the compression and decompression of quantum states. Can
we ﬁnd an analogue of Shannon’s noisy channel coding theorem? Considerable progress
on this important question has been made, using the theory of quantum error-correcting
codes; however, a fully satisfactory analogue has not yet been found. We review some of
what is known about the quantum channel capacity in Chapter 12.
Quantum distinguishability
Thus far all the dynamical processes we have considered – compression, decompression,
noise, encoding and decoding error-correcting codes – arise in both classical and quantum
information theory. However, the introduction of new types of information, such as
quantum states, enlarges the class of dynamical processes beyond those considered in
classical information theory. A good example is the problem of distinguishing quantum
states. Classically, we are used to being able to distinguish different items of information,
at least in principle. In practice, of course, a smudged letter ‘a’ written on a page may be
very difﬁcult to distinguish from a letter ‘o’, but in principle it is possible to distinguish
between the two possibilities with perfect certainty.
On the other hand, quantum mechanically it is not always possible to distinguish
between arbitrary states. For example, there is no process allowed by quantum mechanics
that will reliably distinguish between the states |0⟩and (|0⟩+ |1⟩)/
√
2. Proving this
rigorously requires tools we don’t presently have available (it is done in Chapter 2),
but by considering examples it’s pretty easy to convince oneself that it is not possible.
Suppose, for example, that we try to distinguish the two states by measuring in the
computational basis. Then, if we have been given the state |0⟩, the measurement will
yield 0 with probability 1. However, when we measure (|0⟩+ |1⟩)/
√
2 the measurement
yields 0 with probability 1/2 and 1 with probability 1/2. Thus, while a measurement
result of 1 implies that state must have been (|0⟩+ |1⟩)/
√
2, since it couldn’t have been
|0⟩, we can’t infer anything about the identity of the quantum state from a measurement
result of 0.
This indistinguishability of non-orthogonal quantum states is at the heart of quantum
computation and quantum information. It is the essence of our assertion that a quan-
tum state contains hidden information that is not accessible to measurement, and thus
plays a key role in quantum algorithms and quantum cryptography. One of the central
problems of quantum information theory is to develop measures quantifying how well
non-orthogonal quantum states may be distinguished, and much of Chapters 9 and 12 is
concerned with this goal. In this introduction we’ll limit ourselves to pointing out two
interesting aspects of indistinguishability – a connection with the possibility of faster-
than-light communication, and an application to ‘quantum money.’
Imagine for a moment that we could distinguish between arbitrary quantum states.
We’ll show that this implies the ability to communicate faster than light, using entan-
glement. Suppose Alice and Bob share an entangled pair of qubits in the state (|00⟩+
|11⟩)/
√
2. Then, if Alice measures in the computational basis, the post-measurement
states will be |00⟩with probability 1/2, and |11⟩with probability 1/2. Thus Bob’s sys-
tem is either in the state |0⟩, with probability 1/2, or in the state |1⟩, with probability
1/2. Suppose, however, that Alice had instead measured in the |+⟩, |−⟩basis. Recall that
|0⟩= (|+⟩+ |−⟩)/
√
2 and |1⟩= (|+⟩−|−⟩)/
√
2. A little algebra shows that the initial
state of Alice and Bob’s system may be rewritten as (| + +⟩+ | −−⟩)/
√
2. Therefore,
if Alice measures in the |+⟩, |−⟩basis, the state of Bob’s system after the measurement
Quantum information
57
will be |+⟩or |−⟩with probability 1/2 each. So far, this is all basic quantum mechanics.
But if Bob had access to a device that could distinguish the four states |0⟩, |1⟩, |+⟩, |−⟩
from one another, then he could tell whether Alice had measured in the computational
basis, or in the |+⟩, |−⟩basis. Moreover, he could get that information instantaneously,
as soon as Alice had made the measurement, providing a means by which Alice and Bob
could achieve faster-than-light communication! Of course, we know that it is not possible
to distinguish non-orthogonal quantum states; this example shows that this restriction is
also intimately tied to other physical properties which we expect the world to obey.
The indistinguishability of non-orthogonal quantum states need not always be a hand-
icap. Sometimes it can be a boon. Imagine that a bank produces banknotes imprinted
with a (classical) serial number, and a sequence of qubits each in either the state |0⟩
or (|0⟩+ |1⟩)/
√
2. Nobody but the bank knows what sequence of these two states is
embedded in the note, and the bank maintains a list matching serial numbers to em-
bedded states. The note is impossible to counterfeit exactly, because it is impossible
for a would-be counterfeiter to determine with certainty the state of the qubits in the
original note, without destroying them. When presented with the banknote a merchant
(of certiﬁable repute) can verify that it is not a counterfeit by calling the bank, telling
them the serial number, and then asking what sequence of states were embedded in
the note. They can then check that the note is genuine by measuring the qubits in the
|0⟩, |1⟩or (|0⟩+ |1⟩)/
√
2, (|0⟩−|1⟩)/
√
2 basis, as directed by the bank. With probability
which increases exponentially to one with the number of qubits checked, any would-be
counterfeiter will be detected at this stage! This idea is the basis for numerous other
quantum cryptographic protocols, and demonstrates the utility of the indistinguishability
of non-orthogonal quantum states.
Exercise 1.2:
Explain how a device which, upon input of one of two non-orthogonal
quantum states |ψ⟩or |ϕ⟩correctly identiﬁed the state, could be used to build a
device which cloned the states |ψ⟩and |ϕ⟩, in violation of the no-cloning
theorem. Conversely, explain how a device for cloning could be used to
distinguish non-orthogonal quantum states.
Creation and transformation of entanglement
Entanglement is another elementary static resource of quantum mechanics. Its properties
are amazingly different from those of the resources most familiar from classical informa-
tion theory, and they are not yet well understood; we have at best an incomplete collage
of results related to entanglement. We don’t yet have all the language needed to under-
stand the solutions, but let’s at least look at two information-theoretic problems related
to entanglement.
Creating entanglement is a simple dynamical process of interest in quantum informa-
tion theory. How many qubits must two parties exchange if they are to create a particular
entangled state shared between them, given that they share no prior entanglement? A
second dynamical process of interest is transforming entanglement from one form into
another. Suppose, for example, that Alice and Bob share between them a Bell state, and
wish to transform it into some other type of entangled state. What resources do they
need to accomplish this task? Can they do it without communicating? With classical
communication only? If quantum communication is required then how much quantum
communication is required?
58
Introduction and overview
Answering these and more complex questions about the creation and transformation of
entanglement forms a fascinating area of study in its own right, and also promises to give
insight into tasks such as quantum computation. For example, a distributed quantum
computation may be viewed as simply a method for generating entanglement between
two or more parties; lower bounds on the amount of communication that must be done
to perform such a distributed quantum computation then follow from lower bounds on
the amount of communication that must be performed to create appropriate entangled
states.
1.6.2
Quantum information in a wider context
We have given but the barest glimpse of quantum information theory. Part III of this
book discusses quantum information theory in much greater detail, especially Chapter 11,
which deals with fundamental properties of entropy in quantum and classical information
theory, and Chapter 12, which focuses on pure quantum information theory.
Quantum information theory is the most abstract part of quantum computation and
quantum information, yet in some sense it is also the most fundamental. The question
driving quantum information theory, and ultimately all of quantum computation and
quantum information, is what makes quantum information processing tick? What is
it that separates the quantum and the classical world? What resources, unavailable in a
classical world, are being utilized in a quantum computation? Existing answers to these
questions are foggy and incomplete; it is our hope that the fog may yet lift in the years
to come, and we will obtain a clear appreciation for the possibilities and limitations of
quantum information processing.
Problem 1.1: (Feynman-Gates conversation)
Construct a friendly imaginary
discussion of about 2000 words between Bill Gates and Richard Feynman, set in
the present, on the future of computation. (Comment: You might like to try
waiting until you’ve read the rest of the book before attempting this question.
See the ‘History and further reading’ below for pointers to one possible answer
for this question.)
Problem 1.2:
What is the most signiﬁcant discovery yet made in quantum
computation and quantum information? Write an essay of about 2000 words for
an educated lay audience about the discovery. (Comment: As for the previous
problem, you might like to try waiting until you’ve read the rest of the book
before attempting this question.)
History and further reading
Most of the material in this chapter is revisited in more depth in later chapters. Therefore
the historical references and further reading below are limited to material which does not
recur in later chapters.
Piecing together the historical context in which quantum computation and quantum
information have developed requires a broad overview of the history of many ﬁelds. We
have tried to tie this history together in this chapter, but inevitably much background
material was omitted due to limited space and expertise. The following recommendations
attempt to redress this omission.
History and further reading
59
The history of quantum mechanics has been told in many places. We recommend es-
pecially the outstanding works of Pais[Pai82, Pai86, Pai91]. Of these three, [Pai86] is most di-
rectly concerned with the development of quantum mechanics; however, Pais’ biographies
of Einstein[Pai82] and of Bohr[Pai91] also contain much material of interest, at a less intense
level. The rise of technologies based upon quantum mechanics has been described by Mil-
burn[Mil97, Mil98]. Turing’s marvelous paper on the foundations of computer science[Tur36]
is well worth reading. It can be found in the valuable historical collection of Davis[Dav65].
Hofstadter[Hof79] and Penrose[Pen89] contain entertaining and informative discussions of
the foundations of computer science. Shasha and Lazere’s biography of ﬁfteen leading
computer scientists[SL98] gives considerable insight into many different facets of the his-
tory of computer science. Finally, Knuth’s awesome series of books[Knu97, Knu98a, Knu98b]
contain an amazing amount of historical information. Shannon’s brilliant papers founding
information theory make excellent reading[Sha48] (also reprinted in [SW49]). MacWilliams
and Sloane[MS77] is not only an excellent text on error-correcting codes, but also contains
an enormous amount of useful historical information. Similarly, Cover and Thomas[CT91]
is an excellent text on information theory, with extensive historical information. Shan-
non’s collected works, together with many useful historical items have been collected in
a large volume[SW93] edited by Sloane and Wyner. Slepian has also collected a useful set
of reprints on information theory[Sle74]. Cryptography is an ancient art with an intricate
and often interesting history. Kahn[Kah96] is a huge history of cryptography contain-
ing a wealth of information. For more recent developments we recommend the books
by Menezes, van Oorschot, and Vanstone[MvOV96], Schneier[Sch96a], and by Difﬁe and
Landau[DL98].
Quantum teleportation was discovered by Bennett, Brassard, Cr´epeau, Jozsa, Peres,
and Wootters[BBC+93], and later experimentally realized in various different forms by
Boschi, Branca, De Martini, Hardy and Popescu[BBM+98] using optical techniques, by
Bouwmeester, Pan, Mattle, Eibl, Weinfurter, and Zeilinger[BPM+97] using photon polar-
ization, by Furusawa, Sørensen, Braunstein, Fuchs, Kimble, and Polzik using ‘squeezed’
states of light[FSB+98], and by Nielsen, Knill, and Laﬂamme using NMR[NKL98].
Deutsch’s problem was posed by Deutsch[Deu85], and a one-bit solution was given in the
same paper. The extension to the general n-bit case was given by Deutsch and Jozsa[DJ92].
The algorithms in these early papers have been substantially improved subsequently
by Cleve, Ekert, Macchiavello, and Mosca[CEMM98], and independently in unpublished
work by Tapp. In this chapter we have given the improved version of the algorithm,
which ﬁts very nicely into the hidden subgroup problem framework that will later be
discussed in Chapter 5. The original algorithm of Deutsch only worked probabilistically;
Deutsch and Jozsa improved this to obtain a deterministic algorithm, but their method
required two function evaluations, in contrast to the improved algorithms presented in
this chapter. Nevertheless, it is still conventional to refer to these algorithms as Deutsch’s
algorithm and the Deutsch–Jozsa algorithm in honor of two huge leaps forward: the
concrete demonstration by Deutsch that a quantum computer could do something faster
than a classical computer; and the extension by Deutsch and Jozsa which demonstrated
for the ﬁrst time a similar gap for the scaling of the time required to solve a problem.
Excellent discussions of the Stern–Gerlach experiment can be found in standard quan-
tum mechanics textbooks such as the texts by Sakurai[Sak95], Volume III of Feynman,
Leighton and Sands[FLS65a], and Cohen-Tannoudji, Diu and Lalo¨e[CTDL77a, CTDL77b].
Problem 1.1 was suggested by the lovely article of Rahim[Rah99].
2 Introduction to quantum mechanics
I ain’t no physicist but I know what matters.
– Popeye the Sailor
Quantum mechanics: Real Black Magic Calculus
– Albert Einstein
Quantum mechanics is the most accurate and complete description of the world known. It
is also the basis for an understanding of quantum computation and quantum information.
This chapter provides all the necessary background knowledge of quantum mechanics
needed for a thorough grasp of quantum computation and quantum information. No
prior knowledge of quantum mechanics is assumed.
Quantum mechanics is easy to learn, despite its reputation as a difﬁcult subject. The
reputation comes from the difﬁculty of some applications, like understanding the struc-
ture of complicated molecules, which aren’t fundamental to a grasp of the subject; we
won’t be discussing such applications. The only prerequisite for understanding is some
familiarity with elementary linear algebra. Provided you have this background you can
begin working out simple problems in a few hours, even with no prior knowledge of the
subject.
Readers already familiar with quantum mechanics can quickly skim through this chap-
ter, to become familiar with our (mostly standard) notational conventions, and to assure
themselves of familiarity with all the material. Readers with little or no prior knowledge
should work through the chapter in detail, pausing to attempt the exercises. If you have
difﬁculty with an exercise, move on, and return later to make another attempt.
The chapter begins with a review of some material from linear algebra in Section 2.1.
This section assumes familiarity with elementary linear algebra, but introduces the nota-
tion used by physicists to describe quantum mechanics, which is different to that used in
most introductions to linear algebra. Section 2.2 describes the basic postulates of quan-
tum mechanics. Upon completion of the section, you will have understood all of the
fundamental principles of quantum mechanics. This section contains numerous simple
exercises designed to help consolidate your grasp of this material. The remaining sections
of the chapter, and of this book, elucidate upon this material, without introducing fun-
damentally new physical principles. Section 2.3 explains superdense coding, a surprising
and illuminating example of quantum information processing which combines many of
the postulates of quantum mechanics in a simple setting. Sections 2.4 and 2.5 develop
powerful mathematical tools – the density operator, puriﬁcations, and the Schmidt de-
composition – which are especially useful in the study of quantum computation and
quantum information. Understanding these tools will also help you consolidate your un-
derstanding of elementary quantum mechanics. Finally, Section 2.6 examines the question
of how quantum mechanics goes beyond the usual ‘classical’ understanding of the way
the world works.
Linear algebra
61
2.1
Linear algebra
This book is written as much to disturb and annoy as to instruct.
– The ﬁrst line of About Vectors, by Banesh Hoffmann.
Life is complex – it has both real and imaginary parts.
– Anonymous
Linear algebra is the study of vector spaces and of linear operations on those vector
spaces. A good understanding of quantum mechanics is based upon a solid grasp of
elementary linear algebra. In this section we review some basic concepts from linear
algebra, and describe the standard notations which are used for these concepts in the
study of quantum mechanics. These notations are summarized in Figure 2.1 on page 62,
with the quantum notation in the left column, and the linear-algebraic description in the
right column. You may like to glance at the table, and see how many of the concepts in
the right column you recognize.
In our opinion the chief obstacle to assimilation of the postulates of quantum mechan-
ics is not the postulates themselves, but rather the large body of linear algebraic notions
required to understand them. Coupled with the unusual Dirac notation adopted by physi-
cists for quantum mechanics, it can appear (falsely) quite fearsome. For these reasons,
we advise the reader not familiar with quantum mechanics to quickly read through the
material which follows, pausing mainly to concentrate on understanding the absolute ba-
sics of the notation being used. Then proceed to a careful study of the main topic of the
chapter – the postulates of quantum mechanics – returning to study the necessary linear
algebraic notions and notations in more depth, as required.
The basic objects of linear algebra are vector spaces. The vector space of most interest
to us is Cn, the space of all n-tuples of complex numbers, (z1, . . . , zn). The elements of
a vector space are called vectors, and we will sometimes use the column matrix notation
⎡
⎢⎣
z1
...
zn
⎤
⎥⎦
(2.1)
to indicate a vector. There is an addition operation deﬁned which takes pairs of vectors
to other vectors. In Cn the addition operation for vectors is deﬁned by
⎡
⎢⎣
z1
...
zn
⎤
⎥⎦+
⎡
⎢⎣
z′
1...
z′
n
⎤
⎥⎦≡
⎡
⎢⎣
z1 + z′
1
...
zn + z′
n
⎤
⎥⎦,
(2.2)
where the addition operations on the right are just ordinary additions of complex numbers.
Furthermore, in a vector space there is a multiplication by a scalar operation. In Cn
this operation is deﬁned by
z
⎡
⎢⎣
z1
...
zn
⎤
⎥⎦≡
⎡
⎢⎣
zz1
...
zzn
⎤
⎥⎦,
(2.3)
62
Introduction to quantum mechanics
where z is a scalar, that is, a complex number, and the multiplications on the right
are ordinary multiplication of complex numbers. Physicists sometimes refer to complex
numbers as c-numbers.
Quantum mechanics is our main motivation for studying linear algebra, so we will use
the standard notation of quantum mechanics for linear algebraic concepts. The standard
quantum mechanical notation for a vector in a vector space is the following:
|ψ⟩.
(2.4)
ψ is a label for the vector (any label is valid, although we prefer to use simple labels like
ψ and ϕ). The |·⟩notation is used to indicate that the object is a vector. The entire object
|ψ⟩is sometimes called a ket, although we won’t use that terminology often.
A vector space also contains a special zero vector, which we denote by 0. It satisﬁes
the property that for any other vector |v⟩, |v⟩+ 0 = |v⟩. Note that we do not use the
ket notation for the zero vector – it is the only exception we shall make. The reason
for making the exception is because it is conventional to use the ‘obvious’ notation for
the zero vector, |0⟩, to mean something else entirely. The scalar multiplication operation
is such that z0 = 0 for any complex number z. For convenience, we use the notation
(z1, . . . , zn) to denote a column matrix with entries z1, . . . , zn. In Cn the zero element
is (0, 0, . . . , 0). A vector subspace of a vector space V is a subset W of V such that W is
also a vector space, that is, W must be closed under scalar multiplication and addition.
Notation
Description
z∗
Complex conjugate of the complex number z.
(1 + i)∗= 1 −i
|ψ⟩
Vector. Also known as a ket.
⟨ψ|
Vector dual to |ψ⟩. Also known as a bra.
⟨ϕ|ψ⟩
Inner product between the vectors |ϕ⟩and |ψ⟩.
|ϕ⟩⊗|ψ⟩
Tensor product of |ϕ⟩and |ψ⟩.
|ϕ⟩|ψ⟩
Abbreviated notation for tensor product of |ϕ⟩and |ψ⟩.
A∗
Complex conjugate of the A matrix.
AT
Transpose of the A matrix.
A†
Hermitian conjugate or adjoint of the A matrix, A† = (AT )∗.
 a
b
c
d
†
=
 a∗
c∗
b∗
d∗

.
⟨ϕ|A|ψ⟩
Inner product between |ϕ⟩and A|ψ⟩.
Equivalently, inner product between A†|ϕ⟩and |ψ⟩.
Figure 2.1. Summary of some standard quantum mechanical notation for notions from linear algebra. This style of
notation is known as the Dirac notation.
2.1.1
Bases and linear independence
A spanning set for a vector space is a set of vectors |v1⟩, . . . , |vn⟩such that any vector
|v⟩in the vector space can be written as a linear combination |v⟩= 
i ai|vi⟩of vectors
Linear algebra
63
in that set. For example, a spanning set for the vector space C2 is the set
|v1⟩≡
 1
0

;
|v2⟩≡
 0
1

,
(2.5)
since any vector
|v⟩=
 a1
a2

(2.6)
in C2 can be written as a linear combination |v⟩= a1|v1⟩+ a2|v2⟩of the vectors |v1⟩and
|v2⟩. We say that the vectors |v1⟩and |v2⟩span the vector space C2.
Generally, a vector space may have many different spanning sets. A second spanning
set for the vector space C2 is the set
|v1⟩≡
1
√
2
 1
1

;
|v2⟩≡
1
√
2

1
−1

,
(2.7)
since an arbitrary vector |v⟩= (a1, a2) can be written as a linear combination of |v1⟩and
|v2⟩,
|v⟩= a1 + a2
√
2
|v1⟩+ a1 −a2
√
2
|v2⟩.
(2.8)
A set of non-zero vectors |v1⟩, . . . , |vn⟩are linearly dependent if there exists a set of
complex numbers a1, . . . , an with ai ̸= 0 for at least one value of i, such that
a1|v1⟩+ a2|v2⟩+ · · · + an|vn⟩= 0.
(2.9)
A set of vectors is linearly independent if it is not linearly dependent. It can be shown
that any two sets of linearly independent vectors which span a vector space V contain the
same number of elements. We call such a set a basis for V . Furthermore, such a basis
set always exists. The number of elements in the basis is deﬁned to be the dimension of
V . In this book we will only be interested in ﬁnite dimensional vector spaces. There are
many interesting and often difﬁcult questions associated with inﬁnite dimensional vector
spaces. We won’t need to worry about these questions.
Exercise 2.1: (Linear dependence: example)
Show that (1, −1), (1, 2) and (2, 1)
are linearly dependent.
2.1.2
Linear operators and matrices
A linear operator between vector spaces V and W is deﬁned to be any function A :
V →W which is linear in its inputs,
A

i
ai|vi⟩

=

i
aiA

|vi⟩
 .
(2.10)
Usually we just write A|v⟩to denote A(|v⟩). When we say that a linear operator A is
deﬁned on a vector space, V , we mean that A is a linear operator from V to V . An
important linear operator on any vector space V is the identity operator, IV , deﬁned by
the equation IV |v⟩≡|v⟩for all vectors |v⟩. Where no chance of confusion arises we drop
the subscript V and just write I to denote the identity operator. Another important linear
operator is the zero operator, which we denote 0. The zero operator maps all vectors to
64
Introduction to quantum mechanics
the zero vector, 0|v⟩≡0. It is clear from (2.10) that once the action of a linear operator
A on a basis is speciﬁed, the action of A is completely determined on all inputs.
Suppose V, W, and X are vector spaces, and A : V →W and B : W →X are
linear operators. Then we use the notation BA to denote the composition of B with A,
deﬁned by (BA)(|v⟩) ≡B(A(|v⟩)). Once again, we write BA|v⟩as an abbreviation for
(BA)(|v⟩).
The most convenient way to understand linear operators is in terms of their matrix
representations. In fact, the linear operator and matrix viewpoints turn out to be com-
pletely equivalent. The matrix viewpoint may be more familiar to you, however. To see
the connection, it helps to ﬁrst understand that an m by n complex matrix A with entries
Aij is in fact a linear operator sending vectors in the vector space Cn to the vector space
Cm, under matrix multiplication of the matrix A by a vector in Cn. More precisely, the
claim that the matrix A is a linear operator just means that
A

i
ai|vi⟩

=

i
aiA|vi⟩
(2.11)
is true as an equation where the operation is matrix multiplication of A by column vectors.
Clearly, this is true!
We’ve seen that matrices can be regarded as linear operators. Can linear operators
be given a matrix representation? In fact they can, as we now explain. This equivalence
between the two viewpoints justiﬁes our interchanging terms from matrix theory and
operator theory throughout the book. Suppose A : V →W is a linear operator between
vector spaces V and W. Suppose |v1⟩, . . . , |vm⟩is a basis for V and |w1⟩, . . ., |wn⟩is a
basis for W. Then for each j in the range 1, . . . , m, there exist complex numbers A1j
through Anj such that
A|vj⟩=

i
Aij|wi⟩.
(2.12)
The matrix whose entries are the values Aij is said to form a matrix representation of the
operator A. This matrix representation of A is completely equivalent to the operator A,
and we will use the matrix representation and abstract operator viewpoints interchange-
ably. Note that to make the connection between matrices and linear operators we must
specify a set of input and output basis states for the input and output vector spaces of
the linear operator.
Exercise 2.2: (Matrix representations: example)
Suppose V is a vector space
with basis vectors |0⟩and |1⟩, and A is a linear operator from V to V such that
A|0⟩= |1⟩and A|1⟩= |0⟩. Give a matrix representation for A, with respect to
the input basis |0⟩, |1⟩, and the output basis |0⟩, |1⟩. Find input and output bases
which give rise to a different matrix representation of A.
Exercise 2.3: (Matrix representation for operator products)
Suppose A is a
linear operator from vector space V to vector space W, and B is a linear
operator from vector space W to vector space X. Let |vi⟩, |wj⟩, and |xk⟩be
bases for the vector spaces V, W, and X, respectively. Show that the matrix
representation for the linear transformation BA is the matrix product of the
matrix representations for B and A, with respect to the appropriate bases.
Linear algebra
65
Exercise 2.4: (Matrix representation for identity)
Show that the identity operator
on a vector space V has a matrix representation which is one along the diagonal
and zero everywhere else, if the matrix representation is taken with respect to the
same input and output bases. This matrix is known as the identity matrix.
2.1.3
The Pauli matrices
Four extremely useful matrices which we shall often have occasion to use are the Pauli
matrices. These are 2 by 2 matrices, which go by a variety of notations. The matrices,
and their corresponding notations, are depicted in Figure 2.2. The Pauli matrices are so
useful in the study of quantum computation and quantum information that we encourage
you to memorize them by working through in detail the many examples and exercises
based upon them in subsequent sections.
σ0 ≡I ≡
 1
0
0
1

σ2 ≡σy ≡Y ≡
 0
−i
i
0

σ1 ≡σx ≡X ≡
 0
1
1
0

σ3 ≡σz ≡Z ≡
 1
0
0
−1

Figure 2.2. The Pauli matrices. Sometimes I is omitted from the list with just X, Y and Z known as the Pauli
matrices.
2.1.4
Inner products
An inner product is a function which takes as input two vectors |v⟩and |w⟩from a vector
space and produces a complex number as output. For the time being, it will be convenient
to write the inner product of |v⟩and |w⟩as (|v⟩, |w⟩). This is not the standard quantum
mechanical notation; for pedagogical clarity the (·, ·) notation will be useful occasionally in
this chapter. The standard quantum mechanical notation for the inner product (|v⟩, |w⟩)
is ⟨v|w⟩, where |v⟩and |w⟩are vectors in the inner product space, and the notation ⟨v|
is used for the dual vector to the vector |v⟩; the dual is a linear operator from the inner
product space V to the complex numbers C, deﬁned by ⟨v|(|w⟩) ≡⟨v|w⟩≡(|v⟩, |w⟩).
We will see shortly that the matrix representation of dual vectors is just a row vector.
A function (·, ·) from V × V to C is an inner product if it satisﬁes the requirements
that:
(1) (·, ·) is linear in the second argument,

|v⟩,

i
λi|wi⟩

=

i
λi

|v⟩, |wi⟩
 .
(2.13)
(2) (|v⟩, |w⟩) = (|w⟩, |v⟩)∗.
(3) (|v⟩, |v⟩) ≥0 with equality if and only if |v⟩= 0.
For example, Cn has an inner product deﬁned by
((y1, . . . , yn), (z1, . . . , zn)) ≡

i
y∗
i zi =
y∗
1 . . . y∗
n

⎡
⎢⎣
z1
...
zn
⎤
⎥⎦.
(2.14)
66
Introduction to quantum mechanics
We call a vector space equipped with an inner product an inner product space.
Exercise 2.5:
Verify that (·, ·) just deﬁned is an inner product on Cn.
Exercise 2.6:
Show that any inner product (·, ·) is conjugate-linear in the ﬁrst
argument,

i
λi|wi⟩, |v⟩

=

i
λ∗
i (|wi⟩, |v⟩).
(2.15)
Discussions of quantum mechanics often refer to Hilbert space. In the ﬁnite dimen-
sional complex vector spaces that come up in quantum computation and quantum infor-
mation, a Hilbert space is exactly the same thing as an inner product space. From now
on we use the two terms interchangeably, preferring the term Hilbert space. In inﬁnite
dimensions Hilbert spaces satisfy additional technical restrictions above and beyond inner
product spaces, which we will not need to worry about.
Vectors |w⟩and |v⟩are orthogonal if their inner product is zero. For example, |w⟩≡
(1, 0) and |v⟩≡(0, 1) are orthogonal with respect to the inner product deﬁned by (2.14).
We deﬁne the norm of a vector |v⟩by
∥|v⟩∥≡

⟨v|v⟩.
(2.16)
A unit vector is a vector |v⟩such that ∥|v⟩∥= 1. We also say that |v⟩is normalized if
∥|v⟩∥= 1. It is convenient to talk of normalizing a vector by dividing by its norm; thus
|v⟩/∥|v⟩∥is the normalized form of |v⟩, for any non-zero vector |v⟩. A set |i⟩of vectors
with index i is orthonormal if each vector is a unit vector, and distinct vectors in the set
are orthogonal, that is, ⟨i|j⟩= δij, where i and j are both chosen from the index set.
Exercise 2.7:
Verify that |w⟩≡(1, 1) and |v⟩≡(1, −1) are orthogonal. What are the
normalized forms of these vectors?
Suppose |w1⟩, . . . , |wd⟩is a basis set for some vector space V with an inner product.
There is a useful method, the Gram–Schmidt procedure, which can be used to produce an
orthonormal basis set |v1⟩, . . . , |vd⟩for the vector space V . Deﬁne |v1⟩≡|w1⟩/∥|w1⟩∥,
and for 1 ≤k ≤d −1 deﬁne |vk+1⟩inductively by
|vk+1⟩≡
|wk+1⟩−k
i=1⟨vi|wk+1⟩|vi⟩
∥|wk+1⟩−k
i=1⟨vi|wk+1⟩|vi⟩∥
.
(2.17)
It is not difﬁcult to verify that the vectors |v1⟩, . . ., |vd⟩form an orthonormal set which
is also a basis for V . Thus, any ﬁnite dimensional vector space of dimension d has an
orthonormal basis, |v1⟩, . . . , |vd⟩.
Exercise 2.8:
Prove that the Gram–Schmidt procedure produces an orthonormal basis
for V .
From now on, when we speak of a matrix representation for a linear operator, we mean
a matrix representation with respect to orthonormal input and output bases. We also use
the convention that if the input and output spaces for a linear operator are the same, then
the input and output bases are the same, unless noted otherwise.
Linear algebra
67
With these conventions, the inner product on a Hilbert space can be given a convenient
matrix representation. Let |w⟩= 
i wi|i⟩and |v⟩= 
j vj|j⟩be representations of
vectors |w⟩and |v⟩with respect to some orthonormal basis |i⟩. Then, since ⟨i|j⟩= δij,
⟨v|w⟩=
⎛
⎝
i
vi|i⟩,

j
wj|j⟩
⎞
⎠=

ij
v∗
i wjδij =

i
v∗
i wi
(2.18)
=
v∗
1 . . . v∗
n

⎡
⎢⎣
w1
...
wn
⎤
⎥⎦.
(2.19)
That is, the inner product of two vectors is equal to the vector inner product between
two matrix representations of those vectors, provided the representations are written
with respect to the same orthonormal basis. We also see that the dual vector ⟨v| has a
nice interpretation as the row vector whose components are complex conjugates of the
corresponding components of the column vector representation of |v⟩.
There is a useful way of representing linear operators which makes use of the inner
product, known as the outer product representation. Suppose |v⟩is a vector in an inner
product space V , and |w⟩is a vector in an inner product space W. Deﬁne |w⟩⟨v| to be
the linear operator from V to W whose action is deﬁned by

|w⟩⟨v|
 
|v′⟩
 ≡|w⟩⟨v|v′⟩= ⟨v|v′⟩|w⟩.
(2.20)
This equation ﬁts beautifully into our notational conventions, according to which the
expression |w⟩⟨v|v′⟩could potentially have one of two meanings: we will use it to denote
the result when the operator |w⟩⟨v| acts on |v′⟩, and it has an existing interpretation as
the result of multiplying |w⟩by the complex number ⟨v|v′⟩. Our deﬁnitions are chosen
so that these two potential meanings coincide. Indeed, we deﬁne the former in terms of
the latter!
We can take linear combinations of outer product operators |w⟩⟨v| in the obvious way.
By deﬁnition 
i ai|wi⟩⟨vi| is the linear operator which, when acting on |v′⟩, produces

i ai|wi⟩⟨vi|v′⟩as output.
The usefulness of the outer product notation can be discerned from an important result
known as the completeness relation for orthonormal vectors. Let |i⟩be any orthonormal
basis for the vector space V , so an arbitrary vector |v⟩can be written |v⟩= 
i vi|i⟩for
some set of complex numbers vi. Note that ⟨i|v⟩= vi and therefore

i
|i⟩⟨i|

|v⟩=

i
|i⟩⟨i|v⟩=

i
vi|i⟩= |v⟩.
(2.21)
Since the last equation is true for all |v⟩it follows that

i
|i⟩⟨i| = I.
(2.22)
This equation is known as the completeness relation. One application of the completeness
relation is to give a means for representing any operator in the outer product notation.
Suppose A : V →W is a linear operator, |vi⟩is an orthonormal basis for V , and |wj⟩
an orthonormal basis for W. Using the completeness relation twice we obtain
A = IW AIV
(2.23)
68
Introduction to quantum mechanics
=

ij
|wj⟩⟨wj|A|vi⟩⟨vi|
(2.24)
=

ij
⟨wj|A|vi⟩|wj⟩⟨vi|,
(2.25)
which is the outer product representation for A. We also see from this equation that A
has matrix element ⟨wj|A|vi⟩in the ith column and jth row, with respect to the input
basis |vi⟩and output basis |wj⟩.
A second application illustrating the usefulness of the completeness relation is the
Cauchy–Schwarz inequality. This important result is discussed in Box 2.1, on this
page.
Exercise 2.9: (Pauli operators and the outer product)
The Pauli matrices
(Figure 2.2 on page 65) can be considered as operators with respect to an
orthonormal basis |0⟩, |1⟩for a two-dimensional Hilbert space. Express each of
the Pauli operators in the outer product notation.
Exercise 2.10:
Suppose |vi⟩is an orthonormal basis for an inner product space V .
What is the matrix representation for the operator |vj⟩⟨vk|, with respect to the
|vi⟩basis?
Box 2.1: The Cauchy-Schwarz inequality
The Cauchy–Schwarz inequality is an important geometric fact about Hilbert
spaces. It states that for any two vectors |v⟩and |w⟩, |⟨v|w⟩|2 ≤⟨v|v⟩⟨w|w⟩. To
see this, use the Gram–Schmidt procedure to construct an orthonormal basis |i⟩
for the vector space such that the ﬁrst member of the basis |i⟩is |w⟩/

⟨w|w⟩.
Using the completeness relation 
i |i⟩⟨i| = I, and dropping some non-negative
terms gives
⟨v|v⟩⟨w|w⟩=

i
⟨v|i⟩⟨i|v⟩⟨w|w⟩
(2.26)
≥⟨v|w⟩⟨w|v⟩
⟨w|w⟩
⟨w|w⟩
(2.27)
= ⟨v|w⟩⟨w|v⟩= |⟨v|w⟩|2,
(2.28)
as required. A little thought shows that equality occurs if and only if |v⟩and |w⟩
are linearly related, |v⟩= z|w⟩or |w⟩= z|v⟩, for some scalar z.
2.1.5
Eigenvectors and eigenvalues
An eigenvector of a linear operator A on a vector space is a non-zero vector |v⟩such that
A|v⟩= v|v⟩, where v is a complex number known as the eigenvalue of A corresponding
to |v⟩. It will often be convenient to use the notation v both as a label for the eigenvector,
and to represent the eigenvalue. We assume that you are familiar with the elementary
properties of eigenvalues and eigenvectors – in particular, how to ﬁnd them, via the
characteristic equation. The characteristic function is deﬁned to be c(λ) ≡det |A−λI|,
Linear algebra
69
where det is the determinant function for matrices; it can be shown that the characteristic
function depends only upon the operator A, and not on the speciﬁc matrix representation
used for A. The solutions of the characteristic equation c(λ) = 0 are the eigenvalues
of the operator A. By the fundamental theorem of algebra, every polynomial has at least
one complex root, so every operator A has at least one eigenvalue, and a corresponding
eigenvector. The eigenspace corresponding to an eigenvalue v is the set of vectors which
have eigenvalue v. It is a vector subspace of the vector space on which A acts.
A diagonal representation for an operator A on a vector space V is a representation
A = 
i λi|i⟩⟨i|, where the vectors |i⟩form an orthonormal set of eigenvectors for A,
with corresponding eigenvalues λi. An operator is said to be diagonalizable if it has a
diagonal representation. In the next section we will ﬁnd a simple set of necessary and
sufﬁcient conditions for an operator on a Hilbert space to be diagonalizable. As an example
of a diagonal representation, note that the Pauli Z matrix may be written
Z =
 1
0
0
−1

= |0⟩⟨0| −|1⟩⟨1|,
(2.29)
where the matrix representation is with respect to orthonormal vectors |0⟩and |1⟩, re-
spectively. Diagonal representations are sometimes also known as orthonormal decom-
positions.
When an eigenspace is more than one dimensional we say that it is degenerate. For
example, the matrix A deﬁned by
A ≡
⎡
⎣
2
0
0
0
2
0
0
0
0
⎤
⎦
(2.30)
has a two-dimensional eigenspace corresponding to the eigenvalue 2. The eigenvectors
(1, 0, 0) and (0, 1, 0) are said to be degenerate because they are linearly independent
eigenvectors of A with the same eigenvalue.
Exercise 2.11: (Eigendecomposition of the Pauli matrices)
Find the
eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices
X, Y , and Z.
Exercise 2.12:
Prove that the matrix
 1
0
1
1

(2.31)
is not diagonalizable.
2.1.6
Adjoints and Hermitian operators
Suppose A is any linear operator on a Hilbert space, V . It turns out that there exists a
unique linear operator A† on V such that for all vectors |v⟩, |w⟩∈V ,
(|v⟩, A|w⟩) = (A†|v⟩, |w⟩).
(2.32)
This linear operator is known as the adjoint or Hermitian conjugate of the operator
A. From the deﬁnition it is easy to see that (AB)† = B†A†. By convention, if |v⟩is
a vector, then we deﬁne |v⟩† ≡⟨v|. With this deﬁnition it is not difﬁcult to see that
(A|v⟩)† = ⟨v|A†.
70
Introduction to quantum mechanics
Exercise 2.13:
If |w⟩and |v⟩are any two vectors, show that (|w⟩⟨v|)† = |v⟩⟨w|.
Exercise 2.14: (Anti-linearity of the adjoint)
Show that the adjoint operation is
anti-linear,

i
aiAi
†
=

i
a∗
i A†
i.
(2.33)
Exercise 2.15:
Show that (A†)† = A.
In a matrix representation of an operator A, the action of the Hermitian conjugation
operation is to take the matrix of A to the conjugate-transpose matrix, A† ≡(A∗)T ,
where the ∗indicates complex conjugation, and T indicates the transpose operation. For
example, we have
 1 + 3i
2i
1 + i
1 −4i
†
=
 1 −3i
1 −i
−2i
1 + 4i

.
(2.34)
An operator A whose adjoint is A is known as a Hermitian or self-adjoint op-
erator. An important class of Hermitian operators is the projectors. Suppose W is a
k-dimensional vector subspace of the d-dimensional vector space V . Using the Gram–
Schmidt procedure it is possible to construct an orthonormal basis |1⟩, . . ., |d⟩for V
such that |1⟩, . . . , |k⟩is an orthonormal basis for W. By deﬁnition,
P ≡
k

i=1
|i⟩⟨i|
(2.35)
is the projector onto the subspace W. It is easy to check that this deﬁnition is independent
of the orthonormal basis |1⟩, . . . , |k⟩used for W. From the deﬁnition it can be shown that
|v⟩⟨v| is Hermitian for any vector |v⟩, so P is Hermitian, P † = P. We will often refer
to the ‘vector space’ P, as shorthand for the vector space onto which P is a projector.
The orthogonal complement of P is the operator Q ≡I −P. It is easy to see that Q is
a projector onto the vector space spanned by |k + 1⟩, . . . , |d⟩, which we also refer to as
the orthogonal complement of P, and may denote by Q.
Exercise 2.16:
Show that any projector P satisﬁes the equation P 2 = P.
An operator A is said to be normal if AA† = A†A. Clearly, an operator which
is Hermitian is also normal. There is a remarkable representation theorem for normal
operators known as the spectral decomposition, which states that an operator is a normal
operator if and only if it is diagonalizable. This result is proved in Box 2.2 on page 72,
which you should read closely.
Exercise 2.17:
Show that a normal matrix is Hermitian if and only if it has real
eigenvalues.
A matrix U is said to be unitary if U †U = I. Similarly an operator U is unitary if
U †U = I. It is easily checked that an operator is unitary if and only if each of its matrix
representations is unitary. A unitary operator also satisﬁes UU † = I, and therefore U is
normal and has a spectral decomposition. Geometrically, unitary operators are important
because they preserve inner products between vectors. To see this, let |v⟩and |w⟩be any
Linear algebra
71
two vectors. Then the inner product of U|v⟩and U|w⟩is the same as the inner product
of |v⟩and |w⟩,

U|v⟩, U|w⟩
 = ⟨v|U †U|w⟩= ⟨v|I|w⟩= ⟨v|w⟩.
(2.36)
This result suggests the following elegant outer product representation of any unitary U.
Let |vi⟩be any orthonormal basis set. Deﬁne |wi⟩≡U|vi⟩, so |wi⟩is also an orthonormal
basis set, since unitary operators preserve inner products. Note that U = 
i |wi⟩⟨vi|.
Conversely, if |vi⟩and |wi⟩are any two orthonormal bases, then it is easily checked that
the operator U deﬁned by U ≡
i |wi⟩⟨vi| is a unitary operator.
Exercise 2.18:
Show that all eigenvalues of a unitary matrix have modulus 1, that is,
can be written in the form eiθ for some real θ.
Exercise 2.19: (Pauli matrices: Hermitian and unitary)
Show that the Pauli
matrices are Hermitian and unitary.
Exercise 2.20: (Basis changes)
Suppose A′ and A′′ are matrix representations of an
operator A on a vector space V with respect to two different orthonormal bases,
|vi⟩and |wi⟩. Then the elements of A′ and A′′ are A′
ij = ⟨vi|A|vj⟩and
A′′
ij = ⟨wi|A|wj⟩. Characterize the relationship between A′ and A′′.
A special subclass of Hermitian operators is extremely important. This is the positive
operators. A positive operator A is deﬁned to be an operator such that for any vector |v⟩,
(|v⟩, A|v⟩) is a real, non-negative number. If (|v⟩, A|v⟩) is strictly greater than zero for
all |v⟩̸= 0 then we say that A is positive deﬁnite. In Exercise 2.24 on this page you will
show that any positive operator is automatically Hermitian, and therefore by the spectral
decomposition has diagonal representation 
i λi|i⟩⟨i|, with non-negative eigenvalues λi.
Exercise 2.21:
Repeat the proof of the spectral decomposition in Box 2.2 for the case
when M is Hermitian, simplifying the proof wherever possible.
Exercise 2.22:
Prove that two eigenvectors of a Hermitian operator with different
eigenvalues are necessarily orthogonal.
Exercise 2.23:
Show that the eigenvalues of a projector P are all either 0 or 1.
Exercise 2.24: (Hermiticity of positive operators)
Show that a positive operator
is necessarily Hermitian. (Hint: Show that an arbitrary operator A can be
written A = B + iC where B and C are Hermitian.)
Exercise 2.25:
Show that for any operator A, A†A is positive.
2.1.7
Tensor products
The tensor product is a way of putting vector spaces together to form larger vector spaces.
This construction is crucial to understanding the quantum mechanics of multiparticle
systems. The following discussion is a little abstract, and may be difﬁcult to follow if
you’re not already familiar with the tensor product, so feel free to skip ahead now and
revisit later when you come to the discussion of tensor products in quantum mechanics.
Suppose V and W are vector spaces of dimension m and n respectively. For conve-
nience we also suppose that V and W are Hilbert spaces. Then V ⊗W (read ‘V tensor
72
Introduction to quantum mechanics
Box 2.2: The spectral decomposition – important!
The spectral decomposition is an extremely useful representation theorem for nor-
mal operators.
Theorem 2.1: (Spectral decomposition) Any normal operator M on a vector
space V is diagonal with respect to some orthonormal basis for V .
Conversely, any diagonalizable operator is normal.
Proof
The converse is a simple exercise, so we prove merely the forward implication,
by induction on the dimension d of V . The case d = 1 is trivial. Let λ be an
eigenvalue of M, P the projector onto the λ eigenspace, and Q the projector onto
the orthogonal complement. Then M = (P + Q)M(P + Q) = PMP + QMP +
PMQ + QMQ. Obviously PMP = λP. Furthermore, QMP = 0, as M takes
the subspace P into itself. We claim that PMQ = 0 also. To see this, let |v⟩
be an element of the subspace P. Then MM †|v⟩= M †M|v⟩= λM †|v⟩. Thus,
M †|v⟩has eigenvalue λ and therefore is an element of the subspace P. It follows
that QM †P = 0. Taking the adjoint of this equation gives PMQ = 0. Thus
M = PMP + QMQ. Next, we prove that QMQ is normal. To see this, note that
QM = QM(P + Q) = QMQ, and QM † = QM †(P + Q) = QM †Q. Therefore,
by the normality of M, and the observation that Q2 = Q,
QMQ QM †Q = QMQM †Q
(2.37)
= QMM †Q
(2.38)
= QM †MQ
(2.39)
= QM †QMQ
(2.40)
= QM †Q QMQ ,
(2.41)
so QMQ is normal. By induction, QMQ is diagonal with respect to some or-
thonormal basis for the subspace Q, and PMP is already diagonal with respect
to some orthonormal basis for P. It follows that M = PMP + QMQ is diagonal
with respect to some orthonormal basis for the total vector space.
In terms of the outer product representation, this means that M can be written as
M = 
i λi|i⟩⟨i|, where λi are the eigenvalues of M, |i⟩is an orthonormal basis
for V , and each |i⟩an eigenvector of M with eigenvalue λi. In terms of projectors,
M = 
i λiPi, where λi are again the eigenvalues of M, and Pi is the projector
onto the λi eigenspace of M. These projectors satisfy the completeness relation

i Pi = I, and the orthonormality relation PiPj = δijPi.
W’) is an mn dimensional vector space. The elements of V ⊗W are linear combinations
of ‘tensor products’ |v⟩⊗|w⟩of elements |v⟩of V and |w⟩of W. In particular, if |i⟩and
|j⟩are orthonormal bases for the spaces V and W then |i⟩⊗|j⟩is a basis for V ⊗W. We
often use the abbreviated notations |v⟩|w⟩, |v, w⟩or even |vw⟩for the tensor product
Linear algebra
73
|v⟩⊗|w⟩. For example, if V is a two-dimensional vector space with basis vectors |0⟩and
|1⟩then |0⟩⊗|0⟩+ |1⟩⊗|1⟩is an element of V ⊗V .
By deﬁnition the tensor product satisﬁes the following basic properties:
(1) For an arbitrary scalar z and elements |v⟩of V and |w⟩of W,
z

|v⟩⊗|w⟩
 =

z|v⟩
 ⊗|w⟩= |v⟩⊗

z|w⟩
 .
(2.42)
(2) For arbitrary |v1⟩and |v2⟩in V and |w⟩in W,

|v1⟩+ |v2⟩
 ⊗|w⟩= |v1⟩⊗|w⟩+ |v2⟩⊗|w⟩.
(2.43)
(3) For arbitrary |v⟩in V and |w1⟩and |w2⟩in W,
|v⟩⊗

|w1⟩+ |w2⟩
 = |v⟩⊗|w1⟩+ |v⟩⊗|w2⟩.
(2.44)
What sorts of linear operators act on the space V ⊗W? Suppose |v⟩and |w⟩are
vectors in V and W, and A and B are linear operators on V and W, respectively. Then
we can deﬁne a linear operator A ⊗B on V ⊗W by the equation
(A ⊗B)(|v⟩⊗|w⟩) ≡A|v⟩⊗B|w⟩.
(2.45)
The deﬁnition of A ⊗B is then extended to all elements of V ⊗W in the natural way
to ensure linearity of A ⊗B, that is,
(A ⊗B)

i
ai|vi⟩⊗|wi⟩

≡

i
aiA|vi⟩⊗B|wi⟩.
(2.46)
It can be shown that A ⊗B deﬁned in this way is a well-deﬁned linear operator on
V ⊗W. This notion of the tensor product of two operators extends in the obvious way
to the case where A : V →V ′ and B : W →W ′ map between different vector spaces.
Indeed, an arbitrary linear operator C mapping V ⊗W to V ′ ⊗W ′ can be represented
as a linear combination of tensor products of operators mapping V to V ′ and W to W ′,
C =

i
ciAi ⊗Bi,
(2.47)
where by deﬁnition

i
ciAi ⊗Bi

|v⟩⊗|w⟩≡

i
ciAi|v⟩⊗Bi|w⟩.
(2.48)
The inner products on the spaces V and W can be used to deﬁne a natural inner
product on V ⊗W. Deﬁne
⎛
⎝
i
ai|vi⟩⊗|wi⟩,

j
bj|v′
j⟩⊗|w′
j⟩
⎞
⎠≡

ij
a∗
i bj⟨vi|v′
j⟩⟨wi|w′
j⟩.
(2.49)
It can be shown that the function so deﬁned is a well-deﬁned inner product. From this
inner product, the inner product space V ⊗W inherits the other structure we are familiar
with, such as notions of an adjoint, unitarity, normality, and Hermiticity.
All this discussion is rather abstract. It can be made much more concrete by moving
74
Introduction to quantum mechanics
to a convenient matrix representation known as the Kronecker product. Suppose A is
an m by n matrix, and B is a p by q matrix. Then we have the matrix representation:
nq
 
!"
#
A ⊗B ≡
⎡
⎢⎢⎢⎣
A11B
A12B
. . .
A1nB
A21B
A22B
. . .
A2nB
...
...
...
...
Am1B
Am2B
. . .
AmnB
⎤
⎥⎥⎥⎦
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
mp .
(2.50)
In this representation terms like A11B denote p by q submatrices whose entries are
proportional to B, with overall proportionality constant A11. For example, the tensor
product of the vectors (1, 2) and (2, 3) is the vector
 1
2

⊗
 2
3

=
⎡
⎢⎢⎣
1 × 2
1 × 3
2 × 2
2 × 3
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
2
3
4
6
⎤
⎥⎥⎦.
(2.51)
The tensor product of the Pauli matrices X and Y is
X ⊗Y =
 0 · Y
1 · Y
1 · Y
0 · Y

=
⎡
⎢⎢⎣
0
0
0
−i
0
0
i
0
0
−i
0
0
i
0
0
0
⎤
⎥⎥⎦.
(2.52)
Finally, we mention the useful notation |ψ⟩⊗k, which means |ψ⟩tensored with itself k
times. For example |ψ⟩⊗2 = |ψ⟩⊗|ψ⟩. An analogous notation is also used for operators
on tensor product spaces.
Exercise 2.26:
Let |ψ⟩= (|0⟩+ |1⟩)/
√
2. Write out |ψ⟩⊗2 and |ψ⟩⊗3 explicitly, both
in terms of tensor products like |0⟩|1⟩, and using the Kronecker product.
Exercise 2.27:
Calculate the matrix representation of the tensor products of the Pauli
operators (a) X and Z; (b) I and X; (c) X and I. Is the tensor product
commutative?
Exercise 2.28:
Show that the transpose, complex conjugation, and adjoint operations
distribute over the tensor product,
(A ⊗B)∗= A∗⊗B∗; (A ⊗B)T = AT ⊗BT ; (A ⊗B)† = A† ⊗B†.(2.53)
Exercise 2.29:
Show that the tensor product of two unitary operators is unitary.
Exercise 2.30:
Show that the tensor product of two Hermitian operators is Hermitian.
Exercise 2.31:
Show that the tensor product of two positive operators is positive.
Exercise 2.32:
Show that the tensor product of two projectors is a projector.
Exercise 2.33:
The Hadamard operator on one qubit may be written as
H =
1
√
2

(|0⟩+ |1⟩)⟨0| + (|0⟩−|1⟩)⟨1|
	
.
(2.54)
Linear algebra
75
Show explicitly that the Hadamard transform on n qubits, H⊗n, may be written
as
H⊗n =
1
√
2n

x,y
(−1)x·y|x⟩⟨y|.
(2.55)
Write out an explicit matrix representation for H⊗2.
2.1.8
Operator functions
There are many important functions which can be deﬁned for operators and matri-
ces. Generally speaking, given a function f from the complex numbers to the com-
plex numbers, it is possible to deﬁne a corresponding matrix function on normal ma-
trices (or some subclass, such as the Hermitian matrices) by the following construc-
tion. Let A = 
a a|a⟩⟨a| be a spectral decomposition for a normal operator A. Deﬁne
f(A) ≡
a f(a)|a⟩⟨a|. A little thought shows that f(A) is uniquely deﬁned. This pro-
cedure can be used, for example, to deﬁne the square root of a positive operator, the
logarithm of a positive-deﬁnite operator, or the exponential of a normal operator. As an
example,
exp(θZ) =
 eθ
0
0
e−θ

,
(2.56)
since Z has eigenvectors |0⟩and |1⟩.
Exercise 2.34:
Find the square root and logarithm of the matrix
 4
3
3
4

.
(2.57)
Exercise 2.35: (Exponential of the Pauli matrices)
Let ⃗v be any real,
three-dimensional unit vector and θ a real number. Prove that
exp(iθ⃗v · ⃗σ) = cos(θ)I + i sin(θ)⃗v · ⃗σ,
(2.58)
where ⃗v ·⃗σ ≡3
i=1 viσi. This exercise is generalized in Problem 2.1 on page 117.
Another important matrix function is the trace of a matrix. The trace of A is deﬁned
to be the sum of its diagonal elements,
tr(A) ≡

i
Aii.
(2.59)
The trace is easily seen to be cyclic, tr(AB) = tr(BA), and linear, tr(A + B) =
tr(A)+tr(B), tr(zA) = z tr(A), where A and B are arbitrary matrices, and z is a complex
number. Furthermore, from the cyclic property it follows that the trace of a matrix
is invariant under the unitary similarity transformation A →UAU †, as tr(UAU †) =
tr(U †UA) = tr(A). In light of this result, it makes sense to deﬁne the trace of an operator
A to be the trace of any matrix representation of A. The invariance of the trace under
unitary similarity transformations ensures that the trace of an operator is well deﬁned.
As an example of the trace, suppose |ψ⟩is a unit vector and A is an arbitrary op-
erator. To evaluate tr(A|ψ⟩⟨ψ|) use the Gram–Schmidt procedure to extend |ψ⟩to an
76
Introduction to quantum mechanics
orthonormal basis |i⟩which includes |ψ⟩as the ﬁrst element. Then we have
tr(A|ψ⟩⟨ψ|) =

i
⟨i|A|ψ⟩⟨ψ|i⟩
(2.60)
= ⟨ψ|A|ψ⟩.
(2.61)
This result, that tr(A|ψ⟩⟨ψ|) = ⟨ψ|A|ψ⟩is extremely useful in evaluating the trace of an
operator.
Exercise 2.36:
Show that the Pauli matrices except for I have trace zero.
Exercise 2.37: (Cyclic property of the trace)
If A and B are two linear operators
show that
tr(AB) = tr(BA).
(2.62)
Exercise 2.38: (Linearity of the trace)
If A and B are two linear operators, show
that
tr(A + B) = tr(A) + tr(B)
(2.63)
and if z is an arbitrary complex number show that
tr(zA) = ztr(A).
(2.64)
Exercise 2.39: (The Hilbert–Schmidt inner product on operators)
The set LV
of linear operators on a Hilbert space V is obviously a vector space – the sum of
two linear operators is a linear operator, zA is a linear operator if A is a linear
operator and z is a complex number, and there is a zero element 0. An important
additional result is that the vector space LV can be given a natural inner product
structure, turning it into a Hilbert space.
(1) Show that the function (·, ·) on LV × LV deﬁned by
(A, B) ≡tr(A†B)
(2.65)
is an inner product function. This inner product is known as the
Hilbert–Schmidt or trace inner product.
(2) If V has d dimensions show that LV has dimension d2.
(3) Find an orthonormal basis of Hermitian matrices for the Hilbert space LV .
2.1.9
The commutator and anti-commutator
The commutator between two operators A and B is deﬁned to be
[A, B] ≡AB −BA.
(2.66)
If [A, B] = 0, that is, AB = BA, then we say A commutes with B. Similarly, the
anti-commutator of two operators A and B is deﬁned by
{A, B} ≡AB + BA;
(2.67)
we say A anti-commutes with B if {A, B} = 0. It turns out that many important prop-
erties of pairs of operators can be deduced from their commutator and anti-commutator.
Perhaps the most useful relation is the following connection between the commutator and
the property of being able to simultaneously diagonalize Hermitian operators A and B,
Linear algebra
77
that is, write A = 
i ai|i⟩⟨i|, B = 
i bi|i⟩⟨i|, where |i⟩is some common orthonormal
set of eigenvectors for A and B.
Theorem 2.2: (Simultaneous diagonalization theorem) Suppose A and B are
Hermitian operators. Then [A, B] = 0 if and only if there exists an orthonormal
basis such that both A and B are diagonal with respect to that basis. We say that
A and B are simultaneously diagonalizable in this case.
This result connects the commutator of two operators, which is often easy to compute,
to the property of being simultaneously diagonalizable, which is a priori rather difﬁcult
to determine. As an example, consider that
[X, Y ] =
 0
1
1
0
  0
−i
i
0

−
 0
−i
i
0
  0
1
1
0

(2.68)
= 2i
 1
0
0
−1

(2.69)
= 2iZ ,
(2.70)
so X and Y do not commute. You have already shown, in Exercise 2.11, that X and Y
do not have common eigenvectors, as we expect from the simultaneous diagonalization
theorem.
Proof
You can (and should!) easily verify that if A and B are diagonal in the same orthonormal
basis then [A, B] = 0. To show the converse, let |a, j⟩be an orthonormal basis for the
eigenspace Va of A with eigenvalue a; the index j is used to label possible degeneracies.
Note that
AB|a, j⟩= BA|a, j⟩= aB|a, j⟩,
(2.71)
and therefore B|a, j⟩is an element of the eigenspace Va. Let Pa denote the projector
onto the space Va and deﬁne Ba ≡PaBPa. It is easy to see that the restriction of Ba to
the space Va is Hermitian on Va, and therefore has a spectral decomposition in terms of
an orthonormal set of eigenvectors which span the space Va. Let’s call these eigenvectors
|a, b, k⟩, where the indices a and b label the eigenvalues of A and Ba, and k is an extra
index to allow for the possibility of a degenerate Ba. Note that B|a, b, k⟩is an element
of Va, so B|a, b, k⟩= PaB|a, b, k⟩. Moreover we have Pa|a, b, k⟩= |a, b, k⟩, so
B|a, b, k⟩= PaBPa|a, b, k⟩= b|a, b, k⟩.
(2.72)
It follows that |a, b, k⟩is an eigenvector of B with eigenvalue b, and therefore |a, b, k⟩is
an orthonormal set of eigenvectors of both A and B, spanning the entire vector space on
which A and B are deﬁned. That is, A and B are simultaneously diagonalizable.
Exercise 2.40: (Commutation relations for the Pauli matrices)
Verify the
commutation relations
[X, Y ] = 2iZ;
[Y, Z] = 2iX;
[Z, X] = 2iY.
(2.73)
There is an elegant way of writing this using ϵjkl, the antisymmetric tensor on
78
Introduction to quantum mechanics
three indices, for which ϵjkl = 0 except for ϵ123 = ϵ231 = ϵ312 = 1, and
ϵ321 = ϵ213 = ϵ132 = −1:
[σj, σk] = 2i
3

l=1
ϵjklσl.
(2.74)
Exercise 2.41: (Anti-commutation relations for the Pauli matrices)
Verify the
anti-commutation relations
{σi, σj} = 0
(2.75)
where i ̸= j are both chosen from the set 1, 2, 3. Also verify that (i = 0, 1, 2, 3)
σ2
i = I.
(2.76)
Exercise 2.42:
Verify that
AB = [A, B] + {A, B}
2
.
(2.77)
Exercise 2.43:
Show that for j, k = 1, 2, 3,
σjσk = δjkI + i
3

l=1
ϵjklσl.
(2.78)
Exercise 2.44:
Suppose [A, B] = 0, {A, B} = 0, and A is invertible. Show that B
must be 0.
Exercise 2.45:
Show that [A, B]† = [B†, A†].
Exercise 2.46:
Show that [A, B] = −[B, A].
Exercise 2.47:
Suppose A and B are Hermitian. Show that i[A, B] is Hermitian.
2.1.10
The polar and singular value decompositions
The polar and singular value decompositions are useful ways of breaking linear operators
up into simpler parts. In particular, these decompositions allow us to break general linear
operators up into products of unitary operators and positive operators. While we don’t
understand the structure of general linear operators terribly well, we do understand
unitary operators and positive operators in quite some detail. The polar and singular
value decompositions allow us to apply this understanding to better understand general
linear operators.
Theorem 2.3: (Polar decomposition) Let A be a linear operator on a vector space V .
Then there exists unitary U and positive operators J and K such that
A = UJ = KU,
(2.79)
where the unique positive operators J and K satisfying these equations are
deﬁned by J ≡
√
A†A and K ≡
√
AA†. Moreover, if A is invertible then U is
unique.
Linear algebra
79
We call the expression A = UJ the left polar decomposition of A, and A = KU the
right polar decomposition of A. Most often, we’ll omit the ‘right’ or ‘left’ nomenclature,
and use the term ‘polar decomposition’ for both expressions, with context indicating
which is meant.
Proof
J ≡
√
A†A is a positive operator, so it can be given a spectral decomposition, J =

i λi|i⟩⟨i| (λi ≥0). Deﬁne |ψi⟩≡A|i⟩. From the deﬁnition, we see that ⟨ψi|ψi⟩= λ2
i.
Consider for now only those i for which λi ̸= 0. For those i deﬁne |ei⟩≡|ψi⟩/λi, so
the |ei⟩are normalized. Moreover, they are orthogonal, since if i ̸= j then ⟨ei|ej⟩=
⟨i|A†A|j⟩/λiλj = ⟨i|J2|j⟩/λiλj = 0.
We have been considering i such that λi ̸= 0. Now use the Gram–Schmidt procedure
to extend the orthonormal set |ei⟩so it forms an orthonormal basis, which we also label
|ei⟩. Deﬁne a unitary operator U ≡
i |ei⟩⟨i|. When λi ̸= 0 we have UJ|i⟩= λi|ei⟩=
|ψi⟩= A|i⟩. When λi = 0 we have UJ|i⟩= 0 = |ψi⟩. We have proved that the action of
A and UJ agree on the basis |i⟩, and thus that A = UJ.
J is unique, since multiplying A = UJ on the left by the adjoint equation A† = JU †
gives J2 = A†A, from which we see that J =
√
A†A, uniquely. A little thought shows that
if A is invertible, then so is J, so U is uniquely determined by the equation U = AJ−1.
The proof of the right polar decomposition follows, since A = UJ = UJU †U = KU,
where K ≡UJU † is a positive operator. Since AA† = KUU †K = K2 we must have
K =
√
AA†, as claimed.
The singular value decomposition combines the polar decomposition and the spectral
theorem.
Corollary 2.4: (Singular value decomposition) Let A be a square matrix. Then
there exist unitary matrices U and V , and a diagonal matrix D with
non-negative entries such that
A = UDV .
(2.80)
The diagonal elements of D are called the singular values of A.
Proof
By the polar decomposition, A = SJ, for unitary S, and positive J. By the spectral
theorem, J = TDT †, for unitary T and diagonal D with non-negative entries. Setting
U ≡ST and V ≡T † completes the proof.
Exercise 2.48:
What is the polar decomposition of a positive matrix P? Of a unitary
matrix U? Of a Hermitian matrix, H?
Exercise 2.49:
Express the polar decomposition of a normal matrix in the outer
product representation.
Exercise 2.50:
Find the left and right polar decompositions of the matrix
 1
0
1
1

.
(2.81)
80
Introduction to quantum mechanics
2.2
The postulates of quantum mechanics
All understanding begins with our not accepting the world as it appears.
– Alan Kay
The most incomprehensible thing about the world is that it is comprehensible.
– Albert Einstein
Quantum mechanics is a mathematical framework for the development of physical theo-
ries. On its own quantum mechanics doesn’t tell you what laws a physical system must
obey, but it does provide a mathematical and conceptual framework for the development
of such laws. In the next few sections we give a complete description of the basic postu-
lates of quantum mechanics. These postulates provide a connection between the physical
world and the mathematical formalism of quantum mechanics.
The postulates of quantum mechanics were derived after a long process of trial and
(mostly) error, which involved a considerable amount of guessing and fumbling by the
originators of the theory. Don’t be surprised if the motivation for the postulates is not
always clear; even to experts the basic postulates of quantum mechanics appear surprising.
What you should expect to gain in the next few sections is a good working grasp of the
postulates – how to apply them, and when.
2.2.1
State space
The ﬁrst postulate of quantum mechanics sets up the arena in which quantum mechanics
takes place. The arena is our familiar friend from linear algebra, Hilbert space.
Postulate 1: Associated to any isolated physical system is a complex vector space
with inner product (that is, a Hilbert space) known as the state space of the
system. The system is completely described by its state vector, which is a unit
vector in the system’s state space.
Quantum mechanics does not tell us, for a given physical system, what the state space
of that system is, nor does it tell us what the state vector of the system is. Figuring that
out for a speciﬁc system is a difﬁcult problem for which physicists have developed many
intricate and beautiful rules. For example, there is the wonderful theory of quantum
electrodynamics (often known as QED), which describes how atoms and light interact.
One aspect of QED is that it tells us what state spaces to use to give quantum descriptions
of atoms and light. We won’t be much concerned with the intricacies of theories like QED
(except in so far as they apply to physical realizations, in Chapter 7), as we are mostly
interested in the general framework provided by quantum mechanics. For our purposes
it will be sufﬁcient to make some very simple (and reasonable) assumptions about the
state spaces of the systems we are interested in, and stick with those assumptions.
The simplest quantum mechanical system, and the system which we will be most
concerned with, is the qubit. A qubit has a two-dimensional state space. Suppose |0⟩and
|1⟩form an orthonormal basis for that state space. Then an arbitrary state vector in the
state space can be written
|ψ⟩= a|0⟩+ b|1⟩,
(2.82)
The postulates of quantum mechanics
81
where a and b are complex numbers. The condition that |ψ⟩be a unit vector, ⟨ψ|ψ⟩= 1,
is therefore equivalent to |a|2 + |b|2 = 1. The condition ⟨ψ|ψ⟩= 1 is often known as the
normalization condition for state vectors.
We will take the qubit as our fundamental quantum mechanical system. Later, in
Chapter 7, we will see that there are real physical systems which may be described in
terms of qubits. For now, though, it is sufﬁcient to think of qubits in abstract terms,
without reference to a speciﬁc realization. Our discussions of qubits will always be referred
to some orthonormal set of basis vectors, |0⟩and |1⟩, which should be thought of as being
ﬁxed in advance. Intuitively, the states |0⟩and |1⟩are analogous to the two values 0 and
1 which a bit may take. The way a qubit differs from a bit is that superpositions of these
two states, of the form a|0⟩+ b|1⟩, can also exist, in which it is not possible to say that
the qubit is deﬁnitely in the state |0⟩, or deﬁnitely in the state |1⟩.
We conclude with some useful terminology which is often used in connection with
the description of quantum states. We say that any linear combination 
i αi|ψi⟩is a
superposition of the states |ψi⟩with amplitude αi for the state |ψi⟩. So, for example,
the state
|0⟩−|1⟩
√
2
(2.83)
is a superposition of the states |0⟩and |1⟩with amplitude 1/
√
2 for the state |0⟩, and
amplitude −1/
√
2 for the state |1⟩.
2.2.2
Evolution
How does the state, |ψ⟩, of a quantum mechanical system change with time? The following
postulate gives a prescription for the description of such state changes.
Postulate 2: The evolution of a closed quantum system is described by a unitary
transformation. That is, the state |ψ⟩of the system at time t1 is related to the
state |ψ′⟩of the system at time t2 by a unitary operator U which depends only on
the times t1 and t2,
|ψ′⟩= U|ψ⟩.
(2.84)
Just as quantum mechanics does not tell us the state space or quantum state of a
particular quantum system, it does not tell us which unitary operators U describe real-
world quantum dynamics. Quantum mechanics merely assures us that the evolution of
any closed quantum system may be described in such a way. An obvious question to ask
is: what unitary operators are natural to consider? In the case of single qubits, it turns
out that any unitary operator at all can be realized in realistic systems.
Let’s look at a few examples of unitary operators on a single qubit which are impor-
tant in quantum computation and quantum information. We have already seen several
examples of such unitary operators – the Pauli matrices, deﬁned in Section 2.1.3, and
the quantum gates described in Chapter 1. As remarked in Section 1.3.1, the X matrix is
often known as the quantum
gate, by analogy to the classical
gate. The X and
Z Pauli matrices are also sometimes referred to as the bit ﬂip and phase ﬂip matrices: the
X matrix takes |0⟩to |1⟩, and |1⟩to |0⟩, thus earning the name bit ﬂip; and the Z matrix
leaves |0⟩invariant, and takes |1⟩to −|1⟩, with the extra factor of −1 added known as a
phase factor, thus justifying the term phase ﬂip. We will not use the term phase ﬂip for
82
Introduction to quantum mechanics
Z very often, since it is easily confused with the phase gate to be deﬁned in Chapter 4.
(Section 2.2.7 contains more discussion of the many uses of the term ‘phase’.)
Another interesting unitary operator is the Hadamard gate, which we denote H. This
has the action H|0⟩≡(|0⟩+|1⟩)/
√
2, H|1⟩≡(|0⟩−|1⟩)/
√
2, and corresponding matrix
representation
H =
1
√
2
 1
1
1
−1

.
(2.85)
Exercise 2.51:
Verify that the Hadamard gate H is unitary.
Exercise 2.52:
Verify that H2 = I.
Exercise 2.53:
What are the eigenvalues and eigenvectors of H?
Postulate 2 requires that the system being described be closed. That is, it is not
interacting in any way with other systems. In reality, of course, all systems (except the
Universe as a whole) interact at least somewhat with other systems. Nevertheless, there
are interesting systems which can be described to a good approximation as being closed,
and which are described by unitary evolution to some good approximation. Furthermore,
at least in principle every open system can be described as part of a larger closed system
(the Universe) which is undergoing unitary evolution. Later, we’ll introduce more tools
which allow us to describe systems which are not closed, but for now we’ll continue with
the description of the evolution of closed systems.
Postulate 2 describes how the quantum states of a closed quantum system at two
different times are related. A more reﬁned version of this postulate can be given which
describes the evolution of a quantum system in continuous time. From this more reﬁned
postulate we will recover Postulate 2. Before we state the revised postulate, it is worth
pointing out two things. First, a notational remark. The operator H appearing in the
following discussion is not the same as the Hadamard operator, which we just introduced.
Second, the following postulate makes use of the apparatus of differential equations.
Readers with little background in the study of differential equations should be reassured
that they will not be necessary for much of the book, with the exception of parts of
Chapter 7, on real physical implementations of quantum information processing.
Postulate 2′: The time evolution of the state of a closed quantum system is
described by the Schr¨odinger equation,
iℏd|ψ⟩
dt
= H|ψ⟩.
(2.86)
In this equation, ℏis a physical constant known as Planck’s constant whose value
must be experimentally determined. The exact value is not important to us. In
practice, it is common to absorb the factor ℏinto H, effectively setting ℏ= 1. H
is a ﬁxed Hermitian operator known as the Hamiltonian of the closed system.
If we know the Hamiltonian of a system, then (together with a knowledge of ℏ) we
understand its dynamics completely, at least in principle. In general ﬁguring out the
Hamiltonian needed to describe a particular physical system is a very difﬁcult problem
– much of twentieth century physics has been concerned with this problem – which
requires substantial input from experiment in order to be answered. From our point of
The postulates of quantum mechanics
83
view this is a problem of detail to be addressed by physical theories built within the
framework of quantum mechanics – what Hamiltonian do we need to describe atoms
in such-and-such a conﬁguration – and is not a question that needs to be addressed by
the theory of quantum mechanics itself. Most of the time in our discussion of quantum
computation and quantum information we won’t need to discuss Hamiltonians, and when
we do, we will usually just posit that some matrix is the Hamiltonian as a starting point,
and proceed from there, without attempting to justify the use of that Hamiltonian.
Because the Hamiltonian is a Hermitian operator it has a spectral decomposition
H =

E
E|E⟩⟨E|,
(2.87)
with eigenvalues E and corresponding normalized eigenvectors |E⟩. The states |E⟩are
conventionally referred to as energy eigenstates, or sometimes as stationary states, and
E is the energy of the state |E⟩. The lowest energy is known as the ground state energy
for the system, and the corresponding energy eigenstate (or eigenspace) is known as the
ground state. The reason the states |E⟩are sometimes known as stationary states is
because their only change in time is to acquire an overall numerical factor,
|E⟩→exp(−iEt/ℏ)|E⟩.
(2.88)
As an example, suppose a single qubit has Hamiltonian
H = ℏωX.
(2.89)
In this equation ω is a parameter that, in practice, needs to be experimentally determined.
We won’t worry about the parameter overly much here – the point is to give you a feel
for the sort of Hamiltonians that are sometimes written down in the study of quantum
computation and quantum information. The energy eigenstates of this Hamiltonian are
obviously the same as the eigenstates of X, namely (|0⟩+ |1⟩)/
√
2 and (|0⟩−|1⟩)/
√
2,
with corresponding energies ℏω and −ℏω. The ground state is therefore (|0⟩−|1⟩)/
√
2,
and the ground state energy is −ℏω.
What is the connection between the Hamiltonian picture of dynamics, Postulate 2′,
and the unitary operator picture, Postulate 2? The answer is provided by writing down
the solution to Schr¨odinger’s equation, which is easily veriﬁed to be:
|ψ(t2)⟩= exp
−iH(t2 −t1)
ℏ

|ψ(t1)⟩= U(t1, t2)|ψ(t1)⟩,
(2.90)
where we deﬁne
U(t1, t2) ≡exp
−iH(t2 −t1)
ℏ

.
(2.91)
You will show in the exercises that this operator is unitary, and furthermore, that any
unitary operator U can be realized in the form U = exp(iK) for some Hermitian operator
K. There is therefore a one-to-one correspondence between the discrete-time description
of dynamics using unitary operators, and the continuous time description using Hamil-
tonians. For most of the book we use the unitary formulation of quantum dynamics.
Exercise 2.54:
Suppose A and B are commuting Hermitian operators. Prove that
exp(A) exp(B) = exp(A + B). (Hint: Use the results of Section 2.1.9.)
84
Introduction to quantum mechanics
Exercise 2.55:
Prove that U(t1, t2) deﬁned in Equation (2.91) is unitary.
Exercise 2.56:
Use the spectral decomposition to show that K ≡−i log(U) is
Hermitian for any unitary U, and thus U = exp(iK) for some Hermitian K.
In quantum computation and quantum information we often speak of applying a
unitary operator to a particular quantum system. For example, in the context of quantum
circuits we may speak of applying the unitary gate X to a single qubit. Doesn’t this
contradict what we said earlier, about unitary operators describing the evolution of a
closed quantum system? After all, if we are ‘applying’ a unitary operator, then that
implies that there is an external ‘we’ who is interacting with the quantum system, and
the system is not closed.
An example of this occurs when a laser is focused on an atom. After a lot of thought
and hard work it is possible to write down a Hamiltonian describing the total atom–
laser system. The interesting thing is that when we write down the Hamiltonian for the
atom–laser system and consider the effects on the atom alone, the behavior of the state
vector of the atom turns out to be almost but not quite perfectly described by another
Hamiltonian, the atomic Hamiltonian. The atomic Hamiltonian contains terms related
to laser intensity, and other parameters of the laser, which we can vary at will. It is as if
the evolution of the atom were being described by a Hamiltonian which we can vary at
will, despite the atom not being a closed system.
More generally, for many systems like this it turns out to be possible to write down
a time-varying Hamiltonian for a quantum system, in which the Hamiltonian for the
system is not a constant, but varies according to some parameters which are under an
experimentalist’s control, and which may be changed during the course of an experi-
ment. The system is not, therefore, closed, but it does evolve according to Schr¨odinger’s
equation with a time-varying Hamiltonian, to some good approximation.
The upshot is that to begin we will often describe the evolution of quantum systems –
even systems which aren’t closed – using unitary operators. The main exception to this,
quantum measurement, will be described in the next section. Later on we will investigate
in more detail possible deviations from unitary evolution due to the interaction with other
systems, and understand more precisely the dynamics of realistic quantum systems.
2.2.3
Quantum measurement
We postulated that closed quantum systems evolve according to unitary evolution. The
evolution of systems which don’t interact with the rest of the world is all very well, but
there must also be times when the experimentalist and their experimental equipment –
an external physical system in other words – observes the system to ﬁnd out what is
going on inside the system, an interaction which makes the system no longer closed, and
thus not necessarily subject to unitary evolution. To explain what happens when this
is done, we introduce Postulate 3, which provides a means for describing the effects of
measurements on quantum systems.
Postulate 3: Quantum measurements are described by a collection {Mm} of
measurement operators. These are operators acting on the state space of the
system being measured. The index m refers to the measurement outcomes that
may occur in the experiment. If the state of the quantum system is |ψ⟩
immediately before the measurement then the probability that result m occurs is
The postulates of quantum mechanics
85
given by
p(m) = ⟨ψ|M †
mMm|ψ⟩,
(2.92)
and the state of the system after the measurement is
Mm|ψ⟩

⟨ψ|M †
mMm|ψ⟩
.
(2.93)
The measurement operators satisfy the completeness equation,

m
M †
mMm = I .
(2.94)
The completeness equation expresses the fact that probabilities sum to one:
1 =

m
p(m) =

m
⟨ψ|M †
mMm|ψ⟩.
(2.95)
This equation being satisﬁed for all |ψ⟩is equivalent to the completeness equation.
However, the completeness equation is much easier to check directly, so that’s why it
appears in the statement of the postulate.
A simple but important example of a measurement is the measurement of a qubit in
the computational basis. This is a measurement on a single qubit with two outcomes
deﬁned by the two measurement operators M0 = |0⟩⟨0|, M1 = |1⟩⟨1|. Observe that
each measurement operator is Hermitian, and that M 2
0 = M0, M 2
1 = M1. Thus the
completeness relation is obeyed, I = M †
0 M0 + M †
1 M1 = M0 + M1. Suppose the state
being measured is |ψ⟩= a|0⟩+ b|1⟩. Then the probability of obtaining measurement
outcome 0 is
p(0) = ⟨ψ|M †
0 M0|ψ⟩= ⟨ψ|M0|ψ⟩= |a|2.
(2.96)
Similarly, the probability of obtaining the measurement outcome 1 is p(1) = |b|2. The
state after measurement in the two cases is therefore
M0|ψ⟩
|a|
= a
|a||0⟩
(2.97)
M1|ψ⟩
|b|
= b
|b||1⟩.
(2.98)
We will see in Section 2.2.7 that multipliers like a/|a|, which have modulus one, can
effectively be ignored, so the two post-measurement states are effectively |0⟩and |1⟩, just
as described in Chapter 1.
The status of Postulate 3 as a fundamental postulate intrigues many people. Measuring
devices are quantum mechanical systems, so the quantum system being measured and
the measuring device together are part of a larger, isolated, quantum mechanical system.
(It may be necessary to include quantum systems other than the system being measured
and the measuring device to obtain a completely isolated system, but the point is that
this can be done.) According to Postulate 2, the evolution of this larger isolated system
can be described by a unitary evolution. Might it be possible to derive Postulate 3 as a
consequence of this picture? Despite considerable investigation along these lines there is
still disagreement between physicists about whether or not this is possible. We, however,
are going to take the very pragmatic approach that in practice it is clear when to apply
86
Introduction to quantum mechanics
Postulate 2 and when to apply Postulate 3, and not worry about deriving one postulate
from the other.
Over the next few sections we apply Postulate 3 to several elementary but important
measurement scenarios. Section 2.2.4 examines the problem of distinguishing a set of
quantum states. Section 2.2.5 explains a special case of Postulate 3, the projective or
von Neumann measurements. Section 2.2.6 explains another special case of Postulate 3,
known as POVM measurements. Many introductions to quantum mechanics only discuss
projective measurements, omitting a full discussion of Postulate 3 or of POVM elements.
For this reason we have included Box 2.5 on page 91 which comments on the relationship
between the different classes of measurement we describe.
Exercise 2.57: (Cascaded measurements are single measurements)
Suppose
{Ll} and {Mm} are two sets of measurement operators. Show that a
measurement deﬁned by the measurement operators {Ll} followed by a
measurement deﬁned by the measurement operators {Mm} is physically
equivalent to a single measurement deﬁned by measurement operators {Nlm}
with the representation Nlm ≡MmLl.
2.2.4
Distinguishing quantum states
An important application of Postulate 3 is to the problem of distinguishing quantum
states. In the classical world, distinct states of an object are usually distinguishable, at
least in principle. For example, we can always identify whether a coin has landed heads or
tails, at least in the ideal limit. Quantum mechanically, the situation is more complicated.
In Section 1.6 we gave a plausible argument that non-orthogonal quantum states cannot
be distinguished. With Postulate 3 as a ﬁrm foundation we can now give a much more
convincing demonstration of this fact.
Distinguishability, like many ideas in quantum computation and quantum information,
is most easily understood using the metaphor of a game involving two parties, Alice and
Bob. Alice chooses a state |ψi⟩(1 ≤i ≤n) from some ﬁxed set of states known to both
parties. She gives the state |ψi⟩to Bob, whose task it is to identify the index i of the
state Alice has given him.
Suppose the states |ψi⟩are orthonormal. Then Bob can do a quantum measurement
to distinguish these states, using the following procedure. Deﬁne measurement operators
Mi ≡|ψi⟩⟨ψi|, one for each possible index i, and an additional measurement operator
M0 deﬁned as the positive square root of the positive operator I −
i ̸= 0 |ψi⟩⟨ψi|.
These operators satisfy the completeness relation, and if the state |ψi⟩is prepared then
p(i) = ⟨ψi|Mi|ψi⟩= 1, so the result i occurs with certainty. Thus, it is possible to
reliably distinguish the orthonormal states |ψi⟩.
By contrast, if the states |ψi⟩are not orthonormal then we can prove that there is no
quantum measurement capable of distinguishing the states. The idea is that Bob will do
a measurement described by measurement operators Mj, with outcome j. Depending on
the outcome of the measurement Bob tries to guess what the index i was using some rule,
i = f(j), where f(·) represents the rule he uses to make the guess. The key to why Bob
can’t distinguish non-orthogonal states |ψ1⟩and |ψ2⟩is the observation that |ψ2⟩can be
decomposed into a (non-zero) component parallel to |ψ1⟩, and a component orthogonal
to |ψ1⟩. Suppose j is a measurement outcome such that f(j) = 1, that is, Bob guesses
that the state was |ψ1⟩when he observes j. But because of the component of |ψ2⟩parallel
The postulates of quantum mechanics
87
to |ψ1⟩, there is a non-zero probability of getting outcome j when |ψ2⟩is prepared, so
sometimes Bob will make an error identifying which state was prepared. A more rigorous
argument that non-orthogonal states can’t be distinguished is given in Box 2.3, but this
captures the essential idea.
Box 2.3: Proof that non-orthogonal states can’t be reliably distinguished
A proof by contradiction shows that no measurement distinguishing the non-
orthogonal states |ψ1⟩and |ψ2⟩is possible. Suppose such a measurement is possible.
If the state |ψ1⟩(|ψ2⟩) is prepared then the probability of measuring j such that
f(j) = 1 (f(j) = 2) must be 1. Deﬁning Ei ≡
j:f(j)=i M †
j Mj, these observations
may be written as:
⟨ψ1|E1|ψ1⟩= 1;
⟨ψ2|E2|ψ2⟩= 1.
(2.99)
Since 
i Ei = I it follows that 
i⟨ψ1|Ei|ψ1⟩= 1, and since ⟨ψ1|E1|ψ1⟩= 1
we must have ⟨ψ1|E2|ψ1⟩= 0, and thus √E2|ψ1⟩= 0. Suppose we decompose
|ψ2⟩= α|ψ1⟩+β|ϕ⟩, where |ϕ⟩is orthonormal to |ψ1⟩, |α|2 +|β|2 = 1, and |β| < 1
since |ψ1⟩and |ψ2⟩are not orthogonal. Then √E2|ψ2⟩= β√E2|ϕ⟩, which implies
a contradiction with (2.99), as
⟨ψ2|E2|ψ2⟩= |β|2⟨ϕ|E2|ϕ⟩≤|β|2 < 1,
(2.100)
where the second last inequality follows from the observation that
⟨ϕ|E2|ϕ⟩≤

i
⟨ϕ|Ei|ϕ⟩= ⟨ϕ|ϕ⟩= 1.
(2.101)
2.2.5
Projective measurements
In this section we explain an important special case of the general measurement postulate,
Postulate 3. This special class of measurements is known as projective measurements.
For many applications of quantum computation and quantum information we will be
concerned primarily with projective measurements. Indeed, projective measurements ac-
tually turn out to be equivalent to the general measurement postulate, when they are
augmented with the ability to perform unitary transformations, as described in Postu-
late 2. We will explain this equivalence in detail in Section 2.2.8, as the statement of the
measurement postulate for projective measurements is superﬁcially rather different from
the general postulate, Postulate 3.
Projective measurements: A projective measurement is described by an
observable, M, a Hermitian operator on the state space of the system being
observed. The observable has a spectral decomposition,
M =

m
mPm ,
(2.102)
where Pm is the projector onto the eigenspace of M with eigenvalue m. The
possible outcomes of the measurement correspond to the eigenvalues, m, of the
observable. Upon measuring the state |ψ⟩, the probability of getting result m is
88
Introduction to quantum mechanics
given by
p(m) = ⟨ψ|Pm|ψ⟩.
(2.103)
Given that outcome m occurred, the state of the quantum system immediately
after the measurement is
Pm|ψ⟩
√p(m) .
(2.104)
Projective measurements can be understood as a special case of Postulate 3. Suppose the
measurement operators in Postulate 3, in addition to satisfying the completeness relation

m M †
mMm = I, also satisfy the conditions that Mm are orthogonal projectors, that is,
the Mm are Hermitian, and MmMm′ = δm,m′Mm. With these additional restrictions,
Postulate 3 reduces to a projective measurement as just deﬁned.
Projective measurements have many nice properties. In particular, it is very easy to
calculate average values for projective measurements. By deﬁnition, the average (see
Appendix 1 for elementary deﬁnitions and results in probability theory) value of the
measurement is
E(M) =

m
m p(m)
(2.110)
=

m
m⟨ψ|Pm|ψ⟩
(2.111)
= ⟨ψ|

m
mPm

|ψ⟩
(2.112)
= ⟨ψ|M|ψ⟩.
(2.113)
This is a useful formula, which simpliﬁes many calculations. The average value of the
observable M is often written ⟨M⟩≡⟨ψ|M|ψ⟩. From this formula for the average
follows a formula for the standard deviation associated to observations of M,
[Δ(M)]2 = ⟨(M −⟨M⟩)2⟩
(2.114)
= ⟨M 2⟩−⟨M⟩2.
(2.115)
The standard deviation is a measure of the typical spread of the observed values upon mea-
surement of M. In particular, if we perform a large number of experiments in which the
state |ψ⟩is prepared and the observable M is measured, then the standard deviation Δ(M)
of the observed values is determined by the formula Δ(M) =

⟨M 2⟩−⟨M⟩2. This for-
mulation of measurement and standard deviations in terms of observables gives rise in
an elegant way to results such as the Heisenberg uncertainty principle (see Box 2.4).
Exercise 2.58:
Suppose we prepare a quantum system in an eigenstate |ψ⟩of some
observable M, with corresponding eigenvalue m. What is the average observed
value of M, and the standard deviation?
Two widely used nomenclatures for measurements deserve emphasis. Rather than giv-
ing an observable to describe a projective measurement, often people simply list a com-
plete set of orthogonal projectors Pm satisfying the relations 
m Pm = I and PmPm′ =
The postulates of quantum mechanics
89
Box 2.4: The Heisenberg uncertainty principle
Perhaps the best known result of quantum mechanics is the Heisenberg uncer-
tainty principle. Suppose A and B are two Hermitian operators, and |ψ⟩is a
quantum state. Suppose ⟨ψ|AB|ψ⟩= x + iy, where x and y are real. Note that
⟨ψ|[A, B]|ψ⟩= 2iy and ⟨ψ|{A, B}|ψ⟩= 2x. This implies that
|⟨ψ|[A, B]|ψ⟩|2 + |⟨ψ|{A, B}|ψ⟩|2 = 4 |⟨ψ|AB|ψ⟩|2 .
(2.105)
By the Cauchy–Schwarz inequality
|⟨ψ|AB|ψ⟩|2 ≤⟨ψ|A2|ψ⟩⟨ψ|B2|ψ⟩,
(2.106)
which combined with Equation (2.105) and dropping a non-negative term gives
|⟨ψ|[A, B]|ψ⟩|2 ≤4⟨ψ|A2|ψ⟩⟨ψ|B2|ψ⟩.
(2.107)
Suppose C and D are two observables. Substituting A = C−⟨C⟩and B = D−⟨D⟩
into the last equation, we obtain Heisenberg’s uncertainty principle as it is usually
stated:
Δ(C)Δ(D) ≥|⟨ψ|[C, D]|ψ⟩|
2
.
(2.108)
You should be wary of a common misconception about the uncertainty principle,
that measuring an observable C to some ‘accuracy’ Δ(C) causes the value of D to
be ‘disturbed’ by an amount Δ(D) in such a way that some sort of inequality similar
to (2.108) is satisﬁed. While it is true that measurements in quantum mechanics
cause disturbance to the system being measured, this is most emphatically not the
content of the uncertainty principle.
The correct interpretation of the uncertainty principle is that if we prepare a large
number of quantum systems in identical states, |ψ⟩, and then perform measurements
of C on some of those systems, and of D in others, then the standard deviation
Δ(C) of the C results times the standard deviation Δ(D) of the results for D will
satisfy the inequality (2.108).
As an example of the uncertainty principle, consider the observables X and Y
when measured for the quantum state |0⟩. In Equation (2.70) we showed that
[X, Y ] = 2iZ, so the uncertainty principle tells us that
Δ(X)Δ(Y ) ≥⟨0|Z|0⟩= 1 .
(2.109)
One elementary consequence of this is that Δ(X) and Δ(Y ) must both be strictly
greater than 0, as can be veriﬁed by direct calculation.
δmm′Pm. The corresponding observable implicit in this usage is M = 
m mPm. An-
other widely used phrase, to ‘measure in a basis |m⟩’, where |m⟩form an orthonormal ba-
sis, simply means to perform the projective measurement with projectors Pm = |m⟩⟨m|.
Let’s look at an example of projective measurements on single qubits. First is the
measurement of the observable Z. This has eigenvalues +1 and −1 with corresponding
eigenvectors |0⟩and |1⟩. Thus, for example, measurement of Z on the state |ψ⟩=
(|0⟩+ |1⟩)/
√
2 gives the result +1 with probability ⟨ψ|0⟩⟨0|ψ⟩= 1/2, and similarly the
90
Introduction to quantum mechanics
result −1 with probability 1/2. More generally, suppose ⃗v is any real three-dimensional
unit vector. Then we can deﬁne an observable:
⃗v · ⃗σ ≡v1σ1 + v2σ2 + v3σ3.
(2.116)
Measurement of this observable is sometimes referred to as a ‘measurement of spin along
the ⃗v axis’, for historical reasons. The following two exercises encourage you to work out
some elementary but important properties of such a measurement.
Exercise 2.59:
Suppose we have qubit in the state |0⟩, and we measure the observable
X. What is the average value of X? What is the standard deviation of X?
Exercise 2.60:
Show that ⃗v · ⃗σ has eigenvalues ±1, and that the projectors onto the
corresponding eigenspaces are given by P± = (I ± ⃗v · ⃗σ)/2.
Exercise 2.61:
Calculate the probability of obtaining the result +1 for a measurement
of ⃗v · ⃗σ, given that the state prior to measurement is |0⟩. What is the state of the
system after the measurement if +1 is obtained?
2.2.6
POVM measurements
The quantum measurement postulate, Postulate 3, involves two elements. First, it gives
a rule describing the measurement statistics, that is, the respective probabilities of the
different possible measurement outcomes. Second, it gives a rule describing the post-
measurement state of the system. However, for some applications the post-measurement
state of the system is of little interest, with the main item of interest being the probabilities
of the respective measurement outcomes. This is the case, for example, in an experiment
where the system is measured only once, upon conclusion of the experiment. In such
instances there is a mathematical tool known as the POVM formalism which is especially
well adapted to the analysis of the measurements. (The acronym POVM stands for
‘Positive Operator-Valued Measure’, a technical term whose historical origins we won’t
worry about.) This formalism is a simple consequence of the general description of
measurements introduced in Postulate 3, but the theory of POVMs is so elegant and
widely used that it merits a separate discussion here.
Suppose a measurement described by measurement operators Mm is performed upon
a quantum system in the state |ψ⟩. Then the probability of outcome m is given by
p(m) = ⟨ψ|M †
mMm|ψ⟩. Suppose we deﬁne
Em ≡M †
mMm.
(2.117)
Then from Postulate 3 and elementary linear algebra, Em is a positive operator such
that 
m Em = I and p(m) = ⟨ψ|Em|ψ⟩. Thus the set of operators Em are sufﬁcient to
determine the probabilities of the different measurement outcomes. The operators Em
are known as the POVM elements associated with the measurement. The complete set
{Em} is known as a POVM.
As an example of a POVM, consider a projective measurement described by mea-
surement operators Pm, where the Pm are projectors such that PmPm′ = δmm′Pm and

m Pm = I. In this instance (and only this instance) all the POVM elements are the
same as the measurement operators themselves, since Em ≡P †
mPm = Pm.
The postulates of quantum mechanics
91
Box 2.5: General measurements, projective measurements, and POVMs
Most introductions to quantum mechanics describe only projective measurements,
and consequently the general description of measurements given in Postulate 3
may be unfamiliar to many physicists, as may the POVM formalism described in
Section 2.2.6. The reason most physicists don’t learn the general measurement
formalism is because most physical systems can only be measured in a very coarse
manner. In quantum computation and quantum information we aim for an exquisite
level of control over the measurements that may be done, and consequently it helps
to use a more comprehensive formalism for the description of measurements.
Of course, when the other axioms of quantum mechanics are taken into account,
projective measurements augmented by unitary operations turn out to be completely
equivalent to general measurements, as shown in Section 2.2.8. So a physicist
trained in the use of projective measurements might ask to what end we start with
the general formalism, Postulate 3? There are several reasons for doing so. First,
mathematically general measurements are in some sense simpler than projective
measurements, since they involve fewer restrictions on the measurement operators;
there is, for example, no requirement for general measurements analogous to the
condition PiPj = δijPi for projective measurements. This simpler structure also
gives rise to many useful properties for general measurements that are not possessed
by projective measurements. Second, it turns out that there are important problems
in quantum computation and quantum information – such as the optimal way
to distinguish a set of quantum states – the answer to which involves a general
measurement, rather than a projective measurement.
A third reason for preferring Postulate 3 as a starting point is related to a property
of projective measurements known as repeatability. Projective measurements are
repeatable in the sense that if we perform a projective measurement once, and
obtain the outcome m, repeating the measurement gives the outcome m again and
does not change the state. To see this, suppose |ψ⟩was the initial state. After the
ﬁrst measurement the state is |ψm⟩=
'
Pm|ψ⟩
(
/

⟨ψ|Pm|ψ⟩. Applying Pm to
|ψm⟩does not change it, so we have ⟨ψm|Pm|ψm⟩= 1, and therefore repeated
measurement gives the result m each time, without changing the state.
This repeatability of projective measurements tips us off to the fact that many
important measurements in quantum mechanics are not projective measurements.
For instance, if we use a silvered screen to measure the position of a photon we
destroy the photon in the process. This certainly makes it impossible to repeat
the measurement of the photon’s position! Many other quantum measurements
are also not repeatable in the same sense as a projective measurement. For such
measurements, the general measurement postulate, Postulate 3, must be employed.
Where do POVMs ﬁt in this picture? POVMs are best viewed as a special case
of the general measurement formalism, providing the simplest means by which
one can study general measurement statistics, without the necessity for knowing
the post-measurement state. They are a mathematical convenience that sometimes
gives extra insight into quantum measurements.
92
Introduction to quantum mechanics
Exercise 2.62:
Show that any measurement where the measurement operators and the
POVM elements coincide is a projective measurement.
Above we noticed that the POVM operators are positive and satisfy 
m Em = I.
Suppose now that {Em} is some arbitrary set of positive operators such that 
m Em = I.
We will show that there exists a set of measurement operators Mm deﬁning a measurement
described by the POVM {Em}. Deﬁning Mm ≡√Em we see that 
m M †
mMm =

m Em = I, and therefore the set {Mm} describes a measurement with POVM {Em}.
For this reason it is convenient to deﬁne a POVM to be any set of operators {Em} such
that: (a) each operator Em is positive; and (b) the completeness relation 
m Em = I is
obeyed, expressing the fact that probabilities sum to one. To complete the description
of POVMs, we note again that given a POVM {Em}, the probability of outcome m is
given by p(m) = ⟨ψ|Em|ψ⟩.
We’ve looked at projective measurements as an example of the use of POVMs, but
it wasn’t very exciting since we didn’t learn much that was new. The following more
sophisticated example illustrates the use of the POVM formalism as a guide for our
intuition in quantum computation and quantum information. Suppose Alice gives Bob a
qubit prepared in one of two states, |ψ1⟩= |0⟩or |ψ2⟩= (|0⟩+ |1⟩)/
√
2. As explained in
Section 2.2.4 it is impossible for Bob to determine whether he has been given |ψ1⟩or |ψ2⟩
with perfect reliability. However, it is possible for him to perform a measurement which
distinguishes the states some of the time, but never makes an error of mis-identiﬁcation.
Consider a POVM containing three elements,
E1 ≡
√
2
1 +
√
2
|1⟩⟨1|,
(2.118)
E2 ≡
√
2
1 +
√
2

|0⟩−|1⟩
 
⟨0| −⟨1|

2
,
(2.119)
E3 ≡I −E1 −E2.
(2.120)
It is straightforward to verify that these are positive operators which satisfy the com-
pleteness relation 
m Em = I, and therefore form a legitimate POVM.
Suppose Bob is given the state |ψ1⟩= |0⟩. He performs the measurement described
by the POVM {E1, E2, E3}. There is zero probability that he will observe the result
E1, since E1 has been cleverly chosen to ensure that ⟨ψ1|E1|ψ1⟩= 0. Therefore, if the
result of his measurement is E1 then Bob can safely conclude that the state he received
must have been |ψ2⟩. A similar line of reasoning shows that if the measurement outcome
E2 occurs then it must have been the state |ψ1⟩that Bob received. Some of the time,
however, Bob will obtain the measurement outcome E3, and he can infer nothing about
the identity of the state he was given. The key point, however, is that Bob never makes a
mistake identifying the state he has been given. This infallibility comes at the price that
sometimes Bob obtains no information about the identity of the state.
This simple example demonstrates the utility of the POVM formalism as a simple
and intuitive way of gaining insight into quantum measurements in instances where
only the measurement statistics matter. In many instances later in the book we will only
be concerned with measurement statistics, and will therefore use the POVM formalism
rather than the more general formalism for measurements described in Postulate 3.
Exercise 2.63:
Suppose a measurement is described by measurement operators Mm.
The postulates of quantum mechanics
93
Show that there exist unitary operators Um such that Mm = Um
√Em, where
Em is the POVM associated to the measurement.
Exercise 2.64:
Suppose Bob is given a quantum state chosen from a set |ψ1⟩, . . ., |ψm⟩
of linearly independent states. Construct a POVM {E1, E2, . . . , Em+1} such that
if outcome Ei occurs, 1 ≤i ≤m, then Bob knows with certainty that he was
given the state |ψi⟩. (The POVM must be such that ⟨ψi|Ei|ψi⟩> 0 for each i.)
2.2.7
Phase
‘Phase’ is a commonly used term in quantum mechanics, with several different mean-
ings dependent upon context. At this point it is convenient to review a couple of these
meanings. Consider, for example, the state eiθ|ψ⟩, where |ψ⟩is a state vector, and θ is a
real number. We say that the state eiθ|ψ⟩is equal to |ψ⟩, up to the global phase factor
eiθ. It is interesting to note that the statistics of measurement predicted for these two
states are the same. To see this, suppose Mm is a measurement operator associated to
some quantum measurement, and note that the respective probabilities for outcome m
occurring are ⟨ψ|M †
mMm|ψ⟩and ⟨ψ|e−iθM †
mMmeiθ|ψ⟩= ⟨ψ|M †
mMm|ψ⟩. Therefore,
from an observational point of view these two states are identical. For this reason we may
ignore global phase factors as being irrelevant to the observed properties of the physical
system.
There is another kind of phase known as the relative phase, which has quite a different
meaning. Consider the states
|0⟩+ |1⟩
√
2
and
|0⟩−|1⟩
√
2
.
(2.121)
In the ﬁrst state the amplitude of |1⟩is 1/
√
2. For the second state the amplitude is
−1/
√
2. In each case the magnitude of the amplitudes is the same, but they differ in
sign. More generally, we say that two amplitudes, a and b, differ by a relative phase if
there is a real θ such that a = exp(iθ)b. More generally still, two states are said to differ
by a relative phase in some basis if each of the amplitudes in that basis is related by such
a phase factor. For example, the two states displayed above are the same up to a relative
phase shift because the |0⟩amplitudes are identical (a relative phase factor of 1), and
the |1⟩amplitudes differ only by a relative phase factor of −1. The difference between
relative phase factors and global phase factors is that for relative phase the phase factors
may vary from amplitude to amplitude. This makes the relative phase a basis-dependent
concept unlike global phase. As a result, states which differ only by relative phases in
some basis give rise to physically observable differences in measurement statistics, and
it is not possible to regard these states as physically equivalent, as we do with states
differing by a global phase factor
Exercise 2.65:
Express the states (|0⟩+ |1⟩)/
√
2 and (|0⟩−|1⟩)/
√
2 in a basis in
which they are not the same up to a relative phase shift.
2.2.8
Composite systems
Suppose we are interested in a composite quantum system made up of two (or more)
distinct physical systems. How should we describe states of the composite system? The
following postulate describes how the state space of a composite system is built up from
the state spaces of the component systems.
94
Introduction to quantum mechanics
Postulate 4: The state space of a composite physical system is the tensor product
of the state spaces of the component physical systems. Moreover, if we have
systems numbered 1 through n, and system number i is prepared in the state
|ψi⟩, then the joint state of the total system is |ψ1⟩⊗|ψ2⟩⊗· · · ⊗|ψn⟩.
Why is the tensor product the mathematical structure used to describe the state space of
a composite physical system? At one level, we can simply accept it as a basic postulate, not
reducible to something more elementary, and move on. After all, we certainly expect that
there be some canonical way of describing composite systems in quantum mechanics.
Is there some other way we can arrive at this postulate? Here is one heuristic that is
sometimes used. Physicists sometimes like to speak of the superposition principle of
quantum mechanics, which states that if |x⟩and |y⟩are two states of a quantum system,
then any superposition α|x⟩+ β|y⟩should also be an allowed state of a quantum system,
where |α|2 + |β|2 = 1. For composite systems, it seems natural that if |A⟩is a state of
system A, and |B⟩is a state of system B, then there should be some corresponding state,
which we might denote |A⟩|B⟩, of the joint system AB. Applying the superposition
principle to product states of this form, we arrive at the tensor product postulate given
above. This is not a derivation, since we are not taking the superposition principle as a
fundamental part of our description of quantum mechanics, but it gives you the ﬂavor of
the various ways in which these ideas are sometimes reformulated.
A variety of different notations for composite systems appear in the literature. Part of
the reason for this proliferation is that different notations are better adapted for different
applications, and we will also ﬁnd it convenient to introduce some specialized notations
on occasion. At this point it sufﬁces to mention a useful subscript notation to denote
states and operators on different systems, when it is not clear from context. For example,
in a system containing three qubits, X2 is the Pauli σx operator acting on the second
qubit.
Exercise 2.66:
Show that the average value of the observable X1Z2 for a two qubit
system measured in the state (|00⟩+ |11⟩)/
√
2 is zero.
In Section 2.2.5 we claimed that projective measurements together with unitary dy-
namics are sufﬁcient to implement a general measurement. The proof of this statement
makes use of composite quantum systems, and is a nice illustration of Postulate 4 in
action. Suppose we have a quantum system with state space Q, and we want to per-
form a measurement described by measurement operators Mm on the system Q. To do
this, we introduce an ancilla system, with state space M, having an orthonormal basis
|m⟩in one-to-one correspondence with the possible outcomes of the measurement we
wish to implement. This ancilla system can be regarded as merely a mathematical device
appearing in the construction, or it can be interpreted physically as an extra quantum
system introduced into the problem, which we assume has a state space with the required
properties.
Letting |0⟩be any ﬁxed state of M, deﬁne an operator U on products |ψ⟩|0⟩of states
|ψ⟩from Q with the state |0⟩by
U|ψ⟩|0⟩≡

m
Mm|ψ⟩|m⟩.
(2.122)
Using the orthonormality of the states |m⟩and the completeness relation 
m M †
mMm =
The postulates of quantum mechanics
95
I, we can see that U preserves inner products between states of the form |ψ⟩|0⟩,
⟨ϕ|⟨0|U †U|ψ⟩|0⟩=

m,m′
⟨ϕ|M †
mMm′|ψ⟩⟨m|m′⟩
(2.123)
=

m
⟨ϕ|M †
mMm|ψ⟩
(2.124)
= ⟨ϕ|ψ⟩.
(2.125)
By the results of Exercise 2.67 it follows that U can be extended to a unitary operator on
the space Q ⊗M, which we also denote by U.
Exercise 2.67:
Suppose V is a Hilbert space with a subspace W. Suppose
U : W →V is a linear operator which preserves inner products, that is, for any
|w1⟩and |w2⟩in W,
⟨w1|U †U|w2⟩= ⟨w1|w2⟩.
(2.126)
Prove that there exists a unitary operator U ′ : V →V which extends U. That is,
U ′|w⟩= U|w⟩for all |w⟩in W, but U ′ is deﬁned on the entire space V . Usually
we omit the prime symbol ′ and just write U to denote the extension.
Next, suppose we perform a projective measurement on the two systems described by
projectors Pm ≡IQ ⊗|m⟩⟨m|. Outcome m occurs with probability
p(m) = ⟨ψ|⟨0|U †PmU|ψ⟩|0⟩
(2.127)
=

m′,m′′
⟨ψ|M †
m′⟨m′|(IQ ⊗|m⟩⟨m|)Mm′′|ψ⟩|m′′⟩
(2.128)
= ⟨ψ|M †
mMm|ψ⟩,
(2.129)
just as given in Postulate 3. The joint state of the system QM after measurement,
conditional on result m occurring, is given by
PmU|ψ⟩|0⟩

⟨ψ|U †PmU|ψ⟩=
Mm|ψ⟩|m⟩

⟨ψ|M †
mMm|ψ⟩
.
(2.130)
It follows that the state of system M after the measurement is |m⟩, and the state of
system Q is
Mm|ψ⟩

⟨ψ|M †
mMm|ψ⟩
,
(2.131)
just as prescribed by Postulate 3. Thus unitary dynamics, projective measurements, and
the ability to introduce ancillary systems, together allow any measurement of the form
described in Postulate 3 to be realized.
Postulate 4 also enables us to deﬁne one of the most interesting and puzzling ideas
associated with composite quantum systems – entanglement. Consider the two qubit state
|ψ⟩= |00⟩+ |11⟩
√
2
.
(2.132)
This state has the remarkable property that there are no single qubit states |a⟩and |b⟩
such that |ψ⟩= |a⟩|b⟩, a fact which you should now convince yourself of:
96
Introduction to quantum mechanics
Exercise 2.68:
Prove that |ψ⟩̸= |a⟩|b⟩for all single qubit states |a⟩and |b⟩.
We say that a state of a composite system having this property (that it can’t be written
as a product of states of its component systems) is an entangled state. For reasons which
nobody fully understands, entangled states play a crucial role in quantum computation
and quantum information, and arise repeatedly through the remainder of this book. We
have already seen entanglement play a crucial role in quantum teleportation, as described
in Section 1.3.7. In this chapter we give two examples of the strange effects enabled by
entangled quantum states, superdense coding (Section 2.3), and the violation of Bell’s
inequality (Section 2.6).
2.2.9
Quantum mechanics: a global view
We have now explained all the fundamental postulates of quantum mechanics. Most of
the rest of the book is taken up with deriving consequences of these postulates. Let’s
quickly review the postulates and try to place them in some kind of global perspective.
Postulate 1 sets the arena for quantum mechanics, by specifying how the state of an
isolated quantum system is to be described. Postulate 2 tells us that the dynamics of
closed quantum systems are described by the Schr¨odinger equation, and thus by unitary
evolution. Postulate 3 tells us how to extract information from our quantum systems by
giving a prescription for the description of measurement. Postulate 4 tells us how the
state spaces of different quantum systems may be combined to give a description of the
composite system.
What’s odd about quantum mechanics, at least by our classical lights, is that we can’t
directly observe the state vector. It’s a little bit like a game of chess where you can
never ﬁnd out exactly where each piece is, but only know the rank of the board they
are on. Classical physics – and our intuition – tells us that the fundamental properties
of an object, like energy, position, and velocity, are directly accessible to observation. In
quantum mechanics these quantities no longer appear as fundamental, being replaced by
the state vector, which can’t be directly observed. It is as though there is a hidden world
in quantum mechanics, which we can only indirectly and imperfectly access. Moreover,
merely observing a classical system does not necessarily change the state of the system.
Imagine how difﬁcult it would be to play tennis if each time you looked at the ball its
position changed! But according to Postulate 3, observation in quantum mechanics is an
invasive procedure that typically changes the state of the system.
What conclusions should we draw from these strange features of quantum mechanics?
Might it be possible to reformulate quantum mechanics in a mathematically equivalent
way so that it had a structure more like classical physics? In Section 2.6 we’ll prove
Bell’s inequality, a surprising result that shows any attempt at such a reformulation is
doomed to failure. We’re stuck with the counter-intuitive nature of quantum mechanics.
Of course, the proper reaction to this is glee, not sorrow! It gives us an opportunity
to develop tools of thought that make quantum mechanics intuitive. Moreover, we can
exploit the hidden nature of the state vector to do information processing tasks beyond
what is possible in the classical world. Without this counter-intuitive behavior, quantum
computation and quantum information would be a lot less interesting.
We can also turn this discussion about, and ask ourselves: ‘If quantum mechanics is
so different from classical physics, then how come the everyday world looks so classical?’
Why do we see no evidence of a hidden state vector in our everyday lives? It turns out
Application: superdense coding
97
that the classical world we see can be derived from quantum mechanics as an approximate
description of the world that will be valid on the sort of time, length and mass scales
we commonly encounter in our everyday lives. Explaining the details of how quantum
mechanics gives rise to classical physics is beyond the scope of this book, but the interested
reader should check out the discussion of this topic in ‘History and further reading’at
the end of Chapter 8.
2.3
Application: superdense coding
Superdense coding is a simple yet surprising application of elementary quantum mechan-
ics. It combines in a concrete, non-trivial way all the basic ideas of elementary quantum
mechanics, as covered in the previous sections, and is therefore an ideal example of the
information processing tasks that can be accomplished using quantum mechanics.
Superdense coding involves two parties, conventionally known as ‘Alice’ and ‘Bob’,
who are a long way away from one another. Their goal is to transmit some classical
information from Alice to Bob. Suppose Alice is in possession of two classical bits of
information which she wishes to send Bob, but is only allowed to send a single qubit to
Bob. Can she achieve her goal?
Superdense coding tells us that the answer to this question is yes. Suppose Alice and
Bob initially share a pair of qubits in the entangled state
|ψ⟩= |00⟩+ |11⟩
√
2
.
(2.133)
Alice is initially in possession of the ﬁrst qubit, while Bob has possession of the second
qubit, as illustrated in Figure 2.3. Note that |ψ⟩is a ﬁxed state; there is no need for Alice
to have sent Bob any qubits in order to prepare this state. Instead, some third party may
prepare the entangled state ahead of time, sending one of the qubits to Alice, and the
other to Bob.



 

	
 
 
    
  
Figure 2.3. The initial setup for superdense coding, with Alice and Bob each in possession of one half of an
entangled pair of qubits. Alice can use superdense coding to transmit two classical bits of information to Bob, using
only a single qubit of communication and this preshared entanglement.
By sending the single qubit in her possession to Bob, it turns out that Alice can
communicate two bits of classical information to Bob. Here is the procedure she uses. If
she wishes to send the bit string ‘00’ to Bob then she does nothing at all to her qubit. If
she wishes to send ‘01’ then she applies the phase ﬂip Z to her qubit. If she wishes to
send ‘10’ then she applies the quantum
gate, X, to her qubit. If she wishes to send
‘11’ then she applies the iY gate to her qubit. The four resulting states are easily seen
98
Introduction to quantum mechanics
to be:
00 : |ψ⟩→|00⟩+ |11⟩
√
2
(2.134)
01 : |ψ⟩→|00⟩−|11⟩
√
2
(2.135)
10 : |ψ⟩→|10⟩+ |01⟩
√
2
(2.136)
11 : |ψ⟩→|01⟩−|10⟩
√
2
.
(2.137)
As we noted in Section 1.3.6, these four states are known as the Bell basis, Bell states,
or EPR pairs, in honor of several of the pioneers who ﬁrst appreciated the novelty of
entanglement. Notice that the Bell states form an orthonormal basis, and can therefore
be distinguished by an appropriate quantum measurement. If Alice sends her qubit to
Bob, giving Bob possession of both qubits, then by doing a measurement in the Bell basis
Bob can determine which of the four possible bit strings Alice sent.
Summarizing, Alice, interacting with only a single qubit, is able to transmit two bits
of information to Bob. Of course, two qubits are involved in the protocol, but Alice
never need interact with the second qubit. Classically, the task Alice accomplishes would
have been impossible had she only transmitted a single classical bit, as we will show
in Chapter 12. Furthermore, this remarkable superdense coding protocol has received
partial veriﬁcation in the laboratory. (See ‘History and further reading’ for references to
the experimental veriﬁcation.) In later chapters we will see many other examples, some
of them much more spectacular than superdense coding, of quantum mechanics being
harnessed to perform information processing tasks. However, a key point can already be
seen in this beautiful example: information is physical, and surprising physical theories
such as quantum mechanics may predict surprising information processing abilities.
Exercise 2.69:
Verify that the Bell basis forms an orthonormal basis for the two qubit
state space.
Exercise 2.70:
Suppose E is any positive operator acting on Alice’s qubit. Show that
⟨ψ|E ⊗I|ψ⟩takes the same value when |ψ⟩is any of the four Bell states.
Suppose some malevolent third party (‘Eve’) intercepts Alice’s qubit on the way
to Bob in the superdense coding protocol. Can Eve infer anything about which
of the four possible bit strings 00, 01, 10, 11 Alice is trying to send? If so, how, or
if not, why not?
2.4
The density operator
We have formulated quantum mechanics using the language of state vectors. An alternate
formulation is possible using a tool known as the density operator or density matrix.
This alternate formulation is mathematically equivalent to the state vector approach,
but it provides a much more convenient language for thinking about some commonly
encountered scenarios in quantum mechanics. The next three sections describe the density
operator formulation of quantum mechanics. Section 2.4.1 introduces the density operator
using the concept of an ensemble of quantum states. Section 2.4.2 develops some general
The density operator
99
properties of the density operator. Finally, Section 2.4.3 describes an application where
the density operator really shines – as a tool for the description of individual subsystems
of a composite quantum system.
2.4.1
Ensembles of quantum states
The density operator language provides a convenient means for describing quantum
systems whose state is not completely known. More precisely, suppose a quantum system
is in one of a number of states |ψi⟩, where i is an index, with respective probabilities pi.
We shall call {pi, |ψi⟩} an ensemble of pure states. The density operator for the system
is deﬁned by the equation
ρ ≡

i
pi|ψi⟩⟨ψi|.
(2.138)
The density operator is often known as the density matrix; we will use the two terms
interchangeably. It turns out that all the postulates of quantum mechanics can be re-
formulated in terms of the density operator language. The purpose of this section and
the next is to explain how to perform this reformulation, and explain when it is useful.
Whether one uses the density operator language or the state vector language is a matter of
taste, since both give the same results; however it is sometimes much easier to approach
problems from one point of view rather than the other.
Suppose, for example, that the evolution of a closed quantum system is described by
the unitary operator U. If the system was initially in the state |ψi⟩with probability pi then
after the evolution has occurred the system will be in the state U|ψi⟩with probability
pi. Thus, the evolution of the density operator is described by the equation
ρ =

i
pi|ψi⟩⟨ψi|
U
−→

i
piU|ψi⟩⟨ψi|U † = UρU †.
(2.139)
Measurements are also easily described in the density operator language. Suppose we
perform a measurement described by measurement operators Mm. If the initial state was
|ψi⟩, then the probability of getting result m is
p(m|i) = ⟨ψi|M †
mMm|ψi⟩= tr(M †
mMm|ψi⟩⟨ψi|),
(2.140)
where we have used Equation (2.61) to obtain the last equality. By the law of total
probability (see Appendix 1 for an explanation of this and other elementary notions of
probability theory) the probability of obtaining result m is
p(m) =

i
p(m|i)pi
(2.141)
=

i
pitr(M †
mMm|ψi⟩⟨ψi|)
(2.142)
= tr(M †
mMmρ).
(2.143)
What is the density operator of the system after obtaining the measurement result m? If
the initial state was |ψi⟩then the state after obtaining the result m is
|ψm
i ⟩=
Mm|ψi⟩

⟨ψi|M †
mMm|ψi⟩
.
(2.144)
100
Introduction to quantum mechanics
Thus, after a measurement which yields the result m we have an ensemble of states |ψm
i ⟩
with respective probabilities p(i|m). The corresponding density operator ρm is therefore
ρm =

i
p(i|m)|ψm
i ⟩⟨ψm
i | =

i
p(i|m)Mm|ψi⟩⟨ψi|M †
m
⟨ψi|M †
mMm|ψi⟩
.
(2.145)
But by elementary probability theory, p(i|m) = p(m, i)/p(m) = p(m|i)pi/p(m). Substi-
tuting from (2.143) and (2.140) we obtain
ρm =

i
pi
Mm|ψi⟩⟨ψi|M †
m
tr(M †
mMmρ)
(2.146)
=
MmρM †
m
tr(M †
mMmρ)
.
(2.147)
What we have shown is that the basic postulates of quantum mechanics related to
unitary evolution and measurement can be rephrased in the language of density operators.
In the next section we complete this rephrasing by giving an intrinsic characterization of
the density operator that does not rely on the idea of a state vector.
Before doing so, however, it is useful to introduce some more language, and one more
fact about the density operator. First, the language. A quantum system whose state |ψ⟩
is known exactly is said to be in a pure state. In this case the density operator is simply
ρ = |ψ⟩⟨ψ|. Otherwise, ρ is in a mixed state; it is said to be a mixture of the different
pure states in the ensemble for ρ. In the exercises you will be asked to demonstrate a
simple criterion for determining whether a state is pure or mixed: a pure state satisﬁes
tr(ρ2) = 1, while a mixed state satisﬁes tr(ρ2) < 1. A few words of warning about the
nomenclature: sometimes people use the term ‘mixed state’ as a catch-all to include both
pure and mixed quantum states. The origin for this usage seems to be that it implies that
the writer is not necessarily assuming that a state is pure. Second, the term ‘pure state’
is often used in reference to a state vector |ψ⟩, to distinguish it from a density operator
ρ.
Finally, imagine a quantum system is prepared in the state ρi with probability pi. It is
not difﬁcult to convince yourself that the system may be described by the density matrix

i piρi. A proof of this is to suppose that ρi arises from some ensemble {pij, |ψij⟩}
(note that i is ﬁxed) of pure states, so the probability for being in the state |ψij⟩is pipij.
The density matrix for the system is thus
ρ =

ij
pipij|ψij⟩⟨ψij|
(2.148)
=

i
piρi,
(2.149)
where we have used the deﬁnition ρi = 
j pij|ψij⟩⟨ψij|. We say that ρ is a mixture
of the states ρi with probabilities pi. This concept of a mixture comes up repeatedly in
the analysis of problems like quantum noise, where the effect of the noise is to introduce
ignorance into our knowledge of the quantum state. A simple example is provided by the
measurement scenario described above. Imagine that, for some reason, our record of the
result m of the measurement was lost. We would have a quantum system in the state
ρm with probability p(m), but would no longer know the actual value of m. The state of
The density operator
101
such a quantum system would therefore be described by the density operator
ρ =

m
p(m)ρm
(2.150)
=

m
tr(M †
mMmρ) MmρM †
m
tr(M †
mMmρ)
(2.151)
=

m
MmρM †
m,
(2.152)
a nice compact formula which may be used as the starting point for analysis of further
operations on the system.
2.4.2
General properties of the density operator
The density operator was introduced as a means of describing ensembles of quantum
states. In this section we move away from this description to develop an intrinsic char-
acterization of density operators that does not rely on an ensemble interpretation. This
allows us to complete the program of giving a description of quantum mechanics that
does not take as its foundation the state vector. We also take the opportunity to develop
numerous other elementary properties of the density operator.
The class of operators that are density operators are characterized by the following
useful theorem:
Theorem 2.5: (Characterization of density operators) An operator ρ is the density
operator associated to some ensemble {pi, |ψi⟩} if and only if it satisﬁes the
conditions:
(1) (Trace condition) ρ has trace equal to one.
(2) (Positivity condition) ρ is a positive operator.
Proof
Suppose ρ = 
i pi|ψi⟩⟨ψi| is a density operator. Then
tr(ρ) =

i
pitr(|ψi⟩⟨ψi|) =

i
pi = 1,
(2.153)
so the trace condition tr(ρ) = 1 is satisﬁed. Suppose |ϕ⟩is an arbitrary vector in state
space. Then
⟨ϕ|ρ|ϕ⟩=

i
pi⟨ϕ|ψi⟩⟨ψi|ϕ⟩
(2.154)
=

i
pi|⟨ϕ|ψi⟩|2
(2.155)
≥0,
(2.156)
so the positivity condition is satisﬁed.
Conversely, suppose ρ is any operator satisfying the trace and positivity conditions.
Since ρ is positive, it must have a spectral decomposition
ρ =

j
λj|j⟩⟨j|,
(2.157)
where the vectors |j⟩are orthogonal, and λj are real, non-negative eigenvalues of ρ.
102
Introduction to quantum mechanics
From the trace condition we see that 
j λj = 1. Therefore, a system in state |j⟩with
probability λj will have density operator ρ. That is, the ensemble {λj, |j⟩} is an ensemble
of states giving rise to the density operator ρ.
This theorem provides a characterization of density operators that is intrinsic to the
operator itself: we can deﬁne a density operator to be a positive operator ρ which has
trace equal to one. Making this deﬁnition allows us to reformulate the postulates of
quantum mechanics in the density operator picture. For ease of reference we state all the
reformulated postulates here:
Postulate 1: Associated to any isolated physical system is a complex vector space
with inner product (that is, a Hilbert space) known as the state space of the
system. The system is completely described by its density operator, which is a
positive operator ρ with trace one, acting on the state space of the system. If a
quantum system is in the state ρi with probability pi, then the density operator for
the system is 
i piρi.
Postulate 2: The evolution of a closed quantum system is described by a unitary
transformation. That is, the state ρ of the system at time t1 is related to the state
ρ′ of the system at time t2 by a unitary operator U which depends only on the
times t1 and t2,
ρ′ = UρU †.
(2.158)
Postulate 3: Quantum measurements are described by a collection {Mm} of
measurement operators. These are operators acting on the state space of the
system being measured. The index m refers to the measurement outcomes that
may occur in the experiment. If the state of the quantum system is ρ immediately
before the measurement then the probability that result m occurs is given by
p(m) = tr(M †
mMmρ),
(2.159)
and the state of the system after the measurement is
MmρM †
m
tr(M †
mMmρ)
.
(2.160)
The measurement operators satisfy the completeness equation,

m
M †
mMm = I.
(2.161)
Postulate 4: The state space of a composite physical system is the tensor product
of the state spaces of the component physical systems. Moreover, if we have
systems numbered 1 through n, and system number i is prepared in the state ρi,
then the joint state of the total system is ρ1 ⊗ρ2 ⊗. . . ρn.
These reformulations of the fundamental postulates of quantum mechanics in terms of
the density operator are, of course, mathematically equivalent to the description in terms
of the state vector. Nevertheless, as a way of thinking about quantum mechanics, the
density operator approach really shines for two applications: the description of quantum
systems whose state is not known, and the description of subsystems of a composite
The density operator
103
quantum system, as will be described in the next section. For the remainder of this
section we ﬂesh out the properties of the density matrix in more detail.
Exercise 2.71: (Criterion to decide if a state is mixed or pure)
Let ρ be a
density operator. Show that tr(ρ2) ≤1, with equality if and only if ρ is a pure
state.
It is a tempting (and surprisingly common) fallacy to suppose that the eigenvalues
and eigenvectors of a density matrix have some special signiﬁcance with regard to the
ensemble of quantum states represented by that density matrix. For example, one might
suppose that a quantum system with density matrix
ρ = 3
4|0⟩⟨0| + 1
4|1⟩⟨1| .
(2.162)
must be in the state |0⟩with probability 3/4 and in the state |1⟩with probability 1/4.
However, this is not necessarily the case. Suppose we deﬁne
|a⟩≡
)
3
4|0⟩+
)
1
4|1⟩
(2.163)
|b⟩≡
)
3
4|0⟩−
)
1
4|1⟩,
(2.164)
and the quantum system is prepared in the state |a⟩with probability 1/2 and in the state
|b⟩with probability 1/2. Then it is easily checked that the corresponding density matrix
is
ρ = 1
2|a⟩⟨a| + 1
2|b⟩⟨b| = 3
4|0⟩⟨0| + 1
4|1⟩⟨1|.
(2.165)
That is, these two different ensembles of quantum states give rise to the same density
matrix. In general, the eigenvectors and eigenvalues of a density matrix just indicate one
of many possible ensembles that may give rise to a speciﬁc density matrix, and there is
no reason to suppose it is an especially privileged ensemble.
A natural question to ask in the light of this discussion is what class of ensembles does
give rise to a particular density matrix? The solution to this problem, which we now give,
has surprisingly many applications in quantum computation and quantum information,
notably in the understanding of quantum noise and quantum error-correction (Chapters 8
and 10). For the solution it is convenient to make use of vectors | ˜ψi⟩which may not be
normalized to unit length. We say the set | ˜ψi⟩generates the operator ρ ≡
i | ˜ψi⟩⟨˜ψi|,
and thus the connection to the usual ensemble picture of density operators is expressed
by the equation | ˜ψi⟩= √pi|ψi⟩. When do two sets of vectors, | ˜ψi⟩and | ˜ϕj⟩generate the
same operator ρ? The solution to this problem will enable us to answer the question of
what ensembles give rise to a given density matrix.
Theorem 2.6: (Unitary freedom in the ensemble for density matrices) The sets
| ˜ψi⟩and | ˜ϕj⟩generate the same density matrix if and only if
| ˜ψi⟩=

j
uij| ˜ϕj⟩,
(2.166)
where uij is a unitary matrix of complex numbers, with indices i and j, and we
104
Introduction to quantum mechanics
‘pad’ whichever set of vectors | ˜ψi⟩or | ˜ϕj⟩is smaller with additional vectors 0 so
that the two sets have the same number of elements.
As a consequence of the theorem, note that ρ = 
i pi|ψi⟩⟨ψi| = 
j qj|ϕj⟩⟨ϕj| for
normalized states |ψi⟩, |ϕj⟩and probability distributions pi and qj if and only if
√pi|ψi⟩=

j
uij√qj|ϕj⟩,
(2.167)
for some unitary matrix uij, and we may pad the smaller ensemble with entries having
probability zero in order to make the two ensembles the same size. Thus, Theorem 2.6
characterizes the freedom in ensembles {pi, |ψi⟩} giving rise to a given density matrix ρ.
Indeed, it is easily checked that our earlier example of a density matrix with two different
decompositions, (2.162), arises as a special case of this general result. Let’s turn now to
the proof of the theorem.
Proof
Suppose | ˜ψi⟩= 
j uij| ˜ϕj⟩for some unitary uij. Then

i
| ˜ψi⟩⟨˜ψi| =

ijk
uiju∗
ik| ˜ϕj⟩⟨˜ϕk|
(2.168)
=

jk

i
u†
kiuij

| ˜ϕj⟩⟨˜ϕk|
(2.169)
=

jk
δkj| ˜ϕj⟩⟨˜ϕk|
(2.170)
=

j
| ˜ϕj⟩⟨˜ϕj|,
(2.171)
which shows that | ˜ψi⟩and | ˜ϕj⟩generate the same operator.
Conversely, suppose
A =

i
| ˜ψi⟩⟨˜ψi| =

j
| ˜ϕj⟩⟨˜ϕj| .
(2.172)
Let A = 
k λk|k⟩⟨k| be a decomposition for A such that the states |k⟩are orthonormal,
and the λk are strictly positive. Our strategy is to relate the states | ˜ψi⟩to the states
|˜k⟩≡√λk|k⟩, and similarly relate the states | ˜ϕj⟩to the states |˜k⟩. Combining the two
relations will give the result. Let |ψ⟩be any vector orthonormal to the space spanned by
the |˜k⟩, so ⟨ψ|˜k⟩⟨˜k|ψ⟩= 0 for all k, and thus we see that
0 = ⟨ψ|A|ψ⟩=

i
⟨ψ| ˜ψi⟩⟨˜ψi|ψ⟩=

i
|⟨ψ| ˜ψi⟩|2.
(2.173)
Thus ⟨ψ| ˜ψi⟩= 0 for all i and all |ψ⟩orthonormal to the space spanned by the |˜k⟩.
It follows that each | ˜ψi⟩can be expressed as a linear combination of the |˜k⟩, | ˜ψi⟩=

k cik|˜k⟩. Since A = 
k |˜k⟩⟨˜k| = 
i | ˜ψi⟩⟨˜ψi| we see that

k
|˜k⟩⟨˜k| =

kl

i
cikc∗
il

|˜k⟩⟨˜l|.
(2.174)
The operators |˜k⟩⟨˜l| are easily seen to be linearly independent, and thus it must be that
The density operator
105

i cikc∗
il = δkl. This ensures that we may append extra columns to c to obtain a unitary
matrix v such that | ˜ψi⟩= 
k vik|˜k⟩, where we have appended zero vectors to the list
of |˜k⟩. Similarly, we can ﬁnd a unitary matrix w such that | ˜ϕj⟩= 
k wjk|˜k⟩. Thus
| ˜ψi⟩= 
j uij| ˜ϕj⟩, where u = vw† is unitary.
Exercise 2.72: (Bloch sphere for mixed states)
The Bloch sphere picture for pure
states of a single qubit was introduced in Section 1.2. This description has an
important generalization to mixed states as follows.
(1) Show that an arbitrary density matrix for a mixed state qubit may be written
as
ρ = I + ⃗r · ⃗σ
2
,
(2.175)
where ⃗r is a real three-dimensional vector such that ∥⃗r∥≤1. This vector is
known as the Bloch vector for the state ρ.
(2) What is the Bloch vector representation for the state ρ = I/2?
(3) Show that a state ρ is pure if and only if ∥⃗r∥= 1.
(4) Show that for pure states the description of the Bloch vector we have given
coincides with that in Section 1.2.
Exercise 2.73:
Let ρ be a density operator. A minimal ensemble for ρ is an ensemble
{pi, |ψi⟩} containing a number of elements equal to the rank of ρ. Let |ψ⟩be
any state in the support of ρ. (The support of a Hermitian operator A is the
vector space spanned by the eigenvectors of A with non-zero eigenvalues.) Show
that there is a minimal ensemble for ρ that contains |ψ⟩, and moreover that in
any such ensemble |ψ⟩must appear with probability
pi =
1
⟨ψi|ρ−1|ψi⟩,
(2.176)
where ρ−1 is deﬁned to be the inverse of ρ, when ρ is considered as an operator
acting only on the support of ρ. (This deﬁnition removes the problem that ρ may
not have an inverse.)
2.4.3
The reduced density operator
Perhaps the deepest application of the density operator is as a descriptive tool for sub-
systems of a composite quantum system. Such a description is provided by the reduced
density operator, which is the subject of this section. The reduced density operator is so
useful as to be virtually indispensable in the analysis of composite quantum systems.
Suppose we have physical systems A and B, whose state is described by a density
operator ρAB. The reduced density operator for system A is deﬁned by
ρA ≡trB(ρAB),
(2.177)
where trB is a map of operators known as the partial trace over system B. The partial
trace is deﬁned by
trB
|a1⟩⟨a2| ⊗|b1⟩⟨b2|
 ≡|a1⟩⟨a2| tr(|b1⟩⟨b2|),
(2.178)
where |a1⟩and |a2⟩are any two vectors in the state space of A, and |b1⟩and |b2⟩are any
two vectors in the state space of B. The trace operation appearing on the right hand side
106
Introduction to quantum mechanics
is the usual trace operation for system B, so tr(|b1⟩⟨b2|) = ⟨b2|b1⟩. We have deﬁned the
partial trace operation only on a special subclass of operators on AB; the speciﬁcation is
completed by requiring in addition to Equation (2.178) that the partial trace be linear in
its input.
It is not obvious that the reduced density operator for system A is in any sense a
description for the state of system A. The physical justiﬁcation for making this identiﬁ-
cation is that the reduced density operator provides the correct measurement statistics for
measurements made on system A. This is explained in more detail in Box 2.6 on page 107.
The following simple example calculations may also help understand the reduced density
operator. First, suppose a quantum system is in the product state ρAB = ρ ⊗σ, where
ρ is a density operator for system A, and σ is a density operator for system B. Then
ρA = trB(ρ ⊗σ) = ρ tr(σ) = ρ,
(2.184)
which is the result we intuitively expect. Similarly, ρB = σ for this state. A less trivial
example is the Bell state (|00⟩+ |11⟩)/
√
2. This has density operator
ρ =
|00⟩+ |11⟩
√
2
 ⟨00| + ⟨11|
√
2

(2.185)
= |00⟩⟨00| + |11⟩⟨00| + |00⟩⟨11| + |11⟩⟨11|
2
.
(2.186)
Tracing out the second qubit, we ﬁnd the reduced density operator of the ﬁrst qubit,
ρ1 = tr2(ρ)
(2.187)
= tr2(|00⟩⟨00|) + tr2(|11⟩⟨00|) + tr2(|00⟩⟨11|) + tr2(|11⟩⟨11|)
2
(2.188)
= |0⟩⟨0|⟨0|0⟩+ |1⟩⟨0|⟨0|1⟩+ |0⟩⟨1|⟨1|0⟩+ |1⟩⟨1|⟨1|1⟩
2
(2.189)
= |0⟩⟨0| + |1⟩⟨1|
2
(2.190)
= I
2 .
(2.191)
Notice that this state is a mixed state, since tr((I/2)2) = 1/2 < 1. This is quite a
remarkable result. The state of the joint system of two qubits is a pure state, that is,
it is known exactly; however, the ﬁrst qubit is in a mixed state, that is, a state about
which we apparently do not have maximal knowledge. This strange property, that the
joint state of a system can be completely known, yet a subsystem be in mixed states, is
another hallmark of quantum entanglement.
Exercise 2.74:
Suppose a composite of systems A and B is in the state |a⟩|b⟩, where
|a⟩is a pure state of system A, and |b⟩is a pure state of system B. Show that
the reduced density operator of system A alone is a pure state.
Exercise 2.75:
For each of the four Bell states, ﬁnd the reduced density operator for
each qubit.
Quantum teleportation and the reduced density operator
A useful application of the reduced density operator is to the analysis of quantum telepor-
tation. Recall from Section 1.3.7 that quantum teleportation is a procedure for sending
The density operator
107
Box 2.6: Why the partial trace?
Why is the partial trace used to describe part of a larger quantum system? The
reason for doing this is because the partial trace operation is the unique operation
which gives rise to the correct description of observable quantities for subsystems
of a composite system, in the following sense.
Suppose M is any observable on system A, and we have some measuring device
which is capable of realizing measurements of M. Let ˜M denote the corresponding
observable for the same measurement, performed on the composite system AB.
Our immediate goal is to argue that ˜M is necessarily equal to M ⊗IB. Note that
if the system AB is prepared in the state |m⟩|ψ⟩, where |m⟩is an eigenstate of M
with eigenvalue m, and |ψ⟩is any state of B, then the measuring device must yield
the result m for the measurement, with probability one. Thus, if Pm is the projector
onto the m eigenspace of the observable M, then the corresponding projector for
˜M is Pm ⊗IB. We therefore have
˜M =

m
mPm ⊗IB = M ⊗IB.
(2.179)
The next step is to show that the partial trace procedure gives the correct mea-
surement statistics for observations on part of a system. Suppose we perform a
measurement on system A described by the observable M. Physical consistency
requires that any prescription for associating a ‘state’, ρA, to system A, must have
the property that measurement averages be the same whether computed via ρA or
ρAB,
tr(MρA) = tr( ˜MρAB) = tr((M ⊗IB)ρAB).
(2.180)
This equation is certainly satisﬁed if we choose ρA ≡trB(ρAB). In fact, the partial
trace turns out to be the unique function having this property. To see this unique-
ness property, let f(·) be any map of density operators on AB to density operators
on A such that
tr(Mf(ρAB)) = tr((M ⊗IB)ρAB),
(2.181)
for all observables M. Let Mi be an orthonormal basis of operators for the space of
Hermitian operators with respect to the Hilbert–Schmidt inner product (X, Y ) ≡
tr(XY ) (compare Exercise 2.39 on page 76). Then expanding f(ρAB) in this basis
gives
f(ρAB) =

i
Mitr(Mif(ρAB))
(2.182)
=

i
Mitr((Mi ⊗IB)ρAB).
(2.183)
It follows that f is uniquely determined by Equation (2.180). Moreover, the partial
trace satisﬁes (2.180), so it is the unique function having this property.
quantum information from Alice to Bob, given that Alice and Bob share an EPR pair,
and have a classical communications channel.
108
Introduction to quantum mechanics
At ﬁrst sight it appears as though teleportation can be used to do faster than light
communication, a big no-no according to the theory of relativity. We surmised in Sec-
tion 1.3.7 that what prevents faster than light communication is the need for Alice to
communicate her measurement result to Bob. The reduced density operator allows us to
make this rigorous.
Recall that immediately before Alice makes her measurement the quantum state of the
three qubits is (Equation (1.32)):
|ψ2⟩= 1
2

|00⟩

α|0⟩+ β|1⟩
 + |01⟩

α|1⟩+ β|0⟩

+|10⟩

α|0⟩−β|1⟩
 + |11⟩

α|1⟩−β|0⟩
	
.
(2.192)
Measuring in Alice’s computational basis, the state of the system after the measurement
is:
|00⟩

α|0⟩+ β|1⟩
	
with probability 1
4
(2.193)
|01⟩

α|1⟩+ β|0⟩
	
with probability 1
4
(2.194)
|10⟩

α|0⟩−β|1⟩
	
with probability 1
4
(2.195)
|11⟩

α|1⟩−β|0⟩
	
with probability 1
4.
(2.196)
The density operator of the system is thus
ρ = 1
4

|00⟩⟨00|(α|0⟩+ β|1⟩)(α∗⟨0| + β∗⟨1|) + |01⟩⟨01|(α|1⟩+ β|0⟩)(α∗⟨1| + β∗⟨0|)
+|10⟩⟨10|(α|0⟩−β|1⟩)(α∗⟨0| −β∗⟨1|) + |11⟩⟨11|(α|1⟩−β|0⟩)(α∗⟨1| −β∗⟨0|)
	
.
(2.197)
Tracing out Alice’s system, we see that the reduced density operator of Bob’s system is
ρB = 1
4

(α|0⟩+ β|1⟩)(α∗⟨0| + β∗⟨1|) + (α|1⟩+ β|0⟩)(α∗⟨1| + β∗⟨0|)
+(α|0⟩−β|1⟩)(α∗⟨0| −β∗⟨1|) + (α|1⟩−β|0⟩)(α∗⟨1| −β∗⟨0|)
	
(2.198)
= 2(|α|2 + |β|2)|0⟩⟨0| + 2(|α|2 + |β|2)|1⟩⟨1|
4
(2.199)
= |0⟩⟨0| + |1⟩⟨1|
2
(2.200)
= I
2,
(2.201)
where we have used the completeness relation in the last line. Thus, the state of Bob’s
system after Alice has performed the measurement but before Bob has learned the mea-
surement result is I/2. This state has no dependence upon the state |ψ⟩being teleported,
and thus any measurements performed by Bob will contain no information about |ψ⟩,
thus preventing Alice from using teleportation to transmit information to Bob faster than
light.
The Schmidt decomposition and puriﬁcations
109
2.5
The Schmidt decomposition and puriﬁcations
Density operators and the partial trace are just the beginning of a wide array of tools
useful for the study of composite quantum systems, which are at the heart of quan-
tum computation and quantum information. Two additional tools of great value are the
Schmidt decomposition and puriﬁcations. In this section we present both these tools,
and try to give the ﬂavor of their power.
Theorem 2.7: (Schmidt decomposition) Suppose |ψ⟩is a pure state of a composite
system, AB. Then there exist orthonormal states |iA⟩for system A, and
orthonormal states |iB⟩of system B such that
|ψ⟩=

i
λi|iA⟩|iB⟩,
(2.202)
where λi are non-negative real numbers satisfying 
i λ2
i = 1 known as Schmidt
co-efﬁcients.
This result is very useful. As a taste of its power, consider the following consequence:
let |ψ⟩be a pure state of a composite system, AB. Then by the Schmidt decomposition
ρA = 
i λ2
i|iA⟩⟨iA| and ρB = 
i λ2
i|iB⟩⟨iB|, so the eigenvalues of ρA and ρB are
identical, namely λ2
i for both density operators. Many important properties of quantum
systems are completely determined by the eigenvalues of the reduced density operator of
the system, so for a pure state of a composite system such properties will be the same for
both systems. As an example, consider the state of two qubits, (|00⟩+ |01⟩+ |11⟩)/
√
3.
This has no obvious symmetry property, yet if you calculate tr

(ρA)2 and tr

(ρB)2
you will discover that they have the same value, 7/9 in each case. This is but one small
consequence of the Schmidt decomposition.
Proof
We give the proof for the case where systems A and B have state spaces of the same
dimension, and leave the general case to Exercise 2.76. Let |j⟩and |k⟩be any ﬁxed
orthonormal bases for systems A and B, respectively. Then |ψ⟩can be written
|ψ⟩=

jk
ajk|j⟩|k⟩,
(2.203)
for some matrix a of complex numbers ajk. By the singular value decomposition, a = udv,
where d is a diagonal matrix with non-negative elements, and u and v are unitary matrices.
Thus
|ψ⟩=

ijk
ujidiivik|j⟩|k⟩.
(2.204)
Deﬁning |iA⟩≡
j uji|j⟩, |iB⟩≡
k vik|k⟩, and λi ≡dii, we see that this gives
|ψ⟩=

i
λi|iA⟩|iB⟩.
(2.205)
It is easy to check that |iA⟩forms an orthonormal set, from the unitarity of u and the
orthonormality of |j⟩, and similarly that the |iB⟩form an orthonormal set.
110
Introduction to quantum mechanics
Exercise 2.76:
Extend the proof of the Schmidt decomposition to the case where A
and B may have state spaces of different dimensionality.
Exercise 2.77:
Suppose ABC is a three component quantum system. Show by
example that there are quantum states |ψ⟩of such systems which can not be
written in the form
|ψ⟩=

i
λi|iA⟩|iB⟩|iC⟩,
(2.206)
where λi are real numbers, and |iA⟩, |iB⟩, |iC⟩are orthonormal bases of the
respective systems.
The bases |iA⟩and |iB⟩are called the Schmidt bases for A and B, respectively, and
the number of non-zero values λi is called the Schmidt number for the state |ψ⟩. The
Schmidt number is an important property of a composite quantum system, which in
some sense quantiﬁes the ‘amount’ of entanglement between systems A and B. To get
some idea of why this is the case, consider the following obvious but important property:
the Schmidt number is preserved under unitary transformations on system A or system
B alone. To see this, notice that if 
i λi|iA⟩|iB⟩is the Schmidt decomposition for |ψ⟩
then 
i λi(U|iA⟩)|iB⟩is the Schmidt decomposition for U|ψ⟩, where U is a unitary
operator acting on system A alone. Algebraic invariance properties of this type make the
Schmidt number a very useful tool.
Exercise 2.78:
Prove that a state |ψ⟩of a composite system AB is a product state if
and only if it has Schmidt number 1. Prove that |ψ⟩is a product state if and only
if ρA (and thus ρB) are pure states.
A second, related technique for quantum computation and quantum information is
puriﬁcation. Suppose we are given a state ρA of a quantum system A. It is possible to
introduce another system, which we denote R, and deﬁne a pure state |AR⟩for the joint
system AR such that ρA = trR(|AR⟩⟨AR|). That is, the pure state |AR⟩reduces to ρA
when we look at system A alone. This is a purely mathematical procedure, known as
puriﬁcation, which allows us to associate pure states with mixed states. For this reason
we call system R a reference system: it is a ﬁctitious system, without a direct physical
signiﬁcance.
To prove that puriﬁcation can be done for any state, we explain how to construct
a system R and puriﬁcation |AR⟩for ρA. Suppose ρA has orthonormal decomposition
ρA = 
i pi|iA⟩⟨iA|. To purify ρA we introduce a system R which has the same state
space as system A, with orthonormal basis states |iR⟩, and deﬁne a pure state for the
combined system
|AR⟩≡

i
√pi|iA⟩|iR⟩.
(2.207)
We now calculate the reduced density operator for system A corresponding to the state
|AR⟩:
trR(|AR⟩⟨AR|) =

ij
√pipj|iA⟩⟨jA| tr(|iR⟩⟨jR|)
(2.208)
=

ij
√pipj|iA⟩⟨jA| δij
(2.209)
EPR and the Bell inequality
111
=

i
pi|iA⟩⟨iA|
(2.210)
= ρA.
(2.211)
Thus |AR⟩is a puriﬁcation of ρA.
Notice the close relationship of the Schmidt decomposition to puriﬁcation: the proce-
dure used to purify a mixed state of system A is to deﬁne a pure state whose Schmidt
basis for system A is just the basis in which the mixed state is diagonal, with the Schmidt
coefﬁcients being the square root of the eigenvalues of the density operator being puriﬁed.
In this section we’ve explained two tools for studying composite quantum systems, the
Schmidt decomposition and puriﬁcations. These tools will be indispensable to the study of
quantum computation and quantum information, especially quantum information, which
is the subject of Part III of this book.
Exercise 2.79:
Consider a composite system consisting of two qubits. Find the
Schmidt decompositions of the states
|00⟩+ |11⟩
√
2
;
|00⟩+ |01⟩+ |10⟩+ |11⟩
2
; and |00⟩+ |01⟩+ |10⟩
√
3
. (2.212)
Exercise 2.80:
Suppose |ψ⟩and |ϕ⟩are two pure states of a composite quantum
system with components A and B, with identical Schmidt coefﬁcients. Show
that there are unitary transformations U on system A and V on system B such
that |ψ⟩= (U ⊗V )|ϕ⟩.
Exercise 2.81: (Freedom in puriﬁcations)
Let |AR1⟩and |AR2⟩be two
puriﬁcations of a state ρA to a composite system AR. Prove that there exists a
unitary transformation UR acting on system R such that
|AR1⟩= (IA ⊗UR)|AR2⟩.
Exercise 2.82:
Suppose {pi, |ψi⟩} is an ensemble of states generating a density matrix
ρ = 
i pi|ψi⟩⟨ψi| for a quantum system A. Introduce a system R with
orthonormal basis |i⟩.
(1) Show that 
i
√pi|ψi⟩|i⟩is a puriﬁcation of ρ.
(2) Suppose we measure R in the basis |i⟩, obtaining outcome i. With what
probability do we obtain the result i, and what is the corresponding state of
system A?
(3) Let |AR⟩be any puriﬁcation of ρ to the system AR. Show that there exists
an orthonormal basis |i⟩in which R can be measured such that the
corresponding post-measurement state for system A is |ψi⟩with probability
pi.
2.6
EPR and the Bell inequality
Anybody who is not shocked by quantum theory has not understood it.
– Niels Bohr
112
Introduction to quantum mechanics
I recall that during one walk Einstein suddenly stopped, turned to me and asked
whether I really believed that the moon exists only when I look at it. The rest
of this walk was devoted to a discussion of what a physicist should mean by the
term ‘to exist’.
– Abraham Pais
...quantum phenomena do not occur in a Hilbert space, they occur in a labora-
tory.
– Asher Peres
...what is proved by impossibility proofs is lack of imagination.
– John Bell
This chapter has focused on introducing the tools and mathematics of quantum mechan-
ics. As these techniques are applied in the following chapters of this book, an important
recurring theme is the unusual, non-classical properties of quantum mechanics. But
what exactly is the difference between quantum mechanics and the classical world? Un-
derstanding this difference is vital in learning how to perform information processing
tasks that are difﬁcult or impossible with classical physics. This section concludes the
chapter with a discussion of the Bell inequality, a compelling example of an essential
difference between quantum and classical physics.
When we speak of an object such as a person or a book, we assume that the physical
properties of that object have an existence independent of observation. That is, measure-
ments merely act to reveal such physical properties. For example, a tennis ball has as one
of its physical properties its position, which we typically measure using light scattered
from the surface of the ball. As quantum mechanics was being developed in the 1920s
and 1930s a strange point of view arose that differs markedly from the classical view. As
described earlier in the chapter, according to quantum mechanics, an unobserved particle
does not possess physical properties that exist independent of observation. Rather, such
physical properties arise as a consequence of measurements performed upon the system.
For example, according to quantum mechanics a qubit does not possess deﬁnite proper-
ties of ‘spin in the z direction, σz’, and ‘spin in the x direction, σx’, each of which can
be revealed by performing the appropriate measurement. Rather, quantum mechanics
gives a set of rules which specify, given the state vector, the probabilities for the possible
measurement outcomes when the observable σz is measured, or when the observable σx
is measured.
Many physicists rejected this new view of Nature. The most prominent objector was
Albert Einstein. In the famous ‘EPR paper’, co-authored with Nathan Rosen and Boris
Podolsky, Einstein proposed a thought experiment which, he believed, demonstrated that
quantum mechanics is not a complete theory of Nature.
The essence of the EPR argument is as follows. EPR were interested in what they
termed ‘elements of reality’. Their belief was that any such element of reality must be
represented in any complete physical theory. The goal of the argument was to show that
quantum mechanics is not a complete physical theory, by identifying elements of reality
that were not included in quantum mechanics. The way they attempted to do this was
by introducing what they claimed was a sufﬁcient condition for a physical property to
EPR and the Bell inequality
113
be an element of reality, namely, that it be possible to predict with certainty the value
that property will have, immediately before measurement.
Box 2.7: Anti-correlations in the EPR experiment
Suppose we prepare the two qubit state
|ψ⟩= |01⟩−|10⟩
√
2
,
(2.213)
a state sometimes known as the spin singlet for historical reasons. It is not difﬁcult
to show that this state is an entangled state of the two qubit system. Suppose we
perform a measurement of spin along the ⃗v axis on both qubits, that is, we measure
the observable ⃗v ·⃗σ (deﬁned in Equation (2.116) on page 90) on each qubit, getting
a result of +1 or −1 for each qubit. It turns out that no matter what choice of ⃗v
we make, the results of the two measurements are always opposite to one another.
That is, if the measurement on the ﬁrst qubit yields +1, then the measurement on
the second qubit will yield −1, and vice versa. It is as though the second qubit
knows the result of the measurement on the ﬁrst, no matter how the ﬁrst qubit is
measured. To see why this is true, suppose |a⟩and |b⟩are the eigenstates of ⃗v · ⃗σ.
Then there exist complex numbers α, β, γ, δ such that
|0⟩= α|a⟩+ β|b⟩
(2.214)
|1⟩= γ|a⟩+ δ|b⟩.
(2.215)
Substituting we obtain
|01⟩−|10⟩
√
2
= (αδ −βγ)|ab⟩−|ba⟩
√
2
.
(2.216)
But αδ −βγ is the determinant of the unitary matrix
 α
β
γ
δ

, and thus is equal
to a phase factor eiθ for some real θ. Thus
|01⟩−|10⟩
√
2
= |ab⟩−|ba⟩
√
2
,
(2.217)
up to an unobservable global phase factor. As a result, if a measurement of ⃗v · ⃗σ
is performed on both qubits, then we can see that a result of +1 (−1) on the ﬁrst
qubit implies a result of −1 (+1) on the second qubit.
Consider, for example, an entangled pair of qubits belonging to Alice and Bob, re-
spectively:
|01⟩−|10⟩
√
2
.
(2.218)
Suppose Alice and Bob are a long way away from one another. Alice performs a mea-
surement of spin along the ⃗v axis, that is, she measures the observable ⃗v · ⃗σ (deﬁned in
Equation (2.116) on page 90). Suppose Alice receives the result +1. Then a simple quan-
tum mechanical calculation, given in Box 2.7, shows that she can predict with certainty
114
Introduction to quantum mechanics
that Bob will measure −1 on his qubit if he also measures spin along the ⃗v axis. Similarly,
if Alice measured −1, then she can predict with certainty that Bob will measure +1 on
his qubit. Because it is always possible for Alice to predict the value of the measurement
result recorded when Bob’s qubit is measured in the ⃗v direction, that physical property
must correspond to an element of reality, by the EPR criterion, and should be repre-
sented in any complete physical theory. However, standard quantum mechanics, as we
have presented it, merely tells one how to calculate the probabilities of the respective
measurement outcomes if ⃗v ·⃗σ is measured. Standard quantum mechanics certainly does
not include any fundamental element intended to represent the value of ⃗v ·⃗σ, for all unit
vectors ⃗v.
The goal of EPR was to show that quantum mechanics is incomplete, by demonstrating
that quantum mechanics lacked some essential ‘element of reality’, by their criterion. They
hoped to force a return to a more classical view of the world, one in which systems could
be ascribed properties which existed independently of measurements performed on those
systems. Unfortunately for EPR, most physicists did not accept the above reasoning as
convincing. The attempt to impose on Nature by ﬁat properties which she must obey
seems a most peculiar way of studying her laws.
Indeed, Nature has had the last laugh on EPR. Nearly thirty years after the EPR paper
was published, an experimental test was proposed that could be used to check whether
or not the picture of the world which EPR were hoping to force a return to is valid or not.
It turns out that Nature experimentally invalidates that point of view, while agreeing
with quantum mechanics.
The key to this experimental invalidation is a result known as Bell’s inequality. Bell’s
inequality is not a result about quantum mechanics, so the ﬁrst thing we need to do is
momentarily forget all our knowledge of quantum mechanics. To obtain Bell’s inequality,
we’re going to do a thought experiment, which we will analyze using our common sense
notions of how the world works – the sort of notions Einstein and his collaborators thought
Nature ought to obey. After we have done the common sense analysis, we will perform a
quantum mechanical analysis which we can show is not consistent with the common sense
analysis. Nature can then be asked, by means of a real experiment, to decide between
our common sense notions of how the world works, and quantum mechanics.
Imagine we perform the following experiment, illustrated in Figure 2.4. Charlie pre-
pares two particles. It doesn’t matter how he prepares the particles, just that he is capable
of repeating the experimental procedure which he uses. Once he has performed the prepa-
ration, he sends one particle to Alice, and the second particle to Bob.
Once Alice receives her particle, she performs a measurement on it. Imagine that she
has available two different measurement apparatuses, so she could choose to do one of
two different measurements. These measurements are of physical properties which we
shall label PQ and PR, respectively. Alice doesn’t know in advance which measurement
she will choose to perform. Rather, when she receives the particle she ﬂips a coin or
uses some other random method to decide which measurement to perform. We suppose
for simplicity that the measurements can each have one of two outcomes, +1 or −1.
Suppose Alice’s particle has a value Q for the property PQ. Q is assumed to be an
objective property of Alice’s particle, which is merely revealed by the measurement,
much as we imagine the position of a tennis ball to be revealed by the particles of light
being scattered off it. Similarly, let R denote the value revealed by a measurement of the
property PR.
EPR and the Bell inequality
115
Similarly, suppose that Bob is capable of measuring one of two properties, PS or PT,
once again revealing an objectively existing value S or T for the property, each taking
value +1 or −1. Bob does not decide beforehand which property he will measure, but
waits until he has received the particle and then chooses randomly. The timing of the
experiment is arranged so that Alice and Bob do their measurements at the same time
(or, to use the more precise language of relativity, in a causally disconnected manner).
Therefore, the measurement which Alice performs cannot disturb the result of Bob’s
measurement (or vice versa), since physical inﬂuences cannot propagate faster than light.













	
Q
R
S
T
Figure 2.4. Schematic experimental setup for the Bell inequalities. Alice can choose to measure either Q or R, and
Bob chooses to measure either S or T. They perform their measurements simultaneously. Alice and Bob are
assumed to be far enough apart that performing a measurement on one system can not have any effect on the result
of measurements on the other.
We are going to do some simple algebra with the quantity QS + RS + RT −QT.
Notice that
QS + RS + RT −QT = (Q + R)S + (R −Q)T.
(2.219)
Because R, Q = ±1 it follows that either (Q + R)S = 0 or (R −Q)T = 0. In either
case, it is easy to see from (2.219) that QS + RS + RT −QT = ±2. Suppose next that
p(q, r, s, t) is the probability that, before the measurements are performed, the system is
in a state where Q = q, R = r, S = s, and T = t. These probabilities may depend on
how Charlie performs his preparation, and on experimental noise. Letting E(·) denote
the mean value of a quantity, we have
E(QS + RS + RT −QT) =

qrst
p(q, r, s, t)(qs + rs + rt −qt)
(2.220)
≤

qrst
p(q, r, s, t) × 2
(2.221)
= 2.
(2.222)
Also,
E(QS + RS + RT −QT) =

qrst
p(q, r, s, t)qs +

qrst
p(q, r, s, t)rs
+

qrst
p(q, r, s, t)rt −

qrst
p(q, r, s, t)qt
(2.223)
= E(QS) + E(RS) + E(RT) −E(QT).
(2.224)
Comparing (2.222) and (2.224) we obtain the Bell inequality,
E(QS) + E(RS) + E(RT) −E(QT) ≤2.
(2.225)
116
Introduction to quantum mechanics
This result is also often known as the CHSH inequality after the initials of its four
discoverers. It is part of a larger set of inequalities known generically as Bell inequalities,
since the ﬁrst was found by John Bell.
By repeating the experiment many times, Alice and Bob can determine each quantity on
the left hand side of the Bell inequality. For example, after ﬁnishing a set of experiments,
Alice and Bob get together to analyze their data. They look at all the experiments where
Alice measured PQ and Bob measured PS. By multiplying the results of their experiments
together, they get a sample of values for QS. By averaging over this sample, they can
estimate E(QS) to an accuracy only limited by the number of experiments which they
perform. Similarly, they can estimate all the other quantities on the left hand side of the
Bell inequality, and thus check to see whether it is obeyed in a real experiment.
It’s time to put some quantum mechanics back in the picture. Imagine we perform the
following quantum mechanical experiment. Charlie prepares a quantum system of two
qubits in the state
|ψ⟩= |01⟩−|10⟩
√
2
.
(2.226)
He passes the ﬁrst qubit to Alice, and the second qubit to Bob. They perform measure-
ments of the following observables:
Q = Z1
S = −Z2 −X2
√
2
(2.227)
R = X1
T = Z2 −X2
√
2
.
(2.228)
Simple calculations show that the average values for these observables, written in the
quantum mechanical ⟨·⟩notation, are:
⟨QS⟩=
1
√
2
; ⟨RS⟩=
1
√
2
; ⟨RT⟩=
1
√
2
; ⟨QT⟩= −1
√
2
.
(2.229)
Thus,
⟨QS⟩+ ⟨RS⟩+ ⟨RT⟩−⟨QT⟩= 2
√
2.
(2.230)
Hold on! We learned back in (2.225) that the average value of QS plus the average value
of RS plus the average value of RT minus the average value of QT can never exceed
two. Yet here, quantum mechanics predicts that this sum of averages yields 2
√
2!
Fortunately, we can ask Nature to resolve the apparent paradox for us. Clever experi-
ments using photons – particles of light – have been done to check the prediction (2.230)
of quantum mechanics versus the Bell inequality (2.225) which we were led to by our
common sense reasoning. The details of the experiments are outside the scope of the
book, but the results were resoundingly in favor of the quantum mechanical prediction.
The Bell inequality (2.225) is not obeyed by Nature.
What does this mean? It means that one or more of the assumptions that went into
the derivation of the Bell inequality must be incorrect. Vast tomes have been written
analyzing the various forms in which this type of argument can be made, and analyzing
the subtly different assumptions which must be made to reach Bell-like inequalities. Here
we merely summarize the main points.
There are two assumptions made in the proof of (2.225) which are questionable:
Chapter problems
117
(1) The assumption that the physical properties PQ, PR, PS, PT have deﬁnite values
Q, R, S, T which exist independent of observation. This is sometimes known as the
assumption of realism.
(2) The assumption that Alice performing her measurement does not inﬂuence the
result of Bob’s measurement. This is sometimes known as the assumption of
locality.
These two assumptions together are known as the assumptions of local realism. They are
certainly intuitively plausible assumptions about how the world works, and they ﬁt our
everyday experience. Yet the Bell inequalities show that at least one of these assumptions
is not correct.
What can we learn from Bell’s inequality? For physicists, the most important lesson
is that their deeply held commonsense intuitions about how the world works are wrong.
The world is not locally realistic. Most physicists take the point of view that it is the
assumption of realism which needs to be dropped from our worldview in quantum me-
chanics, although others have argued that the assumption of locality should be dropped
instead. Regardless, Bell’s inequality together with substantial experimental evidence now
points to the conclusion that either or both of locality and realism must be dropped from
our view of the world if we are to develop a good intuitive understanding of quantum
mechanics.
What lessons can the ﬁelds of quantum computation and quantum information learn
from Bell’s inequality? Historically the most useful lesson has perhaps also been the most
vague: there is something profoundly ‘up’ with entangled states like the EPR state. A lot
of mileage in quantum computation and, especially, quantum information, has come from
asking the simple question: ‘what would some entanglement buy me in this problem?’
As we saw in teleportation and superdense coding, and as we will see repeatedly later
in the book, by throwing some entanglement into a problem we open up a new world
of possibilities unimaginable with classical information. The bigger picture is that Bell’s
inequality teaches us that entanglement is a fundamentally new resource in the world that
goes essentially beyond classical resources; iron to the classical world’s bronze age. A major
task of quantum computation and quantum information is to exploit this new resource to
do information processing tasks impossible or much more difﬁcult with classical resources.
Problem 2.1: (Functions of the Pauli matrices)
Let f(·) be any function from
complex numbers to complex numbers. Let ⃗n be a normalized vector in three
dimensions, and let θ be real. Show that
f (θ⃗n · ⃗σ) = f(θ) + f(−θ)
2
I + f(θ) −f(−θ)
2
⃗n · ⃗σ.
(2.231)
Problem 2.2: (Properties of the Schmidt number)
Suppose |ψ⟩is a pure state of
a composite system with components A and B.
(1) Prove that the Schmidt number of |ψ⟩is equal to the rank of the reduced
density matrix ρA ≡trB(|ψ⟩⟨ψ|). (Note that the rank of a Hermitian
operator is equal to the dimension of its support.)
(2) Suppose |ψ⟩= 
j |αj⟩|βj⟩is a representation for |ψ⟩, where |αj⟩and |βj⟩
are (un-normalized) states for systems A and B, respectively. Prove that the
118
Introduction to quantum mechanics
number of terms in such a decomposition is greater than or equal to the
Schmidt number of |ψ⟩, Sch(ψ).
(3) Suppose |ψ⟩= α|ϕ⟩+ β|γ⟩. Prove that
Sch(ψ) ≥|Sch(ϕ) −Sch(γ)| .
(2.232)
Problem 2.3: (Tsirelson’s inequality)
Suppose
Q = ⃗q · ⃗σ, R = ⃗r · ⃗σ, S = ⃗s · ⃗σ, T = ⃗t · ⃗σ, where ⃗q,⃗r,⃗s and ⃗t are real unit vectors
in three dimensions. Show that
(Q ⊗S + R ⊗S + R ⊗T −Q ⊗T)2 = 4I + [Q, R] ⊗[S, T].
(2.233)
Use this result to prove that
⟨Q ⊗S⟩+ ⟨R ⊗S⟩+ ⟨R ⊗T⟩−⟨Q ⊗T⟩≤2
√
2,
(2.234)
so the violation of the Bell inequality found in Equation (2.230) is the maximum
possible in quantum mechanics.
History and further reading
There are an enormous number of books on linear algebra at levels ranging from High
School through to Graduate School. Perhaps our favorites are the two volume set by
Horn and Johnson[HJ85, HJ91], which cover an extensive range of topics in an accessible
manner. Other useful references include Marcus and Minc[MM92], and Bhatia[Bha97]. Good
introductions to linear algebra include Halmos[Hal58], Perlis[Per52], and Strang[Str76].
There are many excellent books on quantum mechanics. Unfortunately, most of
these books focus on topics of tangential interest to quantum information and computa-
tion. Perhaps the most relevant in the existing literature is Peres’ superb book[Per93].
Beside an extremely clear exposition of elementary quantum mechanics, Peres gives
an extensive discussion of the Bell inequalities and related results. Good introductory
level texts include Sakurai’s book[Sak95], Volume III of the superb series by Feynman,
Leighton, and Sands[FLS65a], and the two volume work by Cohen-Tannoudji, Diu and
Lalo¨e[CTDL77a, CTDL77b]. All three of these works are somewhat closer in spirit to quan-
tum computation and quantum information than are most other quantum mechanics
texts, although the great bulk of each is still taken up by applications far removed from
quantum computation and quantum information. As a result, none of these texts need
be read in detail by someone interested in learning about quantum computation and
quantum information. However, any one of these texts may prove handy as a reference,
especially when reading articles by physicists. References for the history of quantum
mechanics may be found at the end of Chapter 1.
Many texts on quantum mechanics deal only with projective measurements. For ap-
plications to quantum computing and quantum information it is more convenient – and,
we believe, easier for novices – to start with the general description of measurements,
of which projective measurements can be regarded as a special case. Of course, ulti-
mately, as we have shown, the two approaches are equivalent. The theory of generalized
measurements which we have employed was developed between the 1940s and 1970s.
Much of the history can be distilled from the book of Kraus[Kra83]. Interesting discus-
sion of quantum measurements may be found in Section 2.2 of Gardiner[Gar91], and in
the book by Braginsky and Khahili[BK92]. The POVM measurement for distinguishing
History and further reading
119
non-orthogonal states described in Section 2.2.6 is due to Peres[Per88]. The extension
described in Exercise 2.64 appeared in Duan and Guo[DG98].
Superdense coding was invented by Bennett and Wiesner[BW92]. An experiment im-
plementing a variant of superdense coding using entangled photon pairs was performed
by Mattle, Weinfurter, Kwiat, and Zeilinger[MWKZ96].
The density operator formalism was introduced independently by Landau[Lan27] and
by von Neumann[von27]. The unitary freedom in the ensemble for density matrices, The-
orem 2.6, was ﬁrst pointed out by Schro¨dinger[Sch36], and was later rediscovered and
extended by Jaynes[Jay57] and by Hughston, Jozsa and Wootters[HJW93]. The result of Ex-
ercise 2.73 is from the paper by Jaynes, and the results of Exercises 2.81 and 2.82 appear
in the paper by Hughston, Jozsa and Wootters. The class of probability distributions
which may appear in a density matrix decomposition for a given density matrix has been
studied by Uhlmann[Uhl70] and by Nielsen[Nie99b]. Schmidt’s eponymous decomposition
appeared in[Sch06]. The result of Exercise 2.77 was noted by Peres[Per95].
The EPR thought experiment is due to Einstein, Podolsky and Rosen[EPR35], and
was recast in essentially the form we have given here by Bohm[Boh51]. It is sometimes
misleadingly referred to as the EPR ‘paradox’. The Bell inequality is named in honour
of Bell[Bel64], who ﬁrst derived inequalities of this type. The form we have presented is
due to Clauser, Horne, Shimony, and Holt[CHSH69], and is often known as the CHSH
inequality. This inequality was derived independently by Bell, who did not publish the
result.
Part 3 of Problem 2.2 is due to Thapliyal (private communication). Tsirelson’s in-
equality is due to Tsirelson[Tsi80].
3 Introduction to computer science
In natural science, Nature has given us a world and we’re just to discover its
laws. In computers, we can stuff laws into it and create a world.
– Alan Kay
Our ﬁeld is still in its embryonic stage. It’s great that we haven’t been around
for 2000 years. We are still at a stage where very, very important results occur
in front of our eyes.
– Michael Rabin, on computer science
Algorithms are the key concept of computer science. An algorithm is a precise recipe for
performing some task, such as the elementary algorithm for adding two numbers which
we all learn as children. This chapter outlines the modern theory of algorithms developed
by computer science. Our fundamental model for algorithms will be the Turing machine.
This is an idealized computer, rather like a modern personal computer, but with a simpler
set of basic instructions, and an idealized unbounded memory. The apparent simplicity
of Turing machines is misleading; they are very powerful devices. We will see that they
can be used to execute any algorithm whatsoever, even one running on an apparently
much more powerful computer.
The fundamental question we are trying to address in the study of algorithms is: what
resources are required to perform a given computational task? This question splits up
naturally into two parts. First, we’d like to understand what computational tasks are pos-
sible, preferably by giving explicit algorithms for solving speciﬁc problems. For example,
we have many excellent examples of algorithms that can quickly sort a list of numbers
into ascending order. The second aspect of this question is to demonstrate limitations
on what computational tasks may be accomplished. For example, lower bounds can be
given for the number of operations that must be performed by any algorithm which
sorts a list of numbers into ascending order. Ideally, these two tasks – the ﬁnding of
algorithms for solving computational problems, and proving limitations on our ability to
solve computational problems – would dovetail perfectly. In practice, a signiﬁcant gap
often exists between the best techniques known for solving a computational problem, and
the most stringent limitations known on the solution. The purpose of this chapter is to
give a broad overview of the tools which have been developed to aid in the analysis of
computational problems, and in the construction and analysis of algorithms to solve such
problems.
Why should a person interested in quantum computation and quantum information
spend time investigating classical computer science? There are three good reasons for this
effort. First, classical computer science provides a vast body of concepts and techniques
which may be reused to great effect in quantum computation and quantum informa-
tion. Many of the triumphs of quantum computation and quantum information have
come by combining existing ideas from computer science with novel ideas from quantum
Introduction to computer science
121
mechanics. For example, some of the fast algorithms for quantum computers are based
upon the Fourier transform, a powerful tool utilized by many classical algorithms. Once
it was realized that quantum computers could perform a type of Fourier transform much
more quickly than classical computers this enabled the development of many important
quantum algorithms.
Second, computer scientists have expended great effort understanding what resources
are required to perform a given computational task on a classical computer. These results
can be used as the basis for a comparison with quantum computation and quantum
information. For example, much attention has been focused on the problem of ﬁnding the
prime factors of a given number. On a classical computer this problem is believed to have
no ‘efﬁcient’ solution, where ‘efﬁcient’ has a meaning we’ll explain later in the chapter.
What is interesting is that an efﬁcient solution to this problem is known for quantum
computers. The lesson is that, for this task of ﬁnding prime factors, there appears to be a
gap between what is possible on a classical computer and what is possible on a quantum
computer. This is both intrinsically interesting, and also interesting in the broader sense
that it suggests such a gap may exist for a wider class of computational problems than
merely the ﬁnding of prime factors. By studying this speciﬁc problem further, it may be
possible to discern features of the problem which make it more tractable on a quantum
computer than on a classical computer, and then act on these insights to ﬁnd interesting
quantum algorithms for the solution of other problems.
Third, and most important, there is learning to think like a computer scientist. Com-
puter scientists think in a rather different style than does a physicist or other natural
scientist. Anybody wanting a deep understanding of quantum computation and quantum
information must learn to think like a computer scientist at least some of the time; they
must instinctively know what problems, what techniques, and most importantly what
problems are of greatest interest to a computer scientist.
The structure of this chapter is as follows. In Section 3.1 we introduce two models for
computation: the Turing machine model, and the circuit model. The Turing machine
model will be used as our fundamental model for computation. In practice, however,
we mostly make use of the circuit model of computation, and it is this model which is
most useful in the study of quantum computation. With our models for computation
in hand, the remainder of the chapter discusses resource requirements for computation.
Section 3.2 begins by overviewing the computational tasks we are interested in as well
as discusing some associated resource questions. It continues with a broad look at the
key concepts of computational complexity, a ﬁeld which examines the time and space
requirements necessary to solve particular computational problems, and provides a broad
classiﬁcation of problems based upon their difﬁculty of solution. Finally, the section
concludes with an examination of the energy resources required to perform computations.
Surprisingly, it turns out that the energy required to perform a computation can be made
vanishingly small, provided one can make the computation reversible. We explain how to
construct reversible computers, and explain some of the reasons they are important both
for computer science and for quantum computation and quantum information. Section 3.3
concludes the chapter with a broad look at the entire ﬁeld of computer science, focusing
on issues of particular relevance to quantum computation and quantum information.
122
Introduction to computer science
3.1
Models for computation
...algorithms are concepts that have existence apart from any programming
language.
– Donald Knuth
What does it mean to have an algorithm for performing some task? As children we all
learn a procedure which enables us to add together any two numbers, no matter how
large those numbers are. This is an example of an algorithm. Finding a mathematically
precise formulation of the concept of an algorithm is the goal of this section.
Historically, the notion of an algorithm goes back centuries; undergraduates learn
Euclid’s two thousand year old algorithm for ﬁnding the greatest common divisor of two
positive integers. However, it wasn’t until the 1930s that the fundamental notions of
the modern theory of algorithms, and thus of computation, were introduced, by Alonzo
Church, Alan Turing, and other pioneers of the computer era. This work arose in response
to a profound challenge laid down by the great mathematician David Hilbert in the early
part of the twentieth century. Hilbert asked whether or not there existed some algorithm
which could be used, in principle, to solve all the problems of mathematics. Hilbert
expected that the answer to this question, sometimes known as the entscheidungsproblem,
would be yes.
Amazingly, the answer to Hilbert’s challenge turned out to be no: there is no algorithm
to solve all mathematical problems. To prove this, Church and Turing had to solve the
deep problem of capturing in a mathematical deﬁnition what we mean when we use the
intuitive concept of an algorithm. In so doing, they laid the foundations for the modern
theory of algorithms, and consequently for the modern theory of computer science.
In this chapter, we use two ostensibly different approaches to the theory of computa-
tion. The ﬁrst approach is that proposed by Turing. Turing deﬁned a class of machines,
now known as Turing machines, in order to capture the notion of an algorithm to perform
a computational task. In Section 3.1.1, we describe Turing machines, and then discuss
some of the simpler variants of the Turing machine model. The second approach is via
the circuit model of computation, an approach that is especially useful as preparation for
our later study of quantum computers. The circuit model is described in Section 3.1.2.
Although these models of computation appear different on the surface, it turns out that
they are equivalent. Why introduce more than one model of computation, you may ask?
We do so because different models of computation may yield different insights into the
solution of speciﬁc problems. Two (or more) ways of thinking about a concept are better
than one.
3.1.1
Turing machines
The basic elements of a Turing machine are illustrated in Figure 3.1. A Turing machine
contains four main elements: (a) a program, rather like an ordinary computer; (b) a ﬁnite
state control, which acts like a stripped-down microprocessor, co-ordinating the other
operations of the machine; (c) a tape, which acts like a computer memory; and (d) a read-
write tape-head, which points to the position on the tape which is currently readable or
writable. We now describe each of these four elements in more detail.
The ﬁnite state control for a Turing machine consists of a ﬁnite set of internal states,
Models for computation
123


!"	 	
"
#$	











%	&'	
(	
Figure 3.1. Main elements of a Turing machine. In the text, blanks on the tape are denoted by a ‘b’. Note the ▷
marking the left hand end of the tape.
q1, . . . , qm. The number m is allowed to be varied; it turns out that for m sufﬁciently
large this does not affect the power of the machine in any essential way, so without loss
of generality we may suppose that m is some ﬁxed constant. The best way to think of
the ﬁnite state control is as a sort of microprocessor, co-ordinating the Turing machine’s
operation. It provides temporary storage off-tape, and a central place where all processing
for the machine may be done. In addition to the states q1, . . . , qm, there are also two special
internal states, labelled qs and qh. We call these the starting state and the halting state,
respectively. The idea is that at the beginning of the computation, the Turing machine
is in the starting state qs. The execution of the computation causes the Turing machine’s
internal state to change. If the computation ever ﬁnishes, the Turing machine ends up
in the state qh to indicate that the machine has completed its operation.
The Turing machine tape is a one-dimensional object, which stretches off to inﬁnity
in one direction. The tape consists of an inﬁnite sequence of tape squares. We number
the tape squares 0, 1, 2, 3, . . .. The tape squares each contain one symbol drawn from
some alphabet, Γ, which contains a ﬁnite number of distinct symbols. For now, it will
be convenient to assume that the alphabet contains four symbols, which we denote by
0, 1, b (the ‘blank’ symbol), and ▷, to mark the left hand edge of the tape. Initially, the
tape contains a ▷at the left hand end, a ﬁnite number of 0s and 1s, and the rest of the
tape contains blanks. The read-write tape-head identiﬁes a single square on the Turing
machine tape as the square that is currently being accessed by the machine.
Summarizing, the machine starts its operation with the ﬁnite state control in the state
qs, and with the read-write head at the leftmost tape square, the square numbered 0. The
computation then proceeds in a step by step manner according to the program, to be
deﬁned below. If the current state is qh, then the computation has halted, and the output
of the computation is the current (non-blank) contents of the tape.
A program for a Turing machine is a ﬁnite ordered list of program lines of the form
⟨q, x, q′, x′, s⟩. The ﬁrst item in the program line, q, is a state from the set of internal
states of the machine. The second item, x, is taken from the alphabet of symbols which
may appear on the tape, Γ. The way the program works is that on each machine cycle,
the Turing machine looks through the list of program lines in order, searching for a
line ⟨q, x, ·, ·, ·⟩, such that the current internal state of the machine is q, and the symbol
124
Introduction to computer science
being read on the tape is x. If it doesn’t ﬁnd such a program line, the internal state of the
machine is changed to qh, and the machine halts operation. If such a line is found, then
that program line is executed. Execution of a program line involves the following steps:
the internal state of the machine is changed to q′; the symbol x on the tape is overwritten
by the symbol x′, and the tape-head moves left, right, or stands still, depending on
whether s is −1, +1, or 0, respectively. The only exception to this rule is if the tape-head
is at the leftmost tape square, and s = −1, in which case the tape-head stays put.
Now that we know what a Turing machine is, let’s see how it may be used to compute
a simple function. Consider the following example of a Turing machine. The machine
starts with a binary number, x, on the tape, followed by blanks. The machine has three
internal states, q1, q2, and q3, in addition to the starting state qs and halting state qh. The
program contains the following program lines (the numbers on the left hand side are for
convenience in referring to the program lines in later discussion, and do not form part
of the program):
1 : ⟨qs, ▷, q1, ▷, +1⟩
2 : ⟨q1, 0, q1, b, +1⟩
3 : ⟨q1, 1, q1, b, +1⟩
4 : ⟨q1, b, q2, b, −1⟩
5 : ⟨q2, b, q2, b, −1⟩
6 : ⟨q2, ▷, q3, ▷, +1⟩
7 : ⟨q3, b, qh, 1, 0⟩.
What function does this program compute? Initially the machine is in the state qs and
at the left-most tape position so line 1, ⟨qs, ▷, q1, ▷, +1⟩, is executed, which causes the
tape-head to move right without changing what is written on the tape, but changing the
internal state of the machine to q1. The next three lines of the program ensure that while
the machine is in the state q1 the tape-head will continue moving right while it reads
either 0s (line 2) or 1s (line 3) on the tape, over-writing the tape contents with blanks as
it goes and remaining in the state q1, until it reaches a tape square that is already blank,
at which point the tape-head is moved one position to the left, and the internal state is
changed to q2 (line 4). Line 5 then ensures that the tape-head keeps moving left while
blanks are being read by the tape-head, without changing the contents of the tape. This
keeps up until the tape-head returns to its starting point, at which point it reads a ▷on
the tape, changes the internal state to q3, and moves one step to the right (line 6). Line
7 completes the program, simply printing the number 1 onto the tape, and then halting.
The preceding analysis shows that this program computes the constant function f(x) =
1. That is, regardless of what number is input on the tape the number 1 is output. More
generally, a Turing machine can be thought of as computing functions from the non-
negative integers to the non-negative integers; the initial state of the tape is used to
represent the input to the function, and the ﬁnal state of the tape is used to represent
the output of the function.
It seems as though we have gone to a very great deal of trouble to compute this
simple function using our Turing machines. Is it possible to build up more complicated
functions using Turing machines? For example, could we construct a machine such that
when two numbers, x and y, are input on the tape with a blank to demarcate them, it will
Models for computation
125
output the sum x + y on the tape? More generally, what class of functions is it possible
to compute using a Turing machine?
It turns out that the Turing machine model of computation can be used to compute an
enormous variety of functions. For example, it can be used to do all the basic arithmetical
operations, to search through text represented as strings of bits on the tape, and many
other interesting operations. Surprisingly, it turns out that a Turing machine can be
used to simulate all the operations performed on a modern computer! Indeed, according
to a thesis put forward independently by Church and by Turing, the Turing machine
model of computation completely captures the notion of computing a function using an
algorithm. This is known as the Church–Turing thesis:
The class of functions computable by a Turing machine corresponds exactly to
the class of functions which we would naturally regard as being computable by an
algorithm.
The Church–Turing thesis asserts an equivalence between a rigorous mathematical
concept – function computable by a Turing machine – and the intuitive concept of
what it means for a function to be computable by an algorithm. The thesis derives its
importance from the fact that it makes the study of real-world algorithms, prior to 1936
a rather vague concept, amenable to rigorous mathematical study. To understand the
signiﬁcance of this point it may be helpful to consider the deﬁnition of a continuous
function from real analysis. Every child can tell you what it means for a line to be
continuous on a piece of paper, but it is far from obvious how to capture that intuition in
a rigorous deﬁnition. Mathematicians in the nineteenth century spent a great deal of time
arguing about the merits of various deﬁnitions of continuity before the modern deﬁnition
of continuity came to be accepted. When making fundamental deﬁnitions like that of
continuity or of computability it is important that good deﬁnitions be chosen, ensuring
that one’s intuitive notions closely match the precise mathematical deﬁnition. From this
point of view the Church–Turing thesis is simply the assertion that the Turing machine
model of computation provides a good foundation for computer science, capturing the
intuitive notion of an algorithm in a rigorous deﬁnition.
A priori it is not obvious that every function which we would intuitively regard as
computable by an algorithm can be computed using a Turing machine. Church, Tur-
ing and many other people have spent a great deal of time gathering evidence for the
Church–Turing thesis, and in sixty years no evidence to the contrary has been found.
Nevertheless, it is possible that in the future we will discover in Nature a process which
computes a function not computable on a Turing machine. It would be wonderful if
that ever happened, because we could then harness that process to help us perform new
computations which could not be performed before. Of course, we would also need to
overhaul the deﬁnition of computability, and with it, computer science.
Exercise 3.1: (Non-computable processes in Nature)
How might we recognize
that a process in Nature computes a function not computable by a Turing
machine?
Exercise 3.2: (Turing numbers)
Show that single-tape Turing machines can each
be given a number from the list 1, 2, 3, . . . in such a way that the number
uniquely speciﬁes the corresponding machine. We call this number the Turing
number of the corresponding Turing machine. (Hint: Every positive integer has
126
Introduction to computer science
a unique prime factorization pa1
1 pa2
2 . . . pak
k , where pi are distinct prime numbers,
and a1, . . . , ak are non-negative integers.)
In later chapters, we will see that quantum computers also obey the Church–Turing
thesis. That is, quantum computers can compute the same class of functions as is com-
putable by a Turing machine. The difference between quantum computers and Turing
machines turns out to lie in the efﬁciency with which the computation of the function
may be performed – there are functions which can be computed much more efﬁciently
on a quantum computer than is believed to be possible with a classical computing device
such as a Turing machine.
Demonstrating in complete detail that the Turing machine model of computation can
be used to build up all the usual concepts used in computer programming languages is
beyond the scope of this book (see ‘History and further reading’ at the end of the chapter
for more information). When specifying algorithms, instead of explicitly specifying the
Turing machine used to compute the algorithm, we shall usually use a much higher level
pseudocode, trusting in the Church–Turing thesis that this pseudocode can be translated
into the Turing machine model of computation. We won’t give any sort of rigorous
deﬁnition for pseudocode. Think of it as a slightly more formal version of English or, if
you like, a sloppy version of a high-level programming language such as C++ or BASIC.
Pseudocode provides a convenient way of expressing algorithms, without going into the
extreme level of detail required by a Turing machine. An example use of pseudocode
may be found in Box 3.2 on page 130; it is also used later in the book to describe quantum
algorithms.
There are many variants on the basic Turing machine model. We might imagine
Turing machines with different kinds of tapes. For example, one could consider two-way
inﬁnite tapes, or perhaps computation with tapes of more than one dimension. So far
as is presently known, it is not possible to change any aspect of the Turing model in
a way that is physically reasonable, and which manages to extend the class of functions
computable by the model.
As an example consider a Turing machine equipped with multiple tapes. For simplicity
we consider the two-tape case, as the generalization to more than two tapes is clear from
this example. Like the basic Turing machine, a two-tape Turing machine has a ﬁnite
number of internal states q1, . . . , qm, a start state qs, and a halt state qh. It has two tapes,
each of which contain symbols from some ﬁnite alphabet of symbols, Γ. As before we
ﬁnd it convenient to assume that the alphabet contains four symbols, 0, 1, b and ▷, where
▷marks the left hand edge of each tape. The machine has two tape-heads, one for each
tape. The main difference between the two-tape Turing machine and the basic Turing
machine is in the program. Program lines are of the form ⟨q, x1, x2, q′, x′
1, x′
2, s1, s2⟩,
meaning that if the internal state of the machine is q, tape one is reading x1 at its current
position, and tape two is reading x2 at its current position, then the internal state of the
machine should be changed to q′, x1 overwritten with x′
1, x2 overwritten with x′
2, and
the tape-heads for tape one and tape two moved according to whether s1 or s2 are equal
to +1, −1 or 0, respectively.
In what sense are the basic Turing machine and the two-tape Turing machine equiv-
alent models of computation? They are equivalent in the sense that each computational
model is able to simulate the other. Suppose we have a two-tape Turing machine which
takes as input a bit string x on the ﬁrst tape and blanks on the remainder of both tapes,
Models for computation
127
except the endpoint marker ▷. This machine computes a function f(x), where f(x) is
deﬁned to be the contents of the ﬁrst tape after the Turing machine has halted. Rather
remarkably, it turns out that given a two-tape Turing machine to compute f, there exists
an equivalent single-tape Turing machine that is also able to compute f. We won’t ex-
plain how to do this explicitly, but the basic idea is that the single-tape Turing machine
simulates the two-tape Turing machine, using its single tape to store the contents of both
tapes of the two-tape Turing machine. There is some computational overhead required
to do this simulation, but the important point is that in principle it can always be done.
In fact, there exists a Universal Turing machine (see Box 3.1) which can simulate any
other Turing machine!
Another interesting variant of the Turing machine model is to introduce randomness
into the model. For example, imagine that the Turing machine can execute a program
line whose effect is the following: if the internal state is q and the tape-head reads x,
then ﬂip an unbiased coin. If the coin lands heads, change the internal state to qiH,
and if it lands tails, change the internal state to qiT, where qiH and qiT are two internal
states of the Turing machine. Such a program line can be represented as ⟨q, x, qiH, qiT⟩.
However, even this variant doesn’t change the essential power of the Turing machine
model of computation. It is not difﬁcult to see that we can simulate the effect of the above
algorithm on a deterministic Turing machine by explicitly ‘searching out’ all the possible
computational paths corresponding to different values of the coin tosses. Of course, this
deterministic simulation may be far less efﬁcient than the random model, but the key
point for the present discussion is that the class of functions computable is not changed
by introducing randomness into the underlying model.
Exercise 3.3: (Turing machine to reverse a bit string)
Describe a Turing
machine which takes a binary number x as input, and outputs the bits of x in
reverse order. (Hint: In this exercise and the next it may help to use a multi-tape
Turing machine and/or symbols other than ▷, 0, 1 and the blank.)
Exercise 3.4: (Turing machine to add modulo 2)
Describe a Turing machine to
add two binary numbers x and y modulo 2. The numbers are input on the
Turing machine tape in binary, in the form x, followed by a single blank,
followed by a y. If one number is not as long as the other then you may assume
that it has been padded with leading 0s to make the two numbers the same
length.
Let us return to Hilbert’s entscheidungsproblem, the original inspiration for the
founders of computer science. Is there an algorithm to decide all the problems of math-
ematics? The answer to this question was shown by Church and Turing to be no. In
Box 3.2, we explain Turing’s proof of this remarkable fact. This phenomenon of unde-
cidability is now known to extend far beyond the examples which Church and Turing
constructed. For example, it is known that the problem of deciding whether two topo-
logical spaces are topologically equivalent (‘homeomorphic’) is undecidable. There are
simple problems related to the behavior of dynamical systems which are undecidable, as
you will show in Problem 3.4. References for these and other examples are given in the
end of chapter ‘History and further reading’.
Besides its intrinsic interest, undecidability foreshadows a topic of great concern in
computer science, and also to quantum computation and quantum information: the dis-
128
Introduction to computer science
Box 3.1: The Universal Turing Machine
We’ve described Turing machines as containing three elements which may vary
from machine to machine – the initial conﬁguration of the tape, the internal states
of the ﬁnite state control, and the program for the machine. A clever idea known
as the Universal Turing Machine (UTM) allows us to ﬁx the program and ﬁnite
state control once and for all, leaving the initial contents of the tape as the only part
of the machine which needs to be varied.
The Universal Turing Machine (see the ﬁgure below) has the following property.
Let M be any Turing machine, and let TM be the Turing number associated to
machine M. Then on input of the binary representation for TM followed by a blank,
followed by any string of symbols x on the remainder of the tape, the Universal
Turing Machine gives as output whatever machine M would have on input of x.
Thus, the Universal Turing Machine is capable of simulating any other Turing
machine!
)#*
#*


)#*
*
$	
The Universal Turing Machine is similar in spirit to a modern programmable
computer, in which the action to be taken by the computer – the ‘program’ – is
stored in memory, analogous to the bit string TM stored at the beginning of the
tape by the Universal Turing Machine. The data to be processed by the program
is stored in a separate part of memory, analogous to the role of x in the Universal
Turing Machine. Then some ﬁxed hardware is used to run the program, producing
the output. This ﬁxed hardware is analogous to the internal states and the (ﬁxed)
program being executed by the Universal Turing Machine.
Describing the detailed construction of a Universal Turing Machine is beyond the
scope of this book. (Though industrious readers may like to attempt the construc-
tion.) The key point is the existence of such a machine, showing that a single ﬁxed
machine can be used to run any algorithm whatsoever. The existence of a Univer-
sal Turing Machine also explains our earlier statement that the number of internal
states in a Turing machine does not matter much, for provided that number m
exceeds the number needed for a Universal Turing Machine, such a machine can
be used to simulate a Turing machine with any number of internal states.
tinction between problems which are easy to solve, and problems which are hard to solve.
Undecidability provides the ultimate example of problems which are hard to solve – so
hard that they are in fact impossible to solve.
Exercise 3.5: (Halting problem with no inputs)
Show that given a Turing
Models for computation
129
machine M there is no algorithm to determine whether M halts when the input
to the machine is a blank tape.
Exercise 3.6: (Probabilistic halting problem)
Suppose we number the
probabilistic Turing machines using a scheme similar to that found in
Exercise 3.2 and deﬁne the probabilistic halting function hp(x) to be 1 if
machine x halts on input of x with probability at least 1/2 and 0 if machine x
halts on input of x with probability less than 1/2. Show that there is no
probabilistic Turing machine which can output hp(x) with probability of
correctness strictly greater than 1/2 for all x.
Exercise 3.7: (Halting oracle)
Suppose a black box is made available to us which
takes a non-negative integer x as input, and then outputs the value of h(x),
where h(·) is the halting function deﬁned in Box 3.2 on page 130. This type of
black box is sometimes known as an oracle for the halting problem. Suppose we
have a regular Turing machine which is augmented by the power to call the
oracle. One way of accomplishing this is to use a two-tape Turing machine, and
add an extra program instruction to the Turing machine which results in the
oracle being called, and the value of h(x) being printed on the second tape,
where x is the current contents of the second tape. It is clear that this model for
computation is more powerful than the conventional Turing machine model,
since it can be used to compute the halting function. Is the halting problem for
this model of computation undecidable? That is, can a Turing machine aided by
an oracle for the halting problem decide whether a program for the Turing
machine with oracle will halt on a particular input?
3.1.2
Circuits
Turing machines are rather idealized models of computing devices. Real computers are
ﬁnite in size, whereas for Turing machines we assumed a computer of unbounded size.
In this section we investigate an alternative model of computation, the circuit model, that
is equivalent to the Turing machine in terms of computational power, but is more conve-
nient and realistic for many applications. In particular the circuit model of computation
is especially important as preparation for our investigation of quantum computers.
A circuit is made up of wires and gates, which carry information around, and perform
simple computational tasks, respectively. For example, Figure 3.2 shows a simple circuit
which takes as input a single bit, a. This bit is passed through a
gate, which ﬂips
the bit, taking 1 to 0 and 0 to 1. The wires before and after the
gate serve merely to
carry the bit to and from the
gate; they can represent movement of the bit through
space, or perhaps just through time.
More generally, a circuit may involve many input and output bits, many wires, and
many logical gates. A logic gate is a function f : {0, 1}k →{0, 1}l from some ﬁxed
number k of input bits to some ﬁxed number l of output bits. For example, the
gate
is a gate with one input bit and one output bit which computes the function f(a) = 1⊕a,
where a is a single bit, and ⊕is modulo 2 addition. It is also usual to make the convention
that no loops are allowed in the circuit, to avoid possible instabilities, as illustrated in
Figure 3.3. We say such a circuit is acyclic, and we adhere to the convention that circuits
in the circuit model of computation be acyclic.
130
Introduction to computer science
Box 3.2: The halting problem
In Exercise 3.2 you showed that each Turing machine can be uniquely associated
with a number from the list 1, 2, 3, . . .. To solve Hilbert’s problem, Turing used this
numbering to pose the halting problem: does the machine with Turing number x
halt upon input of the number y? This is a well posed and interesting mathematical
problem. After all, it is a matter of some considerable interest to us whether our
algorithms halt or not. Yet it turns out that there is no algorithm which is capable of
solving the halting problem. To see this, Turing asked whether there is an algorithm
to solve an even more specialized problem: does the machine with Turing number
x halt upon input of the same number x? Turing deﬁned the halting function,
h(x) ≡
* 0
if machine number x does not halt upon input of x
1
if machine number x halts upon input of x.
If there is an algorithm to solve the halting problem, then there surely is an al-
gorithm to evaluate h(x). We will try to reach a contradiction by supposing such
an algorithm exists, denoted by HALT(x). Consider an algorithm computing the
function TURING(x), with pseudocode
TURING(x)
y = HALT(x)
if y = 0 then
halt
else
loop forever
end if
Since HALT is a valid program, TURING must also be a valid program, with
some Turing number, t. By deﬁnition of the halting function, h(t) = 1 if and only
if TURING halts on input of t. But by inspection of the program for TURING,
we see that TURING halts on input of t if and only if h(t) = 0. Thus h(t) = 1 if
and only if h(t) = 0, a contradiction. Therefore, our initial assumption that there
is an algorithm to evaluate h(x) must have been wrong. We conclude that there is
no algorithm allowing us to solve the halting problem.


Figure 3.2. Elementary circuit performing a single
gate on a single input bit.
There are many other elementary logic gates which are useful for computation. A
partial list includes the
gate, the
gate, the
gate, the
gate, and the
gate. Each of these gates takes two bits as input, and produces a single bit as output.
The
gate outputs 1 if and only if both of its inputs are 1. The
gate outputs 1 if
Models for computation
131
Figure 3.3. Circuits containing cycles can be unstable, and are not usually permitted in the circuit model of
computation.
and only if at least one of its inputs is 1. The
gate outputs the sum, modulo 2, of
its inputs. The
and
gates take the
and
, respectively, of their inputs,
and then apply a
to whatever is output. The action of these gates is illustrated in
Figure 3.4.



  


  


  


  



  

 




	


Figure 3.4. Elementary circuits performing the
,
,
,
, and
gates.
There are two important ‘gates’ missing from Figure 3.4, namely the
gate and
the
gate. In circuits we often allow bits to ‘divide’, replacing a bit with two
copies of itself, an operation referred to as
. We also allow bits to
,
that is, the value of two bits are interchanged. A third operation missing from Figure 3.4,
not really a logic gate at all, is to allow the preparation of extra ancilla or work bits, to
allow extra working space during the computation.
These simple circuit elements can be put together to perform an enormous variety of
computations. Below we’ll show that these elements can be used to compute any function
whatsoever. In the meantime, let’s look at a simple example of a circuit which adds two
n bit integers, using essentially the same algorithm taught to school-children around the
132
Introduction to computer science
world. The basic element in this circuit is a smaller circuit known as a half-adder, shown
in Figure 3.5. A half-adder takes two bits, x and y, as input, and outputs the sum of
the bits x ⊕y modulo 2, together with a carry bit set to 1 if x and y are both 1, or 0
otherwise.



  
Figure 3.5. Half-adder circuit. The carry bit c is set to 1 when x and y are both 1, otherwise it is 0.
Two cascaded half-adders may be used to build a full-adder, as shown in Figure 3.6.
A full-adder takes as input three bits, x, y, and c. The bits x and y should be thought
of as data to be added, while c is a carry bit from an earlier computation. The circuit
outputs two bits. One output bit is the modulo 2 sum, x ⊕y ⊕c of all three input bits.
The second output bit, c′, is a carry bit, which is set to 1 if two or more of the inputs is
1, and is 0 otherwise.



(
(
  

Figure 3.6. Full-adder circuit.
By cascading many of these full-adders together we obtain a circuit to add two n-bit
integers, as illustrated in Figure 3.7 for the case n = 3.
		

!
!
(
Figure 3.7. Addition circuit for two three-bit integers, x = x2x1x0 and y = y2y1y0, using the elementary
algorithm taught to school-children.
We claimed earlier that just a few ﬁxed gates can be used to compute any function
f : {0, 1}n →{0, 1}m whatsoever. We will now prove this for the simpliﬁed case of a
function f : {0, 1}n →{0, 1} with n input bits and a single output bit. Such a function
Models for computation
133
is known as a Boolean function, and the corresponding circuit is a Boolean circuit. The
general universality proof follows immediately from the special case of Boolean functions.
The proof is by induction on n. For n = 1 there are four possible functions: the identity,
which has a circuit consisting of a single wire; the bit ﬂip, which is implemented using
a single
gate; the function which replaces the input bit with a 0, which can be
obtained by
ing the input with a work bit initially in the 0 state; and the function
which replaces the input with a 1, which can be obtained by
ing the input with a work
bit initially in the 1 state.
To complete the induction, suppose that any function on n bits may be computed
by a circuit, and let f be a function on n + 1 bits. Deﬁne n-bit functions f0 and f1
by f0(x1, . . . , xn) ≡f(0, x1, . . . , xn) and f1(x1, . . . , xn) ≡f(1, x1, . . . , xn). These are
both n-bit functions, so by the inductive hypothesis there are circuits to compute these
functions.
It is now an easy matter to design a circuit which computes f. The circuit computes
both f0 and f1 on the last n bits of the input. Then, depending on whether the ﬁrst bit of
the input was a 0 or a 1 it outputs the appropriate answer. A circuit to do this is shown
in Figure 3.8. This completes the induction.









+#
,
,
-+%
Figure 3.8. Circuit to compute an arbitrary function f on n + 1 bits, assuming by induction that there are circuits
to compute the n-bit functions f0 and f1.
Five elements may be identiﬁed in the universal circuit construction: (1) wires, which
preserve the states of the bits; (2) ancilla bits prepared in standard states, used in the
n = 1 case of the proof; (3) the
operation, which takes a single bit as input
and outputs two copies of that bit; (4) the
operation, which interchanges
the value of two bits; and (5) the
,
, and
gates. In Chapter 4 we’ll deﬁne
the quantum circuit model of computation in a manner analogous to classical circuits. It
is interesting to note that many of these ﬁve elements pose some interesting challenges
when extending to the quantum case: it is not necessarily obvious that good quantum
wires for the preservation of qubits can be constructed, even in principle, the
134
Introduction to computer science
operation cannot be performed in a straightforward manner in quantum mechanics, due
to the no-cloning theorem (as explained in Section 1.3.5), and the
and
gates
are not invertible, and thus can’t be implemented in a straightforward manner as unitary
quantum gates. There is certainly plenty to think about in deﬁning a quantum circuit
model of computation!
Exercise 3.8: (Universality of
)
Show that the
gate can be used to
simulate the
,
and
gates, provided wires, ancilla bits and
are available.
Let’s return from our brief quantum digression, to the properties of classical circuits.
We claimed earlier that the Turing machine model is equivalent to the circuit model of
computation. In what sense do we mean the two models are equivalent? On the face of
it, the two models appear quite different. The unbounded nature of a Turing machine
makes them more useful for abstractly specifying what it is we mean by an algorithm,
while circuits more closely capture what an actual physical computer does.
The two models are connected by introducing the notion of a uniform circuit family.
A circuit family consists of a collection of circuits, {Cn}, indexed by a positive integer
n. The circuit Cn has n input bits, and may have any ﬁnite number of extra work bits,
and output bits. The output of the circuit Cn, upon input of a number x of at most n
bits in length, is denoted by Cn(x). We require that the circuits be consistent, that is, if
m < n and x is at most m bits in length, then Cm(x) = Cn(x). The function computed
by the circuit family {Cn} is the function C(·) such that if x is n bits in length then
C(x) = Cn(x). For example, consider a circuit Cn that squares an n-bit number. This
deﬁnes a family of circuits {Cn} that computes the function, C(x) = x2, where x is any
positive integer.
It’s not enough to consider unrestricted families of circuits, however. In practice, we
need an algorithm to build the circuit. Indeed, if we don’t place any restrictions on the
circuit family then it becomes possible to compute all sorts of functions which we do
not expect to be able to compute in a reasonable model of computation. For example, let
hn(x) denote the halting function, restricted to values of x which are n bits in length.
Thus hn is a function from n bits to 1 bit, and we have proved there exists a circuit
Cn to compute hn(·). Therefore the circuit family {Cn} computes the halting function!
However, what prevents us from using this circuit family to solve the halting problem is
that we haven’t speciﬁed an algorithm which will allow us to build the circuit Cn for all
values of n. Adding this requirement results in the notion of a uniform circuit family.
That is, a family of circuits {Cn} is said to be a uniform circuit family if there is some
algorithm running on a Turing machine which, upon input of n, generates a description
of Cn. That is, the algorithm outputs a description of what gates are in the circuit Cn,
how those gates are connected together to form a circuit, any ancilla bits needed by
the circuit,
and
operations, and where the output from the circuit
should be read out. For example, the family of circuits we described earlier for squaring
n-bit numbers is certainly a uniform circuit family, since there is an algorithm which,
given n, outputs a description of the circuit needed to square an n-bit number. You can
think of this algorithm as the means by which an engineer is able to generate a description
of (and thus build) the circuit for any n whatsoever. By contrast, a circuit family that is
not uniform is said to be a non-uniform circuit family. There is no algorithm to construct
The analysis of computational problems
135
the circuit for arbitrary n, which prevents our engineer from building circuits to compute
functions like the halting function.
Intuitively, a uniform circuit family is a family of circuits that can be generated by some
reasonable algorithm. It can be shown that the class of functions computable by uniform
circuit families is exactly the same as the class of functions which can be computed on a
Turing machine. With this uniformity restriction, results in the Turing machine model
of computation can usually be given a straightforward translation into the circuit model
of computation, and vice versa. Later we give similar attention to issues of uniformity in
the quantum circuit model of computation.
3.2
The analysis of computational problems
The analysis of computational problems depends upon the answer to three fundamental
questions:
(1) What is a computational problem? Multiplying two numbers together is a
computational problem; so is programming a computer to exceed human abilities in
the writing of poetry. In order to make progress developing a general theory for the
analysis of computational problems we are going to isolate a special class of
problems known as decision problems, and concentrate our analysis on those.
Restricting ourselves in this way enables the development of a theory which is both
elegant and rich in structure. Most important, it is a theory whose principles have
application far beyond decision problems.
(2) How may we design algorithms to solve a given computational problem?
Once a problem has been speciﬁed, what algorithms can be used to solve the
problem? Are there general techniques which can be used to solve wide classes of
problems? How can we be sure an algorithm behaves as claimed?
(3) What are the minimal resources required to solve a given computational
problem? Running an algorithm requires the consumption of resources, such as
time, space, and energy. In different situations it may be desirable to minimize
consumption of one or more resource. Can we classify problems according to the
resource requirements needed to solve them?
In the next few sections we investigate these three questions, especially questions 1
and 3. Although question 1, ‘what is a computational problem?’, is perhaps the most
fundamental of the questions, we shall defer answering it until Section 3.2.3, pausing ﬁrst
to establish some background notions related to resource quantiﬁcation in Section 3.2.1,
and then reviewing the key ideas of computational complexity in Section 3.2.2.
Question 2, how to design good algorithms, is the subject of an enormous amount of
ingenious work by many researchers. So much so that in this brief introduction we cannot
even begin to describe the main ideas employed in the design of good algorithms. If you
are interested in this beautiful subject, we refer you to the end of chapter ‘History and
further reading’. Our closest direct contact with this subject will occur later in the book,
when we study quantum algorithms. The techniques involved in the creation of quantum
algorithms have typically involved a blend of deep existing ideas in algorithm design for
classical computers, and the creation of new, wholly quantum mechanical techniques for
algorithm design. For this reason, and because the spirit of quantum algorithm design
136
Introduction to computer science
is so similar in many ways to classical algorithm design, we encourage you to become
familiar with at least the basic ideas of algorithm design.
Question 3, what are the minimal resources required to solve a given computational
problem, is the main focus of the next few sections. For example, suppose we are given
two numbers, each n bits in length, which we wish to multiply. If the multiplication
is performed on a single-tape Turing machine, how many computational steps must be
executed by the Turing machine in order to complete the task? How much space is used
on the Turing machine while completing the task?
These are examples of the type of resource questions we may ask. Generally speak-
ing, computers make use of many different kinds of resources, however we will focus
most of our attention on time, space, and energy. Traditionally in computer science, time
and space have been the two major resource concerns in the study of algorithms, and
we study these issues in Sections 3.2.2 through 3.2.4. Energy has been a less impor-
tant consideration; however, the study of energy requirements motivates the subject of
reversible classical computation, which in turn is a prerequisite for quantum computa-
tion, so we examine energy requirements for computation in some considerable detail in
Section 3.2.5.
3.2.1
How to quantify computational resources
Different models of computation lead to different resource requirements for computa-
tion. Even something as simple as changing from a single-tape to a two-tape Turing
machine may change the resources required to solve a given computational problem. For
a computational task which is extremely well understood, like addition of integers, for
example, such differences between computational models may be of interest. However,
for a ﬁrst pass at understanding a problem, we would like a way of quantifying resource
requirements that is independent of relatively trivial changes in the computational model.
One of the tools which has been developed to do this is the asymptotic notation, which
can be used to summarize the essential behavior of a function. This asymptotic notation
can be used, for example, to summarize the essence of how many time steps it takes a
given algorithm to run, without worrying too much about the exact time count. In this
section we describe this notation in detail, and apply it to a simple problem illustrating
the quantiﬁcation of computational resources – the analysis of algorithms for sorting a
list of names into alphabetical order.
Suppose, for example, that we are interested in the number of gates necessary to add
together two n-bit numbers. Exact counts of the number of gates required obscure the
big picture: perhaps a speciﬁc algorithm requires 24n + 2⌈log n⌉+ 16 gates to perform
this task. However, in the limit of large problem size the only term which matters is the
24n term. Furthermore, we disregard constant factors as being of secondary importance
to the analysis of the algorithm. The essential behavior of the algorithm is summed up
by saying that the number of operations required scales like n, where n is the number of
bits in the numbers being added. The asymptotic notation consists of three tools which
make this notion precise.
The O (‘big O’) notation is used to set upper bounds on the behavior of a function.
Suppose f(n) and g(n) are two functions on the non-negative integers. We say ‘f(n) is
in the class of functions O(g(n))’, or just ‘f(n) is O(g(n))’, if there are constants c and
n0 such that for all values of n greater than n0, f(n) ≤cg(n). That is, for sufﬁciently
large n, the function g(n) is an upper bound on f(n), up to an unimportant constant
The analysis of computational problems
137
factor. The big O notation is particularly useful for studying the worst-case behavior of
speciﬁc algorithms, where we are often satisﬁed with an upper bound on the resources
consumed by an algorithm.
When studying the behaviors of a class of algorithms – say the entire class of algorithms
which can be used to multiply two numbers – it is interesting to set lower bounds on
the resources required. For this the Ω (‘big Omega’) notation is used. A function f(n)
is said to be Ω(g(n)) if there exist constants c and n0 such that for all n greater than n0,
cg(n) ≤f(n). That is, for sufﬁciently large n, g(n) is a lower bound on f(n), up to an
unimportant constant factor.
Finally, the Θ (‘big Theta’) notation is used to indicate that f(n) behaves the same as
g(n) asymptotically, up to unimportant constant factors. That is, we say f(n) is Θ(g(n))
if it is both O(g(n)) and Ω(g(n)).
Asymptotic notation: examples
Let’s consider a few simple examples of the asymptotic notation. The function 2n is
in the class O(n2), since 2n ≤2n2 for all positive n. The function 2n is Ω(n3), since
n3 ≤2n for sufﬁciently large n. Finally, the function 7n2 + √n log(n) is Θ(n2), since
7n2 ≤7n2 + √n log(n) ≤8n2 for all sufﬁciently large values of n. In the following
few exercises you will work through some of the elementary properties of the asymptotic
notation that make it a useful tool in the analysis of algorithms.
Exercise 3.9:
Prove that f(n) is O(g(n)) if and only if g(n) is Ω(f(n)). Deduce that
f(n) is Θ(g(n)) if and only if g(n) is Θ(f(n)).
Exercise 3.10:
Suppose g(n) is a polynomial of degree k. Show that g(n) is O(nl) for
any l ≥k.
Exercise 3.11:
Show that log n is O(nk) for any k > 0.
Exercise 3.12: (nlog n is super-polynomial)
Show that nk is O(nlog n) for any k, but
that nlog n is never O(nk).
Exercise 3.13: (nlog n is sub-exponential)
Show that cn is Ω(nlog n) for any c > 1,
but that nlog n is never Ω(cn).
Exercise 3.14:
Suppose e(n) is O(f(n)) and g(n) is O(h(n)). Show that e(n)g(n) is
O(f(n)h(n)).
An example of the use of the asymptotic notation in quantifying resources is the
following simple application to the problem of sorting an n element list of names into
alphabetical order. Many sorting algorithms are based upon the ‘compare-and-swap’
operation: two elements of an n element list are compared, and swapped if they are in
the wrong order. If this compare-and-swap operation is the only means by which we can
access the list, how many such operations are required in order to ensure that the list has
been correctly sorted?
A simple compare-and-swap algorithm for solving the sorting problem is as follows:
(note that compare-and-swap(j,k) compares the list entries numbered j and k, and
swaps them if they are out of order)
138
Introduction to computer science
for j = 1 to n-1
for k = j+1 to n
compare-and-swap(j,k)
end k
end j
It is clear that this algorithm correctly sorts a list of n names into alphabetical order.
Note that the number of compare-and-swap operations executed by the algorithm is
(n −1) + (n −2) + · · · + 1 = n(n −1)/2. Thus the number of compare-and-swap
operations used by the algorithm is Θ(n2). Can we do better than this? It turns out that we
can. Algorithms such as ‘heapsort’ are known which run using O(n log n) compare-and-
swap operations. Furthermore, in Exercise 3.15 you’ll work through a simple counting
argument that shows any algorithm based upon the compare-and-swap operation requires
Ω(n log n) such operations. Thus, the sorting problem requires Θ(n log n) compare-and-
swap operations, in general.
Exercise 3.15: (Lower bound for compare-and-swap based sorts)
Suppose an n
element list is sorted by applying some sequence of compare-and-swap
operations to the list. There are n! possible initial orderings of the list. Show that
after k of the compare-and-swap operations have been applied, at most 2k of the
possible initial orderings will have been sorted into the correct order. Conclude
that Ω(n log n) compare-and-swap operations are required to sort all possible
initial orderings into the correct order.
3.2.2
Computational complexity
The idea that there won’t be an algorithm to solve it – this is something fun-
damental that won’t ever change – that idea appeals to me.
– Stephen Cook
Sometimes it is good that some things are impossible. I am happy there are
many things that nobody can do to me.
– Leonid Levin
It should not come as a surprise that our choice of polynomial algorithms as
the mathematical concept that is supposed to capture the informal notion of
‘practically efﬁcient computation’ is open to criticism from all sides. [. . . ] Ul-
timately, our argument for our choice must be this: Adopting polynomial
worst-case performance as our criterion of efﬁciency results in an
elegant and useful theory that says something meaningful about
practical computation, and would be impossible without this sim-
pliﬁcation.
– Christos Papadimitriou
What time and space resources are required to perform a computation? In many cases
these are the most important questions we can ask about a computational problem. Prob-
lems like addition and multiplication of numbers are regarded as efﬁciently solvable
because we have fast algorithms to perform addition and multiplication, which consume
The analysis of computational problems
139
little space when running. Many other problems have no known fast algorithm, and are
effectively impossible to solve, not because we can’t ﬁnd an algorithm to solve the prob-
lem, but because all known algorithms consume such vast quantities of space or time as
to render them practically useless.
Computational complexity is the study of the time and space resources required to
solve computational problems. The task of computational complexity is to prove lower
bounds on the resources required by the best possible algorithm for solving a problem,
even if that algorithm is not explicitly known. In this and the next two sections, we
give an overview of computational complexity, its major concepts, and some of the more
important results of the ﬁeld. Note that computational complexity is in a sense comple-
mentary to the ﬁeld of algorithm design; ideally, the most efﬁcient algorithms we could
design would match perfectly with the lower bounds proved by computational complex-
ity. Unfortunately, this is often not the case. As already noted, in this book we won’t
examine classical algorithm design in any depth.
One difﬁculty in formulating a theory of computational complexity is that different
computational models may require different resources to solve the same problem. For in-
stance, multiple-tape Turing machines can solve many problems substantially faster than
single-tape Turing machines. This difﬁculty is resolved in a rather coarse way. Suppose
a problem is speciﬁed by giving n bits as input. For instance, we might be interested in
whether a particular n-bit number is prime or not. The chief distinction made in com-
putational complexity is between problems which can be solved using resources which
are bounded by a polynomial in n, or which require resources which grow faster than
any polynomial in n. In the latter case we usually say that the resources required are
exponential in the problem size, abusing the term exponential, since there are functions
like nlog n which grow faster than any polynomial (and thus are ‘exponential’ accord-
ing to this convention), yet which grow slower than any true exponential. A problem
is regarded as easy, tractable or feasible if an algorithm for solving the problem using
polynomial resources exists, and as hard, intractable or infeasible if the best possible
algorithm requires exponential resources.
As a simple example, suppose we have two numbers with binary expansions x1 . . . xm1
and y1 . . . ym2, and we wish to determine the sum of the two numbers. The total size of
the input is n ≡m1 + m2. It’s easy to see that the two numbers can be added using a
number of elementary operations that scales as Θ(n); this algorithm uses a polynomial
(indeed, linear) number of operations to perform its tasks. By contrast, it is believed
(though it has never been proved!) that the problem of factoring an integer into its prime
factors is intractable. That is, the belief is that there is no algorithm which can factor
an arbitrary n-bit integer using O(p(n)) operations, where p is some ﬁxed polynomial
function of n. We will later give many other examples of problems which are believed to
be intractable in this sense.
The polynomial versus exponential classiﬁcation is rather coarse. In practice, an algo-
rithm that solves a problem using 2n/1000 operations is probably more useful than one
which runs in n1000 operations. Only for very large input sizes (n ≈108) will the ‘efﬁ-
cient’ polynomial algorithm be preferable to the ‘inefﬁcient’ exponential algorithm, and
for many purposes it may be more practical to prefer the ‘inefﬁcient’ algorithm.
Nevertheless, there are many reasons to base computational complexity primarily on
the polynomial versus exponential classiﬁcation. First, historically, with few exceptions,
polynomial resource algorithms have been much faster than exponential algorithms. We
140
Introduction to computer science
might speculate that the reason for this is lack of imagination: coming up with algorithms
requiring n, n2 or some other low degree polynomial number of operations is often much
easier than ﬁnding a natural algorithm which requires n1000 operations, although examples
like the latter do exist. Thus, the predisposition for the human mind to come up with
relatively simple algorithms has meant that in practice polynomial algorithms usually do
perform much more efﬁciently than their exponential cousins.
A second and more fundamental reason for emphasizing the polynomial versus expo-
nential classiﬁcation is derived from the strong Church–Turing thesis. As discussed in
Section 1.1, it was observed in the 1960s and 1970s that probabilistic Turing machines
appear to be the strongest ‘reasonable’ model of computation. More precisely, researchers
consistently found that if it was possible to compute a function using k elementary opera-
tions in some model that was not the probabilistic Turing machine model of computation,
then it was always possible to compute the same function in the probabilistic Turing ma-
chine model, using at most p(k) elementary operations, where p(·) is some polynomial
function. This statement is known as the strong Church–Turing thesis:
Strong Church–Turing thesis: Any model of computation can be simulated
on a probabilistic Turing machine with at most a polynomial increase in the
number of elementary operations required.
The strong Church–Turing thesis is great news for the theory of computational complex-
ity, for it implies that attention may be restricted to the probabilistic Turing machine
model of computation. After all, if a problem has no polynomial resource solution on
a probabilistic Turing machine, then the strong Church–Turing thesis implies that it
has no efﬁcient solution on any computing device. Thus, the strong Church–Turing
thesis implies that the entire theory of computational complexity will take on an ele-
gant, model-independent form if the notion of efﬁciency is identiﬁed with polynomial
resource algorithms, and this elegance has provided a strong impetus towards acceptance
of the identiﬁcation of ‘solvable with polynomial resources’ and ‘efﬁciently solvable’. Of
course, one of the prime reasons for interest in quantum computers is that they cast
into doubt the strong Church–Turing thesis, by enabling the efﬁcient solution of a prob-
lem which is believed to be intractable on all classical computers, including probabilistic
Turing machines! Nevertheless, it is useful to understand and appreciate the role the
strong Church–Turing thesis has played in the search for a model-independent theory
of computational complexity.
Finally, we note that, in practice, computer scientists are not only interested in the
polynomial versus exponential classiﬁcation of problems. This is merely the ﬁrst and
coarsest way of understanding how difﬁcult a computational problem is. However, it
is an exceptionally important distinction, and illustrates many broader points about the
nature of resource questions in computer science. For most of this book, it will be our
central concern in evaluating the efﬁciency of a given algorithm.
Having examined the merits of the polynomial versus exponential classiﬁcation, we
now have to confess that the theory of computational complexity has one remarkable
outstanding failure: it seems very hard to prove that there are interesting classes of prob-
lems which require exponential resources to solve. It is quite easy to give non-constructive
proofs that most problems require exponential resources (see Exercise 3.16, below), and
furthermore many interesting problems are conjectured to require exponential resources
for their solution, but rigorous proofs seem very hard to come by, at least with the present
The analysis of computational problems
141
state of knowledge. This failure of computational complexity has important implications
for quantum computation, because it turns out that the computational power of quantum
computers can be related to some major open problems in classical computational com-
plexity theory. Until these problems are resolved, it cannot be stated with certainty how
computationally powerful a quantum computer is, or even whether it is more powerful
than a classical computer!
Exercise 3.16: (Hard-to-compute functions exist)
Show that there exist Boolean
functions on n inputs which require at least 2n/ log n logic gates to compute.
3.2.3
Decision problems and the complexity classes P and NP
Many computational problems are most cleanly formulated as decision problems – prob-
lems with a yes or no answer. For example, is a given number m a prime number or not?
This is the primality decision problem. The main ideas of computational complexity are
most easily and most often formulated in terms of decision problems, for two reasons:
the theory takes its simplest and most elegant form in this form, while still generalizing
in a natural way to more complex scenarios; and historically computational complexity
arose primarily from the study of decision problems.
Although most decision problems can easily be stated in simple, familiar language,
discussion of the general properties of decision problems is greatly helped by the termi-
nology of formal languages. In this terminology, a language L over the alphabet Σ is a
subset of the set Σ∗of all (ﬁnite) strings of symbols from Σ. For example, if Σ = {0, 1},
then the set of binary representations of even numbers, L = {0, 10, 100, 110, . . .} is a
language over Σ.
Decision problems may be encoded in an obvious way as problems about languages.
For instance, the primality decision problem can be encoded using the binary alphabet
Σ = {0, 1}. Strings from Σ∗can be interpreted in a natural way as non-negative integers.
For example, 0010 can be interpreted as the number 2. The language L is deﬁned to
consist of all binary strings such that the corresponding number is prime.
To solve the primality decision problem, what we would like is a Turing machine
which, when started with a given number n on its input tape, eventually outputs some
equivalent of ‘yes’ if n is prime, and outputs ‘no’ if n is not prime. To make this idea
precise, it is convenient to modify our old Turing machine deﬁnition (of Section 3.1.1)
slightly, replacing the halting state qh with two states qY and qN to represent the answers
‘yes’ and ‘no’ respectively. In all other ways the machine behaves in the same way, and
it still halts when it enters the state qY or qN. More generally, a language L is decided
by a Turing machine if the machine is able to decide whether an input x on its tape is
a member of the language of L or not, eventually halting in the state qY if x ∈L, and
eventually halting in the state qN if x ̸∈L. We say that the machine has accepted or
rejected x depending on which of these two cases comes about.
How quickly can we determine whether or not a number is prime? That is, what is the
fastest Turing machine which decides the language representing the primality decision
problem? We say that a problem is in TIME(f(n)) if there exists a Turing machine
which decides whether a candidate x is in the language in time O(f(n)), where n is the
length of x. A problem is said to be solvable in polynomial time if it is in TIME(nk)
for some ﬁnite k. The collection of all languages which are in TIME(nk), for some k,
is denoted P. P is our ﬁrst example of a complexity class. More generally, a complexity
142
Introduction to computer science
class is deﬁned to be a collection of languages. Much of computational complexity theory
is concerned with the deﬁnition of various complexity classes, and understanding the
relationship between different complexity classes.
Not surprisingly, there are problems which cannot be solved in polynomial time.
Unfortunately, proving that any given problem can’t be solved in polynomial time seems
to be very difﬁcult, although conjectures abound! A simple example of an interesting
decision problem which is believed not to be in P is the factoring decision problem:
: Given a composite integer m and l < m, does m have a non-trivial
factor less than l?
An interesting property of factoring is that if somebody claims that the answer is ‘yes,
m does have a non-trivial factor less than l’ then they can establish this by exhibiting
such a factor, which can then be efﬁciently checked by other parties, simply by doing
long-division. We call such a factor a witness to the fact that m has a factor less than l.
This idea of an easily checkable witness is the key idea in the deﬁnition of the complexity
class NP, below. We have phrased factoring as a decision problem, but you can easily
verify that the decision problem is equivalent to ﬁnding the factors of a number:
Exercise 3.17:
Prove that a polynomial-time algorithm for ﬁnding the factors of a
number m exists if and only if the factoring decision problem is in P.
Factoring is an example of a problem in an important complexity class known as NP.
What distinguishes problems in NP is that ‘yes’ instances of a problem can easily be
veriﬁed with the aid of an appropriate witness. More rigorously, a language L is in NP
if there is a Turing machine M with the following properties:
(1) If x ∈L then there exists a witness string w such that M halts in the state qY after
a time polynomial in |x| when the machine is started in the state x-blank-w.
(2) If x ̸∈L then for all strings w which attempt to play the role of a witness, the
machine halts in state qN after a time polynomial in |x| when M is started in the
state x-blank-w.
There is an interesting asymmetry in the deﬁnition of NP. While we have to be able
to quickly decide whether a possible witness to x ∈L is truly a witness, there is no such
need to produce a witness to x ̸∈L. For instance, in the factoring problem, we have
an easy way of proving that a given number has a factor less than m, but exhibiting a
witness to prove that a number has no factors less than m is more daunting. This suggests
deﬁning coNP, the class of languages which have witnesses to ‘no’ instances; obviously
the languages in coNP are just the complements of languages in NP.
How are P and NP related? It is clear that P is a subset of NP. The most famous
open problem in computer science is whether or not there are problems in NP which are
not in P, often abbreviated as the P ̸= NP problem. Most computer scientists believe
that P ̸= NP, but despite decades of work nobody has been able to prove this, and the
possibility remains that P = NP.
Exercise 3.18:
Prove that if coNP ̸= NP then P ̸= NP.
Upon ﬁrst acquaintance it’s tempting to conclude that the conjecture P ̸= NP ought
to be pretty easy to resolve. To see why it’s actually rather subtle it helps to see couple of
The analysis of computational problems
143
examples of problems that are in P and NP. We’ll draw the examples from graph theory,
a rich source of decision problems with surprisingly many practical applications. A graph
is a ﬁnite collection of vertices {v1, . . . , vn} connected by edges, which are pairs (vi, vj)
of vertices. For now, we are only concerned with undirected graphs, in which the order
of the vertices (in each edge pair) does not matter; similar ideas can be investigated for
directed graphs in which the order of vertices does matter. A typical graph is illustrated
in Figure 3.9.
   
   	
   
   	
   
   	
   
   	
   
   	
   
   	
 j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j
? ? ? ? ? ? ? ? ? ? ? ? ? ?


Figure 3.9. A graph.
A cycle in a graph is a sequence v1, . . . , vm of vertices such that each pair (vj, vj+1) is
an edge, as is (v1, vm). A simple cycle is a cycle in which none of the vertices is repeated,
except for the ﬁrst and last vertices. A Hamiltonian cycle is a simple cycle which visits
every vertex in the graph. Examples of graphs with and without Hamiltonian cycles are
shown in Figure 3.10.
   
   	
   
   	
   
   	
   
   	
   
   	
   
   	



j j j j j j j j j j j j j j j j j
j j j j j j j j j j j j j j j j j
???????
???????





   
   	
   
   	
   
   	
   
   	
   
   	
   
   	
 j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j


Figure 3.10. The graph on the left contains a Hamiltonian cycle, 0, 1, 2, 3, 4, 5, 0. The graph on the right contains
no Hamiltonian cycle, as can be veriﬁed by inspection.
The Hamiltonian cycle problem (
) is to determine whether a given graph contains
a Hamiltonian cycle or not.
is a decision problem in NP, since if a given graph has a
Hamiltonian cycle, then that cycle can be used as an easily checkable witness. Moreover,
has no known polynomial time algorithm. Indeed,
is a problem in the class of
so-called NP-complete problems, which can be thought of as the ‘hardest’ problems in
NP, in the sense that solving
in time t allows any other problem in NP to be solved
in time O(poly(t)). This also means that if any NP-complete problem has a polynomial
time solution then it will follow that P = NP.
There is a problem, the Euler cycle decision problem, which is superﬁcially similar to
, but which has astonishingly different properties. An Euler cycle is an ordering of
the edges of a graph G so that every edge in the graph is visited exactly once. The Euler
144
Introduction to computer science
cycle decision problem (
) is to determine, given a graph G on n vertices, whether that
graph contains an Euler cycle or not.
is, in fact, exactly the same problem as
, only
the path visits edges, rather than vertices. Consider the following remarkable theorem,
to be proven in Exercise 3.20:
Theorem 3.1: (Euler’s theorem) A connected graph contains an Euler cycle if and
only if every vertex has an even number of edges incident upon it.
Euler’s theorem gives us a method for efﬁciently solving
. First, check to see whether
the graph is connected; this is easily done with O(n2) operations, as shown in Exer-
cise 3.19. If the graph is not connected, then obviously no Euler cycle exists. If the graph
is connected then for each vertex check whether there is an even number of edges incident
upon the vertex. If a vertex is found for which this is not the case, then there is no Euler
cycle, otherwise an Euler cycle exists. Since there are n vertices, and at most n(n −1)/2
edges, this algorithm requires O(n3) elementary operations. Thus
is in P! Somehow,
there is a structure present in the problem of visiting each edge that can be exploited
to provide an efﬁcient algorithm for
, yet which does not seem to be reﬂected in the
problem of visiting each vertex; it is not at all obvious why such a structure should be
present in one case, but not in the other, if indeed it is absent for the
problem.
Exercise 3.19:
The
problem is to determine whether there is a path
between two speciﬁed vertices in a graph. Show that
can be
solved using O(n) operations if the graph has n vertices. Use the solution to
to show that it is possible to decide whether a graph is connected
in O(n2) operations.
Exercise 3.20: (Euler’s theorem)
Prove Euler’s theorem. In particular, if each
vertex has an even number of incident edges, give a constructive procedure for
ﬁnding an Euler cycle.
The equivalence between the factoring decision problem and the factoring problem
proper is a special instance of one of the most important ideas in computer science, an
idea known as reduction. Intuitively, we know that some problems can be viewed as
special instances of other problems. A less trivial example of reduction is the reduction
of
to the traveling salesman decision problem (
). The traveling salesman decision
problem is as follows: we are given n cities 1, 2, . . . , n and a non-negative integer distance
dij between each pair of cities. Given a distance d the problem is to determine if there
is a tour of all the cities of distance less than d.
The reduction of
to
goes as follows. Suppose we have a graph containing n
vertices. We turn this into an instance of
by thinking of each vertex of the graph as a
‘city’ and deﬁning the distance dij between cities i and j to be one if vertices i and j are
connected, and the distance to be two if the vertices are unconnected. Then a tour of the
cities of distance less than n+1 must be of distance n, and be a Hamiltonian cycle for the
graph. Conversely, if a Hamiltonian cycle exists then a tour of the cities of distance less
than n + 1 must exist. In this way, given an algorithm for solving
, we can convert
it into an algorithm for solving
without much overhead. Two consequences can be
inferred from this. First, if
is a tractable problem, then
is also tractable. Second, if
is hard then
must also be hard. This is an example of a general technique known
The analysis of computational problems
145
as reduction: we’ve reduced the problem
to the problem
. This is a technique we
will use repeatedly throughout this book.
A more general notion of reduction is illustrated in Figure 3.11. A language B is
said to be reducible to another language A if there exists a Turing machine operating
in polynomial time such that given as input x it outputs R(x), and x ∈B if and only
if R(x) ∈A. Thus, if we have an algorithm for deciding A, then with a little extra
overhead we can decide the language B. In this sense, the language B is essentially no
more difﬁcult to decide than the language A.
x ∈B
R x
Is R x ∈A



Figure 3.11. Reduction of B to A.
Exercise 3.21: (Transitive property of reduction)
Show that if a language L1 is
reducible to the language L2 and the language L2 is reducible to L3 then the
language L1 is reducible to the language L3.
Some complexity classes have problems which are complete with respect to that com-
plexity class, meaning there is a language L in the complexity class which is the ‘most
difﬁcult’ to decide, in the sense that every other language in the complexity class can
be reduced to L. Not all complexity classes have complete problems, but many of the
complexity classes we are concerned with do have complete problems. A trivial example
is provided by P. Let L be any language in P which is not empty or equal to the set
of all words. That is, there exists a string x1 such that x1 ̸∈L and a string x2 such
that x2 ∈L. Then any other language L′ in P can be reduced to L using the following
reduction: given an input x, use the polynomial time decision procedure to determine
whether x ∈L′ or not. If it is not, then set R(x) = x1, otherwise set R(x) = x2.
Exercise 3.22:
Suppose L is complete for a complexity class, and L′ is another
language in the complexity class such that L reduces to L′. Show that L′ is
complete for the complexity class.
Less trivially, NP also contains complete problems. An important example of such a
problem and the prototype for all other NP-complete problems is the circuit satisﬁability
problem or
: given a Boolean circuit composed of
,
and
gates, is there
an assignment of values to the inputs to the circuit that results in the circuit outputting 1,
that is, is the circuit satisﬁable for some input? The NP-completeness of
is known
as the Cook–Levin theorem, for which we now outline a proof.
Compute
in polynomial time
Is
( )
?
‘‘Yes’’ or ‘‘No’’
( )
146
Introduction to computer science
Theorem 3.2: (Cook–Levin)
is NP-complete.
Proof
The proof has two parts. The ﬁrst part of the proof is to show that
is in NP, and
the second part is to show that any language in NP can be reduced to
. Both parts
of the proof are based on simulation techniques: the ﬁrst part of the proof is essentially
showing that a Turing machine can efﬁciently simulate a circuit, while the second part of
the proof is essentially showing that a circuit can efﬁciently simulate a Turing machine.
Both parts of the proof are quite straightforward; for the purposes of illustration we give
the second part in some detail.
The ﬁrst part of the proof is to show that
is in NP. Given a circuit containing
n circuit elements, and a potential witness w, it is obviously easy to check in polynomial
time on a Turing machine whether or not w satisﬁes the circuit, which establishes that
is in NP.
The second part of the proof is to show that any language L ∈NP can be reduced to
. That is, we aim to show that there is a polynomial time computable reduction R
such that x ∈L if and only if R(x) is a satisﬁable circuit. The idea of the reduction is
to ﬁnd a circuit which simulates the action of the machine M which is used to check
instance-witness pairs, (x, w), for the language L. The input variables for the circuit
will represent the witness; the idea is that ﬁnding a witness which satisﬁes the circuit is
equivalent to M accepting (x, w) for some speciﬁc witness w. Without loss of generality
we may make the following assumptions about M to simplify the construction:
(1) M’s tape alphabet is ▷,0,1 and the blank symbol.
(2) M runs using time at most t(n) and total space at most s(n) where t(n) and s(n)
are polynomials in n.
(3) Machine M can actually be assumed to run using time exactly t(n) for all inputs of
size n. This is done by adding the lines ⟨qY, x, qY, x, 0⟩, and ⟨qN, x, qN, x, 0⟩for
each of x = ▷, 0, 1 and the blank, artiﬁcially halting the machine after exactly t(n)
steps.
The basic idea of the construction to simulate M is outlined in Figure 3.12. Each
internal state of the Turing machine is represented by a single bit in the circuit. We
name the corresponding bits ˜qs, ˜q1, . . . , ˜qm, ˜qY, ˜qN. Initially, ˜qs is set to one, and all the
other bits representing internal states are set to zero. Each square on the Turing machine
tape is represented by three bits: two bits to represent the letter of the alphabet (▷, 0, 1
or blank) currently residing on the tape, and a single ‘ﬂag’ bit which is set to one if the
read-write head is pointing to the square, and set to zero otherwise. We denote the bits
representing the tape contents by (u1, v1), . . . , (us(n), vs(n)) and the corresponding ﬂag
bits by f1, . . . , fs(n). Initially the uj and vj bits are set to represent the inputs x and w,
as appropriate, while f1 = 1 and all other fj = 0. There is also a lone extra ‘global ﬂag’
bit, F, whose function will be explained later. F is initially set to zero. We regard all the
bits input to the circuit as ﬁxed, except for those representing the witness w, which are
the variable bits for the circuit.
The action of M is obtained by repeating t(n) times a ‘simulation step’ which
simulates the execution of a single program line for the Turing machine. Each
simulation step may be broken up into a sequence of steps corresponding in turn to the
respective program lines, with a ﬁnal step which resets the global ﬂag F to zero, as
The analysis of computational problems
147
qs
q1
qm
qY
qN
· · ·

x
b
· · ·
w
· · ·
b
b
· · ·
F
· · ·
/
m+3
/
3n+6
/
3w(n)
/
3s(n)
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
m + 3
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
s n
⎧
⎨
⎩
˜qY
"
# 
!
t n
Figure 3.12. Outline of the procedure used to simulate a Turing machine using a circuit.
illustrated in Figure 3.13. To complete the simulation, we only need to simulate a
program line of the form ⟨qi, x, qj, x′, s⟩. For convenience, we assume qi ̸= qj, but a
similar construction works in the case when qi = qj. The procedure is as follows:
(1) Check to see that ˜qi = 1, indicating that the current state of the machine is qi.
(2) For each tape square:
(a) Check to see that the global ﬂag bit is set to zero, indicating that no action has
yet been taken by the Turing machine.
(b) Check that the ﬂag bit is set to one, indicating that the tape head is at this tape
square.
(c) Check that the simulated tape contents at this point are x.
(d) If all conditions check out, then perform the following steps:
1. Set ˜qi = 0 and ˜qj = 1.
2. Update the simulated tape contents at this tape square to x′.
3. Update the ﬂag bit of this and adjacent ‘squares’ as appropriate, depending
on whether s = +1, 0, −1, and whether we are at the left hand end of the
tape.
4. Set the global ﬂag bit to one, indicating that this round of computation has
been completed.
output
bit
Simulation
Step
Simulation
Step
Simulation
Step
fixed
inputbits
fixed
inputbits
variable
inputbits
fixed
inputbits
1 fixed
inputbit
3n + 6
3w(n)
3 ( )
( ) simulation steps
...
...
˜
˜
˜
˜
˜
148
Introduction to computer science
This is a ﬁxed procedure which involves a constant number of bits, and by the universality
result of Section 3.1.2 can be performed using a circuit containing a constant number of
gates.
Figure 3.13. Outline of the simulation step used to simulate a Turing machine using a circuit.
The total number of gates in the entire circuit is easily seen to be O(t(n)(s(n) + n)),
which is polynomial in size. At the end of the circuit, it is clear that ˜qY = 1 if and only
if the machine M accepts (x, w). Thus, the circuit is satisﬁable if and only if there exists
w such that machine M accepts (x, w), and we have found the desired reduction from
L to
.
gives us a foot in the door which enables us to easily prove that many other
problems are NP-complete. Instead of directly proving that a problem is NP-complete,
we can instead prove that it is in NP and that
reduces to it, so by Exercise 3.22 the
problem must be NP-complete. A small sample of NP-complete problems is discussed
in Box 3.3. An example of another NP-complete problem is the satisﬁability problem
(
), which is phrased in terms of a Boolean formula. Recall that a Boolean formula
ϕ is composed of the following elements: a set of Boolean variables, x1, x2, . . .; Boolean
connectives, that is, a Boolean function with one or two inputs and one output, such as
∧(AND), ∨(OR), and ¬ (NOT); and parentheses. The truth or falsity of a Boolean
formula for a given set of Boolean variables is decided according to the usual laws of
Boolean algebra. For example, the formula ϕ = x1 ∨¬x2 has the satisfying assignment
x1 = 0 and x2 = 0, while x1 = 0 and x2 = 1 is not a satisfying assignment. The
satisﬁability problem is to determine, given a Boolean formula ϕ, whether or not it is
satisﬁable by any set of possible inputs.
Exercise 3.23:
Show that
is NP-complete by ﬁrst showing that
is in NP, and
then showing that
reduces to
. (Hint: for the reduction it may help to
represent each distinct wire in an instance of
by different variables in a
Boolean formula.)
An important restricted case of
is also NP-complete, the 3-satisﬁability problem
(
), which is concerned with formulae in 3-conjunctive normal form. A formula is
said to be in conjunctive normal form if it is the AND of a collection of clauses, each of
which is the OR of one or more literals, where a literal is an expression is of the form x
or ¬x. For example, the formula (x1 ∨¬x2) ∧(x2 ∨x3 ∨¬x4) is in conjunctive normal
form. A formula is in 3-conjunctive normal form or 3-CNF if each clause has exactly
three literals. For example, the formula (¬x1∨x2∨¬x2)∧(¬x1∨x3∨¬x4)∧(x2∨x3∨x4)
is in 3-conjunctive normal form. The 3-satisﬁability problem is to determine whether a
formula in 3-conjunctive normal form is satisﬁable or not.
The proof that
is NP-complete is straightforward, but is a little too lengthy to
justify inclusion in this overview. Even more than
and
,
is in some sense
The analysis of computational problems
149
the NP-complete problem, and it is the basis for countless proofs that other problems
are NP-complete. We conclude our discussion of NP-completeness with the surprising
fact that
, the analogue of
in which every clause has two literals, can be solved
in polynomial time:
Exercise 3.24: (
has an efﬁcient solution)
Suppose ϕ is a Boolean formula in
conjunctive normal form, in which each clause contains only two literals.
(1) Construct a (directed) graph G(ϕ) with directed edges in the following way:
the vertices of G correspond to variables xj and their negations ¬xj in ϕ.
There is a (directed) edge (α, β) in G if and only if the clause (¬α ∨β) or
the clause (β ∨¬α) is present in ϕ. Show that ϕ is not satisﬁable if and only
if there exists a variable x such that there are paths from x to ¬x and from
¬x to x in G(ϕ).
(2) Show that given a directed graph G containing n vertices it is possible to
determine whether two vertices v1 and v2 are connected in polynomial time.
(3) Find an efﬁcient algorithm to solve
.
Box 3.3: A zoo of NP-complete problems
The importance of the class NP derives, in part, from the enormous number of
computational problems that are known to be NP-complete. We can’t possibly hope
to survey this topic here (see ‘History and further reading’), but the following ex-
amples, taken from many distinct areas of mathematics, give an idea of the delicious
melange of problems known to be NP-complete.
•
(graph theory): A clique in an undirected graph G is a subset of
vertices, each pair of which is connected by an edge. The size of a clique is the
number of vertices it contains. Given an integer m and a graph G, does G have
a clique of size m?
•
(arithmetic): Given a ﬁnite collection S of positive integers and a
target t, is there any subset of S which sums to t?
•
(linear programming): Given an integer m × n
matrix A and an m-dimensional vector y with integer values, does there exist
an n-dimensional vector x with entries in the set {0, 1} such that Ax ≤y?
•
(graph theory): A vertex cover for an undirected graph G is a
set of vertices V ′ such that every edge in the graph has one or both vertices
contained in V ′. Given an integer m and a graph G, does G have a vertex
cover V ′ containing m vertices?
Assuming that P ̸= NP it is possible to prove that there is a non-empty class of
problems NPI (NP intermediate) which are neither solvable with polynomial resources,
nor are NP-complete. Obviously, there are no problems known to be in NPI (otherwise
we would know that P ̸= NP) but there are several problems which are regarded as
being likely candidates. Two of the strongest candidates are the factoring and graph
isomorphism problems:
150
Introduction to computer science
: Suppose G and G′ are two undirected graphs over the
vertices V ≡{v1, . . . , vn}. Are G and G′ isomorphic? That is, does there exist a
one-to-one function ϕ : V →V such that the edge (vi, vj) is contained in G if
and only if (ϕ(vi), ϕ(vj)) is contained in G?
Problems in NPI are interesting to researchers in quantum computation and quantum
information for two reasons. First, it is desirable to ﬁnd fast quantum algorithms to solve
problems which are not in P. Second, many suspect that quantum computers will not
be able to efﬁciently solve all problems in NP, ruling out NP-complete problems. Thus,
it is natural to focus on the class NPI. Indeed, a fast quantum algorithm for factoring
has been discovered (Chapter 5), and this has motivated the search for fast quantum
algorithms for other problems suspected to be in NPI.
3.2.4
A plethora of complexity classes
We have investigated some of the elementary properties of some important complexity
classes. A veritable pantheon of complexity classes exists, and there are many non-trivial
relationships known or suspected between these classes. For quantum computation and
quantum information, it is not necessary to understand all the different complexity classes
that have been deﬁned. However, it is useful to have some appreciation for the more
important of the complexity classes, many of which have natural analogues in the study
of quantum computation and quantum information. Furthermore, if we are to understand
how powerful quantum computers are, then it behooves us to understand how the class
of problems solvable on a quantum computer ﬁts into the zoo of complexity classes which
may be deﬁned for classical computers.
There are essentially three properties that may be varied in the deﬁnition of a complex-
ity class: the resource of interest (time, space, . . . ), the type of problem being considered
(decision problem, optimization problem, . . . ), and the underlying computational model
(deterministic Turing machine, probabilistic Turing machine, quantum computer, . . . ).
Not surprisingly, this gives us an enormous range to deﬁne complexity classes. In this
section, we brieﬂy review a few of the more important complexity classes and some of
their elementary properties. We begin with a complexity class deﬁned by changing the
resource of interest from time to space.
The most natural space-bounded complexity class is the class PSPACE of decision
problems which may be solved on a Turing machine using a polynomial number of
working bits, with no limitation on the amount of time that may be used by the machine
(see Exercise 3.25). Obviously, P is included in PSPACE, since a Turing machine that
halts after polynomial time can only traverse polynomially many squares, but it is also true
that NP is a subset of PSPACE. To see this, suppose L is any language in NP. Suppose
problems of size n have witnesses of size at most p(n), where p(n) is some polynomial
in n. To determine whether or not the problem has a solution, we may sequentially
test all 2p(n) possible witnesses. Each test can be run in polynomial time, and therefore
polynomial space. If we erase all the intermediate working between tests then we can test
all the possibilities using polynomial space.
Unfortunately, at present it is not even known whether PSPACE contains problems
which are not in P! This is a pretty remarkable situation – it seems fairly obvious that
having unlimited time and polynomial spatial resources must be more powerful than
having only a polynomial amount of time. However, despite considerable effort and in-
The analysis of computational problems
151
genuity, this has never been shown. We will see later that the class of problems solvable
on a quantum computer in polynomial time is a subset of PSPACE, so proving that a
problem efﬁciently solvable on a quantum computer is not efﬁciently solvable on a clas-
sical computer would establish that P ̸= PSPACE, and thus solve a major outstanding
problem of computer science. An optimistic way of looking at this result is that ideas
from quantum computation might be useful in proving that P ̸= PSPACE. Pessimisti-
cally, one might conclude that it will be a long time before anyone rigorously proves that
quantum computers can be used to efﬁciently solve problems that are intractable on a
classical computer. Even more pessimistically, it is possible that P = PSPACE, in which
case quantum computers offer no advantage over classical computers! However, very few
(if any) computational complexity theorists believe that P = PSPACE.
Exercise 3.25: (PSPACE ⊆EXP)
The complexity class EXP (for exponential time)
contains all decision problems which may be decided by a Turing machine
running in exponential time, that is time O(2nk), where k is any constant. Prove
that PSPACE ⊆EXP. (Hint: If a Turing machine has l internal states, an m
letter alphabet, and uses space p(n), argue that the machine can exist in one of at
most lmp(n) different states, and that if the Turing machine is to avoid inﬁnite
loops then it must halt before revisiting a state.)
Exercise 3.26: (L ⊆P)
The complexity class L (for logarithmic space) contains all
decision problems which may be decided by a Turing machine running in
logarithmic space, that is, in space O(log(n)). More precisely, the class L is
deﬁned using a two-tape Turing machine. The ﬁrst tape contains the problem
instance, of size n, and is a read-only tape, in the sense that only program lines
which don’t change the contents of the ﬁrst tape are allowed. The second tape is
a working tape which initially contains only blanks. The logarithmic space
requirement is imposed on the second, working tape only. Show that L ⊆P.
Does allowing more time or space give greater computational power? The answer
to this question is yes in both cases. Roughly speaking, the time hierarchy theorem
states that TIME(f(n)) is a proper subset of TIME(f(n) log2(f(n))). Similarly, the space
hierarchy theorem states that SPACE(f(n)) is a proper subset of SPACE(f(n) log(f(n))),
where SPACE(f(n)) is, of course, the complexity class consisting of all languages that
can be decided with spatial resources O(f(n)). The hierarchy theorems have interesting
implications with respect to the equality of complexity classes. We know that
L ⊆P ⊆NP ⊆PSPACE ⊆EXP.
(3.1)
Unfortunately, although each of these inclusions is widely believed to be strict, none of
them has ever been proved to be strict. However, the time hierarchy theorem implies
that P is a strict subset of EXP, and the space hierarchy theorem implies that L is a strict
subset of PSPACE! So we can conclude that at least one of the inclusions in (3.1) must
be strict, although we do not know which one.
What should we do with a problem once we know that it is NP-complete, or that
some other hardness criterion holds? It turns out that this is far from being the end of
the story in problem analysis. One possible line of attack is to identify special cases of
the problem which may be amenable to attack. For example, in Exercise 3.24 we saw
that the
problem has an efﬁcient solution, despite the NP-completeness of
.
152
Introduction to computer science
Another approach is to change the type of problem which is being considered, a tactic
which typically results in the deﬁnition of new complexity classes. For example, instead
of ﬁnding exact solutions to an NP-complete problem, we can instead try to ﬁnd good
algorithms for ﬁnding approximate solutions to a problem. For example, the
problem is an NP-complete problem, yet in Exercise 3.27 we show that it is
possible to efﬁciently ﬁnd an approximation to the minimal vertex cover which is correct
to within a factor two! On the other hand, in Problem 3.6 we show that it is not possible
to ﬁnd approximations to solutions of
correct to within any factor, unless P = NP!
Exercise 3.27: (Approximation algorithm for
)
Let G = (V, E)
be an undirected graph. Prove that the following algorithm ﬁnds a vertex cover
for G that is within a factor two of being a minimal vertex cover:
V C = ∅
E′ = E
do until E′ = ∅
let (α, β) be any edge of E′
V C = V C ∪{α, β}
remove from E′ every edge incident on α or β
return V C.
Why is it possible to approximate the solution of one NP-complete problem, but
not another? After all, isn’t it possible to efﬁciently transform from one problem to
another? This is certainly true, however it is not necessarily true that this transformation
preserves the notion of a ‘good approximation’ to a solution. As a result, the computational
complexity theory of approximation algorithms for problems in NP has a structure that
goes beyond the structure of NP proper. An entire complexity theory of approximation
algorithms exists, which unfortunately is beyond the scope of this book. The basic idea,
however, is to deﬁne a notion of reduction that corresponds to being able to efﬁciently
reduce one approximation problem to another, in such a way that the notion of good
approximation is preserved. With such a notion, it is possible to deﬁne complexity classes
such as MAXSNP by analogy to the class NP, as the set of problems for which it is
possible to efﬁciently verify approximate solutions to the problem. Complete problems
exist for MAXSNP, just as for NP, and it is an interesting open problem to determine
how the class MAXSNP compares to the class of approximation problems which are
efﬁciently solvable.
We conclude our discussion with a complexity class that results when the underlying
model of computation itself is changed. Suppose a Turing machine is endowed with
the ability to ﬂip coins, using the results of the coin tosses to decide what actions to
take during the computation. Such a Turing machine may only accept or reject inputs
with a certain probability. The complexity class BPP (for bounded-error probabilistic
time) contains all languages L with the property that there exists a probabilistic Turing
machine M such that if x ∈L then M accepts x with probability at least 3/4, and if
x ̸∈L, then M rejects x with probability at least 3/4. The following exercise shows that
the choice of the constant 3/4 is essentially arbitrary:
The analysis of computational problems
153
Exercise 3.28: (Arbitrariness of the constant in the deﬁnition of BPP)
Suppose
k is a ﬁxed constant, 1/2 < k ≤1. Suppose L is a language such that there
exists a Turing machine M with the property that whenever x ∈L, M accepts
x with probability at least k, and whenever x ̸∈L, M rejects x with probability
at least k. Show that L ∈BPP.
Indeed, the Chernoff bound, discussed in Box 3.4, implies that with just a few repetitions
of an algorithm deciding a language in BPP the probability of success can be ampliﬁed
to the point where it is essentially equal to one, for all intents and purposes. For this
reason, BPP even more than P is the class of decision problems which is usually regarded
as being efﬁciently solvable on a classical computer, and it is the quantum analogue of
BPP, known as BQP, that is most interesting in our study of quantum algorithms.
3.2.5
Energy and computation
Computational complexity studies the amount of time and space required to solve a
computational problem. Another important computational resource is energy. In this
section, we study the energy requirements for computation. Surprisingly, it turns out that
computation, both classical and quantum, can in principle be done without expending
any energy! Energy consumption in computation turns out to be deeply linked to the
reversibility of the computation. Consider a gate like the
gate, which takes as
input two bits, and produces a single bit as output. This gate is intrinsically irreversible
because, given the output of the gate, the input is not uniquely determined. For example,
if the output of the
gate is 1, then the input could have been any one of 00, 01,
or 10. On the other hand, the
gate is an example of a reversible logic gate because,
given the output of the
gate, it is possible to infer what the input must have been.
Another way of understanding irreversibility is to think of it in terms of information
erasure. If a logic gate is irreversible, then some of the information input to the gate is lost
irretrievably when the gate operates – that is, some of the information has been erased by
the gate. Conversely, in a reversible computation, no information is ever erased, because
the input can always be recovered from the output. Thus, saying that a computation is
reversible is equivalent to saying that no information is erased during the computation.
What is the connection between energy consumption and irreversibility in compu-
tation? Landauer’s principle provides the connection, stating that, in order to erase
information, it is necessary to dissipate energy. More precisely, Landauer’s principle
may be stated as follows:
Landauer’s principle (ﬁrst form): Suppose a computer erases a single bit of
information. The amount of energy dissipated into the environment is at least
kBT ln 2, where kB is a universal constant known as Boltzmann’s constant, and T
is the temperature of the environment of the computer.
According to the laws of thermodynamics, Landauer’s principle can be given an alterna-
tive form stated not in terms of energy dissipation, but rather in terms of entropy:
Landauer’s principle (second form): Suppose a computer erases a single bit of
information. The entropy of the environment increases by at least kB ln 2, where
kB is Boltzmann’s constant.
Justifying Landauer’s principle is a problem of physics that lies beyond the scope of this
154
Introduction to computer science
Box 3.4: BPP and the Chernoff bound
Suppose we have an algorithm for a decision problem which gives the correct answer
with probability 1/2 + ϵ, and the wrong answer with probability 1/2 −ϵ. If we run
the algorithm n times, then it seems reasonable to guess that the correct answer is
whichever appeared most frequently. How reliably does this procedure work? The
Chernoff bound is a simple result from elementary probability which answers this
question.
Theorem 3.3: (The Chernoff bound) Suppose X1, . . . , Xn are independent and
identically distributed random variables, each taking the value 1 with
probability 1/2 + ϵ, and the value 0 with probability 1/2 −ϵ. Then
p
 n

i=1
Xi ≤n/2

≤e−2ϵ2n.
(3.2)
Proof
Consider any sequence (x1, . . . , xn) containing at most n/2 ones. The probability
of such a sequence occurring is maximized when it contains ⌊n/2⌋ones, so
p(X1 = x1, . . . , Xn = xn) ≤
1
2 −ϵ
 n
2 1
2 + ϵ
 n
2
(3.3)
= (1 −4ϵ2)
n
2
2n
.
(3.4)
There can be at most 2n such sequences, so
p

i
Xi ≤n/2

≤2n × (1 −4ϵ2)
n
2
2n
= (1 −4ϵ2)
n
2 .
(3.5)
Finally, by calculus, 1 −x ≤exp(−x), so
p

i
Xi ≤n/2

≤e−4ϵ2n/2 = e−2ϵ2n.
(3.6)
What this tells us is that for ﬁxed ϵ, the probability of making an error decreases
exponentially quickly in the number of repetitions of the algorithm. In the case of
BPP we have ϵ = 1/4, so it takes only a few hundred repetitions of the algorithm
to reduce the probability of error below 10−20, at which point an error in one of
the computer’s components becomes much more likely than an error due to the
probabilistic nature of the algorithm.
book – see the end of chapter ‘History and further reading’ if you wish to understand why
Landauer’s principle holds. However, if we accept Landauer’s principle as given, then it
raises a number of interesting questions. First of all, Landauer’s principle only provides
a lower bound on the amount of energy that must be dissipated to erase information.
The analysis of computational problems
155
How close are existing computers to this lower bound? Not very, turns out to be the
answer – computers circa the year 2000 dissipate roughly 500kBT ln 2 in energy for each
elementary logical operation.
Although existing computers are far from the limit set by Landauer’s principle, it is
still an interesting problem of principle to understand how much the energy consumption
can be reduced. Aside from the intrinsic interest of the problem, a practical reason for the
interest follows from Moore’s law: if computer power keeps increasing then the amount
of energy dissipated must also increase, unless the energy dissipated per operation drops
at least as fast as the rate of increase in computing power.
If all computations could be done reversibly, then Landauer’s principle would imply no
lower bound on the amount of energy dissipated by the computer, since no bits at all are
erased during a reversible computation. Of course, it is possible that some other physical
principle might require that energy be dissipated during the computation; fortunately,
this turns out not to be the case. But is it possible to perform universal computation
without erasing any information? Physicists can cheat on this problem to see in advance
that the answer to this question must be yes, because our present understanding of the
laws of physics is that they are fundamentally reversible. That is, if we know the ﬁnal
state of a closed physical system, then the laws of physics allow us to work out the initial
state of the system. If we believe that those laws are correct, then we must conclude that
hidden in the irreversible logic gates like
and
, there must be some underlying
reversible computation. But where is this hidden reversibility, and can we use it to
construct manifestly reversible computers?
We will use two different techniques to give reversible circuit-based models capable
of universal computation. The ﬁrst model, a computer built entirely of billiard balls and
mirrors, gives a beautiful concrete realization of the principles of reversible computation.
The second model, based on a reversible logic gate known as the Toffoli gate (which we
ﬁrst encountered in Section 1.4.1), is a more abstract view of reversible computation that
will later be of great use in our discussion of quantum computation. It is also possible to
build reversible Turing machines that are universal for computation; however, we won’t
study these here, since the reversible circuit models turn out to be much more useful for
quantum computation.
The basic idea of the billiard ball computer is illustrated in Figure 3.14. Billiard ball
‘inputs’ enter the computer from the left hand side, bouncing off mirrors and each other,
before exiting as ‘outputs’ on the right hand side. The presence or absence of a billiard
ball at a possible input site is used to indicate a logical 1 or a logical 0, respectively. The
fascinating thing about this model is that it is manifestly reversible, insofar as its operation
is based on the laws of classical mechanics. Furthermore, this model of computation turns
out to be universal in the sense that it can be used to simulate an arbitrary computation
in the standard circuit model of computation.
Of course, if a billiard ball computer were ever built it would be highly unstable. As
any billiards player can attest, a billiard ball rolling frictionlessly over a smooth surface is
easily knocked off course by small perturbations. The billiard ball model of computation
depends on perfect operation, and the absence of external perturbations such as those
caused by thermal noise. Periodic corrections can be performed, but information gained
by doing this would have to be erased, requiring work to be performed. Expenditure of
energy thus serves the purpose of reducing this susceptibility to noise, which is necessary
for a practical, real-world computational machine. For the purposes of this introduction,
156
Introduction to computer science






.
Figure 3.14. A simple billiard ball computer, with three input bits and three output bits, shown entering on the left
and leaving on the right, respectively. The presence or absence of a billiard ball indicates a 1 or a 0, respectively.
Empty circles illustrate potential paths due to collisions. This particular computer implements the Fredkin classical
reversible logic gate, discussed in the text.
we will ignore the effects of noise on the billiard ball computer, and concentrate on
understanding the essential elements of reversible computation.
The billiard ball computer provides an elegant means for implementing a reversible
universal logic gate known as the Fredkin gate. Indeed, the properties of the Fredkin gate
provide an informative overview of the general principles of reversible logic gates and
circuits. The Fredkin gate has three input bits and three output bits, which we refer to
as a, b, c and a′, b′, c′, respectively. The bit c is a control bit, whose value is not changed
by the action of the Fredkin gate, that is, c′ = c. The reason c is called the control bit
is because it controls what happens to the other two bits, a and b. If c is set to 0 then a
and b are left alone, a′ = a, b′ = b. If c is set to 1, a and b are swapped, a′ = b, b′ = a.
The explicit truth table for the Fredkin gate is shown in Figure 3.15. It is easy to see
that the Fredkin gate is reversible, because given the output a′, b′, c′, we can determine
the inputs a, b, c. In fact, to recover the original inputs a, b and c we need only apply
another Fredkin gate to a′, b′, c′:
Exercise 3.29: (Fredkin gate is self-inverse)
Show that applying two consecutive
Fredkin gates gives the same outputs as inputs.
Examining the paths of the billiard balls in Figure 3.14, it is not difﬁcult to verify that
this billiard ball computer implements the Fredkin gate:
Exercise 3.30:
Verify that the billiard ball computer in Figure 3.14 computes the
Fredkin gate.
In addition to reversibility, the Fredkin gate also has the interesting property that
the number of 1s is conserved between the input and output. In terms of the billiard
ball computer, this corresponds to the number of billiard balls going into the Fredkin
gate being equal to the number coming out. Thus, it is sometimes referred to as being
a conservative reversible logic gate. Such reversibility and conservative properties are
interesting to a physicist because they can be motivated by fundamental physical princi-
The analysis of computational problems
157
Inputs
Outputs
a
b
c
a′
b′
c′
0
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
1
0
1
1
0
0
1
0
0
1
0
1
0
1
1
1
1
0
1
1
0
1
1
1
1
1
1
Figure 3.15. Fredkin gate truth table and circuit representation. The bits a and b are swapped if the control bit c is
set, and otherwise are left alone.
ples. The laws of Nature are reversible, with the possible exception of the measurement
postulate of quantum mechanics, discussed in Section 2.2.3 on page 84. The conservative
property can be thought of as analogous to properties such as conservation of mass, or
conservation of energy. Indeed, in the billiard ball model of computation the conservative
property corresponds exactly to conservation of mass.
Figure 3.16. Fredkin gate conﬁgured to perform the elementary gates
(left),
(middle), and a primitive
routing function, the
(right). The middle gate also serves to perform the
operation, since it
produces two copies of x at the output. Note that each of these conﬁgurations requires the use of extra ‘ancilla’ bits
prepared in standard states – for example, the 0 input on the ﬁrst line of the
gate – and in general the output
contains ‘garbage’ not needed for the remainder of the computation.
The Fredkin gate is not only reversible and conservative, it’s a universal logic gate
as well! As illustrated in Figure 3.16, the Fredkin gate can be conﬁgured to simulate
,
,
and
functions, and thus can be cascaded to simulate any
classical circuit whatsoever.
To simulate irreversible gates such as
using the Fredkin gate, we made use of two
ideas. First, we allowed the input of ‘ancilla’ bits to the Fredkin gate, in specially prepared
states, either 0 or 1. Second, the output of the Fredkin gate contained extraneous ‘garbage’
not needed for the remainder of the computation. These ancilla and garbage bits are not
directly important to the computation. Their importance lies in the fact that they make
the computation reversible. Indeed the irreversibility of gates like the
and
may
be viewed as a consequence of the ancilla and garbage bits being ‘hidden’. Summarizing,
given any classical circuit computing a function f(x), we can build a reversible circuit
made entirely of Fredkin gates, which on input of x, together with some ancilla bits
158
Introduction to computer science
in a standard state a, computes f(x), together with some extra ‘garbage’ output, g(x).
Therefore, we represent the action of the computation as (x, a) →(f(x), g(x)).
We now know how to compute functions reversibly. Unfortunately, this computation
produces unwanted garbage bits. With some modiﬁcations it turns out to be possible
to perform the computation so that any garbage bits produced are in a standard state.
This construction is crucial for quantum computation, because garbage bits whose value
depends upon x will in general destroy the interference properties crucial to quantum
computation. To understand how this works it is convenient to assume that the
gate
is available in our repertoire of reversible gates, so we may as well assume that the ancilla
bits a all start out as 0s, with
gates being added where necessary to turn the ancilla
0s into 1s. It will also be convenient to assume that the classical controlled-
gate is
available, deﬁned in a manner analogous to the quantum deﬁnition of Section 1.3.2, that
is, the inputs (c, t) are taken to (c, t ⊕c), where ⊕denotes addition modulo 2. Notice
that t = 0 gives (c, 0) →(c, c), so the controlled-
can be thought of as a reversible
copying gate or
, which leaves no garbage bits at the output.
With the additional
gates appended at the beginning of the circuit, the action
of the computation may be written as (x, 0) →(f(x), g(x)). We could also have added
gates to the beginning of the circuit, in order to create a copy of x which is not
changed during the subsequent computation. With this modiﬁcation, the action of the
circuit may be written
(x, 0, 0) →(x, f(x), g(x)) .
(3.7)
Equation (3.7) is a very useful way of writing the action of the reversible circuit, because
it allows an idea known as uncomputation to be used to get rid of the garbage bits, for a
small cost in the running time of the computation. The idea is the following. Suppose we
start with a four register computer in the state (x, 0, 0, y). The second register is used to
store the result of the computation, and the third register is used to provide workspace for
the computation, that is, the garbage bits g(x). The use of the fourth register is described
shortly, and we assume it starts in an arbitrary state y.
We begin as before, by applying a reversible circuit to compute f, resulting in the state
(x, f(x), g(x), y). Next, we use
s to add the result f(x) bitwise to the fourth register,
leaving the machine in the state (x, f(x), g(x), y ⊕f(x)). However, all the steps used to
compute f(x) were reversible and did not affect the fourth register, so by applying the
reverse of the circuit used to compute f we come to the state (x, 0, 0, y⊕f(x)). Typically,
we omit the ancilla 0s from the description of the function evaluation, and just write the
action of the circuit as
(x, y) →(x, y ⊕f(x)) .
(3.8)
In general we refer to this modiﬁed circuit computing f as the reversible circuit computing
f, even though in principle there are many other reversible circuits which could be used
to compute f.
What resource overhead is involved in doing reversible computation? To analyze this
question, we need to count the number of extra ancilla bits needed in a reversible circuit,
and compare the gate counts with classical models. It ought to be clear that the number of
gates in a reversible circuit is the same as in an irreversible circuit to within the constant
factor which represents the number of Fredkin gates needed to simulate a single element
of the irreversible circuit, and an additional factor of two for uncomputation, with an
The analysis of computational problems
159
overhead for the extra
operations used in reversible computation which is linear in
the number of bits involved in the circuit. Similarly, the number of ancilla bits required
scales at most linearly with the number of gates in the irreversible circuit, since each
element in the irreversible circuit can be simulated using a constant number of ancilla
bits. As a result, natural complexity classes such as P and NP are the same no matter
whether a reversible or irreversible model of computation is used. For more elaborate
complexity classes like PSPACE the situation is not so immediately clear; see Problem 3.9
and ‘History and further reading’ for a discussion of some such subtleties.
Exercise 3.31: (Reversible half-adder)
Construct a reversible circuit which, when
two bits x and y are input, outputs (x, y, c, x ⊕y), where c is the carry bit when
x and y are added.
The Fredkin gate and its implementation using the billiard ball computer offers a
beautiful paradigm for reversible computation. There is another reversible logic gate, the
Toffoli gate, which is also universal for classical computation. While the Toffoli gate does
not have quite the same elegant physical simplicity as the billiard ball implementation of
the Fredkin gate, it will be more useful in the study of quantum computation. We have
already met the Toffoli gate in Section 1.4.1, but for convenience we review its properties
here.
The Toffoli gate has three input bits, a, b and c. a and b are known as the ﬁrst and
second control bits, while c is the target bit. The gate leaves both control bits unchanged,
ﬂips the target bit if both control bits are set, and otherwise leaves the target bit alone.
The truth table and circuit representation for the Toffoli gate are shown in Figure 3.17.
Inputs
Outputs
a
b
c
a′
b′
c′
0
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
1
1
1
0
0
1
0
0
1
0
1
1
0
1
1
1
0
1
1
1
1
1
1
1
1
0
Figure 3.17. Truth table and circuit representation of the Toffoli gate.
How can the Toffoli gate be used to do universal computation? Suppose we wish to
the bits a and b. To do this using the Toffoli gate, we input a and b as control
bits, and send in an ancilla bit set to 1 as the target bit, as shown in Figure 3.18. The
of a and b is output as the target bit. As expected from our study of the Fredkin
gate, the Toffoli gate simulation of a
requires the use of a special ancilla input,
and some of the outputs from the simulation are garbage bits.
The Toffoli gate can also be used to implement the
operation by inputting
an ancilla 1 to the ﬁrst control bit, and a to the second control bit, producing the output
1, a, a. This is illustrated in Figure 3.19. Recalling that
and
are together
160
Introduction to computer science
Figure 3.18. Implementing a
gate using a Toffoli gate. The top two bits represent the input to the
,
while the third bit is prepared in the standard state 1, sometimes known as an ancilla state. The output from the
is on the third bit.
universal for computation, we see that an arbitrary circuit can be efﬁciently simulated
using a reversible circuit consisting only of Toffoli gates and ancilla bits, and that useful
additional techniques such as uncomputation may be achieved using the same methods
as were employed with the Fredkin gate.
Figure 3.19.
with the Toffoli gate, with the second bit being the input to the
, and the other two
bits standard ancilla states. The output from
appears on the second and third bits.
Our interest in reversible computation was motivated by our desire to understand the
energy requirements for computation. It is clear that the noise-free billiard ball model
of computation requires no energy for its operation; what about models based upon
the Toffoli gate? This can only be determined by examining speciﬁc models for the
computation of the Toffoli gate. In Chapter 7, we examine several such implementations,
and it turns out that, indeed, the Toffoli gate can be implemented in a manner which
does not require the expenditure of energy.
There is a signiﬁcant caveat attached to the idea that computation can be done without
the expenditure of energy. As we noted earlier, the billiard ball model of computation is
highly sensitive to noise, and this is true of many other models of reversible computation.
To nullify the effects of noise, some form of error-correction needs to be done. Such
error-correction typically involves the performance of measurements on the system to
determine whether the system is behaving as expected, or if an error has occurred.
Because the computer’s memory is ﬁnite, the bits used to store the measurement results
utilized in error-correction must eventually be erased to make way for new measurement
results. According to Landauer’s principle, this erasure carries an associated energy cost
Perspectives on computer science
161
that must be accounted for when tallying the total energy cost of the computation. We
analyze the energy cost associated with error-correction in more detail in Section 12.4.4.
What can we conclude from our study of reversible computation? There are three
key ideas. First, reversibility stems from keeping track of every bit of information; irre-
versibility occurs only when information is lost or erased. Second, by doing computation
reversibly, we obviate the need for energy expenditure during computation. All computa-
tions can be done, in principle, for zero cost in energy. Third, reversible computation can
be done efﬁciently, without the production of garbage bits whose value depends upon the
input to the computation. That is, if there is an irreversible circuit computing a function
f, then there is an efﬁcient simulation of this circuit by a reversible circuit with action
(x, y) →(x, y ⊕f(x)).
What are the implications of these results for physics, computer science, and for
quantum computation and quantum information? From the point of view of a physicist
or hardware engineer worried about heat dissipation, the good news is that, in principle,
it is possible to make computation dissipation-free by making it reversible, although in
practice energy dissipation is required for system stability and immunity from noise. At
an even more fundamental level, the ideas leading to reversible computation also lead to
the resolution of a century-old problem in the foundations of physics, the famous problem
of Maxwell’s demon. The story of this problem and its resolution is outlined in Box 3.5
on page 162. From the point of view of a computer scientist, reversible computation
validates the use of irreversible elements in models of computation such as the Turing
machine (since using them or not gives polynomially equivalent models). Moreover, since
the physical world is fundamentally reversible, one can argue that complexity classes
based upon reversible models of computation are more natural than complexity classes
based upon irreversible models, a point revisited in Problem 3.9 and ‘History and further
reading’. From the point of view of quantum computation and quantum information,
reversible computation is enormously important. To harness the full power of quantum
computation, any classical subroutines in a quantum computation must be performed
reversibly and without the production of garbage bits depending on the classical input.
Exercise 3.32: (From Fredkin to Toffoli and back again)
What is the smallest
number of Fredkin gates needed to simulate a Toffoli gate? What is the smallest
number of Toffoli gates needed to simulate a Fredkin gate?
3.3
Perspectives on computer science
In a short introduction such as this chapter, it is not remotely possible to cover in detail
all the great ideas of a ﬁeld as rich as computer science. We hope to have conveyed
to you something of what it means to think like a computer scientist, and provided a
basic vocabulary and overview of some of the fundamental concepts important in the
understanding of computation. To conclude this chapter, we brieﬂy touch on some more
general issues, in order to provide some perspective on how quantum computation and
quantum information ﬁts into the overall picture of computer science.
Our discussion has revolved around the Turing machine model of computation. How
does the computational power of unconventional models of computation such as massively
parallel computers, DNA computers and analog computers compare with the standard
162
Introduction to computer science
Box 3.5: Maxwell’s demon
The laws of thermodynamics govern the amount of work that can be performed by
a physical system at thermodynamic equilibrium. One of these laws, the second law
of thermodynamics, states that the entropy in a closed system can never decrease.
In 1871, James Clerk Maxwell proposed the existence of a machine that apparently
violated this law. He envisioned a miniature little ‘demon’, like that shown in the
ﬁgure below, which could reduce the entropy of a gas cylinder initially at equilibrium
by individually separating the fast and slow molecules into the two halves of the
cylinder. This demon would sit at a little door at the middle partition. When a
fast molecule approaches from the left side the demon opens a door between the
partitions, allowing the molecule through, and then closes the door. By doing this
many times the total entropy of the cylinder can be decreased, in apparent violation
of the second law of thermodynamics.
The resolution to the Maxwell’s demon paradox lies in the fact that the demon must
perform measurements on the molecules moving between the partitions, in order
to determine their velocities. The result of this measurement must be stored in the
demon’s memory. Because any memory is ﬁnite, the demon must eventually begin
erasing information from its memory, in order to have space for new measurement
results. By Landauer’s principle, this act of erasing information increases the total
entropy of the combined system – demon, gas cylinder, and their environments. In
fact, a complete analysis shows that Landauer’s principle implies that the entropy of
the combined system is increased at least as much by this act of erasing information
as the entropy of the combined system is decreased by the actions of the demon,
thus ensuring that the second law of thermodynamics is obeyed.
Turing machine model of computation and, implicitly, with quantum computation? Let’s
begin with parallel computing architectures. The vast majority of computers in existence
are serial computers, processing instructions one at a time in some central processing
unit. By contrast, parallel computers can process more than one instruction at a time,
leading to a substantial savings in time and money for some applications. Nevertheless,
parallel processing does not offer any fundamental advantage over the standard Turing
machine model when issues of efﬁciency are concerned, because a Turing machine can
simulate a parallel computer with polynomially equivalent total physical resources – the
total space and time used by the computation. What a parallel computer gains in time,
Perspectives on computer science
163
it loses in the total spatial resources required to perform the computation, resulting in a
net of no essential change in the power of the computing model.
An interesting speciﬁc example of massively parallel computing is the technique of
DNA computing. A strand of DNA, deoxyribonucleic acid, is a molecule composed
of a sequence (a polymer) of four kinds of nucleotides distinguished by the bases they
carry, denoted by the letter A (adenine), C (cytosine), G (guanine) and T (thymine).
Two strands, under certain circumstances, can anneal to form a double strand, if the
respective base pairs form complements of each other (A matches T and G matches
C). The ends are also distinct and must match appropriately. Chemical techniques can
be used to amplify the number of strands beginning or ending with speciﬁc sequences
(polymerase chain reaction), separate the strands by length (gel electrophoresis), dissolve
double strands into single strands (changing temperature and pH), read the sequence on
a strand, cut strands at a speciﬁc position (restriction enzymes), and detect if a certain
sequence of DNA is in a test tube. The procedure for using these mechanisms in a robust
manner is rather involved, but the basic idea can be appreciated from an example.
The directed Hamiltonian path problem is a simple and equivalently hard variant of
the Hamiltonian cycle problem of Section 3.2.2, in which the goal is to determine if a
path exists or not between two speciﬁed vertices j1 and jN in a directed graph G of N
vertices, entering each vertex exactly once, and following only allowed edge directions.
This problem can be solved with a DNA computer using the following ﬁve steps, in
which xj are chosen to be unique sequences of bases (and ¯xj their complements), DNA
strands xjxk encode edges, and strands ¯xj ¯xj encode vertices. (1) Generate random paths
through G, by combining a mixture of all possible vertex and edge DNA strands, and
waiting for the strands to anneal. (2) Keep only the paths beginning with j1 and ending
with jN, by amplifying only the double strands beginning with ¯xj1 and ending with ¯xjN.
(3) Select only paths of length N, by separating the strands according to their length. (4)
Select only paths which enter each vertex at least once, by dissolving the DNA into single
strands, and annealing with all possible vertex strands one at a time and ﬁltering out only
those strands which anneal. And (5) detect if any strands have survived the selection
steps; if so, then a path exists, and otherwise, it does not. To ensure the answer is correct
with sufﬁciently high probability, xj may be chosen to contain many (≈30) bases, and
a large number (≈1014 or more are feasible) of strands are used in the reaction.
Heuristic methods are available to improve upon this basic idea. Of course, exhaustive
search methods such as this only work as long as all possible paths can be generated
efﬁciently, and thus the number of molecules used must grow exponentially as the size of
the problem (the number of vertices in the example above). DNA molecules are relatively
small and readily synthesized, and the huge number of DNA combinations one can ﬁt
into a test tube can stave off the exponential complexity cost increase for a while – up
to a few dozen vertices – but eventually the exponential cost limits the applicability of
this method. Thus, while DNA computing offers an attractive and physically realizable
model of computation for the solution of certain problems, it is a classical computing
technique and offers no essential improvement in principle over a Turing machine.
Analog computers offer a yet another paradigm for performing computation. A com-
puter is analog when the physical representation of information it uses for computation
is based on continuous degrees of freedom, instead of zeroes and ones. For example,
a thermometer is an analog computer. Analog circuitry, using resistors, capacitors, and
ampliﬁers, is also said to perform analog computation. Such machines have an inﬁnite
164
Introduction to computer science
resource to draw upon in the ideal limit, since continuous variables like position and
voltage can store an unlimited amount of information. But this is only true in the absence
of noise. The presence of a ﬁnite amount of noise reduces the number of distinguishable
states of a continuous variable to a ﬁnite number – and thus restricts analog computers
to the representation of a ﬁnite amount of information. In practice, noise reduces analog
computers to being no more powerful than conventional digital computers, and through
them Turing machines. One might suspect that quantum computers are just analog com-
puters, because of the use of continuous parameters in describing qubit states; however,
it turns out that the effects of noise on a quantum computer can effectively be digitized.
As a result, their computational advantages remain even in the presence of a ﬁnite amount
of noise, as we shall see in Chapter 10.
What of the effects of noise on digital computers? In the early days of computation,
noise was a very real problem for computers. In some of the original computers a vacuum
tube would malfunction every few minutes. Even today, noise is a problem for compu-
tational devices such as modems and hard drives. Considerable effort was devoted to the
problem of understanding how to construct reliable computers from unreliable compo-
nents. It was proven by von Neumann that this is possible with only a polynomial increase
in the resources required for computation. Ironically, however, modern computers use
none of those results, because the components of modern computers are fantastically
reliable. Failure rates of 10−17 and even less are common in modern electronic compo-
nents. For this reason, failures happen so rarely that the extra effort required to protect
against them is not regarded as being worth making. On the other hand, we shall ﬁnd
that quantum computers are very delicate machines, and will likely require substantial
application of error-correction techniques.
Different architectures may change the effects of noise. For example, if the effect of
noise is ignored, then changing to a computer architecture in which many operations are
performed in parallel may not change the number of operations which need to be done.
However, a parallel system may be substantially more resistant to noise, because the effects
of noise have less time to accumulate. Therefore, in a realistic analysis, the parallel version
of an algorithm may have some substantial advantages over a serial implementation.
Architecture design is a well developed ﬁeld of study for classical computers. Hardly
anything similar has been developed along the same lines for quantum computers, but
the study of noise already suggests some desirable traits for future quantum computer
architectures, such as a high level of parallelism.
A fourth model of computation is distributed computation, in which two or more
spatially separated computational units are available to solve a computational problem.
Obviously, such a model of computation is no more powerful than the Turing machine
model in the sense that it can be efﬁciently simulated on a Turing machine. However,
distributed computation gives rise to an intriguing new resource challenge: how best to
utilize multiple computational units when the cost of communication between the units is
high. This problem of distributed computation becomes especially interesting as comput-
ers are connected through high speed networks; although the total computational capacity
of all the computers on a network might be extremely large, utilization of that potential
is difﬁcult. Most interesting problems do not divide easily into independent chunks that
can be solved separately, and may frequently require global communication between dif-
ferent computational subsystems to exchange intermediate results or synchronize status.
The ﬁeld of communication complexity has been developed to address such issues, by
Chapter problems
165
quantifying the cost of communication requirements in solving problems. When quan-
tum resources are available and can be exchanged between distributed computers, the
communication costs can sometimes be greatly reduced.
A recurring theme through these concluding thoughts and through the entire book is
that despite the traditional independence of computer science from physical constraints,
ultimately physical laws have tremendous impact not only upon how computers are
realized, but also the class of problems they are capable of solving. The success of quantum
computation and quantum information as a physically reasonable alternative model of
computation questions closely held tenets of computer science, and thrusts notions of
computer science into the forefront of physics. The task of the remainder of this book is
to stir together ideas from these disparate ﬁelds, and to delight in what results!
Problem 3.1: (Minsky machines)
A Minsky machine consists of a ﬁnite set of
registers, r1, r2, . . . , rk, each capable of holding an arbitrary non-negative
integer, and a program, made up of orders of one of two types. The ﬁrst type has
the form:


 


 



The interpretation is that at point m in the program register rj is incremented
by one, and execution proceeds to point n in the program. The second type of
order has the form:


 


 


nnnnnn
nnnnnn
	 	PPPPPP
PPPPPP
The interpretation is that at point m in the program, register rj is decremented
if it contains a positive integer, and execution proceeds to point n in the
program. If register rj is zero then execution simply proceeds to point p in the
program. The program for the Minsky machine consists of a collection of such
orders, of a form like:


 


 




 


 



FFFF
FFF
FFFFFFFFFFFF


The starting and all possible halting points for the program are conventionally
labeled zero. This program takes the contents of register r1 and adds them to
register r2, while decrementing r1 to zero.
166
Introduction to computer science
(1) Prove that all (Turing) computable functions can be computed on a Minsky
machine, in the sense that given a computable function f(·) there is a
Minsky machine program that when the registers start in the state
(n, 0, . . . , 0) gives as output (f(n), 0, . . . , 0).
(2) Sketch a proof that any function which can be computed on a Minsky
machine, in the sense just deﬁned, can also be computed on a Turing
machine.
Problem 3.2: (Vector games)
A vector game is speciﬁed by a ﬁnite list of vectors,
all of the same dimension, and with integer co-ordinates. The game is to start
with a vector x of non-negative integer co-ordinates and to add to x the ﬁrst
vector from the list which preserves the non-negativity of all the components,
and to repeat this process until it is no longer possible. Prove that for any
computable function f(·) there is a vector game which when started with the
vector (n, 0, . . . , 0) reaches (f(n), 0, . . . , 0). (Hint: Show that a vector game in
k + 2 dimensions can simulate a Minsky machine containing k registers.)
Problem 3.3: (Fractran)
A Fractran program is deﬁned by a list of positive rational
numbers q1, . . . , qn. It acts on a positive integer m by replacing it by qim, where
i is the least number such that qim is an integer. If there is ever a time when
there is no i such that qim is an integer, then execution stops. Prove that for any
computable function f(·) there is a Fractran program which when started with
2n reaches 2f(n) without going through any intermediate powers of 2. (Hint: use
the previous problem.)
Problem 3.4: (Undecidability of dynamical systems)
A Fractran program is
essentially just a very simple dynamical system taking positive integers to
positive integers. Prove that there is no algorithm to decide whether such a
dynamical system ever reaches 1.
Problem 3.5: (Non-universality of two bit reversible logic)
Suppose we are
trying to build circuits using only one and two bit reversible logic gates, and
ancilla bits. Prove that there are Boolean functions which cannot be computed in
this fashion. Deduce that the Toffoli gate cannot be simulated using one and two
bit reversible gates, even with the aid of ancilla bits.
Problem 3.6: (Hardness of approximation of
)
Let r ≥1 and suppose that
there is an approximation algorithm for
which is guaranteed to ﬁnd the
shortest tour among n cities to within a factor r. Let G = (V, E) be any graph on
n vertices. Deﬁne an instance of
by identifying cities with vertices in V , and
deﬁning the distance between cities i and j to be 1 if (i, j) is an edge of G, and to
be ⌈r⌉|V | + 1 otherwise. Show that if the approximation algorithm is applied to
this instance of
then it returns a Hamiltonian cycle for G if one exists, and
otherwise returns a tour of length more than ⌈r⌉|V |. From the NP-completeness
of
it follows that no such approximation algorithm can exist unless P = NP.
Problem 3.7: (Reversible Turing machines)
(1) Explain how to construct a reversible Turing machine that can compute the
same class of functions as is computable on an ordinary Turing machine.
(Hint: It may be helpful to use a multi-tape construction.)
History and further reading
167
(2) Give general space and time bounds for the operation of your reversible
Turing machine, in terms of the time t(x) and space s(x) required on an
ordinary single-tape Turing machine to compute a function f(x).
Problem 3.8: (Find a hard-to-compute class of functions (Research))
Find a
natural class of functions on n inputs which requires a super-polynomial number
of Boolean gates to compute.
Problem 3.9: (Reversible PSPACE = PSPACE)
It can be shown that the problem
‘quantiﬁed satisﬁability’, or
, is PSPACE-complete. That is, every other
language in PSPACE can be reduced to
in polynomial time. The language
is deﬁned to consist of all Boolean formulae ϕ in n variables x1, . . . , xn,
and in conjunctive normal form, such that:
∃x1∀x2∃x3 . . . ∀xn ϕ if n is even;
(3.9)
∃x1∀x2∃x3 . . . ∃xn ϕ if n is odd.
(3.10)
Prove that a reversible Turing machine operating in polynomial space can be
used to solve
. Thus, the class of languages decidable by a computer
operating reversibly in polynomial space is equal to PSPACE.
Problem 3.10: (Ancilla bits and efﬁciency of reversible computation)
Let pm
be the mth prime number. Outline the construction of a reversible circuit which,
upon input of m and n such that n > m, outputs the product pmpn, that is
(m, n) →(pmpn, g(m, n)), where g(m, n) is the ﬁnal state of the ancilla bits
used by the circuit. Estimate the number of ancilla qubits your circuit requires.
Prove that if a polynomial (in log n) size reversible circuit can be found that uses
O(log(log n)) ancilla bits then the problem of factoring a product of two prime
numbers is in P.
History and further reading
Computer science is a huge subject with many interesting subﬁelds. We cannot hope
for any sort of completeness in this brief space, but instead take the opportunity to
recommend a few titles of general interest, and some works on subjects of speciﬁc interest
in relation to topics covered in this book, with the hope that they may prove stimulating.
Modern computer science dates to the wonderful 1936 paper of Turing[Tur36]. The
Church–Turing thesis was ﬁrst stated by Church[Chu36] in 1936, and was then given
a more complete discussion from a different point of view by Turing. Several other
researchers found their way to similar conclusions at about the same time. Many of
these contributions and a discussion of the history may be found in a volume edited
by Davis[Dav65]. Provocative discussions of the Church–Turing thesis and undecidability
may be found in Hofstadter[Hof79] and Penrose[Pen89].
There are many excellent books on algorithm design. We mention only three. First,
there is the classic series by Knuth[Knu97, Knu98a, Knu98b] which covers an enormous portion
of computer science. Second, there is the marvelous book by Cormen, Leiserson, and
Rivest[CLR90]. This huge book contains a plethora of well-written material on many areas
168
Introduction to computer science
of algorithm design. Finally, the book of Motwani and Raghavan[MR95] is an excellent
survey of the ﬁeld of randomized algorithms.
The modern theory of computational complexity was especially inﬂuenced by the
papers of Cook[Coo71] and Karp[Kar72]. Many similar ideas were arrived at independently
in Russia by Levin[Lev73], but unfortunately took time to propagate to the West. The
classic book by Garey and Johnson[GJ79] has also had an enormous inﬂuence on the
ﬁeld. More recently, Papadimitriou[Pap94] has written a beautiful book that surveys many
of the main ideas of computational complexity theory. Much of the material in this
chapter is based upon Papadimitriou’s book. In this chapter we considered only one type
of reducibility between languages, polynomial time reducibility. There are many other
notions of reductions between languages. An early survey of these notions was given by
Ladner, Lynch and Selman[LLS75]. The study of different notions of reducibility later
blossomed into a subﬁeld of research known as structural complexity, which has been
reviewed by Balc´azar, Diaz, and Gabarr´o[BDG88a, BDG88b].
The connection between information, energy dissipation, and computation has a long
history. The modern understanding is due to a 1961 paper by Landauer[Lan61], in which
Landauer’s principle was ﬁrst formulated. A paper by Szilard[Szi29] and a 1949 lecture
by von Neumann[von66] (page 66) arrive at conclusions close to Landauer’s principle, but
do not fully grasp the essential point that it is the erasure of information that requires
dissipation.
Reversible Turing machines were invented by Lecerf[Lec63] and later, but indepen-
dently, in an inﬂuential paper by Bennett[Ben73]. Fredkin and Toffoli[FT82] introduced
reversible circuit models of computation. Two interesting historical documents are Bar-
ton’s May, 1978 MIT 6.895 term paper[Bar78], and Ressler’s 1981 Master’s thesis[Res81],
which contain designs for a reversible PDP-10! Today, reversible logic is potentially
important in implementations of low-power CMOS circuitry[YK95].
Maxwell’s demon is a fascinating subject, with a long and intricate history. Maxwell
proposed his demon in 1871[Max71]. Szilard published a key paper in 1929[Szi29] which an-
ticipated many of the details of the ﬁnal resolution of the problem of Maxwell’s demon.
In 1965 Feynman[FLS65b] resolved a special case of Maxwell’s demon. Bennett, build-
ing on Landauer’s work[Lan61], wrote two beautiful papers on the subject[BBBW82, Ben87]
which completed the resolution of the problem. An interesting book about the history of
Maxwell’s demon and its exorcism is the collection of papers by Leff and Rex[LR90].
DNA computing was invented by Adleman, and the solution of the directed Hamil-
tonian path problem we describe is his[Adl94]. Lipton has also shown how
and
circuit satisﬁability can be solved in this model[Lip95]. A good general article is Adleman’s
Scientiﬁc American article[Adl98]; for an insightful look into the universality of DNA
operations, see Winfree[Win98]. An interesting place to read about performing reliable
computation in the presence of noise is the book by Winograd and Cowan[WC67]. This
topic will be addressed again in Chapter 10. A good textbook on computer architecture
is by Hennessey, Goldberg, and Patterson.[HGP96].
Problems 3.1 through 3.4 explore a line of thought originated by Minsky (in his
beautiful book on computational machines[Min67]) and developed by Conway[Con72, Con86].
The Fractran programming language is certainly one of the most beautiful and elegant
universal computational models known, as demonstrated by the following example, known
History and further reading
169
as PRIMEGAME[Con86]. PRIMEGAME is deﬁned by the list of rational numbers:
17
91; 78
85; 19
51; 23
38; 29
33; 77
29; 95
23; 77
19; 1
17; 11
13; 13
11; 15
2 ; 1
7; 55
1 .
(3.11)
Amazingly, when PRIMEGAME is started at 2, the other powers of 2 that appear,
namely, 22, 23, 25, 27, 211, 213, . . . , are precisely the prime powers of 2, with the powers
stepping through the prime numbers, in order. Problem 3.9 is a special case of the more
general subject of the spatial requirements for reversible computation. See the papers by
Bennett[Ben89], and by Li, Tromp and Vitanyi[LV96, LTV98].
II Quantum computation
4 Quantum circuits
The theory of computation has traditionally been studied almost entirely in
the abstract, as a topic in pure mathematics. This is to miss the point of it.
Computers are physical objects, and computations are physical processes. What
computers can or cannot compute is determined by the laws of physics alone,
and not by pure mathematics.
– David Deutsch
Like mathematics, computer science will be somewhat different from the other
sciences, in that it deals with artiﬁcial laws that can be proved, instead of
natural laws that are never known with certainty.
– Donald Knuth
The opposite of a profound truth may well be another profound truth.
– Niels Bohr
This chapter begins Part II of the book, in which we explore quantum computation in
detail. The chapter develops the fundamental principles of quantum computation, and
establishes the basic building blocks for quantum circuits, a universal language for de-
scribing sophisticated quantum computations. The two fundamental quantum algorithms
known to date are constructed from these circuits in the following two chapters. Chap-
ter 5 presents the quantum Fourier transform and its applications to phase estimation,
order-ﬁnding and factoring. Chapter 6 describes the quantum search algorithm, and its
applications to database search, counting and speedup of solutions to NP-complete prob-
lems. Chapter 7 concludes Part II with a discussion of how quantum computation may
one day be experimentally realized. Two other topics of great interest for quantum com-
putation, quantum noise and quantum error-correction, are deferred until Part III of the
book, in view of their wide interest also outside quantum computation.
There are two main ideas introduced in this chapter. First, we explain in detail the
fundamental model of quantum computation, the quantum circuit model. Second, we
demonstrate that there exists a small set of gates which are universal, that is, any quantum
computation whatsoever can be expressed in terms of those gates. Along the way we also
have occasion to describe many other basic results of quantum computation. Section 4.1
begins the chapter with an overview of quantum algorithms, focusing on what algorithms
are known, and the unifying techniques underlying their construction. Section 4.2 is a
detailed study of single qubit operations. Despite their simplicity, single qubit operations
offer a rich playground for the construction of examples and techniques, and it is essential
to understand them in detail. Section 4.3 shows how to perform multi-qubit controlled
unitary operations, and Section 4.4 discusses the description of measurement in the
quantum circuits model. These elements are then brought together in Section 4.5 for the
statement and proof of the universality theorem. We summarize all the basic elements
172
Quantum circuits
of quantum computation in Section 4.6, and discuss possible variants of the model, and
the important question of the relationship in computational power between classical and
quantum computers. Section 4.7 concludes the chapter with an important and instructive
application of quantum computation to the simulation of real quantum systems.
This chapter is perhaps the most reader-intensive of all the chapters in the book, with
a high density of exercises for you to complete, and it is worth explaining the reason for
this intensity. Obtaining facility with the basic elements of the quantum circuit model
of computation is quite easy, but requires assimilating a large number of simple results
and techniques that must become second nature if one is to progress to the more difﬁcult
problem of designing quantum algorithms. For this reason we take an example-oriented
approach in this chapter, and ask you to ﬁll in many of the details, in order to acquire
such a facility. A less intensive, but somewhat superﬁcial overview of the basic elements
of quantum computation may be obtained by skipping to Section 4.6.
4.1
Quantum algorithms
What is a quantum computer good for? We’re all familiar with the frustration of needing
more computer resources to solve a computational problem. Practically speaking, many
interesting problems are impossible to solve on a classical computer, not because they
are in principle insoluble, but because of the astronomical resources required to solve
realistic cases of the problem.
The spectacular promise of quantum computers is to enable new algorithms which
render feasible problems requiring exorbitant resources for their solution on a classical
computer. At the time of writing, two broad classes of quantum algorithms are known
which fulﬁll this promise. The ﬁrst class of algorithms is based upon Shor’s quantum
Fourier transform, and includes remarkable algorithms for solving the factoring and dis-
crete logarithm problems, providing a striking exponential speedup over the best known
classical algorithms. The second class of algorithms is based upon Grover’s algorithm
for performing quantum searching. These provide a less striking but still remarkable
quadratic speedup over the best possible classical algorithms. The quantum searching
algorithm derives its importance from the widespread use of search-based techniques in
classical algorithms, which in many instances allows a straightforward adaptation of the
classical algorithm to give a faster quantum algorithm.
Figure 4.1 sketches the state of knowledge about quantum algorithms at the time of
writing, including some sample applications of those algorithms. Naturally, at the core of
the diagram are the quantum Fourier transform and the quantum searching algorithm.
Of particular interest in the ﬁgure is the quantum counting algorithm. This algorithm is
a clever combination of the quantum searching and Fourier transform algorithms, which
can be used to estimate the number of solutions to a search problem more quickly than
is possible on a classical computer.
The quantum searching algorithm has many potential applications, of which but a few
are illustrated. It can be used to extract statistics, such as the minimal element, from
an unordered data set, more quickly than is possible on a classical computer. It can be
used to speed up algorithms for some problems in NP – speciﬁcally, those problems for
which a straightforward search for a solution is the best algorithm known. Finally, it can
be used to speed up the search for keys to cryptosystems such as the widely used Data
Encryption Standard (DES). These and other applications are explained in Chapter 6.
Quantum algorithms
173
Fourier
transform
Quantum
search
NP























	
















Figure 4.1. The main quantum algorithms and their relationships, including some notable applications.
The quantum Fourier transform also has many interesting applications. It can be used
to solve the discrete logarithm and factoring problems. These results, in turn, enable a
quantum computer to break many of the most popular cryptosystems now in use, includ-
ing the RSA cryptosystem. The Fourier transform also turns out to be closely related
to an important problem in mathematics, ﬁnding a hidden subgroup (a generalization of
ﬁnding the period of a periodic function). The quantum Fourier transform and several of
its applications, including fast quantum algorithms for factoring and discrete logarithm,
are explained in Chapter 5.
Why are there so few quantum algorithms known which are better than their classical
counterparts? The answer is that coming up with good quantum algorithms seems to be
a difﬁcult problem. There are at least two reasons for this. First, algorithm design, be
it classical or quantum, is not an easy business! The history of algorithms shows us that
considerable ingenuity is often required to come up with near optimal algorithms, even for
apparently very simple problems, like the multiplication of two numbers. Finding good
quantum algorithms is made doubly difﬁcult because of the additional constraint that we
want our quantum algorithms to be better than the best known classical algorithms. A
second reason for the difﬁculty of ﬁnding good quantum algorithms is that our intuitions
are much better adapted to the classical world than they are to the quantum world. If
we think about problems using our native intuition, then the algorithms which we come
up with are going to be classical algorithms. It takes special insights and special tricks to
come up with good quantum algorithms.
Further study of quantum algorithms will be postponed until the next chapter. In this
chapter we provide an efﬁcient and powerful language for describing quantum algorithms,
the language of quantum circuits – assemblies of discrete sets of components which
describe computational procedures. This construction will enable us to quantify the cost
of an algorithm in terms of things like the total number of gates required, or the circuit
depth. The circuit language also comes with a toolbox of tricks that simpliﬁes algorithm
design and provides ready conceptual understanding.
Quantum
counting
Hidden subgroup
problem
Discrete log
Order-finding
Factoring
Break cryptosystems
(RSA)
Search for
crypto keys
Speed up for some
problems
Statistics
mean,median,min
174
Quantum circuits
4.2
Single qubit operations
The development of our quantum computational toolkit begins with operations on the
simplest quantum system of all – a single qubit. Single qubit gates were introduced in
Section 1.3.1. Let us quickly summarize what we learned there; you may ﬁnd it useful
to refer to the notes on notation on page xxiii as we go along.
A single qubit is a vector |ψ⟩= a|0⟩+ b|1⟩parameterized by two complex numbers
satisfying |a|2 + |b|2 = 1. Operations on a qubit must preserve this norm, and thus are
described by 2×2 unitary matrices. Of these, some of the most important are the Pauli
matrices; it is useful to list them again here:
X ≡
 0
1
1
0

;
Y ≡
 0
−i
i
0

;
Z ≡
 1
0
0
−1

.
(4.1)
Three other quantum gates will play a large part in what follows, the Hadamard gate
(denoted H), phase gate (denoted S), and π/8 gate (denoted T):
H =
1
√
2
 1
1
1
−1

;
S =
 1
0
0
i

;
T =
 1
0
0
exp(iπ/4)

.
(4.2)
A couple of useful algebraic facts to keep in mind are that H = (X +Z)/
√
2 and S = T 2.
You might wonder why the T gate is called the π/8 gate when it is π/4 that appears in
the deﬁnition. The reason is that the gate has historically often been referred to as the
π/8 gate, simply because up to an unimportant global phase T is equal to a gate which
has exp(±iπ/8) appearing on its diagonals.
T = exp(iπ/8)
 exp(−iπ/8)
0
0
exp(iπ/8)

.
(4.3)
Nevertheless, the nomenclature is in some respects rather unfortunate, and we often refer
to this gate as the T gate.
Recall also that a single qubit in the state a|0⟩+b|1⟩can be visualized as a point (θ, ϕ)
on the unit sphere, where a = cos(θ/2), b = eiϕ sin(θ/2), and a can be taken to be real
because the overall phase of the state is unobservable. This is called the Bloch sphere
representation, and the vector (cos ϕ sin θ, sin ϕ sin θ, cos θ) is called the Bloch vector.
We shall return to this picture often as an aid to intuition.
Exercise 4.1:
In Exercise 2.11, which you should do now if you haven’t already done
it, you computed the eigenvectors of the Pauli matrices. Find the points on the
Bloch sphere which correspond to the normalized eigenvectors of the different
Pauli matrices.
The Pauli matrices give rise to three useful classes of unitary matrices when they are
exponentiated, the rotation operators about the ˆx, ˆy, and ˆz axes, deﬁned by the equations:
Rx(θ) ≡e−iθX/2 = cos θ
2I −i sin θ
2X =

cos θ
2
−i sin θ
2
−i sin θ
2
cos θ
2

(4.4)
Ry(θ) ≡e−iθY/2 = cos θ
2I −i sin θ
2Y =
 cos θ
2
−sin θ
2
sin θ
2
cos θ
2

(4.5)
Rz(θ) ≡e−iθZ/2 = cos θ
2I −i sin θ
2Z =
 e−iθ/2
0
0
eiθ/2

.
(4.6)
Single qubit operations
175
Exercise 4.2:
Let x be a real number and A a matrix such that A2 = I. Show that
exp(iAx) = cos(x)I + i sin(x)A.
(4.7)
Use this result to verify Equations (4.4) through (4.6).
Exercise 4.3:
Show that, up to a global phase, the π/8 gate satisﬁes T = Rz(π/4).
Exercise 4.4:
Express the Hadamard gate H as a product of Rx and Rz rotations and
eiϕ for some ϕ.
If ˆn = (nx, ny, nz) is a real unit vector in three dimensions then we generalize the
previous deﬁnitions by deﬁning a rotation by θ about the ˆn axis by the equation
R ˆn(θ) ≡exp(−iθ ˆn · ⃗σ/2) = cos
θ
2

I −i sin
θ
2

(nxX + nyY + nzZ) ,
(4.8)
where ⃗σ denotes the three component vector (X, Y, Z) of Pauli matrices.
Exercise 4.5:
Prove that (ˆn · ⃗σ)2 = I, and use this to verify Equation (4.8).
Exercise 4.6: (Bloch sphere interpretation of rotations)
One reason why the
R ˆn(θ) operators are referred to as rotation operators is the following fact, which
you are to prove. Suppose a single qubit has a state represented by the Bloch
vector ⃗λ. Then the effect of the rotation R ˆn(θ) on the state is to rotate it by an
angle θ about the ˆn axis of the Bloch sphere. This fact explains the rather
mysterious looking factor of two in the deﬁnition of the rotation matrices.
Exercise 4.7:
Show that XY X = −Y and use this to prove that
XRy(θ)X = Ry(−θ).
Exercise 4.8:
An arbitrary single qubit unitary operator can be written in the form
U = exp(iα)R ˆn(θ)
(4.9)
for some real numbers α and θ, and a real three-dimensional unit vector ˆn.
1. Prove this fact.
2. Find values for α, θ, and ˆn giving the Hadamard gate H.
3. Find values for α, θ, and ˆn giving the phase gate
S =
 1
0
0
i

.
(4.10)
An arbitrary unitary operator on a single qubit can be written in many ways as a
combination of rotations, together with global phase shifts on the qubit. The following
theorem provides a means of expressing an arbitrary single qubit rotation that will be
particularly useful in later applications to controlled operations.
Theorem 4.1: (Z-Y decomposition for a single qubit) Suppose U is a unitary
operation on a single qubit. Then there exist real numbers α, β, γ and δ such that
U = eiαRz(β)Ry(γ)Rz(δ).
(4.11)
176
Quantum circuits
Proof
Since U is unitary, the rows and columns of U are orthonormal, from which it follows
that there exist real numbers α, β, γ,and δ such that
U =
 ei(α−β/2−δ/2) cos γ
2
−ei(α−β/2+δ/2) sin γ
2
ei(α+β/2−δ/2) sin γ
2
ei(α+β/2+δ/2) cos γ
2

.
(4.12)
Equation (4.11) now follows immediately from the deﬁnition of the rotation matrices and
matrix multiplication.
Exercise 4.9:
Explain why any single qubit unitary operator may be written in the
form (4.12).
Exercise 4.10: (X-Y decomposition of rotations)
Give a decomposition
analogous to Theorem 4.1 but using Rx instead of Rz.
Exercise 4.11:
Suppose ˆm and ˆn are non-parallel real unit vectors in three
dimensions. Use Theorem 4.1 to show that an arbitrary single qubit unitary U
may be written
U = eiαR ˆn(β)R ˆm(γ)R ˆn(δ),
(4.13)
for appropriate choices of α, β, γ and δ.
The utility of Theorem 4.1 lies in the following mysterious looking corollary, which
is the key to the construction of controlled multi-qubit unitary operations, as explained
in the next section.
Corollary 4.2: Suppose U is a unitary gate on a single qubit. Then there exist unitary
operators A, B, C on a single qubit such that ABC = I and U = eiαAXBXC,
where α is some overall phase factor.
Proof
In the notation of Theorem 4.1, set A ≡Rz(β)Ry(γ/2), B ≡Ry(−γ/2)Rz(−(δ +β)/2)
and C ≡Rz((δ −β)/2). Note that
ABC = Rz(β)Ry
'γ
2
(
Ry
'
−γ
2
(
Rz

−δ + β
2

Rz
δ −β
2

= I .
(4.14)
Since X2 = I, and using Exercise 4.7, we see that
XBX = XRy
'
−γ
2
(
XXRz

−δ + β
2

X = Ry
'γ
2
(
Rz
δ + β
2

.
(4.15)
Thus
AXBXC = Rz(β)Ry
'γ
2
(
Ry
'γ
2
(
Rz
δ + β
2

Rz
δ −β
2

(4.16)
= Rz(β)Ry(γ)Rz(δ) .
(4.17)
Thus U = eiαAXBXC and ABC = I, as required.
Exercise 4.12:
Give A, B, C, and α for the Hadamard gate.
Controlled operations
177
Exercise 4.13: (Circuit identities)
It is useful to be able to simplify circuits by
inspection, using well-known identities. Prove the following three identities:
HXH = Z;
HY H = −Y ;
HZH = X.
(4.18)
Exercise 4.14:
Use the previous exercise to show that HTH = Rx(π/4), up to a
global phase.
Exercise 4.15: (Composition of single qubit operations)
The Bloch
representation gives a nice way to visualize the effect of composing two rotations.
(1) Prove that if a rotation through an angle β1 about the axis ˆn1 is followed by a
rotation through an angle β2 about an axis ˆn2, then the overall rotation is
through an angle β12 about an axis ˆn12 given by
c12 = c1c2 −s1s2 ˆn1 · ˆn2
(4.19)
s12 ˆn12 = s1c2 ˆn1 + c1s2 ˆn2 −s1s2 ˆn2 × ˆn1 ,
(4.20)
where ci = cos(βi/2), si = sin(βi/2), c12 = cos(β12/2), and s12 = sin(β12/2).
(2) Show that if β1 = β2 and ˆn1 = ˆz these equations simplify to
c12 = c2 −s2 ˆz · ˆn2
(4.21)
s12 ˆn12 = sc(ˆz + ˆn2) −s2 ˆn2 × ˆz ,
(4.22)
where c = c1 and s = s1.
Symbols for the common single qubit gates are shown in Figure 4.2. Recall the basic
properties of quantum circuits: time proceeds from left to right; wires represent qubits,
and a ‘/’ may be used to indicate a bundle of qubits.
Hadamard
1
√
2
 1
1
1
−1

Pauli-X
 0
1
1
0

Pauli-Y
 0
−i
i
0

Pauli-Z
 1
0
0
−1

Phase
 1
0
0
i

π/8
 1
0
0
eiπ/4

Figure 4.2. Names, symbols, and unitary matrices for the common single qubit gates.
4.3
Controlled operations
‘If A is true, then do B’. This type of controlled operation is one of the most useful in
computing, both classical and quantum. In this section we explain how complex controlled
operations may be implemented using quantum circuits built from elementary operations.
178
Quantum circuits
The prototypical controlled operation is the controlled-
, which we met in Sec-
tion 1.2.1. Recall that this gate, which we’ll often refer to as
, is a quantum gate
with two input qubits, known as the control qubit and target qubit, respectively. It is
drawn as shown in Figure 4.3. In terms of the computational basis, the action of the
is given by |c⟩|t⟩→|c⟩|t ⊕c⟩; that is, if the control qubit is set to |1⟩then the
target qubit is ﬂipped, otherwise the target qubit is left alone. Thus, in the computational
basis |control, target⟩the matrix representation of
is
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⎤
⎥⎥⎦.
(4.23)
Figure 4.3. Circuit representation for the controlled-
gate. The top line represents the control qubit, the
bottom line the target qubit.
More generally, suppose U is an arbitrary single qubit unitary operation. A controlled-
U operation is a two qubit operation, again with a control and a target qubit. If the control
qubit is set then U is applied to the target qubit, otherwise the target qubit is left alone;
that is, |c⟩|t⟩→|c⟩U c|t⟩. The controlled-U operation is represented by the circuit shown
in Figure 4.4.
Figure 4.4. Controlled-U operation. The top line is the control qubit, and the bottom line is the target qubit. If the
control qubit is set then U is applied to the target, otherwise it is left alone.
Exercise 4.16: (Matrix representation of multi-qubit gates)
What is the 4×4
unitary matrix for the circuit
in the computational basis? What is the unitary matrix for the circuit
Controlled operations
179
in the computational basis?
Exercise 4.17: (Building
from controlled-Z gates)
Construct a
gate
from one controlled-Z gate, that is, the gate whose action in the computational
basis is speciﬁed by the unitary matrix
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1
⎤
⎥⎥⎦,
and two Hadamard gates, specifying the control and target qubits.
Exercise 4.18:
Show that
Exercise 4.19: (
action on density matrices)
The
gate is a simple
permutation whose action on a density matrix ρ is to rearrange the elements in
the matrix. Write out this action explicitly in the computational basis.
Exercise 4.20: (
basis transformations)
Unlike ideal classical gates, ideal
quantum gates do not have (as electrical engineers say) ‘high-impedance’ inputs.
In fact, the role of ‘control’ and ‘target’ are arbitrary – they depend on what basis
you think of a device as operating in. We have described how the
behaves
with respect to the computational basis, and in this description the state of the
control qubit is not changed. However, if we work in a different basis then the
control qubit does change: we will show that its phase is ﬂipped depending on
the state of the ‘target’ qubit! Show that
Introducing basis states |±⟩≡(|0⟩± |1⟩)/
√
2, use this circuit identity to show
that the effect of a
with the ﬁrst qubit as control and the second qubit as
target is as follows:
|+⟩|+⟩→|+⟩|+⟩
(4.24)
|−⟩|+⟩→|−⟩|+⟩
(4.25)
|+⟩|−⟩→|−⟩|−⟩
(4.26)
|−⟩|−⟩→|+⟩|−⟩.
(4.27)
Thus, with respect to this new basis, the state of the target qubit is not changed,
while the state of the control qubit is ﬂipped if the target starts as |−⟩, otherwise
180
Quantum circuits
it is left alone. That is, in this basis, the target and control have essentially
interchanged roles!
Our immediate goal is to understand how to implement the controlled-U operation
for arbitrary single qubit U, using only single qubit operations and the
gate. Our
strategy is a two-part procedure based upon the decomposition U = eiαAXBXC given
in Corollary 4.2 on page 176.
Our ﬁrst step will be to apply the phase shift exp(iα) on the target qubit, controlled
by the control qubit. That is, if the control qubit is |0⟩, then the target qubit is left alone,
while if the control qubit is |1⟩, a phase shift exp(iα) is applied to the target. A circuit
implementing this operation using just a single qubit unitary gate is depicted on the right
hand side of Figure 4.5. To verify that this circuit works correctly, note that the effect
of the circuit on the right hand side is
|00⟩→|00⟩,
|01⟩→|01⟩,
|10⟩→eiα|10⟩,
|11⟩→eiα|11⟩,
(4.28)
which is exactly what is required for the controlled operation on the left hand side.
Figure 4.5. Controlled phase shift gate and an equivalent circuit for two qubits.
We may now complete the construction of the controlled-U operation, as shown in
Figure 4.6. To understand why this circuit works, recall from Corollary 4.2 that U
may be written in the form U = eiαAXBXC, where A, B and C are single qubit
operations such that ABC = I. Suppose that the control qubit is set. Then the operation
eiαAXBXC = U is applied to the second qubit. If, on the other hand, the control qubit
is not set, then the operation ABC = I is applied to the second qubit; that is, no change
is made. That is, this circuit implements the controlled-U operation.
Now that we know how to condition on a single qubit being set, what about condition-
ing on multiple qubits? We’ve already met one example of multiple qubit conditioning,
the Toffoli gate, which ﬂips the third qubit, the target qubit, conditioned on the ﬁrst
two qubits, the control qubits, being set to one. More generally, suppose we have n + k
qubits, and U is a k qubit unitary operator. Then we deﬁne the controlled operation
Cn(U) by the equation
Cn(U)|x1x2 . . . xn⟩|ψ⟩= |x1x2 . . . xn⟩U x1x2...xn|ψ⟩,
(4.29)
where x1x2 . . . xn in the exponent of U means the product of the bits x1, x2, . . . , xn.
That is, the operator U is applied to the last k qubits if the ﬁrst n qubits are all equal
to one, and otherwise, nothing is done. Such conditional operations are so useful that we
Controlled operations
181
Figure 4.6. Circuit implementing the controlled-U operation for single qubit U. α, A, B and C satisfy
U = exp(iα)AXBXC, ABC = I.
introduce a special circuit notation for them, illustrated in Figure 4.7. For the following
we assume that k = 1, for simplicity. Larger k can be dealt with using essentially the
same methods, however for k ≥2 there is the added complication that we don’t (yet)
know how to perform arbitrary operations on k qubits.
Figure 4.7. Sample circuit representation for the Cn(U) operation, where U is a unitary operator on k qubits, for
n = 4 and k = 3.
Suppose U is a single qubit unitary operator, and V is a unitary operator chosen so
that V 2 = U. Then the operation C2(U) may be implemented using the circuit shown
in Figure 4.8.
Exercise 4.21:
Verify that Figure 4.8 implements the C2(U) operation.
Exercise 4.22:
Prove that a C2(U) gate (for any single qubit unitary U) can be
constructed using at most eight one-qubit gates, and six controlled-
s.
Exercise 4.23:
Construct a C1(U) gate for U = Rx(θ) and U = Ry(θ), using only
and single qubit gates. Can you reduce the number of single qubit gates
needed in the construction from three to two?
The familiar Toffoli gate is an especially important special case of the C2(U) operation,
182
Quantum circuits
Figure 4.8. Circuit for the C2(U) gate. V is any unitary operator satisfying V 2 = U. The special case
V ≡(1 −i)(I + iX)/2 corresponds to the Toffoli gate.
the case C2(X). Deﬁning V ≡(1 −i)(I + iX)/2 and noting that V 2 = X, we see that
Figure 4.8 gives an implementation of the Toffoli gate in terms of one and two qubit
operations. From a classical viewpoint this is a remarkable result; recall from Problem 3.5
that one and two bit classical reversible gates are not sufﬁcient to implement the Toffoli
gate, or, more generally, universal computation. By contrast, in the quantum case we see
that one and two qubit reversible gates are sufﬁcient to implement the Toffoli gate, and
will eventually prove that they sufﬁce for universal computation.
Ultimately we will show that any unitary operation can be composed to an arbitrarily
good approximation from just the Hadamard, phase, controlled-
and π/8 gates.
Because of the great usefulness of the Toffoli gate it is interesting to see how it can be
built from just this gate set. Figure 4.9 illustrates a simple circuit for the Toffoli gate
made up of just Hadamard, phase, controlled-
and π/8 gates.
•
•
•
•
•
T
•
•
•
T †
⊕
T † ⊕S
⊕
H ⊕T † ⊕T
⊕T † ⊕
T
H
=
Figure 4.9. Implementation of the Toffoli gate using Hadamard, phase, controlled-
and π/8 gates.
Exercise 4.24:
Verify that Figure 4.9 implements the Toffoli gate.
Exercise 4.25: (Fredkin gate construction)
Recall that the Fredkin
(controlled-swap) gate performs the transform
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(4.30)
Controlled operations
183
(1) Give a quantum circuit which uses three Toffoli gates to construct the
Fredkin gate (Hint: think of the swap gate construction – you can control
each gate, one at a time).
(2) Show that the ﬁrst and last Toffoli gates can be replaced by
gates.
(3) Now replace the middle Toffoli gate with the circuit in Figure 4.8 to obtain
a Fredkin gate construction using only six two-qubit gates.
(4) Can you come up with an even simpler construction, with only ﬁve
two-qubit gates?
Exercise 4.26:
Show that the circuit:
•
•
•
Ry π/
⊕
Ry π/
⊕
Ry −π/
⊕
Ry −π/
differs from a Toffoli gate only by relative phases. That is, the circuit takes
|c1, c2, t⟩to eiθ(c1,c2,t)|c1, c2, t ⊕c1 · c2⟩, where eiθ(c1,c2,t) is some relative phase
factor. Such gates can sometimes be useful in experimental implementations,
where it may be much easier to implement a gate that is the same as the Toffoli
up to relative phases than it is to do the Toffoli directly.
Exercise 4.27:
Using just
s and Toffoli gates, construct a quantum circuit to
perform the transformation
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(4.31)
This kind of partial cyclic permutation operation will be useful later, in
Chapter 7.
How may we implement Cn(U) gates using our existing repertoire of gates, where U
is an arbitrary single qubit unitary operation? A particularly simple circuit for achieving
this task is illustrated in Figure 4.10. The circuit divides up into three stages, and makes
use of a small number (n −1) of working qubits, which all start and end in the state
|0⟩. Suppose the control qubits are in the computational basis state |c1, c2, . . . , cn⟩. The
ﬁrst stage of the circuit is to reversibly
all the control bits c1, . . . , cn together to
produce the product c1 · c2 . . . cn. To do this, the ﬁrst gate in the circuit
s c1 and
c2 together, using a Toffoli gate, changing the state of the ﬁrst work qubit to |c1 · c2⟩.
The next Toffoli gate
s c3 with the product c1 · c2, changing the state of the second
work qubit to |c1 · c2 · c3⟩. We continue applying Toffoli gates in this fashion, until the
ﬁnal work qubit is in the state |c1 · c2 . . . cn⟩. Next, a U operation on the target qubit is
184
Quantum circuits
performed, conditional on the ﬁnal work qubit being set to one. That is, U is applied if
and only if all of c1 through cn are set. Finally, the last part of the circuit just reverses
the steps of the ﬁrst stage, returning all the work qubits to their initial state, |0⟩. The
combined result, therefore, is to apply the unitary operator U to the target qubit, if and
only if all the control bits c1 through cn are set, as desired.
Figure 4.10. Network implementing the Cn(U) operation, for the case n = 5.
Exercise 4.28:
For U = V 2 with V unitary, construct a C5(U) gate analogous to that
in Figure 4.10, but using no work qubits. You may use controlled-V and
controlled-V † gates.
Exercise 4.29:
Find a circuit containing O(n2) Toffoli,
and single qubit gates
which implements a Cn(X) gate (for n > 3), using no work qubits.
Exercise 4.30:
Suppose U is a single qubit unitary operation. Find a circuit
containing O(n2) Toffoli,
and single qubit gates which implements a
Cn(U) gate (for n > 3), using no work qubits.
In the controlled gates we have been considering, conditional dynamics on the target
qubit occurs if the control bits are set to one. Of course, there is nothing special about
one, and it is often useful to consider dynamics which occur conditional on the control
bit being set to zero. For instance, suppose we wish to implement a two qubit gate in
which the second (‘target’) qubit is ﬂipped, conditional on the ﬁrst (‘control’) qubit being
set to zero. In Figure 4.11 we introduce a circuit notation for this gate, together with an
equivalent circuit in terms of the gates we have already introduced. Generically we shall
use the open circle notation to indicate conditioning on the qubit being set to zero, while
a closed circle indicates conditioning on the qubit being set to one.
A more elaborate example of this convention, involving three control qubits, is illus-
trated in Figure 4.12. The operation U is applied to the target qubit if the ﬁrst and third
qubits are set to zero, and the second qubit is set to one. It is easy to verify by inspection
that the circuit on the right hand side of the ﬁgure implements the desired operation.
More generally, it is easy to move between circuits which condition on qubits being set
Measurement
185
    
Figure 4.11. Controlled operation with a
gate being performed on the second qubit, conditional on the ﬁrst
qubit being set to zero.
to one and circuits which condition on qubits being set to zero, by insertion of X gates
in appropriate locations, as illustrated in Figure 4.12.
Another convention which is sometimes useful is to allow controlled-
gates to have
multiple targets, as shown in Figure 4.13. This natural notation means that when the
control qubit is 1, then all the qubits marked with a ⊕are ﬂipped, and otherwise nothing
happens. It is convenient to use, for example, in constructing classical functions such as
permutations, or in encoders and decoders for quantum error-correction circuits, as we
shall see in Chapter 10.
Exercise 4.31: (More circuit identities)
Let subscripts denote which qubit an
operator acts on, and let C be a
with qubit 1 the control qubit and qubit 2
the target qubit. Prove the following identities:
CX1C = X1X2
(4.32)
CY1C = Y1X2
(4.33)
CZ1C = Z1
(4.34)
CX2C = X2
(4.35)
CY2C = Z1Y2
(4.36)
CZ2C = Z1Z2
(4.37)
Rz,1(θ)C = CRz,1(θ)
(4.38)
Rx,2(θ)C = CRx,2(θ).
(4.39)
4.4
Measurement
A ﬁnal element used in quantum circuits, almost implicitly sometimes, is measurement.
In our circuits, we shall denote a projective measurement in the computational basis
(Section 2.2.5) using a ‘meter’ symbol, illustrated in Figure 4.14. In the theory of quan-
tum circuits it is conventional to not use any special symbols to denote more general
measurements, because, as explained in Chapter 2, they can always be represented by
unitary transforms with ancilla qubits followed by projective measurements.
There are two important principles that it is worth bearing in mind about quantum cir-
cuits. Both principles are rather obvious; however, they are of such great utility that they
are worth emphasizing early. The ﬁrst principle is that classically conditioned operations
can be replaced by quantum conditioned operations:
186
Quantum circuits
    
    
Figure 4.12. Controlled-U operation and its equivalent in terms of circuit elements we already know how to
implement. The fourth qubit has U applied if the ﬁrst and third qubits are set to zero, and the second qubit is set
to one.
•
⊕
⊕
≡
• •
⊕
⊕
Figure 4.13. Controlled-
gate with multiple targets.
Principle of deferred measurement: Measurements can always be moved from
an intermediate stage of a quantum circuit to the end of the circuit; if the
measurement results are used at any stage of the circuit then the classically
controlled operations can be replaced by conditional quantum operations.
Often, quantum measurements are performed as an intermediate step in a quantum
circuit, and the measurement results are used to conditionally control subsequent quan-
tum gates. This is the case, for example, in the teleportation circuit of Figure 1.13 on
page 27. However, such measurements can always be moved to the end of the circuit.
Figure 4.15 illustrates how this may be done by replacing all the classical conditional
operations by corresponding quantum conditional operations. (Of course, some of the
interpretation of this circuit as performing ‘teleportation’ is lost, because no classical in-
formation is transmitted from Alice to Bob, but it is clear that the overall action of the
two quantum circuits is the same, which is the key point.)
The second principle is even more obvious – and surprisingly useful!

________

_ _ _ _ _ _ _ _

Figure 4.14. Symbol for projective measurement on a single qubit. In this circuit nothing further is done with the
measurement result, but in more general quantum circuits it is possible to change later parts of the quantum
circuit, conditional on measurement outcomes in earlier parts of the circuit. Such a usage of classical information is
depicted using wires drawn with double lines (not shown here).
Measurement
187

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Figure 4.15. Quantum teleportation circuit in which measurements are done at the end, instead of in the middle of
the circuit. As in Figure 1.13, the top two qubits belong to Alice, and the bottom one to Bob.
Principle of implicit measurement: Without loss of generality, any
unterminated quantum wires (qubits which are not measured) at the end of a
quantum circuit may be assumed to be measured.
To understand why this is true, imagine you have a quantum circuit containing just
two qubits, and only the ﬁrst qubit is measured at the end of the circuit. Then the
measurement statistics observed at this time are completely determined by the reduced
density matrix of the ﬁrst qubit. However, if a measurement had also been performed on
the second qubit, then it would be highly surprising if that measurement could change
the statistics of measurement on the ﬁrst qubit. You’ll prove this in Exercise 4.32 by
showing that the reduced density matrix of the ﬁrst qubit is not affected by performing
a measurement on the second.
As you consider the role of measurements in quantum circuits, it is important to
keep in mind that in its role as an interface between the quantum and classical worlds,
measurement is generally considered to be an irreversible operation, destroying quantum
information and replacing it with classical information. In certain carefully designed cases,
however, this need not be true, as is vividly illustrated by teleportation and quantum
error-correction (Chapter 10). What teleportation and quantum error-correction have in
common is that in neither instance does the measurement result reveal any information
about the identity of the quantum state being measured. Indeed, we will see in Chapter 10
that this is a more general feature of measurement – in order for a measurement to be
reversible, it must reveal no information about the quantum state being measured!
Exercise 4.32:
Suppose ρ is the density matrix describing a two qubit system.
Suppose we perform a projective measurement in the computational basis of the
second qubit. Let P0 = |0⟩⟨0| and P1 = |1⟩⟨1| be the projectors onto the |0⟩and
|1⟩states of the second qubit, respectively. Let ρ′ be the density matrix which
would be assigned to the system after the measurement by an observer who did
not learn the measurement result. Show that
ρ′ = P0ρP0 + P1ρP1 .
(4.40)
Also show that the reduced density matrix for the ﬁrst qubit is not affected by
the measurement, that is, tr2(ρ) = tr2(ρ′).
Exercise 4.33: (Measurement in the Bell basis)
The measurement model we have
speciﬁed for the quantum circuit model is that measurements are performed only
188
Quantum circuits
in the computational basis. However, often we want to perform a measurement
in some other basis, deﬁned by a complete set of orthonormal states. To perform
this measurement, simply unitarily transform from the basis we wish to perform
the measurement in to the computational basis, then measure. For example,
show that the circuit

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

performs a measurement in the basis of the Bell states. More precisely, show that
this circuit results in a measurement being performed with corresponding
POVM elements the four projectors onto the Bell states. What are the
corresponding measurement operators?
Exercise 4.34: (Measuring an operator)
Suppose we have a single qubit operator
U with eigenvalues ±1, so that U is both Hermitian and unitary, so it can be
regarded both as an observable and a quantum gate. Suppose we wish to measure
the observable U. That is, we desire to obtain a measurement result indicating
one of the two eigenvalues, and leaving a post-measurement state which is the
corresponding eigenvector. How can this be implemented by a quantum circuit?
Show that the following circuit implements a measurement of U:
| ⟩
H
•
H

________

_ _ _ _ _ _ _ _

|ψin⟩
U
|ψout⟩
Exercise 4.35: (Measurement commutes with controls)
A consequence of the
principle of deferred measurement is that measurements commute with quantum
gates when the qubit being measured is a control qubit, that is:
•

________

_ _ _ _ _ _ _ _

U
=

________

_ _ _ _ _ _ _ _

•
U
=

________

_ _ _ _ _ _ _ _

U
(Recall that the double lines represent classical bits in this diagram.) Prove the
ﬁrst equality. The rightmost circuit is simply a convenient notation to depict the
use of a measurement result to classically control a quantum gate.
4.5
Universal quantum gates
A small set of gates (e.g.
,
,
) can be used to compute an arbitrary classical
function, as we saw in Section 3.1.2. We say that such a set of gates is universal for clas-
sical computation. In fact, since the Toffoli gate is universal for classical computation,
quantum circuits subsume classical circuits. A similar universality result is true for quan-
tum computation, where a set of gates is said to be universal for quantum computation
if any unitary operation may be approximated to arbitrary accuracy by a quantum circuit
0
Universal quantum gates
189
involving only those gates. We now describe three universality constructions for quantum
computation. These constructions build upon each other, and culminate in a proof that
any unitary operation can be approximated to arbitrary accuracy using Hadamard, phase,
, and π/8 gates. You may wonder why the phase gate appears in this list, since it
can be constructed from two π/8 gates; it is included because of its natural role in the
fault-tolerant constructions described in Chapter 10.
The ﬁrst construction shows that an arbitrary unitary operator may be expressed ex-
actly as a product of unitary operators that each acts non-trivially only on a subspace
spanned by two computational basis states. The second construction combines the ﬁrst
construction with the results of the previous section to show that an arbitrary unitary
operator may be expressed exactly using single qubit and
gates. The third con-
struction combines the second construction with a proof that single qubit operation may
be approximated to arbitrary accuracy using the Hadamard, phase, and π/8 gates. This in
turn implies that any unitary operation can be approximated to arbitrary accuracy using
Hadamard, phase,
, and π/8 gates.
Our constructions say little about efﬁciency – how many (polynomially or exponen-
tially many) gates must be composed in order to create a given unitary transform. In
Section 4.5.4 we show that there exist unitary transforms which require exponentially
many gates to approximate. Of course, the goal of quantum computation is to ﬁnd inter-
esting families of unitary transformations that can be performed efﬁciently.
Exercise 4.36:
Construct a quantum circuit to add two two-bit numbers x and y
modulo 4. That is, the circuit should perform the transformation
|x, y⟩→|x, x + y mod 4⟩.
4.5.1
Two-level unitary gates are universal
Consider a unitary matrix U which acts on a d-dimensional Hilbert space. In this section
we explain how U may be decomposed into a product of two-level unitary matrices;
that is, unitary matrices which act non-trivially only on two-or-fewer vector components.
The essential idea behind this decomposition may be understood by considering the case
when U is 3×3, so suppose that U has the form
U =
⎡
⎣
a
d
g
b
e
h
c
f
j
⎤
⎦.
(4.41)
We will ﬁnd two-level unitary matrices U1, . . . , U3 such that
U3U2U1U = I .
(4.42)
It follows that
U = U †
1 U †
2 U †
3 .
(4.43)
U1, U2 and U3 are all two-level unitary matrices, and it is easy to see that their inverses,
U †
1 , U †
2 and U †
3 are also two-level unitary matrices. Thus, if we can demonstrate (4.42),
then we will have shown how to break U up into a product of two-level unitary matrices.
190
Quantum circuits
Use the following procedure to construct U1: if b = 0 then set
U1 ≡
⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦.
(4.44)
If b ̸= 0 then set
U1 ≡
⎡
⎢⎢⎣
a∗
√
|a|2+|b|2
b∗
√
|a|2+|b|2
0
b
√
|a|2+|b|2
−a
√
|a|2+|b|2
0
0
0
1
⎤
⎥⎥⎦.
(4.45)
Note that in either case U1 is a two-level unitary matrix, and when we multiply the
matrices out we get
U1U =
⎡
⎢⎣
a
′
d
′
g
′
0
e
′
h
′
c
′
f
′
j
′
⎤
⎥⎦.
(4.46)
The key point to note is that the middle entry in the left hand column is zero. We denote
the other entries in the matrix with a generic prime ′; their actual values do not matter.
Now apply a similar procedure to ﬁnd a two-level matrix U2 such that U2U1U has no
entry in the bottom left corner. That is, if c
′ = 0 we set
U2 ≡
⎡
⎢⎣
a
′∗
0
0
0
1
0
0
0
1
⎤
⎥⎦,
(4.47)
while if c
′ ̸= 0 then we set
U2 ≡
⎡
⎢⎢⎢⎣
a
′ ∗
√
|a′|2+|c′|2
0
c
′ ∗
√
|a′|2+|c′|2
0
1
0
c
′
√
|a′|2+|c′|2
0
−a
′
√
|a′|2+|c′|2
⎤
⎥⎥⎥⎦.
(4.48)
In either case, when we carry out the matrix multiplication we ﬁnd that
U2U1U =
⎡
⎢⎣
1
d
′′
g
′′
0
e
′′
h
′′
0
f
′′
j
′′
⎤
⎥⎦.
(4.49)
Since U, U1 and U2 are unitary, it follows that U2U1U is unitary, and thus d
′′ = g
′′ = 0,
since the ﬁrst row of U2U1U must have norm 1. Finally, set
U3 ≡
⎡
⎢⎣
1
0
0
0
e
′′∗
f
′′∗
0
h
′′∗
j
′′∗
⎤
⎥⎦.
(4.50)
It is now easy to verify that U3U2U1U = I, and thus U = U †
1 U †
2 U †
3 , which is a decom-
position of U into two-level unitaries.
More generally, suppose U acts on a d-dimensional space. Then, in a similar fashion
to the 3×3 case, we can ﬁnd two-level unitary matrices U1, . . . , Ud−1 such that the matrix
Universal quantum gates
191
Ud−1Ud−2 . . . U1U has a one in the top left hand corner, and all zeroes elsewhere in the
ﬁrst row and column. We then repeat this procedure for the d −1 by d −1 unitary
submatrix in the lower right hand corner of Ud−1Ud−2 . . . U1U, and so on, with the end
result that an arbitrary d×d unitary matrix may be written
U = V1 . . . Vk,
(4.51)
where the matrices Vi are two-level unitary matrices, and k ≤(d−1)+(d−2)+· · ·+1 =
d(d −1)/2.
Exercise 4.37:
Provide a decomposition of the transform
1
2
⎡
⎢⎢⎣
1
1
1
1
1
i
−1
−i
1
−1
1
−1
1
−i
−1
i
⎤
⎥⎥⎦
(4.52)
into a product of two-level unitaries. This is a special case of the quantum
Fourier transform, which we study in more detail in the next chapter.
A corollary of the above result is that an arbitrary unitary matrix on an n qubit system
may be written as a product of at most 2n−1(2n −1) two-level unitary matrices. For
speciﬁc unitary matrices, it may be possible to ﬁnd much more efﬁcient decompositions,
but as you will now show there exist matrices which cannot be decomposed as a product
of fewer than d −1 two-level unitary matrices!
Exercise 4.38:
Prove that there exists a d×d unitary matrix U which cannot be
decomposed as a product of fewer than d −1 two-level unitary matrices.
4.5.2
Single qubit and
gates are universal
We have just shown that an arbitrary unitary matrix on a d-dimensional Hilbert space
may be written as a product of two-level unitary matrices. Now we show that single
qubit and
gates together can be used to implement an arbitrary two-level unitary
operation on the state space of n qubits. Combining these results we see that single qubit
and
gates can be used to implement an arbitrary unitary operation on n qubits,
and therefore are universal for quantum computation.
Suppose U is a two-level unitary matrix on an n qubit quantum computer. Suppose
in particular that U acts non-trivially on the space spanned by the computational basis
states |s⟩and |t⟩, where s = s1 . . . sn and t = t1 . . . tn are the binary expansions for s
and t. Let ˜U be the non-trivial 2×2 unitary submatrix of U; ˜U can be thought of as a
unitary operator on a single qubit.
Our immediate goal is to construct a circuit implementing U, built from single qubit
and
gates. To do this, we need to make use of Gray codes. Suppose we have
distinct binary numbers, s and t. A Gray code connecting s and t is a sequence of binary
numbers, starting with s and concluding with t, such that adjacent members of the list
differ in exactly one bit. For instance, with s = 101001 and t = 110011 we have the Gray
192
Quantum circuits
code
1
0
1
0
0
1
1
0
1
0
1
1
1
0
0
0
1
1
1
1
0
0
1
1
(4.53)
Let g1 through gm be the elements of a Gray code connecting s and t, with g1 = s and
gm = t. Note that we can always ﬁnd a Gray code such that m ≤n + 1 since s and t can
differ in at most n locations.
The basic idea of the quantum circuit implementing U is to perform a sequence of gates
effecting the state changes |g1⟩→|g2⟩→. . . →|gm−1⟩, then to perform a controlled- ˜U
operation, with the target qubit located at the single bit where gm−1 and gm differ, and
then to undo the ﬁrst stage, transforming |gm−1⟩→|gm−2⟩→. . . →|g1⟩. Each of these
steps can be easily implemented using operations developed earlier in this chapter, and
the ﬁnal result is an implementation of U.
A more precise description of the implementation is as follows. The ﬁrst step is to swap
the states |g1⟩and |g2⟩. Suppose g1 and g2 differ at the ith digit. Then we accomplish
the swap by performing a controlled bit ﬂip on the ith qubit, conditional on the values
of the other qubits being identical to those in both g1 and g2. Next we use a controlled
operation to swap |g2⟩and |g3⟩. We continue in this fashion until we swap |gm−2⟩with
|gm−1⟩. The effect of this sequence of m −2 operations is to achieve the operation
|g1⟩→|gm−1⟩
(4.54)
|g2⟩→|g1⟩
(4.55)
|g3⟩→|g2⟩
(4.56)
. . . . . . . . .
|gm−1⟩→|gm−2⟩.
(4.57)
All other computational basis states are left unchanged by this sequence of operations.
Next, suppose gm−1 and gm differ in the jth bit. We apply a controlled- ˜U operation
with the jth qubit as target, conditional on the other qubits having the same values as
appear in both gm and gm−1. Finally, we complete the U operation by undoing the swap
operations: we swap |gm−1⟩with |gm−2⟩, then |gm−2⟩with |gm−3⟩and so on, until we
swap |g2⟩with |g1⟩.
A simple example illuminates the procedure further. Suppose we wish to implement
the two-level unitary transformation
U =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a
0
0
0
0
0
0
c
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
b
0
0
0
0
0
0
d
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(4.58)
Here, a, b, c and d are any complex numbers such that ˜U ≡
 a
c
b
d

is a unitary matrix.
Universal quantum gates
193
Notice that U acts non-trivially only on the states |000⟩and |111⟩. We write a Gray code
connecting 000 and 111:
A
B
C
0
0
0
0
0
1
0
1
1
1
1
1
.
(4.59)
From this we read off the required circuit, shown in Figure 4.16. The ﬁrst two gates
shufﬂe the states so that |000⟩gets swapped with |011⟩. Next, the operation ˜U is applied
to the ﬁrst qubit of the states |011⟩and |111⟩, conditional on the second and third qubits
being in the state |11⟩. Finally, we unshufﬂe the states, ensuring that |011⟩gets swapped
back with the state |000⟩.
    
    
    
    
    
    
Figure 4.16. Circuit implementing the two-level unitary operation deﬁned by (4.58).
Returning to the general case, we see that implementing the two-level unitary operation
U requires at most 2(n−1) controlled operations to swap |g1⟩with |gm−1⟩and then back
again. Each of these controlled operations can be realized using O(n) single qubit and
gates; the controlled- ˜U operation also requires O(n) gates. Thus, implementing
U requires O(n2) single qubit and
gates. We saw in the previous section that an
arbitrary unitary matrix on the 2n-dimensional state space of n qubits may be written as
a product of O(22n) = O(4n) two-level unitary operations. Combining these results, we
see that an arbitrary unitary operation on n qubits can be implemented using a circuit
containing O(n24n) single qubit and
gates. Obviously, this construction does not
provide terribly efﬁcient quantum circuits! However, we show in Section 4.5.4 that the
construction is close to optimal in the sense that there are unitary operations that require
an exponential number of gates to implement. Thus, to ﬁnd fast quantum algorithms we
will clearly need a different approach than is taken in the universality construction.
Exercise 4.39:
Find a quantum circuit using single qubit operations and
s to
implement the transformation
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
a
0
0
0
0
c
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
b
0
0
0
0
d
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(4.60)
194
Quantum circuits
where ˜U =
 a
c
b
d

is an arbitrary 2×2 unitary matrix.
4.5.3
A discrete set of universal operations
In the previous section we proved that the
and single qubit unitaries together form
a universal set for quantum computation. Unfortunately, no straightforward method is
known to implement all these gates in a fashion which is resistant to errors. Fortunately,
in this section we’ll ﬁnd a discrete set of gates which can be used to perform universal
quantum computation, and in Chaper 10 we’ll show how to perform these gates in an
error-resistant fashion, using quantum error-correcting codes.
Approximating unitary operators
Obviously, a discrete set of gates can’t be used to implement an arbitrary unitary operation
exactly, since the set of unitary operations is continuous. Rather, it turns out that a
discrete set can be used to approximate any unitary operation. To understand how this
works, we ﬁrst need to study what it means to approximate a unitary operation. Suppose
U and V are two unitary operators on the same state space. U is the target unitary operator
that we wish to implement, and V is the unitary operator that is actually implemented
in practice. We deﬁne the error when V is implemented instead of U by
E(U, V ) ≡max
|ψ⟩∥(U −V )|ψ⟩∥,
(4.61)
where the maximum is over all normalized quantum states |ψ⟩in the state space. In
Box 4.1 on page 195 we show that this measure of error has the interpretation that if
E(U, V ) is small, then any measurement performed on the state V |ψ⟩will give approx-
imately the same measurement statistics as a measurement of U|ψ⟩, for any initial state
|ψ⟩. More precisely, we show that if M is a POVM element in an arbitrary POVM, and
PU (or PV ) is the probability of obtaining this outcome if U (or V ) were performed with
a starting state |ψ⟩, then
|PU −PV | ≤2E(U, V ) .
(4.62)
Thus, if E(U, V ) is small, then measurement outcomes occur with similar probabilities,
regardless of whether U or V were performed. Also shown in Box 4.1 is that if we
perform a sequence of gates V1, . . . , Vm intended to approximate some other sequence
of gates U1, . . . , Um, then the errors add at most linearly,
E(UmUm−1 . . . U1, VmVm−1 . . . V1) ≤
m

j=1
E(Uj, Vj) .
(4.63)
The approximation results (4.62) and (4.63) are extremely useful. Suppose we wish
to perform a quantum circuit containing m gates, U1 through Um. Unfortunately, we
are only able to approximate the gate Uj by the gate Vj. In order that the probabilities
of different measurement outcomes obtained from the approximate circuit be within a
tolerance Δ > 0 of the correct probabilities, it sufﬁces that E(Uj, Vj) ≤Δ/(2m), by the
results (4.62) and (4.63).
Universality of Hadamard + phase +
+ π/8 gates
We’re now in a good position to study the approximation of arbitrary unitary operations
by discrete sets of gates. We’re going to consider two different discrete sets of gates, both
Universal quantum gates
195
Box 4.1: Approximating quantum circuits
Suppose a quantum system starts in the state |ψ⟩, and we perform either the unitary
operation U, or the unitary operation V . Following this, we perform a measurement.
Let M be a POVM element associated with the measurement, and let PU (or PV )
be the probability of obtaining the corresponding measurement outcome if the
operation U (or V ) was performed. Then
|PU −PV | =
++⟨ψ|U †MU|ψ⟩−⟨ψ|V †MV |ψ⟩
++ .
(4.64)
Let |Δ⟩≡(U −V )|ψ⟩. Simple algebra and the Cauchy–Schwarz inequality show
that
|PU −PV | =
++⟨ψ|U †M|Δ⟩+ ⟨Δ|MV |ψ⟩
++ .
(4.65)
≤|⟨ψ|U †M|Δ⟩| + |⟨Δ|MV |ψ⟩|
(4.66)
≤∥|Δ⟩∥+ ∥|Δ⟩∥
(4.67)
≤2E(U, V ).
(4.68)
The inequality |PU −PV | ≤2E(U, V ) gives quantitative expression to the idea
that when the error E(U, V ) is small, the difference in probabilities between mea-
surement outcomes is also small.
Suppose we perform a sequence V1, V2, . . . , Vm of gates intended to approximate
some other sequence of gates, U1, U2, . . . , Um. Then it turns out that the error
caused by the entire sequence of imperfect gates is at most the sum of the errors
in the individual gates,
E(UmUm−1 . . . U1, VmVm−1 . . . V1) ≤
m

j=1
E(Uj, Vj).
(4.69)
To prove this we start with the case m = 2. Note that for some state |ψ⟩we have
E(U2U1, V2V1) = ∥(U2U1 −V2V1)|ψ⟩∥
(4.70)
= ∥(U2U1 −V2U1)|ψ⟩+ (V2U1 −V2V1)|ψ⟩∥.
(4.71)
Using the triangle inequality ∥|a⟩+ |b⟩∥≤∥|a⟩∥+ ∥|b⟩∥, we obtain
E(U2U1, V2V1) ≤∥(U2 −V2)U1|ψ⟩∥+ ∥V2(U1 −V1)|ψ⟩∥
(4.72)
≤E(U2, V2) + E(U1, V1),
(4.73)
which was the desired result. The result for general m follows by induction.
of which are universal. The ﬁrst set, the standard set of universal gates, consists of the
Hadamard, phase, controlled-
and π/8 gates. We provide fault-tolerant constructions
for these gates in Chapter 10; they also provide an exceptionally simple universality
construction. The second set of gates we consider consists of the Hadamard gate, phase
gate, the controlled-
gate, and the Toffoli gate. These gates can also all be done fault-
tolerantly; however, the universality proof and fault-tolerance construction for these gates
is a little less appealing.
We begin the universality proof by showing that the Hadamard and π/8 gates can be
196
Quantum circuits
used to approximate any single qubit unitary operation to arbitrary accuracy. Consider
the gates T and HTH. T is, up to an unimportant global phase, a rotation by π/4 radians
around the ˆz axis on the Bloch sphere, while HTH is a rotation by π/4 radians around
the ˆx axis on the Bloch sphere (Exercise 4.14). Composing these two operations gives,
up to a global phase,
exp

−iπ
8 Z

exp

−iπ
8 X

=

cos π
8 I −i sin π
8 Z
 
cos π
8 I −i sin π
8 X

(4.74)
= cos2 π
8 I −i

cos π
8 (X + Z) + sin π
8 Y

sin π
8 .
(4.75)
This is a rotation of the Bloch sphere about an axis along ⃗n = (cos π
8 , sin π
8 , cos π
8 ) with
corresponding unit vector ˆn, and through an angle θ deﬁned by cos(θ/2) ≡cos2 π
8 . That
is, using only the Hadamard and π/8 gates we can construct R ˆn(θ). Moreover, this θ
can be shown to be an irrational multiple of 2π. Proving this latter fact is a little beyond
our scope; see the end of chapter ‘History and further reading’.
Next, we show that repeated iteration of R ˆn(θ) can be used to approximate to arbitrary
accuracy any rotation R ˆn(α). To see this, let δ > 0 be the desired accuracy, and let N be
an integer larger than 2π/δ. Deﬁne θk so that θk ∈[0, 2π) and θk = (kθ)mod 2π. Then
the pigeonhole principle implies that there are distinct j and k in the range 1, . . . , N such
that |θk −θj| ≤2π/N < δ. Without loss of generality assume that k > j, so we have
|θk−j| < δ. Since j ̸= k and θ is an irrational multiple of 2π we must have θk−j ̸= 0. It
follows that the sequence θl(k−j) ﬁlls up the interval [0, 2π) as l is varied, so that adjacent
members of the sequence are no more than δ apart. It follows that for any ϵ > 0 there
exists an n such that
E(R ˆn(α), R ˆn(θ)n) < ϵ
3 .
(4.76)
Exercise 4.40:
For arbitrary α and β show that
E(R ˆn(α), R ˆn(α + β)) = |1 −exp(iβ/2)| ,
(4.77)
and use this to justify (4.76).
We are now in position to verify that any single qubit operation can be approximated to
arbitrary accuracy using the Hadamard and π/8 gates. Simple algebra implies that for
any α
HR ˆn(α)H = R ˆm(α) ,
(4.78)
where ˆm is a unit vector in the direction (cos π
8 , −sin π
8 , cos π
8 ), from which it follows
that
E(R ˆm(α), R ˆm(θ)n) < ϵ
3 .
(4.79)
But by Exercise 4.11 an arbitrary unitary U on a single qubit may be written as
U = R ˆn(β)R ˆm(γ)R ˆn(δ),
(4.80)
up to an unimportant global phase shift. The results (4.76) and (4.79), together with the
Universal quantum gates
197
chaining inequality (4.63) therefore imply that for suitable positive integers n1, n2, n3,
E(U, R ˆn(θ)n1HR ˆn(θ)n2HR ˆn(θ)n3) < ϵ .
(4.81)
That is, given any single qubit unitary operator U and any ϵ > 0 it is possible to
approximate U to within ϵ using a circuit composed of Hadamard gates and π/8 gates
alone.
Since the π/8 and Hadamard gates allow us to approximate any single qubit uni-
tary operator, it follows from the arguments of Section 4.5.2 that we can approximate
any m gate quantum circuit, as follows. Given a quantum circuit containing m gates,
either
s or single qubit unitary gates, we may approximate it using Hadamard,
controlled-
and π/8 gates (later, we will ﬁnd that phase gates make it possible to do
the appoximation fault-tolerantly, but for the present universality argument they are not
strictly necessary). If we desire an accuracy of ϵ for the entire circuit, then this may be
achieved by approximating each single qubit unitary using the above procedure to within
ϵ/m and applying the chaining inequality (4.63) to obtain an accuracy of ϵ for the entire
circuit.
How efﬁcient is this procedure for approximating quantum circuits using a discrete
set of gates? This is an important question. Suppose, for example, that approximating
an arbitrary single qubit unitary to within a distance ϵ were to require Ω(21/ϵ) gates
from the discrete set. Then to approximate the m gate quantum circuit considered in
the previous paragraph would require Ω(m2m/ϵ) gates, an exponential increase over
the original circuit size! Fortunately, the rate of convergence is much better than this.
Intuitively, it is plausible that the sequence of angles θk ‘ﬁlls in’ the interval [0, 2π) in a
more or less uniform fashion, so that to approximate an arbitrary single qubit gate ought
to take roughly Θ(1/ϵ) gates from the discrete set. If we use this estimate for the number
of gates required to approximate an arbitrary single qubit gate, then the number required
to approximate an m gate circuit to accuracy ϵ becomes Θ(m2/ϵ). This is a quadratic
increase over the original size of the circuit, m, which for many applications may be
sufﬁcient.
Rather remarkably, however, a much faster rate of convergence can be proved. The
Solovay–Kitaev theorem, proved in Appendix 3, implies that an arbitrary single qubit
gate may be approximated to an accuracy ϵ using O(logc(1/ϵ)) gates from our discrete set,
where c is a constant approximately equal to 2. The Solovay–Kitaev theorem therefore
implies that to approximate a circuit containing m
s and single qubit unitaries to
an accuracy ϵ requires O(m logc(m/ϵ)) gates from the discrete set, a polylogarithmic
increase over the size of the original circuit, which is likely to be acceptable for virtually
all applications.
To sum up, we have shown that the Hadamard, phase, controlled-
and π/8 gates
are universal for quantum computation in the sense that given a circuit containing
s
and arbitrary single qubit unitaries it is possible to simulate this circuit to good accuracy
using only this discrete set of gates. Moreover, the simulation can be performed efﬁ-
ciently, in the sense that the overhead required to perform the simulation is polynomial
in log(m/ϵ), where m is the number of gates in the original circuit, and ϵ is the desired
accuracy of the simulation.
Exercise 4.41:
This and the next two exercises develop a construction showing that
the Hadamard, phase, controlled-
and Toffoli gates are universal. Show that
198
Quantum circuits

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Figure 4.17. Provided both measurement outcomes are 0 this circuit applies Rz(θ) to the target, where
cos θ = 3/5. If some other measurement outcome occurs then the circuit applies Z to the target.
the circuit in Figure 4.17 applies the operation Rz(θ) to the third (target) qubit if
the measurement outcomes are both 0, where cos θ = 3/5, and otherwise applies
Z to the target qubit. Show that the probability of both measurement outcomes
being 0 is 5/8, and explain how repeated use of this circuit and Z = S2 gates
may be used to apply a Rz(θ) gate with probability approaching 1.
Exercise 4.42: (Irrationality of θ)
Suppose cos θ = 3/5. We give a proof by
contradiction that θ is an irrational multiple of 2π.
(1) Using the fact that eiθ = (3 + 4i)/5, show that if θ is rational, then there
must exist a positive integer m such that (3 + 4i)m = 5m.
(2) Show that (3 + 4i)m = 3 + 4i (mod 5) for all m > 0, and conclude that no m
such that (3 + 4i)m = 5m can exist.
Exercise 4.43:
Use the results of the previous two exercises to show that the
Hadamard, phase, controlled-
and Toffoli gates are universal for quantum
computation.
Exercise 4.44:
Show that the three qubit gate G deﬁned by the circuit:
•
•
iRx πα
is universal for quantum computation whenever α is irrational.
Exercise 4.45:
Suppose U is a unitary transform implemented by an n qubit quantum
circuit constructed from H, S,
and Toffoli gates. Show that U is of the
form 2−k/2M, for some integer k, where M is a 2n×2n matrix with only
complex integer entries. Repeat this exercise with the Toffoli gate replaced by
the π/8 gate.
4.5.4
Approximating arbitrary unitary gates is generically hard
We’ve seen that any unitary transformation on n qubits can be built up out of a small set
of elementary gates. Is it always possible to do this efﬁciently? That is, given a unitary
transformation U on n qubits does there always exist a circuit of size polynomial in n
approximating U? The answer to this question turns out to be a resounding no: in fact,
most unitary transformations can only be implemented very inefﬁciently. One way to see
Universal quantum gates
199
this is to consider the question: how many gates does it take to generate an arbitrary state
of n qubits? A simple counting argument shows that this requires exponentially many
operations, in general; it immediately follows that there are unitary operations requiring
exponentially many operations. To see this, suppose we have g different types of gates
available, and each gate works on at most f input qubits. These numbers, f and g,
are ﬁxed by the computing hardware we have available, and may be considered to be
constants. Suppose we have a quantum circuit containing m gates, starting from the
computational basis state |0⟩⊗n. For any particular gate in the circuit there are therefore
at most
 n
f
g
= O(nfg) possible choices. It follows that at most O(nfgm) different
states may be computed using m gates.
Figure 4.18. Visualization of covering the set of possible states with patches of constant radius.
Suppose we wish to approximate a particular state, |ψ⟩, to within a distance ϵ. The idea
of the proof is to cover the set of all possible states with a collection of ‘patches,’ each of
radius ϵ (Figure 4.18), and then to show that the number of patches required rises doubly
exponentially in n; comparing with the exponential number of different states that may
be computed using m gates will imply the result. The ﬁrst observation we need is that the
space of state vectors of n qubits can be regarded as just the unit (2n+1−1)-sphere. To see
this, suppose the n qubit state has amplitudes ψj = Xj + iYj, where Xj and Yj are the
real and imaginary parts, respectively, of the jth amplitude. The normalization condition
for quantum states can be written 
j(X2
j + Y 2
j ) = 1, which is just the condition for a
point to be on the unit sphere in 2n+1 real dimensions, that is, the unit (2n+1 −1)-sphere.
Similarly, the surface area of radius ϵ near |ψ⟩is approximately the same as the volume
of a (2n+1−2)-sphere of radius ϵ. Using the formula Sk(r) = 2π(k+1)/2rk/Γ((k+1)/2) for
the surface area of a k-sphere of radius r, and Vk(r) = 2π(k+1)/2rk+1/[(k+1)Γ((k+1)/2)]
for the volume of a k-sphere of radius r, we see that the number of patches needed to
200
Quantum circuits
cover the state space goes like
S2n+1−1(1)
V2n+1−2(ϵ) =
√πΓ(2n −1
2)(2n+1 −1)
Γ(2n)ϵ2n+1−1
,
(4.82)
where Γ is the usual generalization of the factorial function. But Γ(2n−1/2) ≥Γ(2n)/2n,
so the number of patches required to cover the space is at least
Ω

1
ϵ2n+1−1

.
(4.83)
Recall that the number of patches which can be reached in m gates is O(nfgm), so in
order to reach all the ϵ-patches we must have
O

nfgm ≥Ω

1
ϵ2n+1−1

(4.84)
which gives us
m = Ω
2n log(1/ϵ)
log(n)

.
(4.85)
That is, there are states of n qubits which take Ω(2n log(1/ϵ)/ log(n)) operations to
approximate to within a distance ϵ. This is exponential in n, and thus is ‘difﬁcult’,
in the sense of computational complexity introduced in Chapter 3. Furthermore, this
immediately implies that there are unitary transformations U on n qubits which take
Ω(2n log(1/ϵ)/ log(n)) operations to approximate by a quantum circuit implementing an
operation V such that E(U, V ) ≤ϵ. By contrast, using our universality constructions
and the Solovay–Kitaev theorem it follows that an arbitrary unitary operation U on n
qubits may be approximated to within a distance ϵ using O(n24n logc(n24n/ϵ)) gates.
Thus, to within a polynomial factor the construction for universality we have given is
optimal; unfortunately, it does not address the problem of determining which families of
unitary operations can be computed efﬁciently in the quantum circuits model.
4.5.5
Quantum computational complexity
In Chapter 3 we described a theory of ‘computational complexity’ for classical comput-
ers that classiﬁed the resource requirements to solve computational problems on classi-
cal computers. Not surprisingly there is considerable interest in developing a theory of
quantum computational complexity, and relating it to classical computational complexity
theory. Although only ﬁrst steps have been taken in this direction, it will doubtless be
an enormously fruitful direction for future researchers. We content ourselves with pre-
senting one result about quantum complexity classes, relating the quantum complexity
class BQP to the classical complexity class PSPACE. Our discussion of this result is
rather informal; for more details you are referred to the paper of Bernstein and Vazirani
referenced in the end of chapter ‘History and further reading’.
Recall that PSPACE was deﬁned in Chapter 3 as the class of decision problems which
can be solved on a Turing machine using space polynomial in the problem size and an
arbitrary amount of time. BQP is an essentially quantum complexity class consisting
of those decision problems that can be solved with bounded probability of error using
a polynomial size quantum circuit. Slightly more formally, we say a language L is in
BQP if there is a family of polynomial size quantum circuits which decides the language,
Universal quantum gates
201
accepting strings in the language with probability at least 3/4, and rejecting strings which
aren’t in the language with probability at least 3/4. In practice, what this means is that
the quantum circuit takes as input binary strings, and tries to determine whether they are
elements of the language or not. At the conclusion of the circuit one qubit is measured,
with 0 indicating that the string has been accepted, and 1 indicating rejection. By testing
the string a few times to determine whether it is in L, we can determine with very high
probability whether a given string is in L.
Of course, a quantum circuit is a ﬁxed entity, and any given quantum circuit can only
decide whether strings up to some ﬁnite length are in L. For this reason, we use an
entire family of circuits in the deﬁnition of BQP; for every possible input length there is
a different circuit in the family. We place two restrictions on the circuit in addition to the
acceptance / rejection criterion already described. First, the size of the circuits should
only grow polynomially with the size of the input string x for which we are trying to
determine whether x ∈L. Second, we require that the circuits be uniformly generated,
in a sense similar to that described in Section 3.1.2. This uniformity requirement arises
because, in practice, given a string x of some length n, somebody will have to build
a quantum circuit capable of deciding whether x is in L. To do so, they will need to
have a clear set of instructions – an algorithm – for building the circuit. For this reason,
we require that our quantum circuits be uniformly generated, that is, there is a Turing
machine capable of efﬁciently outputting a description of the quantum circuit. This
restriction may seem rather technical, and in practice is nearly always satisﬁed trivially,
but it does save us from pathological examples such as that described in Section 3.1.2.
(You might also wonder if it matters whether the Turing machine used in the uniformity
requirement is a quantum or classical Turing machine; it turns out that it doesn’t matter
– see ‘History and further reading’.)
One of the most signiﬁcant results in quantum computational complexity is that BQP
⊆PSPACE. It is clear that BPP ⊆BQP, where BPP is the classical complexity class
of decision problems which can be solved with bounded probability of error using poly-
nomial time on a classical Turing machine. Thus we have the chain of inclusions BPP
⊆BQP ⊆PSPACE. Proving that BQP ̸= BPP – intuitively the statement that quan-
tum computers are more powerful than classical computers – will therefore imply that
BPP ̸= PSPACE. However, it is not presently known whether BPP ̸= PSPACE,
and proving this would represent a major breakthrough in classical computer science! So
proving that quantum computers are more powerful than classical computers would have
some very interesting implications for classical computational complexity! Unfortunately,
it also means that providing such a proof may be quite difﬁcult.
Why is it that BQP ⊆PSPACE? Here is an intuitive outline of the proof (a rigorous
proof is left to the references in ‘History and further reading’). Suppose we have an n
qubit quantum computer, and do a computation involving a sequence of p(n) gates, where
p(n) is some polynomial in n. Supposing the quantum circuit starts in the state |0⟩we
will explain how to evaluate in polynomial space on a classical computer the probability
that it ends up in the state |y⟩. Suppose the gates that are executed on the quantum
computer are, in order, U1, U2, . . . , Up(n). Then the probability of ending up in the state
|y⟩is the modulus squared of
⟨y|Up(n) · · · U2U1|0⟩.
(4.86)
This quantity may be estimated in polynomial space on a classical computer. The basic
202
Quantum circuits
idea is to insert the completeness relation 
x |x⟩⟨x| = I between each term in (4.86),
obtaining
⟨y|Up(n) · · · U2U1|0⟩=

x1,...,xp(n)−1
⟨y|Up(n)|xp(n)−1⟩⟨xp(n)−1|Up(n)−2 . . . U2|x1⟩⟨x1|U1|0⟩.
(4.87)
Given that the individual unitary gates appearing in this sum are operations such as the
Hadamard gate,
, and so on, it is clear that each term in the sum can be calculated
to high accuracy using only polynomial space on a classical computer, and thus the sum
as a whole can be calculated using polynomial space, since individual terms in the sum
can be erased after being added to the running total. Of course, this algorithm is rather
slow, since there are exponentially many terms in the sum which need to be calculated
and added to the total; however, only polynomially much space is consumed, and thus
BQP ⊆PSPACE, as we set out to show.
A similar procedure can be used to simulate an arbitrary quantum computation on
a classical computer, no matter the length of the quantum computation. Therefore, the
class of problems solvable on a quantum computer with unlimited time and space re-
sources is no larger than the class of problems solvable on a classical computer. Stated
another way, this means that quantum computers do not violate the Church–Turing the-
sis that any algorithmic process can be simulated efﬁciently using a Turing machine. Of
course, quantum computers may be much more efﬁcient than their classical counterparts,
thereby challenging the strong Church–Turing thesis that any algorithmic process can
be simulated efﬁciently using a probabilistic Turing machine.
4.6
Summary of the quantum circuit model of computation
In this book the term ‘quantum computer’ is synonymous with the quantum circuit
model of computation. This chapter has provided a detailed look at quantum circuits,
their basic elements, universal families of gates, and some applications. Before we move
on to more sophisticated applications, let us summarize the key elements of the quantum
circuit model of computation:
(1) Classical resources: A quantum computer consists of two parts, a classical part
and a quantum part. In principle, there is no need for the classical part of the
computer, but in practice certain tasks may be made much easier if parts of the
computation can be done classically. For example, many schemes for quantum
error-correction (Chapter 10) are likely to involve classical computations in order to
maximize efﬁciency. While classical computations can always be done, in principle,
on a quantum computer, it may be more convenient to perform the calculations on
a classical computer.
(2) A suitable state space: A quantum circuit operates on some number, n, of qubits.
The state space is thus a 2n-dimensional complex Hilbert space. Product states of
the form |x1, . . . , xn⟩, where xi = 0, 1, are known as computational basis states of
the computer. |x⟩denotes a computational basis state, where x is the number
whose binary representation is x1 . . . xn.
(3) Ability to prepare states in the computational basis: It is assumed that any
computational basis state |x1, . . . , xn⟩can be prepared in at most n steps.
Summary of the quantum circuit model of computation
203
(4) Ability to perform quantum gates: Gates can be applied to any subset of qubits
as desired, and a universal family of gates can be implemented. For example, it
should be possible to apply the
gate to any pair of qubits in the quantum
computer. The Hadamard, phase,
and π/8 gates form a family of gates from
which any unitary operation can be approximated, and thus is a universal set of
gates. Other universal families exist.
(5) Ability to perform measurements in the computational basis:
Measurements may be performed in the computational basis of one or more of the
qubits in the computer.
The quantum circuit model of quantum computation is equivalent to many other
models of computation which have been proposed, in the sense that other models result
in essentially the same resource requirements for the same problems. As a simple example
which illustrates the basic idea, one might wonder whether moving to a design based
on three-level quantum systems, rather than the two-level qubits, would confer any
computational advantage. Of course, although there may be some slight advantage in
using three-level quantum systems (qutrits) over two-level systems, any difference will
be essentially negligible from the theoretical point of view. At a less trivial level, the
‘quantum Turing machine’ model
of computation, a quantum generalization of the
classical Turing machine model, has been shown to be equivalent to the model based
upon quantum circuits. We do not consider that model of computation in this book, but
the reader interested in learning more about quantum Turing machines may consult the
references given in the end of chapter ‘History and further reading’.
Despite the simplicity and attraction of the quantum circuit model, it is useful to keep
in mind possible criticisms, modiﬁcations, and extensions. For example, it is by no means
clear that the basic assumptions underlying the state space and starting conditions in the
quantum circuit model are justiﬁed. Everything is phrased in terms of ﬁnite dimensional
state spaces. Might there be anything to be gained by using systems whose state space is
inﬁnite dimensional? Assuming that the starting state of the computer is a computational
basis state is also not necessary; we know that many systems in Nature ‘prefer’ to sit in
highly entangled states of many systems; might it be possible to exploit this preference
to obtain extra computational power? It might be that having access to certain states
allows particular computations to be done much more easily than if we are constrained
to start in the computational basis. Likewise, the ability to efﬁciently perform entangling
measurements in multi-qubit bases might be as useful as being able to perform just
entangling unitary operations. Indeed, it may be possible to harness such measurements
to perform tasks intractable within the quantum circuit model.
A detailed examination and attempted justiﬁcation of the physics underlying the quan-
tum circuit model is outside the scope of the present discussion, and, indeed, outside the
scope of present knowledge! By raising these issues we wish to introduce the question
of the completeness of the quantum circuit model, and re-emphasize the fundamental
point that information is physical. In our attempts to formulate models for information
processing we should always attempt to go back to fundamental physical laws. For the
purposes of this book, we shall stay within the quantum circuit model of computation. It
offers a rich and powerful model of computation that exploits the properties of quantum
mechanics to perform amazing feats of information processing, without classical prece-
204
Quantum circuits
dent. Whether physically reasonable models of computation exist which go beyond the
quantum circuit model is a fascinating question which we leave open for you.
4.7
Simulation of quantum systems
Perhaps [...] we need a mathematical theory of quantum automata. [...] the
quantum state space has far greater capacity than the classical one: for a clas-
sical system with N states, its quantum version allowing superposition accom-
modates cN states. When we join two classical systems, their number of states
N1 and N2 are multiplied, and in the quantum case we get the exponential
growth cN1N2. [...] These crude estimates show that the quantum behavior of
the system might be much more complex than its classical simulation.
– Yu Manin (1980)[Man80], as translated in [Man99]
The quantum-mechanical computation of one molecule of methane requires 1042
grid points. Assuming that at each point we have to perform only 10 elemen-
tary operations, and that the computation is performed at the extremely low
temperature T = 3 × 10−3K, we would still have to use all the energy produced
on Earth during the last century.
– R. P. Poplavskii (1975)[Pop75], as quoted by Manin
Can physics be simulated by a universal computer? [...] the physical world
is quantum mechanical, and therefore the proper problem is the simulation of
quantum physics [...] the full description of quantum mechanics for a large
system with R particles [...] has too many variables, it cannot be simulated
with a normal computer with a number of elements proportional to R [ ... but
it can be simulated with ] quantum computer elements. [...] Can a quantum
system be probabilistically simulated by a classical (probabilistic, I’d assume)
universal computer? [...] If you take the computer to be the classical kind I’ve
described so far [..] the answer is certainly, No!
– Richard P. Feynman (1982)[Fey82]
Let us close out this chapter by providing an interesting and useful application of the
quantum circuit model. One of the most important practical applications of computation
is the simulation of physical systems. For example, in the engineering design of a new
building, ﬁnite element analysis and modeling is used to ensure safety while minimizing
cost. Cars are made lightweight, structurally sound, attractive, and inexpensive, by using
computer aided design. Modern aeronautical engineering depends heavily on computa-
tional ﬂuid dynamics simulations for aircraft designs. Nuclear weapons are no longer
exploded (for the most part), but rather, tested by exhaustive computational modeling.
Examples abound, because of the tremendous practical applications of predictive simula-
tions. We begin by describing some instances of the simulation problem, then we present
a quantum algorithm for simulation and an illustrative example, concluding with some
perspective on this application.
4.7.1
Simulation in action
The heart of simulation is the solution of differential equations which capture the physical
laws governing the dynamical behavior of a system. Some examples include Newton’s
Simulation of quantum systems
205
law,
d
dt

mdx
dt

= F ,
(4.88)
Poisson’s equation,
−⃗∇· (k ⃗∇⃗u) = ⃗Q ,
(4.89)
the electromagnetic vector wave equation,
⃗∇· ⃗∇⃗E = ϵ0μ0
∂2 ⃗E
∂t2 ,
(4.90)
and the diffusion equation,
⃗∇2ψ = 1
a2
∂ψ
∂t ,
(4.91)
just to name a very few. The goal is generally: given an initial state of the system,
what is the state at some other time and/or position? Solutions are usually obtained by
approximating the state with a digital representation, then discretizing the differential
equation in space and time such that an iterative application of a procedure carries the
state from the initial to the ﬁnal conditions. Importantly, the error in this procedure is
bounded, and known not to grow faster than some small power of the number of iterations.
Furthermore, not all dynamical systems can be simulated efﬁciently: generally, only those
systems which can be described efﬁciently can be simulated efﬁciently.
Simulation of quantum systems by classical computers is possible, but generally only
very inefﬁciently. The dynamical behavior of many simple quantum systems is governed
by Schr¨odinger’s equation,
iℏd
dt|ψ⟩= H|ψ⟩.
(4.92)
We will ﬁnd it convenient to absorb ℏinto H, and use this convention for the rest of
this section. For a typical Hamiltonian of interest to physicists dealing with real particles
in space (rather than abstract systems such as qubits, which we have been dealing with!),
this reduces to
i ∂
∂tψ(x) =

−1
2m
∂2
∂x2 + V (x)

ψ(x) ,
(4.93)
using a convention known as the position representation ⟨x|ψ⟩= ψ(x). This is an elliptical
equation very much like Equation (4.91). So just simulating Schr¨odinger’s equation is
not the especial difﬁculty faced in simulating quantum systems. What is the difﬁculty?
The key challenge in simulating quantum systems is the exponential number of
differential equations which must be solved. For one qubit evolving according to the
Schr¨odinger equation, a system of two differential equations must be solved; for two
qubits, four equations; and for n qubits, 2n equations. Sometimes, insightful approxima-
tions can be made which reduce the effective number of equations involved, thus making
classical simulation of the quantum system feasible. However, there are many physically
interesting quantum systems for which no such approximations are known.
Exercise 4.46: (Exponential complexity growth of quantum systems)
Let ρ be
a density matrix describing the state of n qubits. Show that describing ρ requires
4n −1 independent real numbers.
206
Quantum circuits
The reader with a physics background may appreciate that there are many important
quantum systems for which classical simulation is intractable. These include the Hubbard
model, a model of interacting fermionic particles with the Hamiltonian
H =
n

k=1
V0nk↑nk↓+

k,j neighbors,σ
t0c∗
kσcjσ ,
(4.94)
which is useful in the study of superconductivity and magnetism, the Ising model,
H =
n

k=1
⃗σk · ⃗σk+1 ,
(4.95)
and many others. Solutions to such models give many physical properties such as the
dielectric constant, conductivity, and magnetic susceptibility of materials. More sophis-
ticated models such as quantum electrodynamics (QED) and quantum chromodynamics
(QCD) can be used to compute constants such as the mass of the proton.
Quantum computers can efﬁciently simulate quantum systems for which there is no
known efﬁcient classical simulation. Intuitively, this is possible for much the same reason
any quantum circuit can be constructed from a universal set of quantum gates. Moreover,
just as there exist unitary operations which cannot be efﬁciently approximated, it is
possible in principle to imagine quantum systems with Hamiltonians which cannot be
efﬁciently simulated on a quantum computer. Of course, we believe that such systems
aren’t actually realized in Nature, otherwise we’d be able to exploit them to do information
processing beyond the quantum circuit model.
4.7.2
The quantum simulation algorithm
Classical simulation begins with the realization that in solving a simple differential equa-
tion such as dy/dt = f(y), to ﬁrst order, it is known that y(t + Δt) ≈y(t) + f(y)Δt.
Similarly, the quantum case is concerned with the solution of id|ψ⟩/dt = H|ψ⟩, which,
for a time-independent H, is just
|ψ(t)⟩= e−iHt|ψ(0)⟩.
(4.96)
Since H is usually extremely difﬁcult to exponentiate (it may be sparse, but it is also
exponentially large), a good beginning is the ﬁrst order solution |ψ(t + Δt)⟩≈(I −
iHΔt)|ψ(t)⟩. This is tractable, because for many Hamiltonians H it is straightforward to
compose quantum gates to efﬁciently approximate I −iHΔt. However, such ﬁrst order
solutions are generally not very satisfactory.
Efﬁcient approximation of the solution to Equation (4.96), to high order, is possible for
many classes of Hamiltonian. For example, in most physical systems, the Hamiltonian
can be written as a sum over many local interactions. Speciﬁcally, for a system of n
particles,
H =
L

k=1
Hk ,
(4.97)
where each Hk acts on at most a constant c number of systems, and L is a polynomial in
n. For example, the terms Hk are often just two-body interactions such as XiXj and one-
body Hamiltonians such as Xi. Both the Hubbard and Ising models have Hamiltonians
of this form. Such locality is quite physically reasonable, and originates in many systems
Simulation of quantum systems
207
from the fact that most interactions fall off with increasing distance or difference in energy.
There are sometimes additional global symmetry constraints such as particle statistics;
we shall come to those shortly. The important point is that although e−iHt is difﬁcult to
compute, e−iHkt acts on a much smaller subsystem, and is straightforward to approximate
using quantum circuits. But because [Hj, Hk] ̸= 0 in general, e−iHt ̸= ,
k e−iHkt! How,
then, can e−iHkt be useful in constructing e−iHt?
Exercise 4.47:
For H = L
k Hk, prove that e−iHt = e−iH1te−iH2t . . . e−iHLt for all t
if [Hj, Hk] = 0, for all j, k.
Exercise 4.48:
Show that the restriction of Hk to involve at most c particles implies
that in the sum (4.97), L is upper bounded by a polynomial in n.
The heart of quantum simulation algorithms is the following asymptotic approximation
theorem:
Theorem 4.3: (Trotter formula) Let A and B be Hermitian operators. Then for any
real t,
lim
n→∞(eiAt/neiBt/n)n = ei(A+B)t .
(4.98)
Note that (4.98) is true even if A and B do not commute. Even more interestingly,
perhaps, it can be generalized to hold for A and B which are generators of certain kinds
of semigroups, which correspond to general quantum operations; we shall describe such
generators (the ‘Lindblad form’) in Section 8.4.1 of Chapter 8. For now, we only consider
the case of A and B being Hermitian matrices.
Proof
By deﬁnition,
eiAt/n = I + 1
niAt + O
 1
n2

,
(4.99)
and thus
eiAt/neiBt/n = I + 1
ni(A + B)t + O
 1
n2

.
(4.100)
Taking products of these gives us
(eiAt/neiBt/n)n = I +
n

k=1
 n
k
 1
nk

i(A + B)t
	k
+ O
 1
n

,
(4.101)
and since
 n
k
 1
nk =

1 + O
 1
n

/k!, this gives
lim
n→∞(eiAt/neiBt/n)n = lim
n→∞
n

k=0
(i(A + B)t)k
k!

1 + O
 1
n

+ O
 1
n

= ei(A+B)t .
(4.102)
Modiﬁcations of the Trotter formula provide the methods by which higher order
208
Quantum circuits
approximations can be derived for performing quantum simulations. For example, using
similar reasoning to the proof above, it can be shown that
ei(A+B)Δt = eiAΔteiBΔt + O(Δt2) .
(4.103)
Similarly,
ei(A+B)Δt = eiAΔt/2eiBΔteiAΔt/2 + O(Δt3) .
(4.104)
An overview of the quantum simulation algorithm is given below, and an explicit exam-
ple of simulating the one-dimensional non-relativistic Schr¨odinger equation is shown in
Box 4.2.
Algorithm:
Quantum simulation
Inputs: (1) A Hamiltonian H = 
k Hk acting on an N-dimensional system,
where each Hk acts on a small subsystem of size independent of N, (2) an initial
state |ψ0⟩, of the system at t = 0, (3) a positive, non-zero accuracy δ, and (3) a
time tf at which the evolved state is desired.
Outputs: A state | ˜ψ(tf)⟩such that |⟨˜ψ(tf)|e−iHtf |ψ0⟩|2 ≥1 −δ.
Runtime: O(poly(1/δ)) operations.
Procedure: Choose a representation such that the state | ˜ψ⟩of n = poly(log N)
qubits approximates the system and the operators e−iHkΔt have efﬁcient
quantum circuit approximations. Select an approximation method (see for
example Equations (4.103)–(4.105)) and Δt such that the expected error is
acceptable (and jΔt = tf for an integer j), construct the corresponding quantum
circuit UΔt for the iterative step, and do:
1.
| ˜ψ0⟩←|ψ0⟩; j = 0
initialize state
2.
→| ˜ψj+1⟩= UΔt| ˜ψj⟩
iterative update
3.
→j = j + 1 ; goto 2 until jΔt ≥tf
loop
4.
→| ˜ψ(tf)⟩= | ˜ψj⟩
ﬁnal result
Exercise 4.49: (Baker–Campbell–Hausdorf formula)
Prove that
e(A+B)Δt = eAΔteBΔte−1
2 [A,B]Δt2 + O(Δt3) ,
(4.105)
and also prove Equations (4.103) and (4.104).
Exercise 4.50:
Let H = L
k Hk, and deﬁne
UΔt =

e−iH1Δte−iH2Δt . . . e−iHLΔt	 
e−iHLΔte−iHL−1Δt . . . e−iH1Δt	
.
(4.106)
(a) Prove that UΔt = e−2iHΔt + O(Δt3).
(b) Use the results in Box 4.1 to prove that for a positive integer m,
E(U m
Δt, e−2miHΔt) ≤mαΔt3 ,
(4.107)
for some constant α.
Simulation of quantum systems
209
Box 4.2: Quantum simulation of Schr¨odinger’s equation
The methods and limitations of quantum simulation may be illustrated by the fol-
lowing example, drawn from the conventional models studied by physicists, rather
than the abstract qubit model. Consider a single particle living on a line, in a one-
dimensional potential V (x), governed by the Hamiltonian
H = p2
2m + V (x) ,
(4.108)
where p is the momentum operator and x is the position operator. The eigenvalues
of x are continuous, and the system state |ψ⟩resides in an inﬁnite dimensional
Hilbert space; in the x basis, it can be written as
|ψ⟩=
- ∞
−∞
|x⟩⟨x|ψ⟩dx .
(4.109)
In practice, only some ﬁnite region is of interest, which we may take to be the
range −d ≤x ≤d. Furthermore, it is possible to choose a differential step size Δx
sufﬁciently small compared to the shortest wavelength in the system such that
| ˜ψ⟩=
d/Δx

k=−d/Δx
ak|kΔx⟩
(4.110)
provides a good physical approximation of |ψ⟩. This state can be represented using
n = ⌈log(2d/Δx + 1)⌉qubits; we simply replace the basis |kΔx⟩(an eigenstate of
the x operator) with |k⟩, a computational basis state of n qubits. Note that only
n qubits are required for this simulation, whereas classically 2n complex numbers
would have to be kept track of, thus leading to an exponential resource saving when
performing the simulation on a quantum computer.
Computation of | ˜ψ(t)⟩= e−iHt| ˜ψ(0)⟩must utilize one of the approximations of
Equations (4.103)–(4.105) because in general H1 = V (x) does not commute with
H0 = p2/2m. Thus, we must be able to compute e−iH1Δt and e−iH0Δt. Because | ˜ψ⟩
is expressed in the eigenbasis of H1, e−iH1Δt is a diagonal transformation of the
form
|k⟩→e−iV (kΔx)Δt|k⟩.
(4.111)
It is straightforward to compute this, since we can compute V (kΔx)Δt. (See
also Problem 4.1.) The second term is also simple, because x and p are conju-
gate variables related by a quantum Fourier transform UFFTxU †
FFT = p, and thus
e−iH0Δt = UFFTe−ix2Δt/2mU †
FFT; to compute e−iH0Δt, do
|k⟩→UFFTe−ix2/2mU †
FFT|k⟩.
(4.112)
The construction of UFFT is discussed in Chapter 5.
4.7.3
An illustrative example
The procedure we have described for quantum simulations has concentrated on simulat-
ing Hamiltonians which are sums of local interations. However, this is not a fundamental
210
Quantum circuits
requirement! As the following example illustrates, efﬁcient quantum simulations are pos-
sible even for Hamiltonians which act non-trivially on all or nearly all parts of a large
system.
Suppose we have the Hamiltonian
H = Z1 ⊗Z2 ⊗· · · ⊗Zn ,
(4.113)
which acts on an n qubit system. Despite this being an interaction involving all of the
system, indeed, it can be simulated efﬁciently. What we desire is a simple quantum circuit
which implements e−iHΔt, for arbitrary values of Δt. A circuit doing precisely this, for
n = 3, is shown in Figure 4.19. The main insight is that although the Hamiltonian
involves all the qubits in the system, it does so in a classical manner: the phase shift
applied to the system is e−iΔt if the parity of the n qubits in the computational basis is
even; otherwise, the phase shift should be eiΔt. Thus, simple simulation of H is possible
by ﬁrst classically computing the parity (storing the result in an ancilla qubit), then
applying the appropriate phase shift conditioned on the parity, then uncomputing the
parity (to erase the ancilla). This strategy clearly works not only for n = 3, but also for
arbitrary values of n.
•
•
•
•
•
•
⊕⊕⊕
e−iΔtZ
⊕⊕⊕
| ⟩
| ⟩
Figure 4.19. Quantum circuit for simulating the Hamiltonian H = Z1 ⊗Z2 ⊗Z3 for time Δt.
Furthermore, extending the same procedure allows us to simulate more complicated
extended Hamiltonians. Speciﬁcally, we can efﬁciently simulate any Hamiltonian of the
form
H =
n
.
k=1
σk
c(k) ,
(4.114)
where σk
c(k) is a Pauli matrix (or the identity) acting on the kth qubit, with c(k) ∈
{0, 1, 2, 3} specifying one of {I, X, Y, Z}. The qubits upon which the identity operation
is performed can be disregarded, and X or Y terms can be transformed by single qubit
gates to Z operations. This leaves us with a Hamiltonian of the form of (4.113), which
is simulated as described above.
Exercise 4.51:
Construct a quantum circuit to simulate the Hamiltonian
H = X1 ⊗Y2 ⊗Z3 ,
(4.115)
performing the unitary transform e−iΔtH for any Δt.
Using this procedure allows us to simulate a wide class of Hamiltonians containing
terms which are not local. In particular, it is possible to simulate a Hamiltonian of the form
0
0
Simulation of quantum systems
211
H = L
k=1 Hk where the only restriction is that the individual Hk have a tensor product
structure, and that L is polynomial in the total number of particles n. More generally, all
that is required is that there be an efﬁcient circuit to simulate each Hk separately. As an
example, the Hamiltonian H = n
k=1 Xk + Z⊗n can easily be simulated using the above
techniques. Such Hamiltonians typically do not arise in Nature. However, they provide
a new and possibly valuable vista on the world of quantum algorithms.
4.7.4
Perspectives on quantum simulation
The quantum simulation algorithm is very similar to classical methods, but also differs
in a fundamental way. Each iteration of the quantum algorithm must completely replace
the old state with the new one; there is no way to obtain (non-trivial) information from
an intermediate step without signiﬁcantly changing the algorithm, because the state is a
quantum one. Furthermore, the ﬁnal measurement must be chosen cleverly to provide the
desired result, because it disturbs the quantum state. Of course, the quantum simulation
can be repeated to obtain statistics, but it is desirable to repeat the algorithm only at
most a polynomial number of times. It may be that even though the simulation can be
performed efﬁciently, there is no way to efﬁciently perform a desired measurement.
Also, there are Hamiltonians which simply can’t be simulated efﬁciently. In Sec-
tion 4.5.4, we saw that there exist unitary transformations which quantum computers
cannot efﬁciently approximate. As a corollary, not all Hamiltonian evolutions can be ef-
ﬁciently simulated on a quantum computer, for if this were possible, then all unitary
transformations could be efﬁciently approximated!
Another difﬁcult problem – one which is very interesting – is the simulation of equi-
libration processes. A system with Hamiltonian H in contact with an environment at
temperature T will generally come to thermal equilibrium in a state known as the Gibbs
state, ρtherm = e−H/kBT /Z, where kB is Boltzmann’s constant, and Z = tr e−H/kBT is
the usual partition function normalization, which ensures that tr(ρ) = 1. The process
by which this equilibration occurs is not very well understood, although certain require-
ments are known: the environment must be large, it must have non-zero population in
states with energies matching the eigenstates of H, and its coupling with the system
should be weak. Obtaining ρtherm for arbitrary H and T is generally an exponentially
difﬁcult problem for a classical computer. Might a quantum computer be able to solve
this efﬁciently? We do not yet know.
On the other hand, as we discussed above many interesting quantum problems can
indeed be simulated efﬁciently with a quantum computer, even when they have extra
constraints beyond the simple algorithms presented here. A particular class of these
involve global symmetries originating from particle statistics. In the everyday world, we
are used to being able to identify different particles; tennis balls can be followed around a
tennis court, keeping track of which is which. This ability to keep track of which object is
which is a general feature of classical objects – by continuously measuring the position of a
classical particle it can be tracked at all times, and thus uniquely distinguished from other
particles. However, this breaks down in quantum mechanics, which prevents us from
following the motion of individual particles exactly. If the two particles are inherently
different, say a proton and an electron, then we can distinguish them by measuring the
sign of the charge to tell which particle is which. But in the case of identical particles,
like two electrons, it is found that they are truly indistinguishable.
Indistinguishability of particles places a constraint on the state vector of a system which
212
Quantum circuits
manifests itself in two ways. Experimentally, particles in Nature are found to come in
two distinct ﬂavors, known as bosons and fermions. The state vector of a system of
bosons remains unchanged under permutation of any two constituents, reﬂecting their
fundamental indistinguishability. Systems of fermions, in contrast, experience a sign
change in their state vector under interchange of any two constituents. Both kinds of
systems can be simulated efﬁciently on a quantum computer. The detailed description
of how this is done is outside the scope of this book; sufﬁce it to say the procedure is
fairly straightforward. Given an initial state of the wrong symmetry, it can be properly
symmetrized before the simulation begins. And the operators used in the simulation can
be constructed to respect the desired symmetry, even allowing for the effects of higher
order error terms. The reader who is interested in pursuing this and other topics further
will ﬁnd pointers to the literature in ‘History and further reading,’ at the end of the
chapter.
Problem 4.1: (Computable phase shifts)
Let m and n be positive integers.
Suppose f : {0, . . . , 2m −1} →{0, . . . , 2n −1} is a classical function from m to
n bits which may be computed reversibly using T Toffoli gates, as described in
Section 3.2.5. That is, the function (x, y) →(x, y ⊕f(x)) may be implemented
using T Toffoli gates. Give a quantum circuit using 2T + n (or fewer) one, two,
and three qubit gates to implement the unitary operation deﬁned by
|x⟩→exp
−2iπf(x)
2n

|x⟩.
(4.116)
Problem 4.2:
Find a depth O(log n) construction for the Cn(X) gate. (Comment:
The depth of a circuit is the number of distinct timesteps at which gates are
applied; the point of this problem is that it is possible to parallelize the Cn(X)
construction by applying many gates in parallel during the same timestep.)
Problem 4.3: (Alternate universality construction)
Suppose U is a unitary
matrix on n qubits. Deﬁne H ≡i ln(U). Show that
(1) H is Hermitian, with eigenvalues in the range 0 to 2π.
(2) H can be written
H =

g
hgg ,
(4.117)
where hg are real numbers and the sum is over all n-fold tensor products g
of the Pauli matrices {I, X, Y, Z}.
(3) Let Δ = 1/k, for some positive integer k. Explain how the unitary operation
exp(−ihggΔ) may be implemented using O(n) one and two qubit operations.
(4) Show that
exp(−iHΔ) =
/
g
exp(−ihggΔ) + O(4nΔ2) ,
(4.118)
where the product is taken with respect to any ﬁxed ordering of the n-fold
tensor products of Pauli matrices, g.
Chapter problems
213
(5) Show that
U =
0/
g
exp(−ihggΔ)
1k
+ O(4nΔ).
(4.119)
(6) Explain how to approximate U to within a distance ϵ > 0 using O(n16n/ϵ)
one and two qubit unitary operations.
Problem 4.4: (Minimal Toffoli construction)
(Research)
(1) What is the smallest number of two qubit gates that can be used to
implement the Toffoli gate?
(2) What is the smallest number of one qubit gates and
gates that can be
used to implement the Toffoli gate?
(3) What is the smallest number of one qubit gates and controlled-Z gates that
can be used to implement the Toffoli gate?
Problem 4.5:
(Research) Construct a family of Hamiltonians, {Hn}, on n qubits,
such that simulating Hn requires a number of operations super-polynomial in n.
(Comment: This problem seems to be quite difﬁcult.)
Problem 4.6: (Universality with prior entanglement)
Controlled-
gates and
single qubit gates form a universal set of quantum logic gates. Show that an
alternative universal set of resources is comprised of single qubit unitaries, the
ability to perform measurements of pairs of qubits in the Bell basis, and the
ability to prepare arbitrary four qubit entangled states.
Summary of Chapter 4: Quantum circuits
• Universality: Any unitary operation on n qubits may be implemented exactly by
composing single qubit and controlled-
gates.
• Universality with a discrete set: The Hadamard gate, phase gate, controlled-
gate, and π/8 gate are universal for quantum computation, in the sense that
an arbitrary unitary operation on n qubits can be approximated to an arbitrary
accuracy ϵ > 0 using a circuit composed of only these gates. Replacing the π/8
gate in this list with the Toffoli gate also gives a universal family.
• Not all unitary operations can be efﬁciently implemented: There are uni-
tary operations on n qubits which require Ω(2n log(1/ϵ)/ log(n)) gates to approx-
imate to within a distance ϵ using any ﬁnite set of gates.
• Simulation: For a Hamiltonian H = 
k Hk which is a sum of polynomially
many terms Hk such that efﬁcient quantum circuits for Hk can be constructed, a
quantum computer can efﬁciently simulate the evolution e−iHt and approximate
|ψ(t)⟩= e−iHt|ψ(0)⟩, given |ψ(0)⟩.
214
Quantum circuits
History and further reading
The gate constructions in this chapter are drawn from a wide variety of sources. The
paper by Barenco, Bennett, Cleve, DiVincenzo, Margolus, Shor, Sleator, Smolin, and
Weinfurter[BBC+95] was the source of many of the circuit constructions in this chapter,
and for the universality proof for single qubit and controlled-
gates. Another useful
source of insights about quantum circuits is the paper by Beckman, Chari, Devabhak-
tuni, and Preskill[BCDP96]. A gentle and accessible introduction has been provided by
DiVincenzo[DiV98]. The fact that measurements commute with control qubit terminals
was pointed out by Grifﬁths and Niu[GN96].
The universality proof for two-level unitaries is due to Reck, Zeilinger, Bernstein, and
Bertani[RZBB94]. The universality of the controlled-
and single qubit gates was proved
by DiVincenzo[DiV95b]. The universal gate G in Exercise 4.44 is sometimes known as the
Deutsch gate[Deu89]. Deutsch, Barenco, and Ekert[DBE95] and Lloyd[Llo95] independently
proved that almost any two qubit quantum logic gate is universal. That errors caused by
sequences of gates is at most the sum of the errors of the individual gates was proven by
Bernstein and Vazirani [BV97]. The speciﬁc universal set of gates we have focused on – the
Hadamard, phase, controlled-
and π/8 gates, was proved universal in Boykin, Mor,
Pulver, Roychowdhury, and Vatan[BMP+99], which also contains a proof that θ deﬁned by
cos(θ/2) ≡cos2(π/8) is an irrational multiple of π. The bound in Section 4.5.4 is based
on a paper by Knill[Kni95], which does a much more detailed investigation of the hardness
of approximating arbitrary unitary operations using quantum circuits. In particular, Knill
obtains tighter and more general bounds than we do, and his analysis applies also to cases
where the universal set is a continuum of gates, not just a ﬁnite set, as we have considered.
The quantum circuit model of computation is due to Deutsch[Deu89], and was further
developed by Yao[Yao93]. The latter paper showed that the quantum circuit model of
computation is equivalent to the quantum Turing machine model. Quantum Turing
machines were introduced in 1980 by Benioff[Ben80], further developed by Deutsch[Deu85]
and Yao[Yao93], and their modern deﬁnition given by Bernstein and Vazirani[BV97]. The
latter two papers also take ﬁrst steps towards setting up a theory of quantum computational
complexity, analogous to classical computational complexity theory. In particular, the
inclusion BQP ⊆PSPACE and some slightly stronger results was proved by Bernstein
and Vazirani. Knill and Laﬂamme[KL99] develop some fascinating connections between
quantum and classical computational complexity. Other interesting work on quantum
computational complexity includes the paper by Adleman, Demarrais and Huang[ADH97],
and the paper by Watrous[Wat99]. The latter paper gives intriguing evidence to suggest
that quantum computers are more powerful than classical computers in the setting of
‘interactive proof systems’.
The suggestion that non-computational basis starting states may be used to obtain
computational power beyond the quantum circuits model was made by Daniel Gottesman
and Michael Nielson.
That quantum computers might simulate quantum systems more efﬁciently than clas-
sical computers was intimated by Manin[Man80] in 1980, and independently developed in
more detail by Feynman[Fey82] in 1982. Much more detailed investigations were subse-
quently carried out by Abrams and Lloyd[AL97], Boghosian and Taylor[BT97], Sornborger
and Stewart[SS99], Wiesner[Wie96], and Zalka[Zal98]. The Trotter formula is attributed to
Trotter[Tro59], and was also proven by Chernoff[Che68], although the simpler form for
History and further reading
215
many-body Fermi systems on a quantum computer. Terhal and DiVincenzo address the
problem of simulating the equilibration of quantum systems to the Gibbs state[TD98].
The method used to simulate the Schr¨odinger equation in Box 4.2 is due to Zalka[Zal98]
and Wiesner[Wie96].
Exercise 4.25 is due to Vandersypen, and is related to work by Chau and Wilczek[CW95].
Exercise 4.45 is due to Boykin, Mor, Pulver, Roychowdhury, and Vatan[BMP+99]. Prob-
lem 4.2 is due to Gottesman. Problem 4.6 is due to Gottesman and Chuang[GC99].
unitary operators is much older, and goes back to the time of Sophus Lie. The third
order version of the Baker–Campbell–Hausdorff formula, Equation (4.104), was given by
Sornborger and Stewart[SS99]. Abrams and Lloyd[AL97] give a procedure for simulating
5 The quantum Fourier transform and its applications
If computers that you build are quantum,
Then spies everywhere will all want ’em.
Our codes will all fail,
And they’ll read our email,
Till we get crypto that’s quantum, and daunt ’em.
– Jennifer and Peter Shor
To read our E-mail, how mean
of the spies and their quantum machine;
be comforted though,
they do not yet know
how to factorize twelve or ﬁfteen.
– Volker Strassen
Computer programming is an art form, like the creation of poetry or music.
– Donald Knuth
The most spectacular discovery in quantum computing to date is that quantum com-
puters can efﬁciently perform some tasks which are not feasible on a classical computer.
For example, ﬁnding the prime factorization of an n-bit integer is thought to require
exp(Θ(n1/3 log2/3 n)) operations using the best classical algorithm known at the time of
writing, the so-called number ﬁeld sieve. This is exponential in the size of the num-
ber being factored, so factoring is generally considered to be an intractable problem on
a classical computer: it quickly becomes impossible to factor even modest numbers. In
contrast, a quantum algorithm can accomplish the same task using O(n2 log n log log n)
operations. That is, a quantum computer can factor a number exponentially faster than
the best known classical algorithms. This result is important in its own right, but per-
haps the most exciting aspect is the question it raises: what other problems can be done
efﬁciently on a quantum computer which are infeasible on a classical computer?
In this chapter we develop the quantum Fourier transform, which is the key ingredient
for quantum factoring and many other interesting quantum algorithms. The quantum
Fourier transform, with which we begin in Section 5.1, is an efﬁcient quantum algorithm
for performing a Fourier transform of quantum mechanical amplitudes. It does not speed
up the classical task of computing Fourier transforms of classical data. But one important
task which it does enable is phase estimation, the approximation of the eigenvalues of
a unitary operator under certain circumstances, as described in Section 5.2. This allows
us to solve several other interesting problems, including the order-ﬁnding problem and
the factoring problem, which are covered in Section 5.3. Phase estimation can also be
combined with the quantum search algorithm to solve the problem of counting solutions
to a search problem, as described in the next chapter. Section 5.4 concludes the chapter
with a discussion of how the quantum Fourier transform may be used to solve the hidden
The quantum Fourier transform
217
subgroup problem, a generalization of the phase estimation and order-ﬁnding problems
that has among its special cases an efﬁcient quantum algorithm for the discrete logarithm
problem, another problem thought to be intractable on a classical computer.
5.1
The quantum Fourier transform
A good idea has a way of becoming simpler and solving problems other than
that for which it was intended.
– Robert Tarjan
One of the most useful ways of solving a problem in mathematics or computer science
is to transform it into some other problem for which a solution is known. There are a
few transformations of this type which appear so often and in so many different contexts
that the transformations are studied for their own sake. A great discovery of quantum
computation has been that some such transformations can be computed much faster on
a quantum computer than on a classical computer, a discovery which has enabled the
construction of fast algorithms for quantum computers.
One such transformation is the discrete Fourier transform. In the usual mathematical
notation, the discrete Fourier transform takes as input a vector of complex numbers,
x0, . . . , xN−1 where the length N of the vector is a ﬁxed parameter. It outputs the
transformed data, a vector of complex numbers y0, . . . , yN−1, deﬁned by
yk ≡
1
√
N
N−1

j=0
xje2πijk/N .
(5.1)
The quantum Fourier transform is exactly the same transformation, although the
conventional notation for the quantum Fourier transform is somewhat different. The
quantum Fourier transform on an orthonormal basis |0⟩, . . . , |N −1⟩is deﬁned to be a
linear operator with the following action on the basis states,
|j⟩−→
1
√
N
N−1

k=0
e2πijk/N|k⟩.
(5.2)
Equivalently, the action on an arbitrary state may be written
N−1

j=0
xj|j⟩−→
N−1

k=0
yk|k⟩,
(5.3)
where the amplitudes yk are the discrete Fourier transform of the amplitudes xj. It is not
obvious from the deﬁnition, but this transformation is a unitary transformation, and thus
can be implemented as the dynamics for a quantum computer. We shall demonstrate
the unitarity of the Fourier transform by constructing a manifestly unitary quantum
circuit computing the Fourier transform. It is also easy to prove directly that the Fourier
transform is unitary:
Exercise 5.1:
Give a direct proof that the linear transformation deﬁned by
Equation (5.2) is unitary.
Exercise 5.2:
Explicitly compute the Fourier transform of the n qubit state |00 . . . 0⟩.
218
The quantum Fourier transform and its applications
In the following, we take N = 2n, where n is some integer, and the basis |0⟩, . . . , |2n−
1⟩is the computational basis for an n qubit quantum computer. It is helpful to write the
state |j⟩using the binary representation j = j1j2 . . . jn. More formally, j = j12n−1 +
j22n−2 +· · ·+jn20. It is also convenient to adopt the notation 0.jljl+1 . . . jm to represent
the binary fraction jl/2 + jl+1/4 + · · · + jm/2m−l+1.
With a little algebra the quantum Fourier transform can be given the following useful
product representation:
|j1, . . . , jn⟩→

|0⟩+ e2πi0.jn|1⟩
 
|0⟩+ e2πi0.jn−1jn|1⟩

· · ·

|0⟩+ e2πi0.j1j2···jn|1⟩

2n/2
.
(5.4)
This product representation is so useful that you may even wish to consider this to be the
deﬁnition of the quantum Fourier transform. As we explain shortly this representation
allows us to construct an efﬁcient quantum circuit computing the Fourier transform, a
proof that the quantum Fourier transform is unitary, and provides insight into algorithms
based upon the quantum Fourier transform. As an incidental bonus we obtain the classical
fast Fourier transform, in the exercises!
The equivalence of the product representation (5.4) and the deﬁnition (5.2) follows
from some elementary algebra:
|j⟩→
1
2n/2
2n−1

k=0
e2πijk/2n|k⟩
(5.5)
=
1
2n/2
1

k1=0
. . .
1

kn=0
e2πijn
l=1 kl2−l
|k1 . . . kn⟩
(5.6)
=
1
2n/2
1

k1=0
. . .
1

kn=0
n

l=1
e2πijkl2−l|kl⟩
(5.7)
=
1
2n/2
n

l=1
	
1

kl=0
e2πijkl2−l|kl⟩

(5.8)
=
1
2n/2
n

l=1

|0⟩+ e2πij2−l|1⟩

(5.9)
=

|0⟩+ e2πi0.jn|1⟩
 
|0⟩+ e2πi0.jn−1jn|1⟩

· · ·

|0⟩+ e2πi0.j1j2···jn|1⟩

2n/2
.(5.10)
The product representation (5.4) makes it easy to derive an efﬁcient circuit for the
quantum Fourier transform. Such a circuit is shown in Figure 5.1. The gate Rk denotes
the unitary transformation
Rk ≡
	
1
0
0
e2πi/2k

.
(5.11)
To see that the pictured circuit computes the quantum Fourier transform, consider what
happens when the state |j1 . . . jn⟩is input. Applying the Hadamard gate to the ﬁrst bit
produces the state
1
21/2

|0⟩+ e2πi0.j1|1⟩

|j2 . . . jn⟩,
(5.12)
The quantum Fourier transform
219
Figure 5.1. Efﬁcient circuit for the quantum Fourier transform. This circuit is easily derived from the product
representation (5.4) for the quantum Fourier transform. Not shown are swap gates at the end of the circuit which
reverse the order of the qubits, or normalization factors of 1/
√
2 in the output.
since e2πi0.j1 = −1 when j1 = 1, and is +1 otherwise. Applying the controlled-R2 gate
produces the state
1
21/2
'
|0⟩+ e2πi0.j1j2|1⟩
(
|j2 . . . jn⟩.
(5.13)
We continue applying the controlled-R3, R4 through Rn gates, each of which adds an
extra bit to the phase of the co-efﬁcient of the ﬁrst |1⟩. At the end of this procedure we
have the state
1
21/2
'
|0⟩+ e2πi0.j1j2...jn|1⟩
(
|j2 . . . jn⟩.
(5.14)
Next, we perform a similar procedure on the second qubit. The Hadamard gate puts us
in the state
1
22/2
'
|0⟩+ e2πi0.j1j2...jn|1⟩
( '
|0⟩+ e2πi0.j2|1⟩
(
|j3 . . . jn⟩,
(5.15)
and the controlled-R2 through Rn−1 gates yield the state
1
22/2
'
|0⟩+ e2πi0.j1j2...jn|1⟩
( '
|0⟩+ e2πi0.j2...jn|1⟩
(
|j3 . . . jn⟩.
(5.16)
We continue in this fashion for each qubit, giving a ﬁnal state
1
2n/2
'
|0⟩+ e2πi0.j1j2...jn|1⟩
( '
|0⟩+ e2πi0.j2...jn|1⟩
(
. . .
'
|0⟩+ e2πi0.jn|1⟩
(
.
(5.17)
Swap operations (see Section 1.3.4 for a description of the circuit), omitted from Fig-
ure 5.1 for clarity, are then used to reverse the order of the qubits. After the swap
operations, the state of the qubits is
1
2n/2
'
|0⟩+ e2πi0.jn|1⟩
( '
|0⟩+ e2πi0.jn−1jn|1⟩
(
. . .
'
|0⟩+ e2πi0.j1j2···jn|1⟩
(
.
(5.18)
Comparing with Equation (5.4) we see that this is the desired output from the quantum
Fourier transform. This construction also proves that the quantum Fourier transform is
unitary, since each gate in the circuit is unitary. An explicit example showing a circuit
for the quantum Fourier transform on three qubits is given in Box 5.1.
How many gates does this circuit use? We start by doing a Hadamard gate and n −1
conditional rotations on the ﬁrst qubit – a total of n gates. This is followed by a Hadamard
gate and n −2 conditional rotations on the second qubit, for a total of n + (n −1) gates.
Continuing in this way, we see that n+(n−1)+· · · +1 = n(n+1)/2 gates are required,
220
The quantum Fourier transform and its applications
Box 5.1: Three qubit quantum Fourier transform
For concreteness it may help to look at the explicit circuit for the three qubit
quantum Fourier transform:
Recall that S and T are the phase and π/8 gates (see page xxiii). As a matrix the
quantum Fourier transform in this instance may be written out explicitly, using
ω = e2πi/8 =
√
i, as
1
√
8
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
ω
ω2
ω3
ω4
ω5
ω6
ω7
1
ω2
ω4
ω6
1
ω2
ω4
ω6
1
ω3
ω6
ω1
ω4
ω7
ω2
ω5
1
ω4
1
ω4
1
ω4
1
ω4
1
ω5
ω2
ω7
ω4
ω1
ω6
ω3
1
ω6
ω4
ω2
1
ω6
ω4
ω2
1
ω7
ω6
ω5
ω4
ω3
ω2
ω1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(5.19)
plus the gates involved in the swaps. At most n/2 swaps are required, and each swap
can be accomplished using three controlled-
gates. Therefore, this circuit provides a
Θ(n2) algorithm for performing the quantum Fourier transform.
In contrast, the best classical algorithms for computing the discrete Fourier transform
on 2n elements are algorithms such as the Fast Fourier Transform (FFT), which com-
pute the discrete Fourier transform using Θ(n2n) gates. That is, it requires exponentially
more operations to compute the Fourier transform on a classical computer than it does
to implement the quantum Fourier transform on a quantum computer.
At face value this sounds terriﬁc, since the Fourier transform is a crucial step in so many
real-world data processing applications. For example, in computer speech recognition,
the ﬁrst step in phoneme recognition is to Fourier transform the digitized sound. Can
we use the quantum Fourier transform to speed up the computation of these Fourier
transforms? Unfortunately, the answer is that there is no known way to do this. The
problem is that the amplitudes in a quantum computer cannot be directly accessed by
measurement. Thus, there is no way of determining the Fourier transformed amplitudes
of the original state. Worse still, there is in general no way to efﬁciently prepare the
original state to be Fourier transformed. Thus, ﬁnding uses for the quantum Fourier
transform is more subtle than we might have hoped. In this and the next chapter we
develop several algorithms based upon a more subtle application of the quantum Fourier
transform.
Phase estimation
221
Exercise 5.3: (Classical fast Fourier transform)
Suppose we wish to perform a
Fourier transform of a vector containing 2n complex numbers on a classical
computer. Verify that the straightforward method for performing the Fourier
transform, based upon direct evaluation of Equation (5.1) requires Θ(22n)
elementary arithmetic operations. Find a method for reducing this to Θ(n2n)
operations, based upon Equation (5.4).
Exercise 5.4:
Give a decomposition of the controlled-Rk gate into single qubit and
gates.
Exercise 5.5:
Give a quantum circuit to perform the inverse quantum Fourier
transform.
Exercise 5.6: (Approximate quantum Fourier transform)
The quantum circuit
construction of the quantum Fourier transform apparently requires gates of
exponential precision in the number of qubits used. However, such precision is
never required in any quantum circuit of polynomial size. For example, let U be
the ideal quantum Fourier transform on n qubits, and V be the transform which
results if the controlled-Rk gates are performed to a precision Δ = 1/p(n) for
some polynomial p(n). Show that the error E(U, V ) ≡max|ψ⟩∥(U −V )|ψ⟩∥
scales as Θ(n2/p(n)), and thus polynomial precision in each gate is sufﬁcient to
guarantee polynomial accuracy in the output state.
5.2
Phase estimation
The Fourier transform is the key to a general procedure known as phase estimation,
which in turn is the key for many quantum algorithms. Suppose a unitary operator U
has an eigenvector |u⟩with eigenvalue e2πiϕ, where the value of ϕ is unknown. The goal
of the phase estimation algorithm is to estimate ϕ. To perform the estimation we assume
that we have available black boxes (sometimes known as oracles) capable of preparing the
state |u⟩and performing the controlled-U 2j operation, for suitable non-negative integers
j. The use of black boxes indicates that the phase estimation procedure is not a complete
quantum algorithm in its own right. Rather, you should think of phase estimation as a
kind of ‘subroutine’ or ‘module’ that, when combined with other subroutines, can be
used to perform interesting computational tasks. In speciﬁc applications of the phase
estimation procedure we shall do exactly this, describing how these black box operations
are to be performed, and combining them with the phase estimation procedure to do
genuinely useful tasks. For the moment, though, we will continue to imagine them as
black boxes.
The quantum phase estimation procedure uses two registers. The ﬁrst register contains
t qubits initially in the state |0⟩. How we choose t depends on two things: the number
of digits of accuracy we wish to have in our estimate for ϕ, and with what probability
we wish the phase estimation procedure to be successful. The dependence of t on these
quantities emerges naturally from the following analysis.
The second register begins in the state |u⟩, and contains as many qubits as is necessary
to store |u⟩. Phase estimation is performed in two stages. First, we apply the circuit shown
in Figure 5.2. The circuit begins by applying a Hadamard transform to the ﬁrst register,
followed by application of controlled-U operations on the second register, with U raised
222
The quantum Fourier transform and its applications
to successive powers of two. The ﬁnal state of the ﬁrst register is easily seen to be:
1
2t/2
'
|0⟩+ e2πi2t−1ϕ|1⟩
( '
|0⟩+ e2πi2t−2ϕ|1⟩
(
. . .
'
|0⟩+ e2πi20ϕ|1⟩
(
=
1
2t/2
2t−1

k=0
e2πiϕk|k⟩.
(5.20)
We omit the second register from this description, since it stays in the state |u⟩throughout
the computation.
| ⟩
H
· · ·
•
| ⟩
e2πi(2t−1ϕ)| ⟩
| ⟩
H
•
· · ·
| ⟩
e2πi(22ϕ)| ⟩
| ⟩
H
•
· · ·
| ⟩
e2πi(21ϕ)| ⟩
| ⟩
H
•
· · ·
| ⟩
e2πi(20ϕ)| ⟩
|u⟩
U20
U21
U22
· · ·
U2t−1
|u⟩
t
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
⎧
⎪
⎨
⎩
Figure 5.2. The ﬁrst stage of the phase estimation procedure. Normalization factors of 1/
√
2 have been omitted, on
the right.
Exercise 5.7:
Additional insight into the circuit in Figure 5.2 may be obtained by
showing, as you should now do, that the effect of the sequence of controlled-U
operations like that in Figure 5.2 is to take the state |j⟩|u⟩to |j⟩U j|u⟩. (Note
that this does not depend on |u⟩being an eigenstate of U.)
The second stage of phase estimation is to apply the inverse quantum Fourier transform
on the ﬁrst register. This is obtained by reversing the circuit for the quantum Fourier
transform in the previous section (Exercise 5.5), and can be done in Θ(t2) steps. The
third and ﬁnal stage of phase estimation is to read out the state of the ﬁrst register by
doing a measurement in the computational basis. We will show that this provides a pretty
good estimate of ϕ. An overall schematic of the algorithm is shown in Figure 5.3.
To sharpen our intuition as to why phase estimation works, suppose ϕ may be ex-
pressed exactly in t bits, as ϕ = 0.ϕ1 . . . ϕt. Then the state (5.20) resulting from the ﬁrst
stage of phase estimation may be rewritten
1
2t/2
'
|0⟩+ e2πi0.ϕt|1⟩
( '
|0⟩+ e2πi0.ϕt−1ϕt|1⟩
(
. . .
'
|0⟩+ e2πi0.ϕ1ϕ2···ϕt|1⟩
(
. (5.21)
The second stage of phase estimation is to apply the inverse quantum Fourier transform.
But comparing the previous equation with the product form for the Fourier transform,
Equation (5.4), we see that the output state from the second stage is the product state
|ϕ1 . . . ϕt⟩. A measurement in the computational basis therefore gives us ϕ exactly!
0
0
0
0
0
0
0
0
1
1
1
1
Second register
First register
qubits
Phase estimation
223

________

_ _ _ _ _ _ _ _

Figure 5.3. Schematic of the overall phase estimation procedure. The top t qubits (the ‘/’ denotes a bundle of
wires, as usual) are the ﬁrst register, and the bottom qubits are the second register, numbering as many as required
to perform U. |u⟩is an eigenstate of U with eigenvalue e2πiϕ. The output of the measurement is an
approximation to ϕ accurate to t −2
log 
2 + 1
2ϵ
3
bits, with probability of success at least 1 −ϵ.
Summarizing, the phase estimation algorithm allows one to estimate the phase ϕ of an
eigenvalue of a unitary operator U, given the corresponding eigenvector |u⟩. An essential
feature at the heart of this procedure is the ability of the inverse Fourier transform to
perform the transformation
1
2t/2
2t−1

j=0
e2πiϕj|j⟩|u⟩→| ˜ϕ⟩|u⟩,
(5.22)
where | ˜ϕ⟩denotes a state which is a good estimator for ϕ when measured.
5.2.1
Performance and requirements
The above analysis applies to the ideal case, where ϕ can be written exactly with a t
bit binary expansion. What happens when this is not the case? It turns out that the
procedure we have described will produce a pretty good approximation to ϕ with high
probability, as foreshadowed by the notation used in (5.22). Showing this requires some
careful manipulations.
Let b be the integer in the range 0 to 2t −1 such that b/2t = 0.b1 . . . bt is the best t bit
approximation to ϕ which is less than ϕ. That is, the difference δ ≡ϕ −b/2t between
ϕ and b/2t satisﬁes 0 ≤δ ≤2−t. We aim to show that the observation at the end of
the phase estimation procedure produces a result which is close to b, and thus enables us
to estimate ϕ accurately, with high probability. Applying the inverse quantum Fourier
transform to the state (5.20) produces the state
1
2t
2t−1

k,l=0
e
−2πikl
2t
e2πiϕk|l⟩.
(5.23)
Let αl be the amplitude of |(b + l)(mod 2t)⟩,
αl ≡1
2t
2t−1

k=0
'
e2πi(ϕ−(b+l)/2t)(k
.
(5.24)
This is the sum of a geometric series, so
αl = 1
2t

1 −e2πi(2tϕ−(b+l))
1 −e2πi(ϕ−(b+l)/2t)

(5.25)
224
The quantum Fourier transform and its applications
= 1
2t

1 −e2πi(2tδ−l)
1 −e2πi(δ−l/2t)

.
(5.26)
Suppose the outcome of the ﬁnal measurement is m. We aim to bound the probability of
obtaining a value of m such that |m −b| > e, where e is a positive integer characterizing
our desired tolerance to error. The probability of observing such an m is given by
p(|m −b| > e) =

−2t−1<l≤−(e+1)
|αl|2 +

e+1≤l≤2t−1
|αl|2 .
(5.27)
But for any real θ, |1 −exp(iθ)| ≤2, so
|αl| ≤
2
2t |1 −e2πi(δ−l/2t)| .
(5.28)
By elementary geometry or calculus |1 −exp(iθ)| ≥2|θ|/π whenever −π ≤θ ≤π. But
when −2t−1 < l ≤2t−1 we have −π ≤2π(δ −l/2t) ≤π. Thus
|αl| ≤
1
2t+1(δ −l/2t) .
(5.29)
Combining (5.27) and (5.29) gives
p(|m −b| > e) ≤1
4
⎡
⎣
−(e+1)

l=−2t−1+1
1
(l −2tδ)2 +
2t−1

l=e+1
1
(l −2tδ)2
⎤
⎦.
(5.30)
Recalling that 0 ≤2tδ ≤1, we obtain
p(|m −b| > e) ≤1
4
⎡
⎣
−(e+1)

l=−2t−1+1
1
l2 +
2t−1

l=e+1
1
(l −1)2
⎤
⎦
(5.31)
≤1
2
2t−1−1

l=e
1
l2
(5.32)
≤1
2
- 2t−1−1
e−1
dl 1
l2
(5.33)
=
1
2(e −1) .
(5.34)
Suppose we wish to approximate ϕ to an accuracy 2−n, that is, we choose e = 2t−n −1.
By making use of t = n + p qubits in the phase estimation algorithm we see from (5.34)
that the probability of obtaining an approximation correct to this accuracy is at least
1 −1/2(2p −2). Thus to successfully obtain ϕ accurate to n bits with probability of
success at least 1 −ϵ we choose
t = n +
4
log

2 + 1
2ϵ
5
.
(5.35)
In order to make use of the phase estimation algorithm, we need to be able to prepare an
eigenstate |u⟩of U. What if we do not know how to prepare such an eigenstate? Suppose
that we prepare some other state |ψ⟩in place of |u⟩. Expanding this state in terms of
eigenstates |u⟩of U gives |ψ⟩= 
u cu|u⟩. Suppose the eigenstate |u⟩has eigenvalue
e2πiϕu. Intuitively, the result of running the phase estimation algorithm will be to give
Phase estimation
225
as output a state close to 
u cu|6
ϕu⟩|u⟩, where 6
ϕu is a pretty good approximation to the
phase ϕu. Therefore, we expect that reading out the ﬁrst register will give us a good
approximation to ϕu, where u is chosen at random with probability |cu|2. Making this
argument rigorous is left for Exercise 5.8. This procedure allows us to avoid preparing
a (possibly unknown) eigenstate, at the cost of introducing some additional randomness
into the algorithm.
Exercise 5.8:
Suppose the phase estimation algorithm takes the state |0⟩|u⟩to the
state |6
ϕu⟩|u⟩, so that given the input |0⟩


u cu|u⟩
, the algorithm outputs

u cu|6
ϕu⟩|u⟩. Show that if t is chosen according to (5.35), then the probability
for measuring ϕu accurate to n bits at the conclusion of the phase estimation
algorithm is at least |cu|2(1 −ϵ).
Why is phase estimation interesting? For its own sake, phase estimation solves a prob-
lem which is both non-trivial and interesting from a physical point of view: how to
estimate the eigenvalue associated to a given eigenvector of a unitary operator. Its real
use, though, comes from the observation that other interesting problems can be reduced
to phase estimation, as will be shown in subsequent sections. The phase estimation algo-
rithm is summarized below.
Algorithm:
Quantum phase estimation
Inputs: (1) A black box wich performs a controlled-U j operation, for integer j,
(2) an eigenstate |u⟩of U with eigenvalue e2πiϕu, and (3) t = n +
2log

2 + 1
2ϵ
3
qubits initialized to |0⟩.
Outputs: An n-bit approximation 6
ϕu to ϕu.
Runtime: O(t2) operations and one call to controlled-U j black box. Succeeds
with probability at least 1 −ϵ.
Procedure:
1.
|0⟩|u⟩
initial state
2.
→
1
√
2t
2t−1

j=0
|j⟩|u⟩
create superposition
3.
→
1
√
2t
2t−1

j=0
|j⟩U j|u⟩
apply black box
=
1
√
2t
2t−1

j=0
e2πijϕu|j⟩|u⟩
result of black box
4.
→|6
ϕu⟩|u⟩
apply inverse Fourier transform
5.
→6
ϕu
measure ﬁrst register
Exercise 5.9:
Let U be a unitary transform with eigenvalues ±1, which acts on a state
|ψ⟩. Using the phase estimation procedure, construct a quantum circuit to
collapse |ψ⟩into one or the other of the two eigenspaces of U, giving also a
226
The quantum Fourier transform and its applications
classical indicator as to which space the ﬁnal state is in. Compare your result
with Exercise 4.34.
5.3
Applications: order-ﬁnding and factoring
The phase estimation procedure can be used to solve a variety of interesting problems. We
now describe two of the most interesting of these problems: the order-ﬁnding problem,
and the factoring problem. These two problems are, in fact, equivalent to one another, so
in Section 5.3.1 we explain a quantum algorithm for solving the order-ﬁnding problem,
and in Section 5.3.2 we explain how the order-ﬁnding problem implies the ability to
factor as well.
To understand the quantum algorithms for factoring and order-ﬁnding requires a
little background in number theory. All the required materials are collected together in
Appendix 4. The description we give over the next two sections focuses on the quantum
aspects of the problem, and requires only a little familiarity with modular arithmetic to
be readable. Detailed proofs of the number-theoretic results we quote here may be found
in Appendix 4.
The fast quantum algorithms for order-ﬁnding and factoring are interesting for at least
three reasons. First, and most important in our opinion, they provide evidence for the idea
that quantum computers may be inherently more powerful than classical computers, and
provide a credible challenge to the strong Church–Turing thesis. Second, both problems
are of sufﬁcient intrinsic worth to justify interest in any novel algorithm, be it classical
or quantum. Third, and most important from a practical standpoint, efﬁcient algorithms
for order-ﬁnding and factoring can be used to break the RSA public-key cryptosystem
(Appendix 5).
5.3.1
Application: order-ﬁnding
For positive integers x and N, x < N, with no common factors, the order of x modulo N
is deﬁned to be the least positive integer, r, such that xr = 1(mod N). The order-ﬁnding
problem is to determine the order for some speciﬁed x and N. Order-ﬁnding is believed
to be a hard problem on a classical computer, in the sense that no algorithm is known
to solve the problem using resources polynomial in the O(L) bits needed to specify the
problem, where L ≡⌈log(N)⌉is the number of bits needed to specify N. In this section
we explain how phase estimation may be used to obtain an efﬁcient quantum algorithm
for order-ﬁnding.
Exercise 5.10:
Show that the order of x = 5 modulo N = 21 is 6.
Exercise 5.11:
Show that the order of x satisﬁes r ≤N.
The quantum algorithm for order-ﬁnding is just the phase estimation algorithm applied
to the unitary operator
U|y⟩≡|xy(mod N)⟩,
(5.36)
with y ∈{0, 1}L. (Note that here and below, when N ≤y ≤2L −1, we use the
convention that xy(mod N) is just y again. That is, U only acts non-trivially when
Applications: order-ﬁnding and factoring
227
0 ≤y ≤N −1.) A simple calculation shows that the states deﬁned by
|us⟩≡
1
√r
r−1

k=0
exp
−2πisk
r

|xk mod N⟩,
(5.37)
for integer 0 ≤s ≤r −1 are eigenstates of U, since
U|us⟩=
1
√r
r−1

k=0
exp
−2πisk
r

|xk+1 mod N⟩
(5.38)
= exp
2πis
r

|us⟩.
(5.39)
Using the phase estimation procedure allows us to obtain, with high accuracy, the cor-
responding eigenvalues exp(2πis/r), from which we can obtain the order r with a little
bit more work.
Exercise 5.12:
Show that U is unitary (Hint: x is co-prime to N, and therefore has
an inverse modulo N).
There are two important requirements for us to be able to use the phase estimation
procedure: we must have efﬁcient procedures to implement a controlled-U 2j operation
for any integer j, and we must be able to efﬁciently prepare an eigenstate |us⟩with a non-
trivial eigenvalue, or at least a superposition of such eigenstates. The ﬁrst requirement
is satisﬁed by using a procedure known as modular exponentiation, with which we
can implement the entire sequence of controlled-U 2j operations applied by the phase
estimation procedure using O(L3) gates, as described in Box 5.2.
The second requirement is a little tricker: preparing |us⟩requires that we know r, so
this is out of the question. Fortunately, there is a clever observation which allows us to
circumvent the problem of preparing |us⟩, which is that
1
√r
r−1

s=0
|us⟩= |1⟩.
(5.44)
In performing the phase estimation procedure, if we use t = 2L + 1 +
2log

2 + 1
2ϵ
3
qubits in the ﬁrst register (referring to Figure 5.3), and prepare the second register in
the state |1⟩– which is trivial to construct – it follows that for each s in the range 0
through r −1, we will obtain an estimate of the phase ϕ ≈s/r accurate to 2L + 1 bits,
with probability at least (1 −ϵ)/r. The order-ﬁnding algorithm is schematically depicted
in Figure 5.4.
Exercise 5.13:
Prove (5.44). (Hint: r−1
s=0 exp(−2πisk/r) = rδk0.) In fact, prove that
1
√r
r−1

s=0
e2πisk/r|us⟩= |xk mod N⟩.
(5.45)
Exercise 5.14:
The quantum state produced in the order-ﬁnding algorithm, before
the inverse Fourier transform, is
|ψ⟩=
2t−1

j=0
|j⟩U j|1⟩=
2t−1

j=0
|j⟩|xj mod N⟩,
(5.46)
228
The quantum Fourier transform and its applications
Box 5.2: Modular exponentiation
How can we compute the sequence of controlled-U 2j operations used by the phase
estimation procedure as part of the order-ﬁnding algorithm? That is, we wish to
compute the transformation
|z⟩|y⟩→|z⟩U zt2t−1 . . . U z120|y⟩
(5.40)
= |z⟩|xzt2t−1 × · · · × xz120y(mod N)⟩
(5.41)
= |z⟩|xzy(mod N)⟩.
(5.42)
Thus the sequence of controlled-U 2j operations used in phase estimation is equiva-
lent to multiplying the contents of the second register by the modular exponential
xz(mod N), where z is the contents of the ﬁrst register. This operation may be
accomplished easily using the techniques of reversible computation. The basic idea
is to reversibly compute the function xz(mod N) of z in a third register, and then
to reversibly multiply the contents of the second register by xz(mod N), using the
trick of uncomputation to erase the contents of the third register upon completion.
The algorithm for computing the modular exponential has two stages. The ﬁrst stage
uses modular multiplication to compute x2(mod N), by squaring x modulo N, then
computes x4(mod N) by squaring x2(mod N), and continues in this way, computing
x2j(mod N) for all j up to t −1. We use t = 2L + 1 + ⌈log(2 + 1/(2ϵ))⌉= O(L),
so a total of t −1 = O(L) squaring operations is performed at a cost of O(L2)
each (this cost assumes the circuit used to do the squaring implements the familiar
algorithm we all learn as children for multiplication), for a total cost of O(L3) for
the ﬁrst stage. The second stage of the algorithm is based upon the observation
we’ve already noted,
xz(mod N) =
'
xzt2t−1(mod N)
( '
xzt−12t−2(mod N)
(
. . .
'
xz120(mod N)
(
.
(5.43)
Performing t −1 modular multiplications with a cost O(L2) each, we see that this
product can be computed using O(L3) gates. This is sufﬁciently efﬁcient for our
purposes, but more efﬁcient algorithms are possible based on more efﬁcient algo-
rithms for multiplication (see ‘History and further reading’). Using the techniques
of Section 3.2.5, it is now straightforward to construct a reversible circuit with a
t bit register and an L bit register which, when started in the state (z, y) outputs
(z, xzy (mod N)), using O(L3) gates, which can be translated into a quantum circuit
using O(L3) gates computing the transformation |z⟩|y⟩→|z⟩|xzy (mod N)⟩.
if we initialize the second register as |1⟩. Show that the same state is obtained if
we replace U j with a different unitary transform V , which computes
V |j⟩|k⟩= |j⟩|k + xj mod N⟩,
(5.47)
and start the second register in the state |0⟩. Also show how to construct V using
O(L3) gates.
Applications: order-ﬁnding and factoring
229
t
| ⟩/
H⊗t
|j⟩
•
FT †

________

_ _ _ _ _ _ _ _

L
| ⟩/
xj
N
Figure 5.4. Quantum circuit for the order-ﬁnding algorithm. The second register is shown as being initialized to
the |1⟩state, but if the method of Exercise 5.14 is used, it can be initialized to |0⟩instead. This circuit can also be
used for factoring, using the reduction given in Section 5.3.2.
The continued fraction expansion
The reduction of order-ﬁnding to phase estimation is completed by describing how to
obtain the desired answer, r, from the result of the phase estimation algorithm, ϕ ≈s/r.
We only know ϕ to 2L + 1 bits, but we also know a priori that it is a rational number
– the ratio of two bounded integers – and if we could compute the nearest such fraction
to ϕ we might obtain r.
Remarkably, there is an algorithm which accomplishes this task efﬁciently, known as
the continued fractions algorithm. An example of how this works is described in Box 5.3.
The reason this algorithm satisﬁes our needs is the following theorem, which is proved
in Appendix 4:
Theorem 5.1: Suppose s/r is a rational number such that
+++s
r −ϕ
+++ ≤
1
2r2 .
(5.48)
Then s/r is a convergent of the continued fraction for ϕ, and thus can be
computed in O(L3) operations using the continued fractions algorithm.
Since ϕ is an approximation of s/r accurate to 2L + 1 bits, it follows that |s/r −ϕ| ≤
2−2L−1 ≤1/2r2, since r ≤N ≤2L. Thus, the theorem applies.
Summarizing, given ϕ the continued fractions algorithm efﬁciently produces numbers
s′ and r′ with no common factor, such that s′/r′ = s/r. The number r′ is our candidate
for the order. We can check to see whether it is the order by calculating xr′ mod N, and
seeing if the result is 1. If so, then r′ is the order of x modulo N, and we are done!
Performance
How can the order-ﬁnding algorithm fail? There are two possibilities. First, the phase
estimation procedure might produce a bad estimate to s/r. This occurs with probability
at most ϵ, and can be made small with a negligible increase in the size of the circuit.
More seriously, it might be that s and r have a common factor, in which case the
number r′ returned by the continued fractions algorithm be a factor of r, and not r itself.
Fortunately, there are at least three ways around this problem.
Perhaps the most straightforward way is to note that for randomly chosen s in the
range 0 through r −1, it’s actually pretty likely that s and r are co-prime, in which
case the continued fractions algorithm must return r. To see that this is the case, note
that by Problem 4.1 on page 638 the number of prime numbers less than r is at least
mod
Register 1
qubits
Register 2
qubits
1
0
230
The quantum Fourier transform and its applications
Box 5.3: The continued fractions algorithm
The idea of the continued fractions algorithm is to describe real numbers in terms
of integers alone, using expressions of the form
[a0, . . . , aM] ≡a0 +
1
a1 +
1
a2+
1
...+
1
aM
,
(5.49)
where a0, . . . , aM are positive integers. (For applications to quantum computing it
is convenient to allow a0 = 0 as well.) We deﬁne the mth convergent (0 ≤m ≤M)
to this continued fraction to be [a0, . . . , am]. The continued fractions algorithm
is a method for determining the continued fraction expansion of an arbitrary real
number. It is easily understood by example. Suppose we are trying to decompose
31/13 as a continued fraction. The ﬁrst step of the continued fractions algorithm
is to split 31/13 into its integer and fractional part,
31
13 = 2 + 5
13 .
(5.50)
Next we invert the fractional part, obtaining
31
13 = 2 + 1
13
5
.
(5.51)
These steps – split then invert – are now applied to 13/5, giving
31
13 = 2 +
1
2 + 3
5
= 2 +
1
2 + 1
5
3
.
(5.52)
Next we split and invert 5/3:
31
13 = 2 +
1
2 +
1
1+ 2
3
= 2 +
1
2 +
1
1+ 1
3
2
.
(5.53)
The decomposition into a continued fraction now terminates, since
3
2 = 1 + 1
2
(5.54)
may be written with a 1 in the numerator without any need to invert, giving a ﬁnal
continued fraction representation of 31/13 as
31
13 = 2 +
1
2 +
1
1+
1
1+ 1
2
.
(5.55)
It’s clear that the continued fractions algorithm terminates after a ﬁnite number
of ‘split and invert’ steps for any rational number, since the numerators which
appear (31, 5, 3, 2, 1 in the example) are strictly decreasing. How quickly does this
termination occur? It turns out that if ϕ = s/r is a rational number, and s and r
are L bit integers, then the continued fraction expansion for ϕ can be computed
using O(L3) operations – O(L) ‘split and invert’ steps, each using O(L2) gates for
elementary arithmetic.
Applications: order-ﬁnding and factoring
231
r/2 log r, and thus the chance that s is prime (and therefore, co-prime to r) is at least
1/2 log(r) > 1/2 log(N). Thus, repeating the algorithm 2 log(N) times we will, with
high probability, observe a phase s/r such that s and r are co-prime, and therefore the
continued fractions algorithm produces r, as desired.
A second method is to note that if r′ ̸= r, then r′ is guaranteed to be a factor of r,
unless s = 0, which possibility occurs with probability 1/r ≤1/2, and which can be
discounted further by a few repetitions. Suppose we replace a by a′ ≡ar′(mod N). Then
the order of a′ is r/r′. We can now repeat the algorithm, and try to compute the order
of a′, which, if we succeed, allows us to compute the order of a, since r = r′ × r/r′.
If we fail, then we obtain r′′ which is a factor of r/r′, and we now try to compute the
order of a′′ ≡(a′)r′′(mod N). We iterate this procedure until we determine the order of
a. At most log(r) = O(L) iterations are required, since each repetition reduces the order
of the current candidate a
′′... by a factor of at least two.
The third method is better than the ﬁrst two methods, in that it requires only a
constant number of trials, rather than O(L) repetitions. The idea is to repeat the phase
estimation-continued fractions procedure twice, obtaining r′
1, s′
1 the ﬁrst time, and r′
2, s′
2
the second time. Provided s′
1 and s′
2 have no common factors, r may be extracted by
taking the least common multiple of r1 and r2. The probability that s′
1 and s′
2 have no
common factors is given by
1 −

q
p(q|s′
1)p(q|s′
2) ,
(5.56)
where the sum is over all prime numbers q, and p(x|y) here means the probability of x
dividing y. If q divides s′
1 then it must also divide the true value of s, s1, on the ﬁrst
iteration, so to upper bound p(q|s′
1) it sufﬁces to upper bound p(q|s1), where s1 is chosen
uniformly at random from 0 through r −1. It is easy to see that p(q|s1) ≤1/q, and thus
p(q|s′
1) ≤1/q. Similarly, p(q|s′
2) ≤1/q, and thus the probability that s′
1 and s′
2 have no
common factors satisﬁes
1 −

q
p(q|s′
1)p(q|s′
2) ≥1 −

q
1
q2 .
(5.57)
The right hand side can be upper bounded in a number of ways; a simple technique is
provided in Exercise 5.16, which gives
1 −

q
p(q|s′
1)p(q|s′
2) ≥1
4 ,
(5.58)
and thus the probability of obtaining the correct r is at least 1/4.
Exercise 5.15:
Show that the least common multiple of positive integers x and y is
xy/ gcd(x, y), and thus may be computed in O(L2) operations if x and y are L
bit numbers.
Exercise 5.16:
For all x ≥2 prove that
7 x+1
x
1/y2 dy ≥2/3x2. Show that

q
1
q2 ≤3
2
- ∞
2
1
y2 dy = 3
4 ,
(5.59)
and thus that (5.58) holds.
232
The quantum Fourier transform and its applications
What resource requirements does this algorithm consume? The Hadamard transform
requires O(L) gates, and the inverse Fourier transform requires O(L2) gates. The major
cost in the quantum circuit proper actually comes from the modular exponentiation,
which uses O(L3) gates, for a total of O(L3) gates in the quantum circuit proper. The
continued fractions algorithm adds O(L3) more gates, for a total of O(L3) gates to obtain
r′. Using the third method for obtaining r from r′ we need only repeat this procedure a
constant number of times to obtain the order, r, for a total cost of O(L3). The algorithm
is summarized below.
Algorithm:
Quantum order-ﬁnding
Inputs: (1) A black box Ux,N which performs the transformation
|j⟩|k⟩→|j⟩|xjk mod N⟩, for x co-prime to the L-bit number N, (2)
t = 2L + 1 +
2log

2 + 1
2ϵ
3 qubits initialized to |0⟩, and (3) L qubits initialized
to the state |1⟩.
Outputs: The least integer r > 0 such that xr = 1 (mod N).
Runtime: O(L3) operations. Succeeds with probability O(1).
Procedure:
1.
|0⟩|1⟩
initial state
2.
→
1
√
2t
2t−1

j=0
|j⟩|1⟩
create superposition
3.
→
1
√
2t
2t−1

j=0
|j⟩|xj mod N⟩
apply Ux,N
≈
1
√
r2t
r−1

s=0
2t−1

j=0
e2πisj/r|j⟩|us⟩
4.
→
1
√r
r−1

s=0
|8
s/r⟩|us⟩
apply inverse Fourier transform to ﬁrst
register
5.
→8
s/r
measure ﬁrst register
6.
→r
apply continued fractions
algorithm
5.3.2
Application: factoring
The problem of distinguishing prime numbers from composites, and of resolving
composite numbers into their prime factors, is one of the most important and
useful in all of arithmetic. [ . . . ] The dignity of science seems to demand that
every aid to the solution of such an elegant and celebrated problem be zealously
cultivated.
– Carl Friedrich Gauss, as quoted by Donald Knuth
Given a positive composite integer N, what prime numbers when multiplied together
equal it? This factoring problem turns out to be equivalent to the order-ﬁnding problem
Applications: order-ﬁnding and factoring
233
we just studied, in the sense that a fast algorithm for order-ﬁnding can easily be turned
into a fast algorithm for factoring. In this section we explain the method used to reduce
factoring to order-ﬁnding, and give a simple example of this reduction.
The reduction of factoring to order-ﬁnding proceeds in two basic steps. The ﬁrst
step is to show that we can compute a factor of N if we can ﬁnd a non-trivial solution
x ̸= ± 1(mod N) to the equation x2 = 1(mod N). The second step is to show that a
randomly chosen y co-prime to N is quite likely to have an order r which is even, and
such that yr/2 ̸= ± 1(mod N), and thus x ≡yr/2(mod N) is a non-trivial solution to
x2 = 1(mod N). These two steps are embodied in the following theorems, whose proofs
may be found in Section A4.3 of Appendix 4.
Theorem 5.2: Suppose N is an L bit composite number, and x is a non-trivial solution
to the equation x2 = 1(mod N) in the range 1 ≤x ≤N, that is, neither
x = 1(mod N) nor x = N −1 = −1(mod N). Then at least one of
gcd(x −1, N) and gcd(x + 1, N) is a non-trivial factor of N that can be
computed using O(L3) operations.
Theorem 5.3: Suppose N = pα1
1 . . . pαm
m is the prime factorization of an odd composite
positive integer. Let x be an integer chosen uniformly at random, subject to the
requirements that 1 ≤x ≤N −1 and x is co-prime to N. Let r be the order of
x modulo N. Then
p(r is even and xr/2 ̸= −1(mod N)) ≥1 −1
2m .
(5.60)
Theorems 5.2 and 5.3 can be combined to give an algorithm which, with high prob-
ability, returns a non-trivial factor of any composite N. All the steps in the algorithm
can be performed efﬁciently on a classical computer except (so far as is known today) an
order-ﬁnding ‘subroutine’ which is used by the algorithm. By repeating the procedure
we may ﬁnd a complete prime factorization of N. The algorithm is summarized below.
Algorithm:
Reduction of factoring to order-ﬁnding
Inputs: A composite number N
Outputs: A non-trivial factor of N.
Runtime: O((log N)3) operations. Succeeds with probability O(1).
Procedure:
1.
If N is even, return the factor 2.
2.
Determine whether N = ab for integers a ≥1 and b ≥2, and if so
return the factor a (uses the classical algorithm of Exercise 5.17).
3.
Randomly choose x in the range 1 to N −1. If gcd(x, N) > 1 then return
the factor gcd(x, N).
4.
Use the order-ﬁnding subroutine to ﬁnd the order r of x modulo N.
234
The quantum Fourier transform and its applications
5.
If r is even and xr/2 ̸= −1(mod N) then compute gcd(xr/2 −1, N) and
gcd(xr/2 + 1, N), and test to see if one of these is a non-trivial factor,
returning that factor if so. Otherwise, the algorithm fails.
Steps 1 and 2 of the algorithm either return a factor, or else ensure that N is an
odd integer with more than one prime factor. These steps may be performed using
O(1) and O(L3) operations, respectively. Step 3 either returns a factor, or produces
a randomly chosen element x of {0, 1, 2, . . ., N −1}. Step 4 calls the order-ﬁnding
subroutine, computing the order r of x modulo N. Step 5 completes the algorithm,
since Theorem 5.3 guarantees that with probability at least one-half r will be even and
xr/2 ̸= −1(mod N), and then Theorem 5.2 guarantees that either gcd(xr/2 −1, N) or
gcd(xr/2 + 1, N) is a non-trivial factor of N. An example illustrating the use of this
algorithm with the quantum order-ﬁnding subroutine is shown in Box 5.4.
Exercise 5.17:
Suppose N is L bits long. The aim of this exercise is to ﬁnd an
efﬁcient classical algorithm to determine whether N = ab for some integers
a ≥1 and b ≥2. This may be done as follows:
(1) Show that b, if it exists, satisﬁes b ≤L.
(2) Show that it takes at most O(L2) operations to compute log2 N, x = y/b for
b ≤L, and the two integers u1 and u2 nearest to 2x.
(3) Show that it takes at most O(L2) operations to compute ub
1 and ub
2 (use
repeated squaring) and check to see if either is equal to N.
(4) Combine the previous results to give an O(L3) operation algorithm to
determine whether N = ab for integers a and b.
Exercise 5.18: (Factoring 91)
Suppose we wish to factor N = 91. Conﬁrm that
steps 1 and 2 are passed. For step 3, suppose we choose x = 4, which is co-prime
to 91. Compute the order r of x with respect to N, and show that
xr/2 mod 91 = 64 ̸= −1(mod 91), so the algorithm succeeds, giving
gcd(64 −1, 19) = 7.
It is unlikely that this is the most efﬁcient method you’ve seen for factoring 91.
Indeed, if all computations had to be carried out on a classical computer, this
reduction would not result in an efﬁcient factoring algorithm, as no efﬁcient
method is known for solving the order-ﬁnding problem on a classical computer.
Exercise 5.19:
Show that N = 15 is the smallest number for which the order-ﬁnding
subroutine is required, that is, it is the smallest composite number that is not
even or a power of some smaller integer.
5.4
General applications of the quantum Fourier transform
The main applications of the quantum Fourier transform we have described so far in
this chapter are phase estimation and order-ﬁnding. What other problems can be solved
with these techniques? In this section, we deﬁne a very general problem known as the
hidden subgroup problem, and describe an efﬁcient quantum algorithm for solving it. This
problem, which encompasses all known ‘exponentially fast’ applications of the quantum
Fourier transform, can be thought of as a generalization of the task of ﬁnding the unknown
period of a periodic function, in a context where the structure of the domain and range
General applications of the quantum Fourier transform
235
Box 5.4: Factoring 15 quantum-mechanically
The use of order-ﬁnding, phase estimation, and continued fraction expansions in
the quantum factoring algorithm is illustrated by applying it to factor N = 15.
First, we choose a random number which has no common factors with N; suppose
we choose x = 7. Next, we compute the order r of x with respect to N, using the
quantum order-ﬁnding algorithm: begin with the state |0⟩|0⟩and create the state
1
√
2t
2t−1

k=0
|k⟩|0⟩=
1
√
2t

|0⟩+ |1⟩+ |2⟩+ · · · + |2t −1⟩
	
|0⟩
(5.61)
by applying t = 11 Hadamard transforms to the ﬁrst register. Choosing this value
of t ensures an error probability ϵ of at most 1/4. Next, compute f(k) = xk mod N,
leaving the result in the second register,
1
√
2t
2t−1

k=0
|k⟩|xk mod N⟩
(5.62)
=
1
√
2t

|0⟩|1⟩+ |1⟩|7⟩+ |2⟩|4⟩+ |3⟩|13⟩+ |4⟩|1⟩+ |5⟩|7⟩+ |6⟩|4⟩+ · · ·
	
.
We now apply the inverse Fourier transform FT † to the ﬁrst register and measure
it. One way of analyzing the distribution of outcomes obtained is to calculate the
reduced density matrix for the ﬁrst register, and apply FT † to it, and calculate the
measurement statistics. However, since no further operation is applied to the second
register, we can instead apply the principle of implicit measurement (Section 4.4)
and assume that the second register is measured, obtaining a random result from 1,
7, 4, or 13. Suppose we get 4 (any of the results works); this means the state input
to FT † would have been

4
2t

|2⟩+ |6⟩+ |10⟩+ |14⟩+ · · ·
	
. After applying FT †
we obtain some state 
ℓαℓ|ℓ⟩, with the probability distribution
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0.05
0.1
0.15
0.2
0.25
|αℓ|2
ℓ
shown for 2t = 2048. The ﬁnal measurement therefore gives either 0, 512, 1024,
or 1536, each with probability almost exactly 1/4. Suppose we obtain ℓ= 1536
from the measurement; computing the continued fraction expansion thus gives
1536/2048 = 1/(1 + (1/3)), so that 3/4 occurs as a convergent in the expan-
sion, giving r = 4 as the order of x = 7. By chance, r is even, and moreover,
xr/2 mod N = 72 mod 15 = 4 ̸= −1 mod 15, so the algorithm works: computing
the greatest common divisor gcd(x2 −1, 15) = 3 and gcd(x2 + 1, 15) = 5 tells us
that 15 = 3×5.
236
The quantum Fourier transform and its applications
of the function may be very intricate. In order to present this problem in the most
approachable manner, we begin with two more speciﬁc applications: period-ﬁnding (of a
one-dimensional function), and discrete logarithms. We then return to the general hidden
subgroup problem. Note that the presentation in this section is rather more schematic
and conceptual than earlier sections in this chapter; of necessity, this means that the
reader interested in understanding all the details will have to work much harder!
5.4.1
Period-ﬁnding
Consider the following problem. Suppose f is a periodic function producing a single
bit as output and such that f(x + r) = f(x), for some unknown 0 < r < 2L, where
x, r ∈{0, 1, 2, . . .}. Given a quantum black box U which performs the unitary trans-
form U|x⟩|y⟩→|x⟩|y ⊕f(x)⟩(where ⊕denotes addition modulo 2) how many black
box queries and other operations are required to determine r? Note that in practice U
operates on a ﬁnite domain, whose size is determined by the desired accuracy for r. Here
is a quantum algorithm which solves this problem using one query, and O(L2) other
operations:
Algorithm:
Period-ﬁnding
Inputs: (1) A black box which performs the operation U|x⟩|y⟩= |x⟩|y ⊕f(x)⟩,
(2) a state to store the function evaluation, initialized to |0⟩, and (3)
t = O(L + log(1/ϵ)) qubits initialized to |0⟩.
Outputs: The least integer r > 0 such that f(x + r) = f(x).
Runtime: One use of U, and O(L2) operations. Succeeds with probability O(1).
Procedure:
1.
|0⟩|0⟩
initial state
2.
→
1
√
2t
2t−1

x=0
|x⟩|0⟩
create superposition
3.
→
1
√
2t
2t−1

x=0
|x⟩|f(x)⟩
apply U
≈
1
√
r2t
r−1

ℓ=0
2t−1

x=0
e2πiℓx/r|x⟩| ˆf(ℓ)⟩
4.
→
1
√r
r−1

ℓ=0
|8
ℓ/r⟩| ˆf(ℓ)⟩
apply inverse Fourier transform to ﬁrst
register
5.
→8
ℓ/r
measure ﬁrst register
6.
→r
apply continued fractions
algorithm
The key to understanding this algorithm, which is based on phase estimation, and
is nearly identical to the algorithm for quantum order-ﬁnding, is step 3, in which we
introduce the state
| ˆf(ℓ)⟩≡
1
√r
r−1

x=0
e−2πiℓx/r|f(x)⟩,
(5.63)
General applications of the quantum Fourier transform
237
the Fourier transform of |f(x)⟩. The identity used in step 3 is based on
|f(x)⟩=
1
√r
r−1

ℓ=0
e2πiℓx/r| ˆf(ℓ)⟩,
(5.64)
which is easy to verify by noting that r−1
ℓ=0 e2πiℓx/r = r for x an integer multiple of r,
and zero otherwise. The approximate equality in step 3 is required because 2t may not be
an integer multiple of r in general (it need not be: this is taken account of by the phase
estimation bounds). By Equation (5.22), applying the inverse Fourier transform to the
ﬁrst register, in step 4, gives an estimate of the phase ℓ/r, where ℓis chosen randomly.
r can be efﬁciently obtained in the ﬁnal step using a continued fraction expansion.
Box 5.5: The shift-invariance property of the Fourier transform
The Fourier transform, Equation (5.1), has an interesting and very useful property,
known as shift invariance. Using notation which is useful in describing the general
application of this property, let us describe the quantum Fourier transform as

h∈H
αh|h⟩→

g∈G
˜αg|g⟩,
(5.65)
where ˜αg = 
h∈H αh exp(2πigh/|G|), H is some subset of G, and G indexes the
states in an orthonormal basis of the Hilbert space. For example, G may be the set
of numbers from 0 to 2n −1 for an n qubit system. |G| denotes the number of
elements in G. Suppose we apply to the initial state an operator Uk which performs
the unitary transform
Uk|g⟩= |g + k⟩,
(5.66)
then apply the Fourier transform. The result,
Uk

h∈H
αh|h⟩=

h∈H
αh|h + k⟩→

g∈G
e2πigk/|G| ˜αg|g⟩
(5.67)
has the property that the magnitude of the amplitude for |g⟩does not change, no
matter what k is, that is: | exp(2πigk/|G|) ˜αg| = | ˜αg|.
In the language of group theory, G is a group, H a subgroup of G, and we say that
if a function f on G is constant on cosets of H, then the Fourier transform of f is
invariant over cosets of H.
Why does this work? One way to understand this is to realize that (5.63) is approxi-
mately the Fourier transform over {0, 1, . . ., 2L −1} of |f(x)⟩(see Exercise 5.20), and
the Fourier transform has an interesting and very useful property, known as shift invari-
ance, described in Box 5.5. Another is to realize that what the order-ﬁnding algorithm
does is just to ﬁnd the period of the function f(k) = xk mod N, so the ability to ﬁnd the
period of a general periodic function is not unexpected. Yet another way is to realize that
the implementation of the black box U is naturally done using a certain unitary operator
whose eigenvectors are precisely | ˆf(ℓ)⟩, as described in Exercise 5.21 below, so that the
phase estimation procedure of Section 5.2 can be applied.
238
The quantum Fourier transform and its applications
Exercise 5.20:
Suppose f(x + r) = f(x), and 0 ≤x < N, for N an integer multiple
of r. Compute
ˆf(ℓ) ≡
1
√
N
N−1

x=0
e−2πiℓx/Nf(x) ,
(5.68)
and relate the result to (5.63). You will need to use the fact that

k∈{0,r,2r,...,N−r}
e2πikℓ/N =
* 
N/r
if ℓis an integer multiple of N/r
0
otherwise.
(5.69)
Exercise 5.21: (Period-ﬁnding and phase estimation)
Suppose you are given a
unitary operator Uy which performs the transformation Uy|f(x)⟩= |f(x + y)⟩,
for the periodic function described above.
(1) Show that the eigenvectors of Uy are | ˆf(ℓ)⟩, and calculate their eigenvalues.
(2) Show that given |f(x0)⟩for some x0, Uy can be used to realize a black box
which is as useful as U in solving the period-ﬁnding problem.
5.4.2
Discrete logarithms
The period ﬁnding problem we just considered is a simple one, in that the domain and
range of the periodic function were integers. What happens when the function is more
complex? Consider the function f(x1, x2) = asx1+x2 mod N, where all the variables
are integers, and r is the smallest positive integer for which ar mod N = 1. This
function is periodic, since f(x1 +ℓ, x2 −ℓs) = f(x1, x2), but now the period is a 2-tuple,
(ℓ, −ℓs), for integer ℓ. This may seem to be a strange function, but it is very useful
in cryptography, since determining s allows one to solve what is known as the discrete
logarithm problem: given a and b = as, what is s? Here is a quantum algorithm which
solves this problem using one query of a quantum black box U which performs the unitary
transform U|x1⟩|x2⟩|y⟩→|x1⟩|x2⟩|y ⊕f(x)⟩(where ⊕denotes bitwise addition modulo
2), and O(⌈log r⌉2) other operations. We assume knowledge of the minimum r > 0 such
that ar mod N = 1, which can be obtained using the order-ﬁnding algorithm described
previously.
Algorithm:
Discrete logarithm
Inputs: (1) A black box which performs the operation
U|x1⟩|x2⟩|y⟩= |x1⟩|x2⟩|y ⊕f(x1, x2)⟩, for f(x1, x2) = bx1ax2, (2) a state to store
the function evaluation, initialized to |0⟩, and (3) two t = O(⌈log r⌉+ log(1/ϵ))
qubit registers initialized to |0⟩.
Outputs: The least positive integer s such that as = b.
Runtime: One use of U, and O(⌈log r⌉2) operations. Succeeds with probability
O(1).
Procedure:
1.
|0⟩|0⟩|0⟩
initial state
General applications of the quantum Fourier transform
239
2.
→1
2t
2t−1

x1=0
2t−1

x2=0
|x1⟩|x2⟩|0⟩
create superposition
3.
→1
2t
2t−1

x1=0
2t−1

x2=0
|x1⟩|x2⟩|f(x1, x2)⟩
apply U
≈
1
2t√r
r−1

ℓ2=0
2t−1

x1=0
2t−1

x2=0
e2πi(sℓ2x1+ℓ2x2)/r|x1⟩|x2⟩| ˆf(sℓ2, ℓ2)⟩
=
1
2t√r
r−1

ℓ2=0
⎡
⎣
2t−1

x1=0
e2πi(sℓ2x1)/r|x1⟩
⎤
⎦
⎡
⎣
2t−1

x2=0
e2πi(ℓ2x2)/r|x2⟩
⎤
⎦| ˆf(sℓ2, ℓ2)⟩
4.
→
1
√r
r−1

ℓ2=0
| 8
sℓ2/r⟩| 8
ℓ2/r⟩| ˆf(sℓ2, ℓ2)⟩
apply inverse Fourier transform to ﬁrst
two registers
5.
→
' 8
sℓ2/r, 8
ℓ2/r
(
measure ﬁrst two registers
6.
→s
apply generalized continued
fractions algorithm
Again, the key to understanding this algorithm is step 3, in which we introduce the
state
| ˆf(ℓ1, ℓ2)⟩=
1
√r
r−1

j=0
e−2πiℓ2j/r|f(0, j)⟩,
(5.70)
the Fourier transform of |f(x1, x2)⟩(see Exercise 5.22). In this equation, the values of ℓ1
and ℓ2 must satisfy
r−1

k=0
e2πik(ℓ1/s−ℓ2)/r = r .
(5.71)
Otherwise, the amplitude of | ˆf(ℓ1, ℓ2)⟩is nearly zero. The generalized continued fraction
expansion used in the ﬁnal step to determine s is analogous to the procedures used in
Section 5.3.1, and is left as a simple exercise for you to construct.
Exercise 5.22:
Show that
| ˆf(ℓ1, ℓ2)⟩=
r−1

x1=0
r−1

x2=0
e−2πi(ℓ1x1+ℓ2x2)/r|f(x1, x2)⟩=
1
√r
r−1

j=0
e−2πiℓ2j/r|f(0, j)⟩,
(5.72)
and we are constrained to have ℓ1/s −ℓ2 be an integer multiple of r for this
expression to be non-zero.
Exercise 5.23:
Compute
1
r
r−1

ℓ1=0
r−1

ℓ2=0
e−2πi(ℓ1x1+ℓ2x2)/r| ˆf(ℓ1, ℓ2)⟩
(5.73)
using (5.70), and show that the result is f(x1, x2).
Exercise 5.24:
Construct the generalized continued fractions algorithm needed in
240
The quantum Fourier transform and its applications
step 6 of the discrete logarithm algorithm to determine s from estimates of sℓ2/r
and ℓ2/r.
Exercise 5.25:
Construct a quantum circuit for the black box U used in the quantum
discrete logarithm algorithm, which takes a and b as parameters, and performs
the unitary transform |x1⟩|x2⟩|y⟩→|x1⟩|x2⟩|y ⊕bx1ax2⟩. How many elementary
operations are required?
5.4.3
The hidden subgroup problem
By now, a pattern should be coming clear: if we are given a periodic function, even when
the structure of the periodicity is quite complicated, we can often use a quantum algorithm
to determine the period efﬁciently. Importantly, however, not all periods of periodic
functions can be determined. The general problem which deﬁnes a broad framework
for these questions can be succinctly expressed in the language of group theory (see
Appendix 2 for a quick review) as follows:
Let f be a function from a ﬁnitely generated group G to a ﬁnite set X such that
f is constant on the cosets of a subgroup K, and distinct on each coset. Given a
quantum black box for performing the unitary transform U|g⟩|h⟩= |g⟩|h⊕f(g)⟩,
for g ∈G, h ∈X, and ⊕an appropriately chosen binary operation on X, ﬁnd a
generating set for K.
Order-ﬁnding, period-ﬁnding, discrete logarithms, and many other problems are in-
stances of this hidden subgroup problem; some interesting ones are listed in Figure 5.5.
For G a ﬁnite Abelian group, a quantum computer can solve the hidden subgroup
problem using a number of operations polynomial in log |G|, and one use of the black
box function evaluation, using an algorithm very similar to the others in this section.
(In fact, solution for a ﬁnitely generated Abelian group is also possible, along similar
lines, but we’ll stick to the ﬁnite case here.) We shall leave detailed speciﬁcation of the
algorithm to you as an exercise, which should be simple after we explain the basic idea.
Many things remain essentially the same, because ﬁnite Abelian groups are isomorphic
to products of additive groups over the integers in modular arithmetic. This means that
the quantum Fourier transform of f over G is well deﬁned (see Section A2.3), and can
still be done efﬁciently. The ﬁrst non-trivial step of the algorithm is to use a Fourier
transform (generalizing the Hadamard operation) to create a superposition over group
elements, which is then transformed by applying the quantum black box for f in the next
step, to give
1

|G|

g∈G
|g⟩|f(g)⟩.
(5.74)
As before, we would now like to rewrite |f(g)⟩in the Fourier basis. We start with
|f(g)⟩=
1

|G|
|G|−1

ℓ=0
e2πiℓg/|G|| ˆf(ℓ)⟩,
(5.75)
where we have chosen exp[−2πiℓg/|G|] as a representation (see Exercise A2.13) of g ∈G
indexed by ℓ(the Fourier transform maps between group elements and representations:
see Exercise A2.23). The key is to recognize that this expression can be simpliﬁed because
General applications of the quantum Fourier transform
241
Name
G
X
K
Function
Deutsch
{0, 1}, ⊕
{0, 1}
{0} or {0, 1}
K = {0, 1} :
* f(x) = 0
f(x) = 1
K = {0} :
* f(x) = x
f(x) = 1 −x
Simon
{0, 1}n, ⊕
any
ﬁnite
set
{0, s}
s ∈{0, 1}n
f(x ⊕s) = f(x)
Period-
ﬁnding
Z, +
any
ﬁnite
set
{0, r, 2r, . . .}
r ∈G
f(x + r) = f(x)
Order-
ﬁnding
Z, +
{aj}
j ∈Zr
ar = 1
{0, r, 2r, . . .}
r ∈G
f(x) = ax
f(x + r) = f(x)
Discrete
logarithm
Zr × Zr
+ (mod r)
{aj}
j ∈Zr
ar = 1
(ℓ, −ℓs)
ℓ, s ∈Zr
f(x1, x2) = akx1+x2
f(x1 + ℓ, x2 −ℓs) = f(x1, x2)
Order of a
permutation
Z2m × Z2n
+ (mod 2m)
Z2n
{0, r, 2r, . . .}
r ∈X
f(x, y) = πx(y)
f(x + r, y) = f(x, y)
π = permutation on X
Hidden
linear
function
Z × Z, +
ZN
(ℓ, −ℓs)
ℓ, s ∈X
f(x1, x2) =
π(sx1 + x2 mod N)
π = permutation on X
Abelian
stabilizer
(H, X)
H = any
Abelian
group
any
ﬁnite
set
{s ∈H |
f(s, x) = x,
∀x ∈X}
f(gh, x) = f(g, f(h, x))
f(gs, x) = f(g, x)
Figure 5.5. Hidden subgroup problems. The function f maps from the group G to the ﬁnite set X, and is
promised to be constant on cosets of the hidden subgroup K ⊆G. ZN represents the set {0, 1, . . . , N −1} in
this table, and Z is the integers. The problem is to ﬁnd K (or a generating set for it), given a black box for f.
f is constant and distinct on cosets of the subgroup K, so that
| ˆf(ℓ)⟩=
1

|G|

g∈G
e−2πiℓg/|G||f(g)⟩
(5.76)
has nearly zero amplitude for all values of ℓexcept those which satisfy

h∈K
e−2πiℓh/|G| = |K| .
(5.77)
242
The quantum Fourier transform and its applications
If we can determine ℓ, then using the linear constraints given by this expression allows
us to determine elements of K, and since K is Abelian, this allows us to eventually
determine a generating set for the whole hidden subgroup, solving the problem.
However, life is not so simple. An important reason why the period-ﬁnding and discrete
logarithm algorithms work is because of the success of the continued fraction expansion
in obtaining ℓfrom ℓ/|G|. In those problems, ℓand |G| are arranged to not have any
common factors, with high probability. In the general case, however, this may not be
true, since |G| is free to be a composite number with many factors, and we have no
useful prior information about ℓ.
Fortunately, this problem can be solved: as mentioned above, any ﬁnite Abelian group
G is isomorphic to a product of cyclic groups of prime power order, that is, G = Zp1 ×
Zp2 ×· · ·×ZpM, where pi are primes, and Zpi is the group over integers {0, 1, . . ., pi−1}
with addition modulo pi being the group operation. We can thus re-express the phase
which appears in (5.75) as
e2πiℓg/|G| =
M
/
i=1
e2πiℓ′
igi/pi
(5.78)
for gi ∈Zpi. The phase estimation procedure now gives us ℓ′
i, from which we determine
ℓ, and thus, sample K as described above, to solve the hidden subgroup problem.
Exercise 5.26:
Since K is a subgroup of G, when we decompose G into a product of
cyclic groups of prime power order, this also decomposes K. Re-express (5.77)
to show that determining ℓ′
i allows one to sample from the corresponding cyclic
subgroup Kpi of K.
Exercise 5.27:
Of course, the decomposition of a general ﬁnite Abelian group G into a
product of cyclic groups of prime power order is usually a difﬁcult problem (at
least as hard as factoring integers, for example). Here, quantum algorithms come
to the rescue again: explain how the algorithms in this chapter can be used to
efﬁciently decompose G as desired.
Exercise 5.28:
Write out a detailed speciﬁcation of the quantum algorithm to solve
the hidden subgroup problem, complete with runtime and success probability
estimates, for ﬁnite Abelian groups.
Exercise 5.29:
Give quantum algorithms to solve the Deutsch and Simon problems
listed in Figure 5.5, using the framework of the hidden subgroup problem.
5.4.4
Other quantum algorithms?
One of the most intriguing aspects of this framework for describing quantum algorithms
in terms of the hidden subgroup problem is the suggestion that more difﬁcult prob-
lems might be solvable by considering various groups G and functions f. We have only
described the solution of this problem for Abelian groups. What about non-Abelian
groups? They are quite interesting (see Appendix 2 for a discussion of general Fourier
transforms over non-Abelian groups): for example, the problem of graph isomorphism is
to determine if two given graphs are the same under some permutation of the labels of
the n vertices (Section 3.2.3). These permutations can be described as transformations
under the symmetric group Sn, and algorithms for performing fast Fourier transforms
Chapter problems
243
over these groups exists. However, a quantum algorithm for efﬁciently solving the graph
isomporphism problem remains unknown.
Even if more general cases of the hidden subgroup problem remain unsolvable by
quantum computers, having this unifying framework is useful, because it allows us to
ask questions about how one might be able to step outside its limitations. It is difﬁcult
to believe that all fast quantum algorithms that will ever be discovered will be just ways
to solve the hidden subgroup problem. If one thinks of these problems as being based on
the coset invariance property of the Fourier transform, in searching for new algorithms,
perhaps the thing to do then is to investigate other transforms with different invariances.
Going in another direction, one might ask: what difﬁcult hidden subgroup problems
might be efﬁciently solved given an arbitrary (but speciﬁed independently of the problem)
quantum state as a helper? After all, as discussed in Chapter 4, most quantum states are
actually exponentially hard to construct. Such a state might be a useful resource (a real
‘quantum oracle’), if quantum algorithms existed to utilize them to solve hard problems!
The hidden subgroup problem also captures an important constraint underlying the
class of quantum algorithms which are exponentially faster than their (known) classical
counterparts: this is a promise problem, meaning that it is of the form ‘F(X) is promised
to have such and such property: characterize that property.’ Rather disappointingly,
perhaps, we shall show at the end of the next chapter that, in solving problems without
some sort of promise, quantum computers cannot achieve an exponential speedup over
classical computers; the best speedup is polynomial. On the other hand, this gives us an
important clue as to what kinds of problems quantum computers might be good at: in
retrospect, the hidden subgroup problem might be thought of as a natural candidate for
quantum computation. What other natural problems are there? Think about it!
Problem 5.1:
Construct a quantum circuit to perform the quantum Fourier transform
|j⟩−→
1
√p
p−1

k=0
e2πijk/p|k⟩
(5.79)
where p is prime.
Problem 5.2: (Measured quantum Fourier transform)
Suppose the quantum
Fourier transform is performed as the last step of a quantum computation,
followed by a measurement in the computational basis. Show that the
combination of quantum Fourier transform and measurement is equivalent to a
circuit consisting entirely of one qubit gates and measurement, with classical
control, and no two qubit gates. You may ﬁnd the discussion of Section 4.4
useful.
Problem 5.3: (Kitaev’s algorithm)
Consider the quantum circuit

________

_ _ _ _ _ _ _ _

where |u⟩is an eigenstate of U with eigenvalue e2πiϕ. Show that the top qubit is
244
The quantum Fourier transform and its applications
measured to be 0 with probability p ≡cos2(πϕ). Since the state |u⟩is unaffected
by the circuit it may be reused; if U can be replaced by U k, where k is an
arbitrary integer under your control, show that by repeating this circuit and
increasing k appropriately, you can efﬁciently obtain as many bits of p as desired,
and thus, of ϕ. This is an alternative to the phase estimation algorithm.
Problem 5.4:
The runtime bound O(L3) we have given for the factoring algorithm is
not tight. Show that a better upper bound of O(L2 log L log log L) operations can
be achieved.
Problem 5.5: (Non-Abelian hidden subgroups – Research)
Let f be a function
on a ﬁnite group G to an arbitrary ﬁnite range X, which is promised to be
constant and distinct on distinct left cosets of a subgroup K. Start with the state
1

|G|m

g1,...,gm
|g1, . . . , gm⟩|f(g1), . . . , f(gm)⟩,
(5.80)
and prove that picking m = 4 log |G| + 2 allows K to be identiﬁed with
probability at least 1 −1/|G|. Note that G does not necessarily have to be
Abelian, and being able to perform a Fourier transform over G is not required.
This result shows that one can produce (using only O(log |G|) oracle calls) a ﬁnal
result in which the pure state outcomes corresponding to different possible
hidden subgroups are nearly orthogonal. However, it is unknown whether a
POVM exists or not which allows the hidden subgroup to be identiﬁed
efﬁciently (i.e. using poly(log |G|) operations) from this ﬁnal state.
Problem 5.6: (Addition by Fourier transforms)
Consider the task of constructing
a quantum circuit to compute |x⟩→|x + y mod 2n⟩, where y is a ﬁxed constant,
and 0 ≤x < 2n. Show that one efﬁcient way to do this, for values of y such as
1, is to ﬁrst perform a quantum Fourier transform, then to apply single qubit
phase shifts, then an inverse Fourier transform. What values of y can be added
easily this way, and how many operations are required?
History and further reading
245
Summary of Chapter 5: The quantum Fourier transform and its
applications
• When N = 2n the quantum Fourier transform
|j⟩= |j1, . . . , jn⟩−→
1
√
N
N−1

k=0
e2πi jk
N |k⟩
(5.81)
may be written in the form
|j⟩→
1
2n/2

|0⟩+ e2πi0.jn|1⟩
 
|0⟩+ e2πi0.jn−1jn|1⟩
 . . .

|0⟩+ e2πi0.j1j2...jn|1⟩
 ,
(5.82)
and may be implemented using Θ(n2) gates.
• Phase estimation: Let |u⟩be an eigenstate of the operator U with eigenvalue
e2πiϕ. Starting from the initial state |0⟩⊗t|u⟩, and given the ability to efﬁciently
perform U 2k for integer k, this algorithm (shown in Figure 5.3) can be used
to efﬁciently obtain the state | ˜ϕ⟩|u⟩, where ˜ϕ accurately approximates ϕ to t −
2log

2 + 1
2ϵ
3 bits with probability at least 1 −ϵ.
• Order-ﬁnding: The order of x modulo N is the least positive integer r such that
xr mod N = 1. This number can be computed in O(L3) operations using the
quantum phase estimation algorithm, for L-bit integers x and N.
• Factoring: The prime factors of an L-bit integer N can be determined in O(L3)
operations by reducing this problem to ﬁnding the order of a random number x
co-prime with N.
• Hidden subgroup problem: All the known fast quantum algorithms can be
described as solving the following problem: Let f be a function from a ﬁnitely
generated group G to a ﬁnite set X such that f is constant on the cosets of a
subgroup K, and distinct on each coset. Given a quantum black box for performing
the unitary transform U|g⟩|h⟩= |g⟩|h ⊕f(g)⟩, for g ∈G and h ∈X, ﬁnd a
generating set for K.
History and further reading
The deﬁnition of the Fourier transform may be generalized beyond what we have con-
sidered in this chapter. In the general scenario a Fourier transform is deﬁned on a set
of complex numbers αg, where the index g is chosen from some group, G. In this
chapter we have chosen G to be the additive group of integers modulo 2n, often de-
noted Z2n. Deutsch[Deu85] showed that the Fourier transform over the group Zn
2 could
be implemented efﬁciently on a quantum computer – this is the Hadamard transform
of earlier chapters. Shor [Sho94] realized to spectacular effect that quantum computers
could efﬁciently implement the quantum Fourier transform over groups Zm for certain
special values of m. Inspired by this result Coppersmith[Cop94], Deutsch (unpublished),
and Cleve (unpublished) gave the simple quantum circuits for computing the quantum
Fourier transform over Z2n which we have used in this chapter. Cleve, Ekert, Mac-
246
The quantum Fourier transform and its applications
chiavello and Mosca[CEMM98] and Grifﬁths and Niu[GN96] independently discovered the
product formula (5.4); in fact, this result had been realized much earlier by Danielson
and Lanczos. The simpliﬁed proof starting in Equation (5.5) was suggested by Zhou.
Grifﬁths and Niu[GN96] are responsible for the measured quantum Fourier transform
found in Problem 5.2.
The Fourier transform over Z2n was generalized to obtain a Fourier transform over
an arbitrary ﬁnite Abelian group by Kitaev[Kit95], who also introduced the phase esti-
mation procedure in the form given in Problem 5.3. Cleve, Ekert, Macchiavello and
Mosca[CEMM98] also integrated several of the techniques of Shor and Kitaev into one
nice picture, upon which Section 5.2 is based. A good description of the phase estimation
algorithm can be found in Mosca’s Ph.D. thesis[Mos99].
Shor announced the quantum order-ﬁnding algorithm in a seminal paper in 1994[Sho94],
and noted that the problems of performing discrete logarithms and factoring could be
reduced to order-ﬁnding. The ﬁnal paper, including extended discussion and references,
was published in 1997[Sho97]. This paper also contains a discussion of clever multi-
plication methods that may be used to speed up the algorithm even further than in
our description, which uses relatively naive multiplication techniques. With these faster
multiplication methods the resources required to factor a composite integer n scale as
O(n2 log n log log n), as claimed in the introduction to the chapter. In 1995 Kitaev[Kit95]
announced an algorithm for ﬁnding the stabilizer of a general Abelian group, which he
showed could be used to solve discrete logarithm and factoring as special cases. In addi-
tion, this algorithm contained several elements not present in Shor’s algorithm. A good
review of the factoring algorithm was written by Ekert and Jozsa [EJ96]; also see DiVin-
cenzo [DiV95a]. The discussion of continued fractions is based upon Chapter 10 of Hardy
and Wright[HW60]. At the time of writing, the most efﬁcient classical algorithm for fac-
toring on a classical computer is the number ﬁeld sieve. This is described in a collection
edited by A. K. Lenstra and H. W. Lenstra, Jr.[LL93].
The generalization of quantum algorithms to solving the hidden subgroup problem has
been considered by many authors. Historically, Simon was ﬁrst to note that a quantum
computer could ﬁnd a hidden period of a function satisfying f(x⊕s) = f(x)[Sim94, Sim97].
In fact, Shor found his result by generalizing Simon’s result, and by applying a Fourier
transform over ZN instead of Simon’s Hadamard transforms (a Fourier transform over
Zk
2 ). Boneh and Lipton then noted the connection to the hidden subgroup problem,
and described a quantum algorithm for solving the hidden linear function problem[BL95].
Jozsa was the ﬁrst to explicitly provide a uniform description of the Deutsch–Jozsa, Si-
mon, and Shor algorithms in terms of the hidden subgroup problem[Joz97]. Ekert and
Jozsa’s work in studying the role of the Abelian and non-Abelian Fast Fourier Trans-
form algorithms in speedup of quantum algorithms[EJ98] has also been insightful. Our
description of the hidden subgroup problem in Section 5.4 follows the framework of
Mosca and Ekert[ME99, Mos99]. Cleve has proven that the problem of ﬁnding an order of a
permutation requires an exponential number of queries for a bounded-error probabilistic
classical computer[Cle99]. Generalizations of this method to beyond Abelian groups have
been attempted by Ettinger and Høyer[EH99], by Roetteler and Beth[RB98] and Pueschel,
Roetteler, and Beth[PRB98], by Beals, who also described constructions of quantum Fourier
transforms over the symmetric group[BBC+98], and by Ettinger, Høyer, and Knill[EHK99].
These results have shown, so far, that there exists a quantum algorithm to solve the
History and further reading
247
hidden subgroup problem for non-Abelian groups using only O(log |G|) oracle calls, but
whether this can be realized in polynomial time is unknown (Problem 5.5).
6 Quantum search algorithms
Suppose you are given a map containing many cities, and wish to determine the shortest
route passing through all cities on the map. A simple algorithm to ﬁnd this route is to
search all possible routes through the cities, keeping a running record of which route has
the shortest length. On a classical computer, if there are N possible routes, it obviously
takes O(N) operations to determine the shortest route using this method. Remarkably,
there is a quantum search algorithm, sometimes known as Grover’s algorithm, which
enables this search method to be sped up substantially, requiring only O(
√
N) operations.
Moreover, the quantum search algorithm is general in the sense that it can be applied
far beyond the route-ﬁnding example just described to speed up many (though not all)
classical algorithms that use search heuristics.
In this chapter we explain the fast quantum search algorithm. The basic algorithm is
described in Section 6.1. In Section 6.2 we derive the algorithm from another point of
view, based on the quantum simulation algorithm of Section 4.7. Three important appli-
cations of this algorithm are also described: quantum counting in Section 6.3, speedup of
solution of NP-complete problems in Section 6.4, and search of unstructured databases
in Section 6.5. One might hope to improve upon the search algorithm to do even better
than a square root speedup but, as we show in Section 6.6, it turns out this is not possible.
We conclude in Section 6.7 by showing that this speed limit applies to most unstructured
problems.
6.1
The quantum search algorithm
Let us begin by setting the stage for the search algorithm in terms of an oracle, similar to
that encountered in Section 3.1.1. This allows us to present a very general description of
the search procedure, and a geometric way to visualize its action and see how it performs.
6.1.1
The oracle
Suppose we wish to search through a search space of N elements. Rather than search the
elements directly, we concentrate on the index to those elements, which is just a number
in the range 0 to N −1. For convenience we assume N = 2n, so the index can be stored
in n bits, and that the search problem has exactly M solutions, with 1 ≤M ≤N. A
particular instance of the search problem can conveniently be represented by a function
f, which takes as input an integer x, in the range 0 to N −1. By deﬁnition, f(x) = 1 if
x is a solution to the search problem, and f(x) = 0 if x is not a solution to the search
problem.
Suppose we are supplied with a quantum oracle – a black box whose internal workings
we discuss later, but which are not important at this stage – with the ability to recognize
solutions to the search problem. This recognition is signalled by making use of an oracle
The quantum search algorithm
249
qubit. More precisely, the oracle is a unitary operator, O, deﬁned by its action on the
computational basis:
|x⟩|q⟩
O→|x⟩|q ⊕f(x)⟩,
(6.1)
where |x⟩is the index register, ⊕denotes addition modulo 2, and the oracle qubit |q⟩is
a single qubit which is ﬂipped if f(x) = 1, and is unchanged otherwise. We can check
whether x is a solution to our search problem by preparing |x⟩|0⟩, applying the oracle,
and checking to see if the oracle qubit has been ﬂipped to |1⟩.
In the quantum search algorithm it is useful to apply the oracle with the oracle qubit
initially in the state (|0⟩−|1⟩)/
√
2, just as was done in the Deutsch–Jozsa algorithm of
Section 1.4.4. If x is not a solution to the search problem, applying the oracle to the state
|x⟩(|0⟩−|1⟩)/
√
2 does not change the state. On the other hand, if x is a solution to the
search problem, then |0⟩and |1⟩are interchanged by the action of the oracle, giving a
ﬁnal state −|x⟩(|0⟩−|1⟩)/
√
2. The action of the oracle is thus:
|x⟩
|0⟩−|1⟩
√
2

O
−→(−1)f(x)|x⟩
|0⟩−|1⟩
√
2

.
(6.2)
Notice that the state of the oracle qubit is not changed. It turns out that this remains
(|0⟩−|1⟩)/
√
2 throughout the quantum search algorithm, and can therefore be omitted
from further discussion of the algorithm, simplifying our description.
With this convention, the action of the oracle may be written:
|x⟩
O
−→(−1)f(x)|x⟩.
(6.3)
We say that the oracle marks the solutions to the search problem, by shifting the phase
of the solution. For an N item search problem with M solutions, it turns out that we
need only apply the search oracle O(

N/M) times in order to obtain a solution, on a
quantum computer.
This discussion of the oracle without describing how it works in practice is rather
abstract, and perhaps even puzzling. It seems as though the oracle already knows the
answer to the search problem; what possible use could it be to have a quantum search
algorithm based upon such oracle consultations?! The answer is that there is a distinction
between knowing the solution to a search problem, and being able to recognize the
solution; the crucial point is that it is possible to do the latter without necessarily being
able to do the former.
A simple example to illustrate this is the problem of factoring. Suppose we have been
given a large number, m, and told that it is a product of two primes, p and q – the
same sort of situation as arises in trying to break the RSA public key cryptosystem
(Appendix 5). To determine p and q, the obvious method on a classical computer is to
search all numbers from 2 through m1/2 for the smaller of the two prime factors. That
is, we successively do a trial division of m by each number in the range 2 to m1/2, until
we ﬁnd the smaller prime factor. The other prime factor can then be found by dividing
m by the smaller prime. Obviously, this search-based method requires roughly m1/2 trial
divisions to ﬁnd a factor on a classical computer.
The quantum search algorithm can be used to speed up this process. By deﬁnition,
the action of the oracle upon input of the state |x⟩is to divide m by x, and check to see if
the division is exact, ﬂipping the oracle qubit if this is so. Applying the quantum search
algorithm with this oracle yields the smaller of the two prime factors with high probability.
250
Quantum search algorithms
But to make the algorithm work, we need to construct an efﬁcient circuit implementing
the oracle. How to do this is an exercise in the techniques of reversible computation.
We begin by deﬁning the function f(x) ≡1 if x divides m, and f(x) = 0 otherwise;
f(x) tells us whether the trial division is successful or not. Using the techniques of
reversible computation discussed in Section 3.2.5, construct a classical reversible circuit
which takes (x, q) – representing an input register initially set to x and a one bit output
register initially set to q – to (x, q ⊕f(x)), by modifying the usual (irreversible) classical
circuit for doing trial division. The resource cost of this reversible circuit is the same to
within a factor two as the irreversible classical circuit used for trial division, and therefore
we regard the two circuits as consuming essentially the same resources. Furthermore, the
classical reversible circuit can be immediately translated into a quantum circuit that takes
|x⟩|q⟩to |x⟩|q ⊕f(x)⟩, as required of the oracle. The key point is that even without
knowing the prime factors of m, we can explicitly construct an oracle which recognizes
a solution to the search problem when it sees one. Using this oracle and the quantum
search algorithm we can search the range 2 to m1/2 using O(m1/4) oracle consultations.
That is, we need only perform the trial division roughly m1/4 times, instead of m1/2
times, as with the classical algorithm!
The factoring example is conceptually interesting but not practical: there are classical
algorithms for factoring which work much faster than searching through all possible
divisors. However, it illustrates the general way in which the quantum search algorithm
may be applied: classical algorithms which rely on search-based techniques may be sped
up using the quantum search algorithm. Later in this chapter we examine scenarios where
the quantum search algorithm offers a genuinely useful aid in speeding up the solution
of NP-complete problems.
6.1.2
The procedure
Schematically, the search algorithm operates as shown in Figure 6.1. The algorithm
proper makes use of a single n qubit register. The internal workings of the oracle, in-
cluding the possibility of it needing extra work qubits, are not important to the description
of the quantum search algorithm proper. The goal of the algorithm is to ﬁnd a solution
to the search problem, using the smallest possible number of applications of the oracle.
The algorithm begins with the computer in the state |0⟩⊗n. The Hadamard transform
is used to put the computer in the equal superposition state,
|ψ⟩=
1
N 1/2
N−1

x=0
|x⟩.
(6.4)
The quantum search algorithm then consists of repeated application of a quantum
subroutine, know as the Grover iteration or Grover operator, which we denote G. The
Grover iteration, whose quantum circuit is illustrated in Figure 6.2, may be broken up
into four steps:
(1) Apply the oracle O.
(2) Apply the Hadamard transform H⊗n.
(3) Perform a conditional phase shift on the computer, with every computational basis
state except |0⟩receiving a phase shift of −1,
|x⟩→−(−1)δx0|x⟩.
(6.5)
The quantum search algorithm
251
(4) Apply the Hadamard transform H⊗n.
Exercise 6.1:
Show that the unitary operator corresponding to the phase shift in the
Grover iteration is 2|0⟩⟨0| −I.
Figure 6.1. Schematic circuit for the quantum search algorithm. The oracle may employ work qubits for its
implementation, but the analysis of the quantum search algorithm involves only the n qubit register.
Figure 6.2. Circuit for the Grover iteration, G.
Each of the operations in the Grover iteration may be efﬁciently implemented on
a quantum computer. Steps 2 and 4, the Hadamard transforms, require n = log(N)
operations each. Step 3, the conditional phase shift, may be implemented using the
techniques of Section 4.3, using O(n) gates. The cost of the oracle call depends upon
the speciﬁc application; for now, we merely need note that the Grover iteration requires
only a single oracle call. It is useful to note that the combined effect of steps 2, 3, and 4
is
H⊗n(2|0⟩⟨0| −I)H⊗n = 2|ψ⟩⟨ψ| −I ,
(6.6)
where |ψ⟩is the equally weighted superposition of states, (6.4). Thus the Grover iteration,
G, may be written G = (2|ψ⟩⟨ψ| −I)O.
Exercise 6.2:
Show that the operation (2|ψ⟩⟨ψ| −I) applied to a general state

k αk|k⟩produces

k

−αk + 2⟨α⟩
	
|k⟩,
(6.7)
252
Quantum search algorithms
where ⟨α⟩≡
k αk/N is the mean value of the αk. For this reason,
(2|ψ⟩⟨ψ| −I) is sometimes referred to as the inversion about mean operation.
6.1.3
Geometric visualization
What does the Grover iteration do? We have noted that G = (2|ψ⟩⟨ψ|−I)O. In fact, we
will show that the Grover iteration can be regarded as a rotation in the two-dimensional
space spanned by the starting vector |ψ⟩and the state consisting of a uniform superpo-
sition of solutions to the search problem. To see this it is useful to adopt the convention
that ′
x indicates a sum over all x which are solutions to the search problem, and ′′
x in-
dicates a sum over all x which are not solutions to the search problem. Deﬁne normalized
states
|α⟩≡
1
√
N −M

x
′′ |x⟩
(6.8)
|β⟩≡
1
√
M

x
′ |x⟩.
(6.9)
Simple algebra shows that the initial state |ψ⟩may be re-expressed as
|ψ⟩=
)
N −M
N
|α⟩+
)
M
N |β⟩,
(6.10)
so the initial state of the quantum computer is in the space spanned by |α⟩and |β⟩.
The effect of G can be understood in a beautiful way by realizing that the oracle
operation O performs a reﬂection about the vector |α⟩in the plane deﬁned by |α⟩and
|β⟩. That is, O(a|α⟩+ b|β⟩) = a|α⟩−b|β⟩. Similarly, 2|ψ⟩⟨ψ| −I also performs a
reﬂection in the plane deﬁned by |α⟩and |β⟩, about the vector |ψ⟩. And the product of
two reﬂections is a rotation! This tells us that the state Gk|ψ⟩remains in the space spanned
by |α⟩and |β⟩for all k. It also gives us the rotation angle. Let cos θ/2 =

(N −M)/N,
so that |ψ⟩= cos θ/2|α⟩+ sin θ/2|β⟩. As Figure 6.3 shows, the two reﬂections which
comprise G take |ψ⟩to
G|ψ⟩= cos 3θ
2 |α⟩+ sin 3θ
2 |β⟩,
(6.11)
so the rotation angle is in fact θ. It follows that continued application of G takes the state
to
Gk|ψ⟩= cos
2k + 1
2
θ

|α⟩+ sin
2k + 1
2
θ

|β⟩.
(6.12)
Summarizing, G is a rotation in the two-dimensional space spanned by |α⟩and |β⟩,
rotating the space by θ radians per application of G. Repeated application of the Grover
iteration rotates the state vector close to |β⟩. When this occurs, an observation in the
computational basis produces with high probability one of the outcomes superposed in
|β⟩, that is, a solution to the search problem! An example illustrating the search algorithm
with N = 4 is given in Box 6.1.
Exercise 6.3:
Show that in the |α⟩, |β⟩basis, we may write the Grover iteration as
G =
 cos θ
−sin θ
sin θ
cos θ

,
(6.13)
The quantum search algorithm
253
Figure 6.3. The action of a single Grover iteration, G: the state vector is rotated by θ towards the superposition
|β⟩of all solutions to the search problem. Initially, it is inclined at angle θ/2 from |α⟩, a state orthogonal to |β⟩.
An oracle operation O reﬂects the state about the state |α⟩, then the operation 2|ψ⟩⟨ψ| −I reﬂects it about |ψ⟩.
In the ﬁgure |α⟩and |β⟩are lengthened slightly to reduce clutter (all states should be unit vectors). After repeated
Grover iterations, the state vector gets close to |β⟩, at which point an observation in the computational basis
outputs a solution to the search problem with high probability. The remarkable efﬁciency of the algorithm occurs
because θ behaves like Ω(

M/N), so only O(

N/M) applications of G are required to rotate the state vector
close to |β⟩.
where θ is a real number in the range 0 to π/2 (assuming for simplicity that
M ≤N/2; this limitation will be lifted shortly), chosen so that
sin θ = 2√M(N −M)
N
.
(6.14)
6.1.4
Performance
How many times must the Grover iteration be repeated in order to rotate |ψ⟩near |β⟩?
The initial state of the system is |ψ⟩=

(N −M)/N|α⟩+

M/N|β⟩, so rotating
through arccos

M/N radians takes the system to |β⟩. Let CI(x) denote the integer
closest to the real number x, where by convention we round halves down, CI(3.5) = 3,
for example. Then repeating the Grover iteration
R = CI

arccos

M/N
θ

(6.15)
times rotates |ψ⟩to within an angle θ/2 ≤π/4 of |β⟩. Observation of the state in the
computational basis then yields a solution to the search problem with probability at least
one-half. Indeed, for speciﬁc values of M and N it is possible to achieve a much higher
probability of success. For example, when M ≪N we have θ ≈sin θ ≈2

M/N, and
thus the angular error in the ﬁnal state is at most θ/2 ≈

M/N, giving a probability
of error of at most M/N. Note that R depends on the number of solutions M, but not
254
Quantum search algorithms
on the identity of those solutions, so provided we know M we can apply the quantum
search algorithm as described. In Section 6.3 we will explain how to remove even the
need for a knowledge of M in applying the search algorithm.
The form (6.15) is useful as an exact expression for the number of oracle calls used
to perform the quantum search algorithm, but it would be useful to have a simpler
expression summarizing the essential behavior of R. To achieve this, note from (6.15)
that R ≤⌈π/2θ⌉, so a lower bound on θ will give an upper bound on R. Assuming for
the moment that M ≤N/2, we have
θ
2 ≥sin θ
2 =

M
N ,
(6.16)
from which we obtain an elegant upper bound on the number of iterations required,
R ≤

π
4

N
M

.
(6.17)
That is, R = O(

N/M) Grover iterations (and thus oracle calls) must be performed
in order to obtain a solution to the search problem with high probability, a quadratic
improvement over the O(N/M) oracle calls required classically. The quantum search
algorithm is summarized below, for the case M = 1.
Algorithm:
Quantum search
Inputs: (1) a black box oracle O which performs the transformation
O|x⟩|q⟩= |x⟩|q ⊕f(x)⟩, where f(x) = 0 for all 0 ≤x < 2n except x0, for which
f(x0) = 1; (2) n + 1 qubits in the state |0⟩.
Outputs: x0.
Runtime: O(
√
2n) operations. Succeeds with probability O(1).
Procedure:
1. |0⟩⊗n|0⟩
initial state
2. →
1
√
2n
2n−1

x=0
|x⟩
|0⟩−|1⟩
√
2

apply H⊗n to the ﬁrst n qubits,
and HX to the last qubit
3. →
	
(2|ψ⟩⟨ψ| −I)O

R
1
√
2n
2n−1

x=0
|x⟩
|0⟩−|1⟩
√
2

apply the Grover iteration R ≈
⌈π
√
2n/4⌉times.
≈|x0⟩
|0⟩−|1⟩
√
2

4. →x0
measure the ﬁrst n qubits
Exercise 6.4:
Give explicit steps for the quantum search algorithm, as above, but for
the case of multiple solutions (1 < M < N/2).
What happens when more than half the items are solutions to the search problem, that
is, M ≥N/2? From the expression θ = arcsin(2√M(N −M)/N) (compare (6.14)) we
see that the angle θ gets smaller as M varies from N/2 to N. As a result, the number of
iterations needed by the search algorithm increases with M, for M ≥N/2. Intuitively,
Quantum search as a quantum simulation
255
this is a silly property for a search algorithm to have: we expect that it should become
easier to ﬁnd a solution to the problem as the number of solutions increases. There are
at least two ways around this problem. If M is known in advance to be larger than N/2
then we can just randomly pick an item from the search space, and then check that it is
a solution using the oracle. This approach has a success probability at least one-half, and
only requires one consultation with the oracle. It has the disadvantage that we may not
know the number of solutions M in advance.
In the case where it isn’t known whether M ≥N/2, another approach can be used.
This approach is interesting in its own right, and has a useful application to simplify the
analysis of the quantum algorithm for counting the number of solutions to the search
problem, as presented in Section 6.3. The idea is to double the number of elements in the
search space by adding N extra items to the search space, none of which are solutions.
As a consequence, less than half the items in the new search space are solutions. This is
effected by adding a single qubit |q⟩to the search index, doubling the number of items to
be searched to 2N. A new augmented oracle O′ is constructed which marks an item only
if it is a solution to the search problem and the extra bit is set to zero. In Exercise 6.5 you
will explain how the oracle O′ may be constructed using one call to O. The new search
problem has only M solutions out of 2N entries, so running the search algorithm with
the new oracle O′ we see that at most R = π/4

2N/M calls to O′ are required, and it
follows that O(

N/M) calls to O are required to perform the search.
Exercise 6.5:
Show that the augmented oracle O′ may be constructed using one
application of O, and elementary quantum gates, using the extra qubit |q⟩.
The quantum search algorithm may be used in a wide variety of ways, some of which
will be explored in subsequent sections. The great utility of the algorithm arises because
we do not assume any particular structure to the search problems being performed. This
is the great advantage of posing the problem in terms of a ‘black box’ oracle, and we
adopt this point of view whenever convenient through the remainder of this chapter. In
practical applications, of course, it is necessary to understand how the oracle is being
implemented, and in each of the practical problems we concern ourselves with an explicit
description of the oracle implementation is given.
Exercise 6.6:
Verify that the gates in the dotted box in the second ﬁgure of Box 6.1
perform the conditional phase shift operation 2|00⟩⟨00| −I, up to an
unimportant global phase factor.
6.2
Quantum search as a quantum simulation
The correctness of the quantum search algorithm is easily veriﬁed, but it is by no means
obvious how one would dream up such an algorithm from a state of ignorance. In this
section we sketch a heuristic means by which one can ‘derive’ the quantum search algo-
rithm, in the hope of lending some intuition as to the tricky task of quantum algorithm
design. As a useful side effect we also obtain a deterministic quantum search algorithm.
Because our goal is to obtain insight rather than generality, we assume for the sake of
simplicity that the search problem has exactly one solution, which we label x.
Our method involves two steps. First, we make a guess as to a Hamiltonian which
256
Quantum search algorithms
Box 6.1: Quantum search: a two-bit example
Here is an explicit example illustrating how the quantum search algorithm works
on a search space of size N = 4. The oracle, for which f(x) = 0 for all x except
x = x0, in which case f(x0) = 1, can be taken to be one of the four circuits
    
    
    
    
corresponding to x0 = 0, 1, 2, or 3 from left to right, where the top two qubits carry
the query x, and the bottom qubit carries the oracle’s response. The quantum circuit
which performs the initial Hadamard transforms and a single Grover iteration G is
Initially, the top two qubits are prepared in the state |0⟩, and the bottom one as
|1⟩. The gates in the dotted box perform the conditional phase shift operation
2|00⟩⟨00| −I. How many times must we repeat G to obtain x0? From Equa-
tion (6.15), using M = 1, we ﬁnd that less than one iteration is required. It turns
out that because θ = π/3 in (6.14), only exactly one iteration is required, to per-
fectly obtain x0, in this special case. In the geometric picture of Figure 6.3, our
initial state |ψ⟩= (|00⟩+|01⟩+|10⟩+|11⟩)/2 is 30◦from |α⟩, and a single rotation
by θ = 60◦moves |ψ⟩to |β⟩. You can conﬁrm for yourself directly, using the
quantum circuits, that measurement of the top two qubits gives x0, after using the
oracle only once. In contrast, a classical computer – or classical circuit – trying to
differentiate between the four oracles would require on average 2.25 oracle queries!
solves the search problem. More precisely, we write down a Hamiltonian H which de-
pends on the solution x and an initial state |ψ⟩such that a quantum system evolving
according to H will change from |ψ⟩to |x⟩after some prescribed time. Once we’ve
found such a Hamiltonian and initial state, we can move on to the second step, which is
to attempt to simulate the action of the Hamiltonian using a quantum circuit. Amazingly,
following this procedure leads very quickly to the quantum search algorithm! We have
already met this two-part procedure while studying universality in quantum circuits, in
Problem 4.3, and it also serves well in the study of quantum searching.
We suppose that the algorithm starts with the quantum computer in a state |ψ⟩. We’ll
tie down what |ψ⟩should be later on, but it is convenient to leave |ψ⟩undetermined
until we understand the dynamics of the algorithm. The goal of quantum searching is to
Quantum search as a quantum simulation
257
change |ψ⟩into |x⟩or some approximation thereof. What Hamiltonians might we guess
do a good job of causing such an evolution? Simplicity suggests that we should guess
a Hamiltonian constructed entirely from the terms |ψ⟩and |x⟩. Thus, the Hamiltonian
must be a sum of terms like |ψ⟩⟨ψ|, |x⟩⟨x|, |ψ⟩⟨x| and |x⟩⟨ψ|. Perhaps the simplest
choices along these lines are the Hamiltonians:
H = |x⟩⟨x| + |ψ⟩⟨ψ|
(6.18)
H = |x⟩⟨ψ| + |ψ⟩⟨x|.
(6.19)
It turns out that both these Hamiltonians lead to the quantum search algorithm! For now,
however, we restrict ourselves to analyzing the Hamiltonian in Equation (6.18). Recall
from Section 2.2.2 that after a time t, the state of a quantum system evolving according
to the Hamiltonian H and initially in the state |ψ⟩is given by
exp(−iHt)|ψ⟩.
(6.20)
Intuitively it looks pretty good: for small t the effect of the evolution is to take |ψ⟩to
(I −itH)|ψ⟩= (1 −it)|ψ⟩−it⟨x|ψ⟩|x⟩. That is, the |ψ⟩vector is rotated slightly,
into the |x⟩direction. Let’s actually do a full analysis, with the goal being to determine
whether there is a t such that exp(−iHt)|ψ⟩= |x⟩. Clearly we can restrict the analysis
to the two-dimensional space spanned by |x⟩and |ψ⟩. Performing the Gram–Schmidt
procedure, we can ﬁnd |y⟩such that |x⟩, |y⟩forms an orthonormal basis for this space,
and |ψ⟩= α|x⟩+ β|y⟩, for some α, β such that α2 + β2 = 1, and for convenience we
have chosen the phases of |x⟩and |y⟩so that α and β are real and non-negative. In the
|x⟩, |y⟩basis we have
H =
 1
0
0
0

+
 α2
αβ
αβ
β2

=
 1 + α2
αβ
αβ
1 −α2

= I + α(βX + αZ) .
(6.21)
Thus
exp(−iHt)|ψ⟩= exp(−it)

cos(αt)|ψ⟩−i sin(αt) (βX + αZ) |ψ⟩
	
.
(6.22)
The global phase factor exp(−it) can be ignored, and simple algebra shows that (βX +
αZ)|ψ⟩= |x⟩, so the state of the system after a time t is
cos(αt)|ψ⟩−i sin(αt)|x⟩.
(6.23)
Thus, observation of the system at time t = π/2α yields the result |x⟩with probability
one: we have found a solution to the search problem! Unfortunately, the time of the
observation depends on α, the component of |ψ⟩in the |x⟩direction, and thus on x,
which is what we are trying to determine. The obvious solution is to attempt to arrange
α to be the same for all |x⟩, that is, to choose |ψ⟩to be the uniform superposition state
|ψ⟩=

x |x⟩
√
N
.
(6.24)
Making this choice gives α = 1/
√
N for all x, and thus the time of observation t =
π
√
N/2 does not depend on knowing the value of x. Furthermore, the state (6.24) has
the obvious advantage that we already know how to prepare such a state by doing a
Hadamard transform.
We now know that the Hamiltonian (6.18) rotates the vector |ψ⟩to |x⟩. Can we ﬁnd
258
Quantum search algorithms
a quantum circuit to simulate the Hamiltonian (6.18), and thus obtain a quantum search
algorithm? Applying the method of Section 4.7, we see that a natural way of simulating H
is to alternately simulate the Hamiltonians H1 ≡|x⟩⟨x| and H2 ≡|ψ⟩⟨ψ| for short time
increments Δt. These Hamiltonians are easily simulated using the methods of Chapter 4,
as illustrated in Figures 6.4 and 6.5.
Exercise 6.7:
Verify that the circuits shown in Figures 6.4 and 6.5 implement the
operations exp(−i|x⟩⟨x|Δt) and exp(−i|ψ⟩⟨ψ|Δt), respectively, with |ψ⟩as in
(6.24).
Figure 6.4. Circuit implementing the operation exp(−i|x⟩⟨x|Δt) using two oracle calls.
    
    
    
    
    
    
Figure 6.5. Circuit implementing the operation exp(−i|ψ⟩⟨ψ|Δt), for |ψ⟩as in (6.24).
The number of oracle calls required by the quantum simulation is determined by
how small a time-step is required to obtain reasonably accurate results. Suppose we use a
simulation step of length Δt that is accurate to O(Δt2). The total number of steps required
is t/Δt = Θ(
√
N/Δt), and thus the cumulative error is O(Δt2 ×
√
N/Δt) = O(Δt
√
N).
To obtain a reasonably high success probability we need the error to be O(1), which means
that we must choose Δt = Θ(1/
√
N) which results in a number of oracle calls that scales
like O(N) – no better than the classical solution! What if we use a more accurate method
of quantum simulation, say one that is accurate to O(Δt3)? The cumulative error in this
case is O(Δt2√
N), and thus to achieve a reasonable success probability we need to choose
Δt = Θ(N −1/4), resulting in a total number of oracle calls O(N 3/4), which is a distinct
improvement over the classical situation, although still not as good as achieved by the
quantum search algorithm of Section 6.1! In general going to a more accurate quantum
simulation technique results in a reduction in the number of oracle calls required to
perform the simulation:
Exercise 6.8:
Suppose the simulation step is performed to an accuracy O(Δtr). Show
Quantum search as a quantum simulation
259
that the number of oracle calls required to simulate H to reasonable accuracy is
O(N r/2(r−1)). Note that as r becomes large the exponent of N approaches 1/2.
We have been analyzing the accuracy of the quantum simulation of the Hamilto-
nian (6.18) using general results on quantum simulation from Section 4.7. Of course, in
this instance we are dealing with a speciﬁc Hamiltonian, not the general case, which sug-
gests that it might be interesting to calculate explicitly the effect of a simulation step of
time Δt, rather than relying on the general analysis. We can do this for any speciﬁc choice
of simulation method – it can be a little tedious to work out the effect of the simulation
step, but it is essentially a straightforward calculation. The obvious starting point is to ex-
plicitly calculate the action of the lowest-order simulation techniques, that is, to calculate
one or both of exp(−i|x⟩⟨x|Δt) exp(−i|ψ⟩⟨ψ|Δt) or exp(−i|ψ⟩⟨ψ|Δt) exp(−i|x⟩⟨x|Δt).
The results are essentially the same in both instances; we will focus on the study of
U(Δt) ≡exp(−i|ψ⟩⟨ψ|Δt) exp(−i|x⟩⟨x|Δt). U(Δt) clearly acts non-trivially only in the
space spanned by |x⟩⟨x| and |ψ⟩⟨ψ|, so we restrict ourselves to that space, working
in the basis |x⟩, |y⟩, where |y⟩is deﬁned as before. Note that in this representation
|x⟩⟨x| = (I + Z)/2 = (I + ˆz · ⃗σ)/2, where ˆz ≡(0, 0, 1) is the unit vector in the z
direction, and |ψ⟩⟨ψ| = (I + ⃗ψ · ⃗σ)/2, where ⃗ψ = (2αβ, 0, (α2 −β2)) (recall that this is
the Bloch vector representation; see Section 4.2). A simple calculation shows that up to
an unimportant global phase factor,
U(Δt) =

cos2
Δt
2

−sin2
Δt
2

⃗ψ · ˆz

I
−2i sin
Δt
2
 
cos
Δt
2
 ⃗ψ + ˆz
2
+ sin
Δt
2
 ⃗ψ × ˆz
2

· ⃗σ .
(6.25)
Exercise 6.9:
Verify Equation (6.25). (Hint: see Exercise 4.15.)
Equation (6.25) implies that U(Δt) is a rotation on the Bloch sphere about an axis of
rotation ⃗r deﬁned by
⃗r = cos
Δt
2
 ⃗ψ + ˆz
2
+ sin
Δt
2
 ⃗ψ × ˆz
2
,
(6.26)
and through an angle θ deﬁned by
cos
θ
2

= cos2
Δt
2

−sin2
Δt
2

⃗ψ · ˆz,
(6.27)
which simpliﬁes upon substitution of ⃗ψ · ˆz = α2 −β2 = (2/N −1) to
cos
θ
2

= 1 −2
N sin2
Δt
2

.
(6.28)
Note that ⃗ψ · ⃗r = ˆz · ⃗r, so both |ψ⟩⟨ψ| and |x⟩⟨x| lie on the same circle of revolution
about the ⃗r axis on the Bloch sphere. Summarizing, the action of U(Δt) is to rotate
|ψ⟩⟨ψ| about the ⃗r axis, through an angle θ for each application of U(Δt), as illustrated
in Figure 6.6. We terminate the procedure when enough rotations have been performed
to rotate |ψ⟩⟨ψ| near to the solution |x⟩⟨x|. Now initially we imagined that Δt was small,
since we were considering the case of quantum simulation, but Equation (6.28) shows
260
Quantum search algorithms
that the smart thing to do is to choose Δt = π, in order to maximize the rotation angle
θ. If we do this, then we obtain cos(θ/2) = 1 −2/N, which for large N corresponds to
θ ≈4/
√
N, and the number of oracle calls required to ﬁnd the solution |x⟩is O(
√
N),
just as for the original quantum search algorithm.
Figure 6.6. Bloch sphere diagram showing the initial state ⃗ψ rotating around the axis of rotation ⃗r going toward the
ﬁnal state ˆz.
Indeed, if we make the choice Δt = π, then this ‘quantum simulation’ is in fact
identical with the original quantum search algorithm, since the operators applied in
the quantum simulation are exp(−iπ|ψ⟩⟨ψ|) = I −2|ψ⟩⟨ψ| and exp(−iπ|x⟩⟨x|) =
I −2|x⟩⟨x|, and up to a global phase shift these are identical to the steps making
up the Grover iteration. Viewed this way, the circuits shown in Figures 6.2 and 6.3 for
the quantum search algorithm are simpliﬁcations of the circuits shown in Figures 6.4
and 6.5 for the simulation, in the special case Δt = π!
Exercise 6.10:
Show that by choosing Δt appropriately we can obtain a quantum
search algorithm which uses O(
√
N) queries, and for which the ﬁnal state is |x⟩
exactly, that is, the algorithm works with probability 1, rather than with some
smaller probability.
We have re-derived the quantum search algorithm from a different point of view, the
point of view of quantum simulation. Why did this approach work? Might it be used to
ﬁnd other fast quantum algorithms? We can’t answer these questions in any deﬁnitive
way, but the following few thoughts may be of some interest. The basic procedure used is
four-fold: (1) specify the problem to be solved, including a description of the desired input
and output from the quantum algorithm; (2) guess a Hamiltonian to solve the problem,
and verify that it does in fact work; (3) ﬁnd a procedure to simulate the Hamiltonian;
and (4) analyze the resource costs of the simulation. This is different from the more
conventional approach in two respects: we guess a Hamiltonian, rather than a quantum
circuit, and there is no analogue to the simulation step in the conventional approach. The
more important of these two differences is the ﬁrst. There is a great deal of freedom in
specifying a quantum circuit to solve a problem. While that freedom is, in part, responsible
Quantum counting
261
for the great power of quantum computation, it makes searching for good circuits rather
difﬁcult. By contrast, specifying a Hamiltonian is a much more constrained problem, and
therefore affords less freedom in the solution of problems, but those same constraints
may in fact make it much easier to ﬁnd an efﬁcient quantum algorithm to solve a problem.
We’ve seen this happen for the quantum search algorithm, and perhaps other quantum
algorithms will be discovered by this method; we don’t know. What seems certain is that
this ‘quantum algorithms as quantum simulations’ point of view offers a useful alternative
viewpoint to stimulate in the development of quantum algorithms.
Exercise 6.11: (Multiple solution continuous quantum search)
Guess a
Hamiltonian with which one may solve the continuous time search problem in
the case where the search problem has M solutions.
Exercise 6.12: (Alternative Hamiltonian for quantum search)
Suppose
H = |x⟩⟨ψ| + |ψ⟩⟨x| .
(6.29)
(1) Show that it takes time O(1) to rotate from the state |ψ⟩to the state |x⟩,
given an evolution according to the Hamiltonian H.
(2) Explain how a quantum simulation of the Hamiltonian H may be performed,
and determine the number of oracle calls your simulation technique requires
to obtain the solution with high probability.
6.3
Quantum counting
How quickly can we determine the number of solutions, M, to an N item search problem,
if M is not known in advance? Clearly, on a classical computer it takes Θ(N) consultations
with an oracle to determine M. On a quantum computer it is possible to estimate the
number of solutions much more quickly than is possible on a classical computer by
combining the Grover iteration with the phase estimation technique based upon the
quantum Fourier transform (Chapter 5). This has some important applications. First, if
we can estimate the number of solutions quickly then it is also possible to ﬁnd a solution
quickly, even if the number of solutions is unknown, by ﬁrst counting the number of
solutions, and then applying the quantum search algorithm to ﬁnd a solution. Second,
quantum counting allows us to decide whether or not a solution even exists, depending on
whether the number of solutions is zero, or non-zero. This has applications, for example,
to the solution of NP-complete problems, which may be phrased in terms of the existence
of a solution to a search problem.
Exercise 6.13:
Consider a classical algorithm for the counting problem which samples
uniformly and independently k times from the search space, and let X1, . . . , Xk
be the results of the oracle calls, that is, Xj = 1 if the jth oracle call revealed a
solution to the problem, and Xj = 0 if the jth oracle call did not reveal a
solution to the problem. This algorithm returns the estimate S ≡N × 
j Xj/k
for the number of solutions to the search problem. Show that the standard
deviation in S is ΔS =

M(N −M)/k. Prove that to obtain a probability at
least 3/4 of estimating M correctly to within an accuracy
√
M for all values of
M we must have k = Ω(N).
262
Quantum search algorithms
Exercise 6.14:
Prove that any classical counting algorithm with a probability at least
3/4 for estimating M correctly to within an accuracy c
√
M for some constant c
and for all values of M must make Ω(N) oracle calls.
Quantum counting is an application of the phase estimation procedure of Section 5.2 to
estimate the eigenvalues of the Grover iteration G, which in turn enables us to determine
the number of solutions M to the search problem. Suppose |a⟩and |b⟩are the two
eigenvectors of the Grover iteration in the space spanned by |α⟩and |β⟩. Let θ be the
angle of rotation determined by the Grover iteration. From Equation (6.13) we see that
the corresponding eigenvalues are eiθ and ei(2π−θ). For ease of analysis it is convenient to
assume that the oracle has been augmented, as described in Section 6.1, expanding the
size of the search space to 2N, and ensuring that sin2(θ/2) = M/2N.
The phase estimation circuit used for quantum counting is shown in Figure 6.7. The
function of the circuit is to estimate θ to m bits of accuracy, with a probability of success
at least 1 −ϵ. The ﬁrst register contains t ≡m +⌈log(2 +1/2ϵ)⌉qubits, as per the phase
estimation algorithm, and the second register contains n+1 qubits, enough to implement
the Grover iteration on the augmented search space. The state of the second register
is initialized to an equal superposition of all possible inputs 
x |x⟩by a Hadamard
transform. As we saw in Section 6.1 this state is a superposition of the eigenstates |a⟩
and |b⟩, so by the results of Section 5.2 the circuit in Figure 6.7 gives us an estimate of
θ or 2π −θ accurate to within |Δθ| ≤2−m, with probability at least 1 −ϵ. Furthermore,
an estimate for 2π −θ is clearly equivalent to an estimate of θ with the same level of
accuracy, so effectively the phase estimation algorithm determines θ to an accuracy 2−m
with probability 1 −ϵ.
Figure 6.7. Circuit for performing approximate quantum counting on a quantum computer.
Speeding up the solution of NP-complete problems
263
Using the equation sin2(θ/2) = M/2N and our estimate for θ we obtain an estimate
of the number of solutions, M. How large an error, ΔM, is there in this estimate?
|ΔM|
2N
=
++++sin2
θ + Δθ
2

−sin2
θ
2
++++
(6.30)
=

sin
θ + Δθ
2

+ sin
θ
2
 ++++sin
θ + Δθ
2

−sin
θ
2
++++ .
(6.31)
Calculus implies that | sin((θ+Δθ)/2)−sin(θ/2)| ≤|Δθ|/2, and elementary trigonometry
that | sin((θ + Δθ)/2)| < sin(θ/2) + |Δθ|/2, so
|ΔM|
2N
<

2 sin
θ
2

+ |Δθ|
2
 |Δθ|
2 .
(6.32)
Substituting sin2(θ/2) = M/2N and |Δθ| ≤2−m gives our ﬁnal estimate for the error
in our estimate of M,
|ΔM| <
√
2MN +
N
2m+1

2−m .
(6.33)
As an example, suppose we choose m = ⌈n/2⌉+ 1, and ϵ = 1/6. Then we have t =
⌈n/2⌉+ 3, so the algorithm requires Θ(
√
N) Grover iterations, and thus Θ(
√
N) oracle
calls. By (6.33) our accuracy is |ΔM| <

M/2 + 1/4 = O(
√
M). Compare this with
Exercise 6.14, according to which it would have required O(N) oracle calls to obtain a
similar accuracy on a classical computer.
Indeed, the example just described serves double duty as an algorithm for determining
whether a solution to the search problem exists at all, that is, whether M = 0 or M ̸= 0. If
M = 0 then we have |ΔM| < 1/4, so the algorithm must produce the estimate zero with
probability at least 5/6. Conversely, if M ̸= 0 then it is easy to verify that the estimate
for M is not equal to 0 with probability at least 5/6.
Another application of quantum counting is to ﬁnd a solution to a search problem
when the number M of solutions is unknown. The difﬁculty in applying the quantum
search algorithm as described in Section 6.1 is that the number of times to repeat the
Grover iteration, Equation (6.15), depends on knowing the number of solutions M. This
problem can be alleviated by using the quantum counting algorithm to ﬁrst estimate θ
and M to high accuracy using phase estimation, and then to apply the quantum search
algorithm as in Section 6.1, repeating the Grover iteration a number of times determined
by (6.15), with the estimates for θ and M obtained by phase estimation substituted to
determine R. The angular error in this case is at most π/4(1 + |Δθ|/θ), so choosing
m = ⌈n/2⌉+ 1 as before gives an angular error at most π/4 × 3/2 = 3π/8, which
corresponds to a success probability of at least cos2(3π/8) = 1/2 −1/2
√
2 ≈0.15 for
the search algorithm. If the probability of obtaining an estimate of θ this accurate is 5/6,
as in our earlier example, then the total probability of obtaining a solution to the search
problem is 5/6 × cos2(3π/8) ≈0.12, a probability which may quickly be boosted close
to 1 by a few repetitions of the combined counting–search procedure.
6.4
Speeding up the solution of NP-complete problems
Quantum searching may be used to speed up the solution to problems in the complexity
class NP (Section 3.2.2). We already saw, in Section 6.1.1, how factoring can be sped
264
Quantum search algorithms
up; here, we illustrate how quantum search can be applied to assist the solution of the
Hamiltonian cycle problem (
). Recall that a Hamiltonian cycle of a graph is a simple
cycle which visits every vertex of the graph. The
problem is to determine whether a
given graph has a Hamiltonian cycle or not. This problem belongs to the class of NP-
complete problems, widely believed (but not yet proved) to be intractable on a classical
computer.
A simple algorithm to solve
is to perform a search through all possible orderings
of the vertices:
(1) Generate each possible ordering (v1, . . . , vn) of vertices for the graph. Repetitions
will be allowed, as they ease the analysis without affecting the essential result.
(2) For each ordering check to see whether it is a Hamiltonian cycle for the graph. If
not, continue checking the orderings.
Since there are nn = 2n log n possible orderings of the vertices which must be searched,
this algorithm requires 2n log n checks for the Hamiltonian cycle property in the worst
case. Indeed, any problem in NP may be solved in a similar way: if a problem of size n has
witnesses which can be speciﬁed using w(n) bits, where w(n) is some polynomial in n,
then searching through all 2w(n) possible witnesses will reveal a solution to the problem,
if one exists.
The quantum search algorithm may be used to speed up this algorithm by increasing
the speed of the search. Speciﬁcally, we use the algorithm described in Section 6.3 to
determine whether a solution to the search problem exists. Let m ≡⌈log n⌉. The search
space for the algorithm will be represented by a string of mn qubits, each block of m
qubits being used to store the index to a single vertex. Thus we can write the computa-
tional basis states as |v1, . . . , vn⟩, where each |vi⟩is represented by the appropriate string
of m qubits, for a total of nm qubits. The oracle for the search algorithm must apply
the transformation:
O|v1, . . . , vn⟩=
*
|v1, . . . , vn⟩
if v1, . . . , vn is not a Hamiltonian cycle
−|v1, . . . , vn⟩
if v1, . . . , vn is a Hamiltonian cycle
(6.34)
Such an oracle is easy to design and implement when one has a description of the graph.
One takes a polynomial size classical circuit recognizing Hamiltonian cycles in the graph,
and converts it to a reversible circuit, also of polynomial size, computing the transfor-
mation (v1, . . . , vn, q) →(v1, . . . , vn, q ⊕f(v1, . . . , vn)), where f(v1, . . ., vn) = 1 if
v1, . . . , vn is a Hamiltonian cycle, and is 0 otherwise. Implementing the corresponding
circuit on a quantum computer with the ﬁnal qubit starting in the state (|0⟩−|1⟩)/
√
2
gives the desired transformation. We won’t explicitly describe the details here, except
to note the key point: the oracle requires a number of gates polynomial in n, as a di-
rect consequence of the fact that Hamiltonian cycles can be recognized using polyno-
mially many gates classically. Applying the variant of the search algorithm which deter-
mines whether a solution to the search problem exists (Section 6.3) we see that it takes
O(2mn/2) = O(2n⌈log n⌉/2) applications of the oracle to determine whether a Hamiltonian
cycle exists. When one does exist it is easy to apply the combined counting–search algo-
rithm to ﬁnd an example of such a cycle, which can then be exhibited as a witness for
the problem.
To summarize:
• The classical algorithm requires O

p(n)2n⌈log n⌉ operations to determine whether a
Quantum search of an unstructured database
265
Hamiltonian cycle exists, where the polynomial factor p(n) is overhead
predominantly due to the implementation of the oracle, that is, the gates checking
whether a candidate path is Hamiltonian or not. The dominant effect in determining
the resources required is the exponent in 2n⌈log n⌉. The classical algorithm is
deterministic, that is, it succeeds with probability 1.
• The quantum algorithm requires O

p(n)2n⌈log n⌉/2 operations to determine whether
a Hamiltonian cycle exists. Once again, the polynomial p(n) is overhead
predominantly due to implementation of the oracle. The dominant effect in
determining the resources required is the exponent in 2n⌈log n⌉/2. There is a constant
probability (say, 1/6) of error for the algorithm, which may be reduced to 1/6r by r
repetitions of the algorithm.
• Asymptotically the quantum algorithm requires the square root of the number of
operations the classical algorithm requires.
6.5
Quantum search of an unstructured database
Suppose somebody gives you a list containing one thousand ﬂower names, and asks
you where ‘Perth Rose’ appears on the list. If the ﬂower appears exactly once on the
list, and the list is not ordered in any obvious way, then you will need to examine ﬁve
hundred names, on average, before you ﬁnd the ‘Perth Rose’. Might it be possible to
speed up this kind of database searching using the quantum search algorithm? Indeed,
the quantum search algorithm is sometimes referred to as a database search algorithm,
but its usefulness for that application is limited, and based on certain assumptions. In this
section we take a look at how the quantum search algorithm can conceptually be used
to search an unstructured database, in a setting rather like that found on a conventional
computer. The picture we construct will clarify what resources are required to enable a
quantum computer to search classical databases.
Suppose we have a database containing N ≡2n items, each of length l bits. We will
label these items d1, . . . , dN. We want to determine where a particular l bit string, s, is
in the database. A classical computer, used to solve this problem, is typically split into
two parts, illustrated in Figure 6.8. One is the central processing unit, or CPU, where
data manipulation takes place, using a small amount of temporary memory. The second
part is a large memory which stores the database in a string of 2n blocks of l bit cells.
The memory is assumed to be passive, in the sense that it is not capable of processing
data on its own. What is possible is to LOAD data from memory into the CPU, and STORE
data from the CPU in memory, and to do manipulations of the data stored temporarily
in the CPU. Of course, classical computers may be designed along different lines, but
this CPU–memory split is a popular and common architecture.
To ﬁnd out where a given string s is in the unstructured database, the most efﬁcient
classical algorithm is as follows. First, an n-bit index to the database elements is set up
in the CPU. We assume that the CPU is large enough to store the n ≡⌈log N⌉bit
index. The index starts out at zero, and is incremented by one on each iteration of the
algorithm. At each iteration, the database entry corresponding to the index is loaded into
the CPU, and compared to the string which is being searched for. If they are the same,
the algorithm outputs the value of the index and halts. If not, the algorithm continues
incrementing the index. Obviously, this algorithm requires that items be loaded from
266
Quantum search algorithms
	 
 	 
















)
/ / /
Figure 6.8. Classical database searching on a computer with distinct central processing unit (CPU) and memory.
Only two operations may be directly performed on the memory – a memory element may be LOADed into the CPU,
or an item from the CPU may be STOREd in memory.
memory 2n times in the worst case. It is also clear that this is the most efﬁcient possible
algorithm for solving the problem in this model of computation.
How efﬁciently can an analogous algorithm be implemented on a quantum computer?
And, even if a quantum speedup is possible, how useful is such an algorithm? We show
ﬁrst that a speedup is possible, and then return to the question of the utility of such an
algorithm. Suppose our quantum computer consists of two units, just like the classical
computer, a CPU and a memory. We assume that the CPU contains four registers: (1)
an n qubit ‘index’ register initialized to |0⟩; (2) an l qubit register initialized to |s⟩and
remaining in that state for the entire computation; (3) an l qubit ‘data’ register initialized
to |0⟩; and (4) a 1 qubit register initialized to (|0⟩−|1⟩)/
√
2.
The memory unit can be implemented in one of two ways. The simplest is a quantum
memory containing N = 2n cells of l qubits each, containing the database entries |dx⟩.
The second implementation is to implement the memory as a classical memory with
N = 2n cells of l bits each, containing the database entries dx. Unlike a traditional classical
memory, however, it can be addressed by an index x which can be in a superposition of
multiple values. This quantum index allows a superposition of cell values to be LOADed
from memory. Memory access works in the following way: if the CPU’s index register
is in the state |x⟩and the data register is in the state |d⟩, then the contents dx of the
xth memory cell are added to the data register: |d⟩→|d ⊕dx⟩, where the addition is
done bitwise, modulo 2. First, let us see how this capability is used to perform quantum
search, then we shall discuss how such a memory might be physically constructed.
The key part of implementing the quantum search algorithm is realization of the oracle,
which must ﬂip the phase of the index which locates s in the memory. Suppose the CPU
is in the state
|x⟩|s⟩|0⟩|0⟩−|1⟩
√
2
.
(6.35)
Applying the LOAD operation puts the computer in the state
|x⟩|s⟩|dx⟩|0⟩−|1⟩
√
2
.
(6.36)
Now the second and third registers are compared, and if they are the same, then a bit
Quantum search of an unstructured database
267
ﬂip is applied to register 4; otherwise nothing is changed. The effect of this operation is
|x⟩|s⟩|dx⟩|0⟩−|1⟩
√
2
→
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−|x⟩|s⟩|dx⟩|0⟩−|1⟩
√
2
if dx = s
|x⟩|s⟩|dx⟩|0⟩−|1⟩
√
2
if dx ̸= s.
(6.37)
The data register is then restored to the state |0⟩by performing the LOAD operation again.
The total action of the oracle thus leaves registers 2, 3 and 4 unaffected, and unentangled
with register 1. Thus, the overall effect is to take the state of register 1 from |x⟩to −|x⟩
if dx = s, and to leave the register alone otherwise. Using the oracle implemented in this
way, we may apply the quantum search algorithm to determine the location of s in the
database, using O(
√
N) LOAD operations, compared to the N LOAD operations that were
required classically.
In order for the oracle to function correctly on superpositions it seems at ﬁrst glance
as though the memory needs to be quantum mechanical. In fact, as we noted above, with
some caveats the memory can actually be implemented classically, which likely makes it
much more resistant to the effects of noise. But a quantum addressing scheme is still
needed; a conceptual picture illustrating how this might be done is shown in Figure 6.9.
The principle of operation is a means by which the binary encoded state of the quantum
index (where 0 to 2n −1 is represented by n qubits) is translated into a unary encoding
(where 0 to 2n −1 is represented by the position of a single probe within 2n possible
locations) which addresses the classical database. The database effects a change on a
degree of freedom within the probe which is unrelated to its position. The binary to
unary encoding is then reversed, leaving the data register with the desired contents.
Are there practical instances in which the quantum search algorithm could be useful
for searching classical databases? Two distinct points may be made. First, databases are
not ordinarily unstructured. Simple databases, like one containing ﬂower names discussed
in the introduction to this section, may be maintained in alphabetical order, such that a
binary search can be used to locate an item in time which is O(log(N)) for an N-element
database. However, some databases may require a much more complex structure, and
although sophisticated techniques exist to optimize classical searches, given queries of a
sufﬁciently complex or unanticipated nature, a predetermined structure may not be of
assistance, and the problem can be regarded as being essentially the unstructured database
search problem we discussed.
Second, for a quantum computer to be able to search a classical database, a quantum
addressing scheme is required. The scheme we depicted requires O(N log N) quantum
switches – about the same amount of hardware as would be required to store the database
itself. Presumably, these switches may someday be as simple and inexpensive as classical
memory elements, but if that is not the case, then building a quantum computer to
perform a quantum search may not be economically advantageous, compared with using
classical computing hardware distributed over the memory elements.
Given these considerations, it appears that the principle use of quantum search al-
gorithms will not be in searching classical databases. Rather, their use will probably be
in searching for solutions to hard problems, as discussed in the last section, such as the
Hamiltonian cycle, traveling salesman, and satisﬁability problems.
268
Quantum search algorithms
////
01
0
0
0
0
0
0
0
0
0
0
02
02
03
03
01
24
25
26
3
32

2
3
1
7
8
3
03
02
0
0
0
0
0
0
03
02
0
0
0
0
Figure 6.9. Conceptual diagram of a 32 cell classical memory with a ﬁve qubit quantum addressing scheme. Each
circle represents a switch, addressed by the qubit inscribed within. For example, when |x4⟩= |0⟩, the
corresponding switch routes the input qubit towards the left; when |x4⟩= |1⟩the switch routes the input qubit to
the right. If |x4⟩= (|0⟩+ |1⟩)/
√
2, then an equal superposition of both routes is taken. The data register qubits
enter at the top of the tree, and are routed down to the database, which changes their state according to the
contents of the memory. The qubits are then routed back into a deﬁnite position, leaving them with the retrieved
information. Physically, this could be realized using, for example, single photons for the data register qubits, which
are steered using nonlinear interferometers (Chapter 7). The classical database could be just a simple sheet of
plastic in which a ‘zero’ (illustrated as white squares) transmits light unchanged, and a ‘one’ (shaded squares) shifts
the polarization of the incident light by 90◦.
Optimality of the search algorithm
269
6.6
Optimality of the search algorithm
We have shown that a quantum computer can search N items, consulting the search
oracle only O(
√
N) times. We now prove that no quantum algorithm can perform this
task using fewer than Ω(
√
N) accesses to the search oracle, and thus the algorithm we
have demonstrated is optimal.
Suppose the algorithm starts in the state |ψ⟩. For simplicity, we prove the lower
bound for the case where the search problem has a single solution, x. To determine x
we are allowed to apply an oracle Ox which gives a phase shift of −1 to the solution
|x⟩and leaves all other states invariant, Ox = I −2|x⟩⟨x|. We suppose the algorithm
starts in a state |ψ⟩and applies the oracle Ox exactly k times, with unitary operations
U1, U2, . . . , Uk interleaved between the oracle operations. Deﬁne
|ψx
k⟩≡UkOxUk−1Ox . . . U1Ox|ψ⟩
(6.38)
|ψk⟩≡UkUk−1 . . . U1|ψ⟩.
(6.39)
That is, |ψk⟩is the state that results when the sequence of unitary operations U1, . . . , Uk
is carried out, without the oracle operations. Let |ψ0⟩= |ψ⟩. Our goal will be to bound
the quantity
Dk ≡

x
∥ψx
k −ψk ∥2 ,
(6.40)
where we use the notation ψ for |ψ⟩as a convenience to simplify formulas. Intuitively,
Dk is a measure of the deviation after k steps caused by the oracle, from the evolution
that would otherwise have ensued. If this quantity is small, then all the states |ψx
k⟩are
roughly the same, and it is not possible to correctly identify x with high probability. The
strategy of the proof is to demonstrate two things: (a) a bound on Dk that shows it can
grow no faster than O(k2); and (b) a proof that Dk must be Ω(N) if it is to be possible to
distinguish N alternatives. Combining these two results gives the desired lower bound.
First, we give an inductive proof that Dk ≤4k2. This is clearly true for k = 0, where
Dk = 0. Note that
Dk+1 =

x
∥Oxψx
k −ψk ∥2
(6.41)
=

x
∥Ox(ψx
k −ψk) + (Ox −I)ψk ∥2.
(6.42)
Applying ∥b+c∥2 ≤∥b∥2+2∥b∥∥c∥+∥c∥2 with b ≡Ox(ψx
k −ψk) and c ≡(Ox−I)ψk =
−2⟨x|ψk⟩|x⟩, gives
Dk+1 ≤

x

∥ψx
k −ψk∥2 + 4∥ψx
k −ψk∥|⟨x|ψk⟩| + 4|⟨ψk|x⟩|2 .
(6.43)
Applying the Cauchy–Schwarz inequality to the second term on the right hand side, and
noting that 
x |⟨x|ψk⟩|2 = 1 gives
Dk+1 ≤Dk + 4

x
∥ψx
k −ψk∥2
 1
2 
x′
|⟨ψk|x′⟩|2
 1
2
+ 4
(6.44)
≤Dk + 4

Dk + 4.
(6.45)
270
Quantum search algorithms
By the inductive hypothesis Dk ≤4k2 we obtain
Dk+1 ≤4k2 + 8k + 4 = 4(k + 1)2,
(6.46)
which completes the induction.
To complete the proof we need to show that the probability of success can only be
high if Dk is Ω(N). We suppose |⟨x|ψx
k⟩|2 ≥1/2 for all x, so that an observation yields a
solution to the search problem with probability at least one-half. Replacing |x⟩by eiθ|x⟩
does not change the probability of success, so without loss of generality we may assume
that ⟨x|ψx
k⟩= |⟨x|ψx
k⟩|, and therefore
∥ψx
k −x∥2 = 2 −2|⟨x|ψx
k⟩| ≤2 −
√
2.
(6.47)
Deﬁning Ek ≡
x ∥ψx
k −x∥2 we see that Ek ≤(2 −
√
2)N. We are now in position to
prove that Dk is Ω(N). Deﬁning Fk ≡
x ∥x −ψk∥2 we have
Dk =

x
∥(ψx
k −x) + (x −ψk)∥2
(6.48)
≥

x
∥ψx
k −x∥2 −2

x
∥ψx
k −x∥∥x −ψk∥+

x
∥x −ψk∥2
(6.49)
= Ek + Fk −2

x
∥ψx
k −x∥∥x −ψk∥.
(6.50)
Applying the Cauchy–Schwarz inequality gives 
x ∥ψx
k −x∥∥x −ψk∥≤√EkFk, so
we have
Dk ≥Ek + Fk −2

EkFk = (

Fk −

Ek)2 .
(6.51)
In Exercise 6.15 you will show that Fk ≥2N −2
√
N. Combining this with the result
Ek ≤(2 −
√
2)N gives Dk ≥cN for sufﬁciently large N, where c is any constant less
than (
√
2 −

2 −
√
2)2 ≈0.42. Since Dk ≤4k2 this implies that
k ≥

cN/4 .
(6.52)
Summarizing, to achieve a probability of success at least one-half for ﬁnding a solution
to the search problem we must call the oracle Ω(
√
N) times.
Exercise 6.15:
Use the Cauchy–Schwarz inequality to show that for any normalized
state vector |ψ⟩and set of N orthonormal basis vectors |x⟩,

x
∥ψ −x∥2 ≥2N −2
√
N .
(6.53)
Exercise 6.16:
Suppose we merely require that the probability of an error being made
is less than 1/2 when averaged uniformly over the possible values for x, instead
of for all values of x. Show that O(
√
N) oracle calls are still required to solve the
search problem.
This result, that the quantum search algorithm is essentially optimal, is both exciting
and disappointing. It is exciting because it tells us that for this problem, at least, we have
fully plumbed the depths of quantum mechanics; no further improvement is possible.
The disappointment arises because we might have hoped to do much better than the
square root speedup offered by the quantum search algorithm. The sort of dream result
Black box algorithm limits
271
we might have hoped for a priori is that it would be possible to search an N item search
space using O(log N) oracle calls. If such an algorithm existed, it would allow us to
solve NP-complete problems efﬁciently on a quantum computer, since it could search
all 2w(n) possible witnesses using roughly w(n) oracle calls, where the polynomial w(n)
is the length of a witness in bits. Unfortunately, such an algorithm is not possible. This
is useful information for would-be algorithm designers, since it indicates that a naive
search-based method for attacking NP-complete problems is guaranteed to fail.
Venturing into the realm of opinion, we note that many researchers believe that the
essential reason for the difﬁculty of NP-complete problems is that their search space has
essentially no structure, and that (up to polynomial factors) the best possible method for
solving such a problem is to adopt a search method. If one takes this point of view, then
it is bad news for quantum computing, indicating that the class of problems efﬁciently
soluble on a quantum computer, BQP, does not contain the NP-complete problems. Of
course, this is merely opinion, and it is still possible that the NP-complete problems
contain some unknown structure that allows them to be efﬁciently solved on a quantum
computer, or perhaps even on a classical computer. A nice example to illustrate this
point is the problem of factoring, widely believed to be in the class NPI of problems
intermediate in difﬁculty between P and the NP-complete problems. The key to the
efﬁcient quantum mechanical solution of the factoring problem was the exploitation of a
structure ‘hidden’ within the problem – a structure revealed by the reduction to order-
ﬁnding. Even with this amazing structure revealed, it has not been found possible to
exploit the structure to develop an efﬁcient classical algorithm for factoring, although, of
course, quantum mechanically the structure can be harnessed to give an efﬁcient factoring
algorithm! Perhaps a similar structure lurks in other problems suspected to be in NPI,
such as the graph isomorphism problem, or perhaps even in the NP-complete problems
themselves.
Exercise 6.17: (Optimality for multiple solutions)
Suppose the search problem
has M solutions. Show that O(

N/M) oracle applications are required to ﬁnd a
solution.
6.7
Black box algorithm limits
We conclude this chapter with a generalization of the quantum search algorithm which
provides insightful bounds on the power of quantum computation. At the beginning of
the chapter, we described the search problem as ﬁnding an n-bit integer x such that
the function f : {0, 1}n →{0, 1} evaluates to f(x) = 1. Related to this is the decision
problem of whether or not there exists x such that f(x) = 1. Solving this decision
problem is equivalently difﬁcult, and can be expressed as computing the Boolean function
F(X) = X0 ∨X1 ∨· · · ∨XN−1, where ∨denotes the binary OR operation, Xk ≡f(k),
and X denotes the set {X0, X1, . . . , XN−1}. More generally, we may wish to compute
some function other than OR. For example, F(X) could be the AND, PARITY (sum
modulo two), or MAJORITY (F(X) = 1 if and only if more Xk = 1 than not) functions.
In general, we can consider F to be any Boolean function. How fast (measured in number
of queries) can a computer, classical or quantum, compute these functions, given an oracle
for f?
It might seem difﬁcult to answer such questions without knowing something about the
272
Quantum search algorithms
function f, but in fact a great deal can be determined even in this ‘black box’ model, where
the means by which the oracle accomplishes its task is taken for granted, and complexity
is measured only in terms of the number of required oracle queries. The analysis of
the search algorithm in the previous sections demonstrated one way to approach such
problems, but a more powerful approach for obtaining query complexities is the method
of polynomials, which we now brieﬂy describe.
Let us begin with some useful deﬁnitions. The deterministic query complexity D(F)
is the minimum number of oracle queries a classical computer must perform to compute
F with certainty. The quantum equivalent, QE(F), is the minimum number of oracle
queries a quantum computer requires to compute F with certainty. Since a quantum
computer produces probabilistic outputs by nature, a more interesting quantity is the
bounded error complexity Q2(F), the minimum number of oracle queries a quantum
computer requires to produce an output which equals F with probability at least 2/3.
(The 2/3 is an arbitrary number – the probability need only be bounded ﬁnitely away
from 1/2 in order to be boosted close to 1 by repetitions.) A related measure is the zero-
error complexity Q0(F), the minimum number of oracle queries a quantum computer
requires to produce an output which either equals F with certainty, or, with probability
less than 1/2, an admission of an inconclusive result. All these bounds must hold for any
oracle function f (or in other words, any input X into F). Note that Q2(F) ≤Q0(F) ≤
QE(F) ≤D(F) ≤N.
The method of polynomials is based upon the properties of minimum-degree multi-
linear polynomials (over the real numbers) which represent Boolean functions. All the
polynomials we shall consider below are functions of Xk ∈{0, 1} and are thus multi-
linear, since X2
k = Xk. We say that a polynomial p : RN →R represents F if p(X) =
F(X) for all X ∈{0, 1}N (where R denotes the real numbers). Such a polynomial p
always exists, since we can explicitly construct a suitable candidate:
p(X) =

Y ∈{0,1}N
F(Y )
N−1
/
k=0

1 −(Yk −Xk)2	
.
(6.54)
That the minimum degree p is unique is left as Exercise 6.18 for the reader. The minimum
degree of such a representation for F, denoted as deg(F), is a useful measure of the
complexity of F. For example, it is known that deg(OR), deg(AND), and deg(PARITY)
are all equal to N. In fact, it is known that the degree of most functions is of order N.
Moreover, it has also been proven that
D(F) ≤2 deg(F)4 .
(6.55)
This result places an upper bound on the performance of deterministic classical com-
putation in calculating most Boolean functions. Extending this concept, if a polynomial
satisﬁes |p(X)−F(X)| ≤1/3 for all X ∈{0, 1}N, we say p approximates F, and 8
deg(F)
denotes the minimum degree of such an approximating polynomial. Such measures are
important in randomized classical computation and, as we shall see, in describing the
quantum case. It is known that 8
deg(PARITY) = N,
8
deg(OR) ∈Θ(
√
N)
and
8
deg(AND) ∈Θ(
√
N) ,
(6.56)
and
D(F) ≤216 8
deg(F)6 .
(6.57)
Black box algorithm limits
273
The bounds of Equations (6.55) and (6.57) are only the best known at the time of writing;
their proof is outside the scope of this book, but you may ﬁnd further information about
them in ‘History and further reading’. It is believed that tighter bounds are possible, but
these will be good enough for our purposes.
Exercise 6.18:
Prove that the minimum degree polynomial representing a Boolean
function F(X) is unique.
Exercise 6.19:
Show that P(X) = 1 −(1 −X0)(1 −X1) . . . (1 −XN−1) represents OR.
Polynomials naturally arise in describing the results of quantum algorithms. Let us
write the output of a quantum algorithm Q which performs T queries to an oracle O as
2n−1

k=0
ck|k⟩.
(6.58)
We will show that the amplitudes ck are polynomials of degree at most T in the vari-
ables X0, X1, . . . , XN−1. Any Q can be realized using the quantum circuit shown in
Figure 6.10. The state |ψ0⟩right before the ﬁrst oracle query can be written as
|ψ0⟩=

ij
'
ai0j|i⟩|0⟩+ ai1j|i⟩|1⟩
(
|j⟩,
(6.59)
where the ﬁrst label corresponds to the n qubit oracle query, the next to a single qubit
in which the oracle leaves its result, and the last to the m −n −1 working qubits used
by Q. After the oracle query, we obtain the state
|ψ1⟩=

ij
'
ai0j|i⟩|Xi⟩+ ai1j|i⟩|Xi ⊕1⟩
(
|j⟩,
(6.60)
but since Xi is either 0 or 1, we can re-express this as
|ψ1⟩=

ij
'
(1 −Xi)ai0j + Xiai1j
(
|i0⟩+
'
(1 −Xi)ai1j + Xiai0j
(
|i1⟩
	
|j⟩.
(6.61)
Note that in |ψ0⟩, the amplitudes of the computational basis states were of degree 0 in X,
while those of |ψ1⟩are of degree 1 (linear in X). The important observation is that any
unitary operation which Q performs before or after the oracle query cannot change the
degree of these polynomials, but each oracle call can increase the degree by at most one.
Thus, after T queries, the amplitudes are polynomials of at most degree T. Moreover,
measuring the ﬁnal output (6.58) in the computational basis produces a result k with
probability Pk(X) = |ck|2, which are real-valued polynomials in X of degree at most 2T.
Figure 6.10. General quantum circuit for a quantum algorithm which performs T queries to an oracle O.
U0, U1, . . . , UT are arbitrary unitary transforms on m qubits, and the oracle acts on n + 1 qubits.
274
Quantum search algorithms
The total probability P(X) of obtaining a one as the output from the algorithm is a
sum over some subset of the polynomials Pk(X), and thus also has degree at most 2T. In
the case that Q produces the correct answer with certainty we must have P(X) = F(X),
and thus deg(F) ≤2T, from which we deduce
QE(F) ≥deg(F)
2
.
(6.62)
In the case where Q produces an answer with bounded probability of error it follows that
P(X) approximates F(X), and thus 8
deg(F) ≤2T, from which we deduce
Q2(F) ≥
8
deg(F)
2
.
(6.63)
Combining (6.55) and (6.62), we ﬁnd that
QE(F) ≥
D(F)
32
1/4
.
(6.64)
Similarly, combining (6.57) and (6.63), we ﬁnd that
Q2(F) ≥
 D(F)
13 824
1/6
.
(6.65)
This means that in computing Boolean functions using a black box, quantum algorithms
may only provide a polynomial speedup over classical algorithms, at best – and even that
is not generally possible (since deg(F) is Ω(N) for most functions). On the other hand,
it is known that for F = OR, D(F) = N, and the randomized classical query complexity
R(F) ∈Θ(N), whereas combining (6.63) and (6.56), and the known performance of the
quantum search algorithm, shows that Q2(F) ∈Θ(
√
N). This square root speedup is just
what the quantum search algorithm achieves, and the method of polynomials indicates
that the result can perhaps be generalized to a somewhat wider class of problems, but
without extra information about the structure of the black box oracle function f, no
exponential speedup over classical algorithms is possible.
Exercise 6.20:
Show that Q0(OR) ≥N by constructing a polynomial which
represents the OR function from the output of a quantum circuit which
computes OR with zero error.
Problem 6.1: (Finding the minimum)
Suppose x1, . . . , xN is a database of
numbers held in memory, as in Section 6.5. Show that only O(log(N)
√
N)
accesses to the memory are required on a quantum computer, in order to ﬁnd
the smallest element on the list, with probability at least one-half.
Problem 6.2: (Generalized quantum searching)
Let |ψ⟩be a quantum state, and
deﬁne U|ψ⟩≡I −2|ψ⟩⟨ψ|. That is, U|ψ⟩gives the state |ψ⟩a −1 phase, and
leaves states orthogonal to |ψ⟩invariant.
(1) Suppose we have a quantum circuit implementing a unitary operator U such
that U|0⟩⊗n = |ψ⟩. Explain how to implement U|ψ⟩.
Chapter problems
275
(2) Let |ψ1⟩= |1⟩, |ψ2⟩= (|0⟩−|1⟩)/
√
2, |ψ3⟩= (|0⟩−i|1⟩)/
√
2. Suppose an
unknown oracle O is selected from the set U|ψ1⟩, U|ψ2⟩, U|ψ3⟩. Give a
quantum algorithm which identiﬁes the oracle with just one application of
the oracle. (Hint: consider superdense coding.)
(3) Research: More generally, given k states |ψ1⟩, . . . , |ψk⟩, and an unknown
oracle O selected from the set U|ψ1⟩, . . . , U|ψk⟩, how many oracle
applications are required to identify the oracle, with high probability?
Problem 6.3: (Database retrieval)
Given a quantum oracle which returns
|k, y ⊕Xk⟩given an n qubit query (and one scratchpad qubit) |k, y⟩, show that
with high probability, all N = 2n bits of X can be obtained using only
N/2 +
√
N queries. This implies the general upper bound Q2(F) ≤N/2 +
√
N
for any F.
Problem 6.4: (Quantum searching and cryptography)
Quantum searching can,
potentially, be used to speed up the search for cryptographic keys. The idea is to
search through the space of all possible keys for decryption, in each case trying
the key, and checking to see whether the decrypted message makes ‘sense’.
Explain why this idea doesn’t work for the Vernam cipher (Section 12.6). When
might it work for cryptosystems such as DES? (For a description of DES see, for
example, [MvOV96] or [Sch96a].)
Summary of Chapter 6: Quantum search algorithms
• Quantum search algorithm: For a search problem with M solutions out of
N = 2n possibilities, prepare 
x |x⟩and then repeat G ≡H⊗nUH⊗nO a total
of O(

N/M) times, where O is the search oracle, |x⟩→−|x⟩if x is a solution,
no change otherwise, and U takes |0⟩→−|0⟩and leaves all other computational
basis states alone. Measuring yields a solution to the search problem with high
probability.
• Quantum counting algorithm: Suppose a search problem has an unknown
number M of solutions. G has eigenvalues exp(±iθ) where sin2(θ/2) = M/N.
The Fourier transform based phase estimation procedure enables us to estimate
M to high accuracy using O(
√
N) oracle applications. Quantum counting, in turn,
allows us to determine whether a given search problem has any solutions, and to
ﬁnd one if there are, even if the number of solutions is not known in advance.
• Polynomial bounds: For problems which are described as evaluations of total
functions F (as opposed to partial functions, or ‘promise’ problems), quantum
algorithms can give no more than a polynomial speedup over classical algorithms.
Speciﬁcally, Q2(F) ≥

D(F)/13 824
	1/6
. Moreover, the performance of the quan-
tum search is optimal: it is Θ(
√
N).
276
Quantum search algorithms
History and further reading
The quantum search algorithm and much of its further development and elaboration is
due to Grover[Gro96, Gro97]. Boyer, Brassard, Høyer and Tapp[BBHT98] wrote an inﬂuential
paper in which they developed the quantum search algorithm for cases where the number
of solutions M is greater than one, and outlined the quantum counting algorithm, later
developed in more detail by Brassard, Høyer, and Tapp[BHT98], and from the point of
view of phase estimation by Mosca[Mos98]. That the Grover iteration can be understood
as a product of two reﬂections was ﬁrst pointed out in a review by Aharonov[Aha99b]. The
continuous-time Hamiltonian (6.18) was ﬁrst investigated by Farhi and Gutmann[FG98],
from a rather different point of view than we take in Section 6.2. That Grover’s algorithm
is the best possible oracle-based search algorithm was proved by Bennett, Bernstein,
Brassard and Vazirani[BBBV97]. The version of this proof we have presented is based upon
that given by Boyer, Brassard, Høyer and Tapp[BBHT98]. Zalka[Zal99] has reﬁned these
proofs to show that the quantum search algorithm is, asymptotically, exactly optimal.
The method of polynomials for bounding the power of quantum algorithms was intro-
duced into quantum computing by Beals, Buhrman, Cleve, Mosca, and de Wolf[BBC+98].
An excellent discussion is also available in Mosca’s Ph.D. thesis[Mos99], on which much
of the discussion in Section 6.7 is based. A number of results are quoted in that sec-
tion without proof; here are the citations: Equation (6.55) is attributed to Nisan and
Smolensky in [BBC+98], but otherwise is presently unpublished, (6.56) is derived from
a theorem by Paturi[Pat92] and (6.57) is derived in [BBC+98]. A better bound than (6.65)
is given in [BBC+98], but requires concepts such as block sensitivity which are outside
the scope of this book. A completely different approach for bounding quantum black box
algorithms, using arguments based on entanglement, was presented by Ambainis[Amb00].
Problem 6.1 is due to D¨urr and Høyer[DH96]. Problem 6.3 is due to van Dam[van98a].
7 Quantum computers: physical realization
Computers in the future may weigh no more than 1.5 tons.
– Popular Mechanics, forecasting the relentless march of science, 1949
I think there is a world market for maybe ﬁve computers.
– Thomas Watson, chairman of IBM, 1943
Quantum computation and quantum information is a ﬁeld of fundamental interest be-
cause we believe quantum information processing machines can actually be realized in
Nature. Otherwise, the ﬁeld would be just a mathematical curiosity! Nevertheless, ex-
perimental realization of quantum circuits, algorithms, and communication systems has
proven extremely challenging. In this chapter we explore some of the guiding princi-
ples and model systems for physical implementation of quantum information processing
devices and systems.
We begin in Section 7.1 with an overview of the tradeoffs in selecting a physical real-
ization of a quantum computer. This discussion provides perspective for an elaboration of
a set of conditions sufﬁcient for the experimental realization of quantum computation in
Section 7.2. These conditions are illustrated in Sections 7.3 through 7.7, through a series
of case studies, which consider ﬁve different model physical systems: the simple harmonic
oscillator, photons and nonlinear optical media, cavity quantum electrodynamics devices,
ion traps, and nuclear magnetic resonance with molecules. For each system, we brieﬂy
describe the physical apparatus, the Hamiltonian which governs its dynamics, means for
controlling the system to perform quantum computation, and its principal drawbacks. We
do not go into much depth in describing the physics of these systems; as each of these are
entire ﬁelds unto themselves, that would be outside the scope of this book! Instead, we
summarize just the concepts relevant to quantum computation and quantum information
such that both the experimental challenge and theoretical potential can be appreciated.
On the other hand, analyzing these systems from the standpoint of quantum information
also provides a fresh perspective which we hope you will ﬁnd insightful and useful, as
it also allows strikingly simple derivations of some important physics. We conclude the
chapter in Section 7.8 by discussing aspects of some other physical systems – quantum
dots, superconducting gates, and spins in semiconductors – which are also of interest
to this ﬁeld. For the beneﬁt of the reader wishing to catch just the highlights of each
implementation, a summary is provided at the end of each section.
7.1
Guiding principles
What are the experimental requirements for building a quantum computer? The elemen-
tary units of the theory are quantum bits – two-level quantum systems; in Section 1.5
we took a brief look at why it is believed that qubits exist in Nature, and what physical
forms they may take on. To realize a quantum computer, we must not only give qubits
278
Quantum computers: physical realization
some robust physical representation (in which they retain their quantum properties), but
also select a system in which they can be made to evolve as desired. Furthermore, we
must be able to prepare qubits in some speciﬁed set of initial states, and to measure the
ﬁnal output state of the system.
The challenge of experimental realization is that these basic requirements can often
only be partially met. A coin has two states, and makes a good bit, but a poor qubit
because it cannot remain in a superposition state (of ‘heads’ and ‘tails’) for very long.
A single nuclear spin can be a very good qubit, because superpositions of being aligned
with or against an external magnetic ﬁeld can last a long time – even for days. But it
can be difﬁcult to build a quantum computer from nuclear spins because their coupling
to the world is so small that it is hard to measure the orientation of single nuclei. The
observation that the constraints are opposing is general: a quantum computer has to be
well isolated in order to retain its quantum properties, but at the same time its qubits
have to be accessible so that they can be manipulated to perform a computation and to
read out the results. A realistic implementation must strike a delicate balance between
these constraints, so that the relevant question is not how to build a quantum computer,
but rather, how good a quantum computer can be built.
System
τQ
τop
nop = λ−1
Nuclear spin
10−2 −108
10−3 −10−6
105 −1014
Electron spin
10−3
10−7
104
Ion trap (In+)
10−1
10−14
1013
Electron – Au
10−8
10−14
106
Electron – GaAs
10−10
10−13
103
Quantum dot
10−6
10−9
103
Optical cavity
10−5
10−14
109
Microwave cavity
100
10−4
104
Figure 7.1. Crude estimates for decoherence times τQ (seconds), operation times τop (seconds), and maximum
number of operations nop = λ−1 = τQ/τop for various candidate physical realizations of interacting systems of
quantum bits. Despite the number of entries in this table, only three fundamentally different qubit representations
are given: spin, charge, and photon. The ion trap utilizes either ﬁne or hyperﬁne transitions of a trapped atom
(Section 7.6), which correspond to electron and nuclear spin ﬂips. The estimates for electrons in gold and GaAs,
and in quantum dots are given for a charge representation, with an electrode or some conﬁned area either
containing an electron or not. In optical and microwave cavities, photons (of frequencies from gigahertz to
hundreds of terahertz) populating different modes of the cavities represent the qubit. Take these estimates with a
grain of salt: they are only meant to give some perspective on the wide range of possibilities.
What physical systems are potentially good candidates for handling quantum infor-
mation? A key concept in understanding the merit of a particular quantum computer
realization is the notion of quantum noise (sometimes called decoherence) , the subject of
Chapter 8: processes corrupting the desired evolution of the system. This is because the
length of the longest possible quantum computation is roughly given by the ratio of τQ,
the time for which a system remains quantum-mechanically coherent, to τop, the time it
takes to perform elementary unitary transformations (which involve at least two qubits).
These two times are actually related to each other in many systems, since they are both
Conditions for quantum computation
279
determined by the strength of coupling of the system to the external world. Nevertheless,
λ = τop/τQ can vary over a surprisingly wide range, as shown in Figure 7.1.
These estimates give some insight into the merits of different possible physical realiza-
tions of a quantum information processing machine, but many other important sources
of noise and imperfections arise in actual implementations. For example, manipulations
of a qubit represented by two electronic levels of an atom by using light to cause tran-
sitions between levels would also cause transitions to other electronic levels with some
probability. These would also be considered noise processes, since they take the system
out of the two states which deﬁne the qubit. Generally speaking, anything which causes
loss of (quantum) information is a noise process – later, in Chapter 8, we discuss the
theory of quantum noise in more depth.
7.2
Conditions for quantum computation
Let us return to discuss in detail the four basic requirements for quantum computation
which were mentioned at the beginning of the previous section. These requirements are
the abilities to:
1. Robustly represent quantum information
2. Perform a universal family of unitary transformations
3. Prepare a ﬁducial initial state
4. Measure the output result
7.2.1
Representation of quantum information
Quantum computation is based on transformation of quantum states. Quantum bits are
two-level quantum systems, and as the simplest elementary building blocks for a quan-
tum computer, they provide a convenient labeling for pairs of states and their physical
realizations. Thus, for example, the four states of a spin-3/2 particle, |m = +3/2⟩, |m =
+1/2⟩, |m = −1/2⟩, |m = −3/2⟩, could be used to represent two qubits.
For the purpose of computation, the crucial realization is that the set of accessible states
should be ﬁnite. The position x of a particle along a one-dimensional line is not generally
a good set of states for computation, even though the particle may be in a quantum state
|x⟩, or even some superposition 
x cx|x⟩. This is because x has a continuous range
of possibilities, and the Hilbert space has inﬁnite size, so that in the absence of noise
the information capacity is inﬁnite. For example, in a perfect world, the entire texts
of Shakespeare could be stored in (and retrieved from) the inﬁnite number of digits in
the binary fraction x = 0.010111011001 . . .. This is clearly unrealistic; what happens in
reality is that the presence of noise reduces the number of distinguishable states to a ﬁnite
number.
In fact, it is generally desirable to have some aspect of symmetry dictate the ﬁniteness of
the state space, in order to minimize decoherence. For example, a spin-1/2 particle lives
in a Hilbert space spanned by the | ↑⟩and | ↓⟩states; the spin state cannot be anything
outside this two-dimensional space, and thus is a nearly ideal quantum bit when well
isolated.
If the choice of representation is poor, then decoherence will result. For example,
as described in Box 7.1, a particle in a ﬁnite square well which is just deep enough to
contain two bound states would make a mediocre quantum bit, because transitions from
280
Quantum computers: physical realization
the bound states to the continuum of unbound states would be possible. These would lead
to decoherence since they could destroy qubit superposition states. For single qubits, the
ﬁgure of merit is the minimum lifetime of arbitrary superposition states; a good measure,
used for spin states and atomic systems, is T2, the (‘transverse’) relaxation time of states
such as (|0⟩+ |1⟩)/
√
2. Note that T1, the (‘longitudinal’) relaxation time of the higher
energy |1⟩state, is just a classical state lifetime, which is usually longer than T2.
Box 7.1: Square wells and qubits
A prototypical quantum system is known as the ‘square well,’ which is a particle in
a one-dimensional box, behaving according to Schr¨odinger’s equation, (2.86). The
Hamiltonian for this system is H = p2/2m+V (x), where V (x) = 0 for 0 < x < L,
and V (x) = ∞otherwise. The energy eigenstates, expressed as wavefunctions in
the position basis, are
|ψn⟩=
)
2
L sin
'nπ
L x
(
,
(7.1)
where n is an integer, and |ψn(t)⟩= e−iEnt|ψn⟩, with En = n2π2m/2L2. These
states have a discrete spectrum. In particular, suppose that we arrange matters such
that only the two lowest energy levels need be considered in an experiment. We
deﬁne an arbitrary wavefunction of interest as |ψ⟩= a |ψ1⟩+ b |ψ2⟩. Since
|ψ(t)⟩= e−i(E1+E2)/2t ae−iωt|ψ1⟩+ beiωt|ψ2⟩
 ,
(7.2)
where ω = (E1 −E2)/2, we can just forget about everything except a and b, and
write our state abstractly as the two-component vector |ψ⟩=
 a
b

. This two-level
system represents a qubit! Does our two-level system transform like a qubit? Under
time evolution, this qubit evolves under the effective Hamiltonian H = ℏωZ, which
can be disregarded by moving into the rotating frame. To perform operations to
this qubit, we perturb H. Consider the effect of adding the additional term
δV (x) = −V0(t) 9π2
16L
 x
L −1
2

(7.3)
to V (x). In the basis of our two-level system, this can be rewritten by taking the
matrix elements Vnm = ⟨ψn|δV (x)|ψm⟩, giving V11 = V22 = 0, and V12 = V21 = V0,
such that, to lowest order in V0, the perturbation to H is H1 = V0(t)X. This
generates rotations about the ˆx axis. Similar techniques can be used to perform
other single qubit operations, by manipulating the potential function.
This shows how a single qubit can be represented by the two lowest levels in a square
well potential, and how simple perturbations of the potential can effect computa-
tional operations on the qubit. However, perturbations also introduce higher order
effects, and in real physical systems boxes are not inﬁnitely deep, other levels begin
to enter the picture, and our two-level approximation begins to fail. Also, in reality,
the controlling system is just another quantum system, and it couples to the one we
are trying to do quantum computation with. These problems lead to decoherence.
Conditions for quantum computation
281
7.2.2
Performance of unitary transformations
Closed quantum systems evolve unitarily as determined by their Hamiltonians, but to
perform quantum computation one must be able to control the Hamiltonian to ef-
fect an arbitrary selection from a universal family of unitary transformations (as de-
scribed in Section 4.5). For example, a single spin might evolve under the Hamiltonian
H = Px(t)X + Py(t)Y , where P{x,y} are classically controllable parameters. From Ex-
ercise 4.10, we know that by manipulating Px and Py appropriately, one can perform
arbitrary single spin rotations.
According to the theorems of Section 4.5, any unitary transform can be composed
from single spin operations and controlled-
gates, and thus realization of those two
kinds of quantum logic gates are natural goals for experimental quantum computation.
However, implicitly required also is the ability to address individual qubits, and to apply
these gates to select qubits or pairs of qubits. This is not simple to accomplish in many
physical systems. For example, in an ion trap, one can direct a laser at one of many
individual ions to selectively excite it, but only as long as the ions are spatially separated
by a wavelength or more.
Unrecorded imperfections in unitary transforms can lead to decoherence. In Chapter 8
we shall see how the average effect of random kicks (small rotations to a single spin about
its ˆz axis) leads to loss of quantum information which is represented by the relative phases
in a quantum state. Similarly, the cumulative effect of systematic errors is decoherence,
when the information needed to be able to reverse them is lost. Furthermore, the control
parameters in the Hamiltonian are only approximately classical controls: in reality, the
controlling system is just another quantum system, and the true Hamiltonian should
include the back-action of the control system upon the quantum computer. For example,
instead of Px(t) in the above example, one actually has a Jaynes–Cummings type atom–
photon interaction Hamiltonian (Section 7.5.2), with Px(t) = 
k ωk(t)(ak + a†
k) or
something similar being the cavity photon ﬁeld. After interacting with a qubit, a photon
can carry away information about the state of the qubit, and this is thus a decoherence
process.
Two important ﬁgures of merit for unitary transforms are the minimum achievable
ﬁdelity F (Chapter 9), and the maximum time top required to perform elementary op-
erations such as single spin rotations or a controlled-
gate.
7.2.3
Preparation of ﬁducial initial states
One of the most important requirements for being able to perform a useful computation,
even classically, is to be able to prepare the desired input. If one has a box which can
perform perfect computations, what use is it if numbers cannot be input? With classical
machines, establishing a deﬁnite input state is rarely a difﬁculty – one merely sets some
switches in the desired conﬁguration and that deﬁnes the input state. However, with
quantum systems this can be very difﬁcult, depending on the realization of qubits.
Note that it is only necessary to be able to (repeatedly) produce one speciﬁc quantum
state with high ﬁdelity, since a unitary transform can turn it into any other desired input
state. For example, being able to put n spins into the |00 . . . 0⟩state is good enough. The
fact that they may not stay there for very long due to thermal heating is a problem with
the choice of representation.
Input state preparation is a signiﬁcant problem for most physical systems. For example,
ions can be prepared in good input states by physically cooling them into their ground state
282
Quantum computers: physical realization
(Section 7.6), but this is challenging. Moreover, for physical systems in which ensembles
of quantum computers are involved, extra concerns arise. In nuclear magnetic resonance
(Section 7.7), each molecule can be thought of as a single quantum computer, and a large
number of molecules is needed to obtain a measurable signal strength. Although qubits
can remain in arbitrary superposition states for relatively long times, it is difﬁcult to put
all of the qubits in all of the molecules into the same state, because the energy difference
ℏω between the |0⟩and |1⟩states is much smaller than kBT. On the other hand, simply
letting the system equilibrate establishes it in a very well-known state, the thermal one,
with the density matrix ρ ≈e−H/kBT /Z, where Z is a normalization factor required to
maintain tr(ρ) = 1.
Two ﬁgures of merit are relevant to input state preparation: the minimum ﬁdelity
with which the initial state can be prepared in a given state ρin, and the entropy of
ρin. The entropy is important because, for example, it is very easy to prepare the state
ρin = I/2n with high ﬁdelity, but that is a useless state for quantum computation, since
it is invariant under unitary transforms! Ideally, the input state is a pure state, with zero
entropy. Generally, input states with non-zero entropy reduce the accessibility of the
answer from the output result.
7.2.4
Measurement of output result
What measurement capability is required for quantum computation? For the purpose of
the present discussion, let us think of measurement as a process of coupling one or more
qubits to a classical system such that after some interval of time, the state of the qubits
is indicated by the state of the classical system. For example, a qubit state a|0⟩+ b|1⟩,
represented by the ground and excited states of a two-level atom, might be measured by
pumping the excited state and looking for ﬂuorescence. If an electrometer indicates that
ﬂuorescence had been detected by a photomultiplier tube, then the qubit would collapse
into the |1⟩state; this would happen with probability |b|2. Otherwise, the electrometer
would detect no charge, and the qubit would collapse into the |0⟩state.
An important characteristic of the measurement process for quantum computation is
the wavefunction collapse which describes what happens when a projective measurement
is performed (Section 2.2.5). The output from a good quantum algorithm is a super-
position state which gives a useful answer with high probability when measured. For
example, one step in Shor’s quantum factoring algorithm is to ﬁnd an integer r from
the measurement result, which is an integer close to qc/r, where q is the dimension
of a Hilbert space. The output state is actually in a nearly uniform superposition of all
possible values of c, but a measurement collapses this into a single, random integer, thus
allowing r to be determined with high probability (using a continued fraction expansion,
as was described in Chapter 5).
Many difﬁculties with measurement can be imagined; for example, inefﬁcient photon
counters and ampliﬁer thermal noise can reduce the information obtained about mea-
sured qubit states in the scheme just described. Furthermore, projective measurements
(sometimes called ‘strong’ measurements) are often difﬁcult to implement. They require
that the coupling between the quantum and classical systems be large, and switchable.
Measurements should not occur when not desired; otherwise they can be a decoherence
process.
Surprisingly, however, strong measurements are not necessary; weak measurements
which are performed continuously and never switched off, are usable for quantum com-
Harmonic oscillator quantum computer
283
putation. This is made possible by completing the computation in time short compared
with the measurement coupling, and by using large ensembles of quantum computers.
These ensembles together give an aggregate signal which is macroscopically observable
and indicative of the quantum state. Use of an ensemble introduces additional problems.
For example, in the factoring algorithm, if the measurement output is q⟨c⟩/r, the algo-
rithm would fail because ⟨c⟩, the average value of c, is not necessarily an integer (and
thus the continued fraction expansion would not be possible). Fortunately, it is possible
to modify quantum algorithms to work with ensemble average readouts. This will be
discussed further in Section 7.7.
A good ﬁgure of merit for measurement capability is the signal to noise ratio (SNR).
This accounts for measurement inefﬁciency as well as inherent signal strength available
from coupling a measurement apparatus to the quantum system.
7.3
Harmonic oscillator quantum computer
Before continuing on to describe a complete physical model for a realizable quantum
computer, let us pause for a moment to consider a very elementary system – the simple
harmonic oscillator – and discuss why it does not serve as a good quantum computer.
The formalism used in this example will also serve as a basis for studying other physical
systems.
7.3.1
Physical apparatus
An example of a simple harmonic oscillator is a particle in a parabolic potential well,
V (x) = mω2x2/2. In the classical world, this could be a mass on a spring, which oscillates
back and forth as energy is transfered between the potential energy of the spring and the
kinetic energy of the mass. It could also be a resonant electrical circuit, where the energy
sloshes back and forth between the inductor and the capacitor. In these systems, the total
energy of the system is a continuous parameter.
In the quantum domain, which is reached when the coupling to the external world
becomes very small, the total energy of the system can only take on a discrete set of
values. An example is given by a single mode of electromagnetic radiation trapped in
a high Q cavity; the total amount of energy (up to a ﬁxed offset) can only be integer
multiples of ℏω, an energy scale which is determined by the fundamental constant ℏand
the frequency of the trapped radiation, ω.
The set of discrete energy eigenstates of a simple harmonic oscillator can be labeled
as |n⟩, where n = 0, 1, . . ., ∞. The relationship to quantum computation comes by
taking a ﬁnite subset of these states to represent qubits. These qubits will have lifetimes
determined by physical parameters such as the cavity quality factor Q, which can be made
very large by increasing the reﬂectivity of the cavity walls. Moreover, unitary transforms
can be applied by simply allowing the system to evolve in time. However, there are
problems with this scheme, as will become clear below. We begin by studying the system
Hamiltonian, then discuss how one might implement simple quantum logic gates such
as the controlled-
.
284
Quantum computers: physical realization
7.3.2
The Hamiltonian
The Hamiltonian for a particle in a one-dimensional parabolic potential is
H = p2
2m + 1
2mω2x2 ,
(7.4)
where p is the particle momentum operator, m is the mass, x is the position operator, and
ω is related to the potential depth. Recall that x and p are operators in this expression
(see Box 7.2), which can be rewritten as
H = ℏω

a†a + 1
2

,
(7.5)
where a† and a are creation and annihilation operators, deﬁned as
a =
1
√
2mℏω
'
mωx + ip
(
(7.6)
a† =
1
√
2mℏω
'
mωx −ip
(
.
(7.7)
The zero point energy ℏω/2 contributes an unobservable overall phase factor, which can
be disregarded for our present purpose.
The eigenstates |n⟩of H, where n = 0, 1, . . ., have the properties
a†a|n⟩= n|n⟩
(7.10)
a†|n⟩=
√
n + 1 |n + 1⟩
(7.11)
a|n⟩= √n |n −1⟩.
(7.12)
Later, we will ﬁnd it convenient to express interactions with a simple harmonic oscillator
by introducing additional terms involving a and a†, and interactions between oscillators
with terms such as a†
1a2 + a1a†
2. For now, however, we conﬁne our attention to a single
oscillator.
Exercise 7.1:
Using the fact that x and p do not commute, and that in fact
[x, p] = iℏ, explicitly show that a†a = H/ℏω −1/2.
Exercise 7.2:
Given that [x, p] = iℏ, compute [a, a†].
Exercise 7.3:
Compute [H, a] and use the result to show that if |ψ⟩is an eigenstate of
H with energy E ≥nℏω, then an|ψ⟩is an eigenstate with energy E −nℏω.
Exercise 7.4:
Show that |n⟩= (a†)n
√
n! |0⟩.
Exercise 7.5:
Verify that Equations (7.11) and (7.12) are consistent with (7.10) and
the normalization condition ⟨n|n⟩= 1.
Time evolution of the eigenstates is given by solving the Schr¨odinger equation, (2.86),
from which we ﬁnd that the state |ψ(0)⟩= 
n cn(0)|n⟩evolves in time to become
|ψ(t)⟩= e−iHt/ℏ|ψ(0)⟩=

n
cne−inωt|n⟩.
(7.13)
We will assume for the purpose of discussion that an arbitrary state can be perfectly
prepared, and that the state of the system can be projectively measured (Section 2.2.3),
Harmonic oscillator quantum computer
285
Box 7.2: The quantum harmonic oscillator
The harmonic oscillator is an extremely important and useful concept in the quan-
tum description of the physical world, and a good way to begin to understand its
properties is to determine the energy eigenstates of its Hamiltonian, (7.4). One way
to do this is simply to solve the Schr¨odinger equation
ℏ2
2m
d2ψn(x)
dx2
+ 1
2mω2x2ψn(x) = Eψn(x)
(7.8)
for ψn(x) and the eigenenergies E, subject to ψ(x) →0 at x = ±∞, and
7 |ψ(x)|2 =
1; the ﬁrst ﬁve solutions are sketched here:
These wavefunctions describe the probability amplitudes that a particle in the har-
monic oscillator will be found at different positions within the potential.
Although these pictures may give some intuition about what a physical system
is doing in co-ordinate space, we will generally be more interested in the abstract
algebraic properties of the states. Speciﬁcally, suppose |ψ⟩satisﬁes (7.8) with energy
E. Then deﬁning operators a and a† as in (7.6)–(7.7), we ﬁnd that since [H, a†] =
ℏωa†,
Ha†|ψ⟩=
'
[H, a†] + a†H
(
|ψ⟩= (ℏω + E)a†|ψ⟩,
(7.9)
that is, a†|ψ⟩is an eigenstate of H, with energy E + ℏω! Similarly, a|ψ⟩is an
eigenstate with energy E −ℏω. Because of this, a† and a are called raising and
lowering operators. It follows that a†n|ψ⟩are eigenstates for any integer n, with
energies E + nℏω. There are thus an inﬁnite number of energy eigenstates, whose
energies are equally spaced apart, by ℏω. Moreover, since H is positive deﬁnite,
there must be some |ψ0⟩for which a|ψ0⟩= 0; this is the ground state – the
eigenstate of H with lowest energy. These results efﬁciently capture the essence of
the quantum harmonic oscillator, and allow us to use a compact notation |n⟩for
the eigenstates, where n is an integer, and H|n⟩= ℏ(n + 1/2)|n⟩. We shall often
work with |n⟩, a, and a† in this chapter, as harmonic oscillators arise in the guise
of many different physical systems.
286
Quantum computers: physical realization
but otherwise, there are no interactions with the external world, so that the system is
perfectly closed.
7.3.3
Quantum computation
Suppose we want to perform quantum computation with the single simple harmonic
oscillator described above. What can be done? The most natural choice for representation
of qubits are the energy eigenstates |n⟩. This choice allows us to perform a controlled-
gate in the following way. Recall that this transformation performs the mapping
|00⟩L
→
|00⟩L
|01⟩L
→
|01⟩L
|10⟩L
→
|11⟩L
|11⟩L
→
|10⟩L ,
(7.14)
on two qubit states (here, the subscript L is used to clearly distinguish ‘logical’ states in
contrast to the harmonic oscillator basis states). Let us encode these two qubits using the
mapping
|00⟩L
=
|0⟩
|01⟩L
=
|2⟩
|10⟩L
=
(|4⟩+ |1⟩)/
√
2
|11⟩L
=
(|4⟩−|1⟩)/
√
2 .
(7.15)
Now suppose that at t = 0 the system is started in a state spanned by these basis states,
and we simply evolve the system forward to time t = π/ℏω. This causes the energy
eigenstates to undergo the transformation |n⟩→exp(−iπa†a)|n⟩= (−1)n|n⟩, such that
|0⟩, |2⟩, and |4⟩stay unchanged, but |1⟩→−|1⟩. As a result, we obtain the desired
controlled-
gate transformation.
In general, a necessary and sufﬁcient condition for a physical system to be able to
perform a unitary transform U is simply that the time evolution operator for the system,
T = exp(−iHt), deﬁned by its Hamiltonian H, has nearly the same eigenvalue spectrum
as U. In the case above, the controlled-
gate was simple to implement because it
only has eigenvalues +1 and −1; it was straightforward to arrange an encoding to obtain
the same eigenvalues from the time evolution operator for the harmonic oscillator. The
Hamiltonian for an oscillator could be perturbed to realize nearly any eigenvalue spec-
trum, and any number of qubits could be represented by simply mapping them into the
inﬁnite number of eigenstates of the system. This suggests that perhaps one might be
able to realize an entire quantum computer in a single simple harmonic oscillator!
7.3.4
Drawbacks
Of course, there are many problems with the above scenario. Clearly, one will not always
know the eigenvalue spectrum of the unitary operator for a certain quantum computation,
even though one may know how to construct the operator from elementary gates. In
fact, for most problems addressed by quantum algorithms, knowledge of the eigenvalue
spectrum is tantamount to knowledge of the solution!
Another obvious problem is that the technique used above does not allow one compu-
tation to be cascaded with another, because in general, cascading two unitary transforms
results in a new transform with unrelated eigenvalues.
Finally, the idea of using a single harmonic oscillator to perform quantum computation
Optical photon quantum computer
287
is ﬂawed because it neglects the principle of digital representation of information. A
Hilbert space of 2n dimensions mapped into the state space of a single harmonic oscillator
would have to allow for the possibility of states with energy 2nℏω. In contrast, the same
Hilbert space could be obtained by using n two-level quantum systems, which has an
energy of at most nℏω. Similar comparisons can be made between a classical dial with
2n settings, and a register of n classical bits. Quantum computation builds upon digital
computation, not analog computation.
The main features of the harmonic oscillator quantum computer are summarized below
(each system we consider will be summarized similarly, at the end of the corresponding
section). With this, we leave behind us the study of single oscillators, and turn next to
systems of harmonic oscillators, made of photons and atoms.
Harmonic oscillator quantum computer
• Qubit representation: Energy levels |0⟩, |1⟩, . . ., |2n⟩of a single quantum
oscillator give n qubits.
• Unitary evolution: Arbitrary transforms U are realized by matching their
eigenvalue spectrums to that given by the Hamiltonian H = a†a.
• Initial state preparation: Not considered.
• Readout: Not considered.
• Drawbacks: Not a digital representation! Also, matching eigenvalues to realize
transformations is not feasible for arbitrary U, which generally have unknown
eigenvalues.
7.4
Optical photon quantum computer
An attractive physical system for representing a quantum bit is the optical photon. Pho-
tons are chargeless particles, and do not interact very strongly with each other, or even
with most matter. They can be guided along long distances with low loss in optical ﬁbers,
delayed efﬁciently using phase shifters, and combined easily using beamsplitters. Photons
exhibit signature quantum phenomena, such as the interference produced in two-slit ex-
periments. Furthermore, in principle, photons can be made to interact with each other,
using nonlinear optical media which mediate interactions. There are problems with this
ideal scenario; nevertheless, many things can be learned from studying the components,
architecture, and drawbacks of an optical photon quantum information processor, as we
shall see in this section.
7.4.1
Physical apparatus
Let us begin by considering what single photons are, how they can represent quantum
states, and the experimental components useful for manipulating photons. The classical
behavior of phase shifters, beamsplitters, and nonlinear optical Kerr media is described.
Photons can represent qubits in the following manner. As we saw in the discussion
of the simple harmonic oscillator, the energy in an electromagnetic cavity is quantized
in units of ℏω. Each such quantum is called a photon. It is possible for a cavity to
contain a superposition of zero or one photon, a state which could be expressed as a qubit
c0|0⟩+ c1|1⟩, but we shall do something different. Let us consider two cavities, whose
total energy is ℏω, and take the two states of a qubit as being whether the photon is in
288
Quantum computers: physical realization
one cavity (|01⟩) or the other (|10⟩). The physical state of a superposition would thus be
written as c0|01⟩+ c1|10⟩; we shall call this the dual-rail representation. Note that we
shall focus on single photons traveling as a wavepacket through free space, rather than
inside a cavity; one can imagine this as having a cavity moving along with the wavepacket.
Each cavity in our qubit state will thus correspond to a different spatial mode.
One scheme for generating single photons in the laboratory is by attenuating the output
of a laser. A laser outputs a state known as a coherent state, |α⟩, deﬁned as
|α⟩= e−|α|2/2
∞

n=0
αn
√
n!
|n⟩,
(7.16)
where |n⟩is an n-photon energy eigenstate. This state, which has been the subject of
thorough study in the ﬁeld of quantum optics, has many beautiful properties which we
shall not describe here. It sufﬁces to understand just that coherent states are naturally
radiated from driven oscillators such as a laser when pumped high above its lasing thresh-
old. Note that the mean energy is ⟨α|n|α⟩= |α|2. When attenuated, a coherent state just
becomes a weaker coherent state, and a weak coherent state can be made to have just one
photon, with high probability.
Exercise 7.6: (Eigenstates of photon annihilation)
Prove that a coherent state is
an eigenstate of the photon annihilation operator, that is, show a|α⟩= λ|α⟩for
some constant λ.
For example, for α =
√
0.1, we obtain the state
√
0.90 |0⟩+
√
0.09 |1⟩+
√
0.002 |2⟩+· · ·.
Thus if light ever makes it through the attenuator, one knows it is a single photon with
probability better than 95%; the failure probability is thus 5%. Note also that 90% of the
time, no photons come through at all; this source thus has a rate of 0.1 photons per unit
time. Finally, this source does not indicate (by means of some classical readout) when a
photon has been output or not; two of these sources cannot be synchronized.
Better synchronicity can be achieved using parametric down-conversion. This involves
sending photons of frequency ω0 into a nonlinear optical medium such as KH2PO4 to
generate photon pairs at frequencies ω1 + ω2 = ω0. Momentum is also conserved, such
that ⃗k1+⃗k2 = ⃗k3, so that when a single ω2 photon is (destructively) detected, then a single
ω1 photon is known to exist (Figure 7.2). By coupling this to a gate, which is opened
only when a single photon (as opposed to two or more) is detected, and by appropriately
delaying the outputs of multiple down-conversion sources, one can, in principle, obtain
multiple single photons propagating in time synchronously, within the time resolution of
the detector and gate.
Single photons can be detected with high quantum efﬁciency for a wide range of
wavelengths, using a variety of technologies. For our purposes, the most important char-
acteristic of a detector is its capability of determining, with high probability, whether
zero or one photon exists in a particular spatial mode. For the dual-rail representation,
this translates into a projective measurement in the computational basis. In practice, im-
perfections reduce the probability of being able to detect a single photon; the quantum
efﬁciency η (0 ≤η ≤1) of a photodetector is the probability that a single photon incident
on the detector generates a photocarrier pair that contributes to detector current. Other
important characteristics of a detector are its bandwidth (time responsivity), noise, and
‘dark counts’ which are photocarriers generated even when no photons are incident.
Optical photon quantum computer
289
9:	


	
;:
Figure 7.2. Parametric down-conversion scheme for generation of single photons.
Three of the most experimentally accessible devices for manipulating photon states are
mirrors, phase shifters and beamsplitters. High reﬂectivity mirrors reﬂect photons and
change their propagation direction in space. Mirrors with 0.01% loss are not unusual.
We shall take these for granted in our scenario. A phase shifter is nothing more than
a slab of transparent medium with index of refraction n different from that of free
space, n0; for example, ordinary borosilicate glass has n ≈1.5n0 at optical wavelengths.
Propagation in such a medium through a distance L changes a photon’s phase by eikL,
where k = nω/c0, and c0 is the speed of light in vacuum. Thus, a photon propagating
through a phase shifter will experience a phase shift of ei(n−n0)Lω/c0 compared to a photon
going the same distance through free space.
Another useful component, the beamsplitter, is nothing more than a partially silvered
piece of glass, which reﬂects a fraction R of the incident light, and transmits 1−R. In the
laboratory, a beamsplitter is usually fabricated from two prisms, with a thin metallic layer
sandwiched in-between, schematically drawn as shown in Figure 7.3. It is convenient to
deﬁne the angle θ of a beamsplitter as cos θ = R; note that the angle parameterizes
the amount of partial reﬂection, and does not necessarily have anything to do with the
physical orientation of the beamsplitter. The two inputs and two outputs of this device
are related by
aout = ain cos θ + bin sin θ
(7.17)
bout = −ain sin θ + bin cos θ ,
(7.18)
where classically we think of a and b as being the electromagnetic ﬁelds of the radiation at
the two ports. Note that in this deﬁnition we have chosen a non-standard phase convention
convenient for our purposes. In the special case of a 50/50 beamsplitter, θ = 45◦.
Nonlinear optics provides one ﬁnal useful component for this exercise: a material


  

 
 



  

 
 

Figure 7.3. Schematic of an optical beamsplitter, showing the two input ports, the two output ports, and the phase
conventions for a 50/50 beamsplitter (θ = π/4). The beamsplitter on the right is the inverse of the one on the left
(the two are distinguished by the dot drawn inside). The input-output relations for the mode operators a and b are
given for θ = π/4.
290
Quantum computers: physical realization
whose index of refraction n is proportional to the total intensity I of light going through
it:
n(I) = n + n2I .
(7.19)
This is known as the optical Kerr effect, and it occurs (very weakly) in materials as
mundane as glass and sugar water. In doped glasses, n2 ranges from 10−14 to 10−7 cm2/W,
and in semiconductors, from 10−10 to 102. Experimentally, the relevant behavior is that
when two beams of light of equal intensity are nearly co-propagated through a Kerr
medium, each beam will experience an extra phase shift of ein2ILω/c0 compared to what
happens in the single beam case. This would be ideal if the length L could be arbitrarily
long, but unfortunately that fails because most Kerr media are also highly absorptive,
or scatter light out of the desired spatial mode. This is the primary reason why a single
photon quantum computer is impractical, as we shall discuss in Section 7.4.3.
We turn next to a quantum description of these optical components.
7.4.2
Quantum computation
Arbitrary unitary transforms can be applied to quantum information, encoded with single
photons in the c0|01⟩+c1|10⟩dual-rail representation, using phase shifters, beamsplitters,
and nonlinear optical Kerr media. How this works can be understood in the following
manner, by giving a quantum-mechanical Hamiltonian description of each of these de-
vices.
The time evolution of a cavity mode of electromagnetic radiation is modeled quantum-
mechanically by a harmonic oscillator, as we saw in Section 7.3.2. |0⟩is the vacuum state,
|1⟩= a†|0⟩is a single photon state, and in general, |n⟩= a†n
√
n!|0⟩is an n-photon state,
where a† is the creation operator for the mode. Free space evolution is described by the
Hamiltonian
H = ℏωa†a ,
(7.20)
and applying (7.13), we ﬁnd that the state |ψ⟩= c0|0⟩+ c1|1⟩evolves in time to be-
come |ψ(t)⟩= c0|0⟩+ c1e−iωt|1⟩. Note that the dual-rail representation is convenient
because free evolution only changes |ϕ⟩= c0|01⟩+ c1|10⟩by an overall phase, which is
undetectable. Thus, for that manifold of states, the evolution Hamiltonian is zero.
Phase shifter. A phase shifter P acts just like normal time evolution, but at a different
rate, and localized to only the modes going through it. That is because light slows down
in a medium with larger index of refraction; speciﬁcally, it takes Δ ≡(n −n0)L/c0 more
time to propagate a distance L in a medium with index of refraction n than in vacuum.
For example, the action of P on the vacuum state is to do nothing: P|0⟩= |0⟩, but on a
single photon state, one obtains P|1⟩= eiΔ|1⟩.
P performs a useful logical operation on a dual-rail state. Placing a phase shifter in
one mode retards its phase evolution with respect to another mode, which travels the
same distance but without going through the shifter. For dual-rail states this transforms
c0|01⟩+c1|10⟩to c0e−iΔ/2|01⟩+c1eiΔ/2|10⟩, up to an irrelevant overall phase. Recall from
Section 4.2 that this operation is nothing more than a rotation,
Rz(Δ) = e−iZΔ/2 ,
(7.21)
where we take as the logical zero |0L⟩= |01⟩and one |1L⟩= |10⟩, and Z is the usual
Optical photon quantum computer
291
Pauli operator. One can thus think of P as resulting from time evolution under the
Hamiltonian
H = (n0 −n)Z ,
(7.22)
where P = exp(−iHL/c0).
Exercise 7.7:
Show that the circuit below transforms a dual-rail state by
|ψout⟩=
 eiπ
0
0
1

|ψin⟩,
(7.23)
if we take the top wire to represent the |01⟩mode, and |10⟩the bottom mode,
and the boxed π to represent a phase shift by π:





Note that in such ‘optical circuits’, propagation in space is explicitly represented
by putting in lumped circuit elements such as in the above, to represent phase
evolution. In the dual-rail representation, evolution according to (7.20) changes
the logical state only by an unobservable global phase, and thus we are free to
disregard it and keep only relative phase shifts.
Exercise 7.8:
Show that P|α⟩= |αeiΔ⟩where |α⟩is a coherent state (note that, in
general, α is a complex number!).
Beamsplitter. A similar Hamiltonian description of the beamsplitter also exists, but
instead of motivating it phenomenologically, let us begin with the Hamiltonian and show
how the expected classical behavior, Equations (7.17)–(7.18) arises from it. Recall that the
beamsplitter acts on two modes, which we shall describe by the creation (annihilation)
operators a (a†) and b (b†). The Hamiltonian is
Hbs = iθ

ab† −a†b
 ,
(7.24)
and the beamsplitter performs the unitary operation
B = exp
θ

a†b −ab† .
(7.25)
The transformations effected by B on a and b, which will later be useful, are found to
be
BaB† = a cos θ + b sin θ
and
BbB† = −a sin θ + b cos θ .
(7.26)
We verify these relations using the Baker–Campbell–Hausdorf formula (also see Exer-
cise 4.49)
eλGAe−λG =
∞

n=0
λn
n! Cn ,
(7.27)
where λ is a complex number, A, G, and Cn are operators, and Cn is deﬁned recursively
as the sequence of commutators C0 = A, C1 = [G, C0], C2 = [G, C1], C3 = [G, C2], . . .,
Cn = [G, Cn−1]. Since it follows from [a, a†] = 1 and [b, b†] = 1 that [G, a] = −b and
[G, b] = a, for G ≡a†b−ab†, we obtain for the expansion of BaB† the series coefﬁcients
292
Quantum computers: physical realization
C0 = a, C1 = [G, a] = −b, C2 = [G, C1] = −a, C3 = [G, C2] = −[G, C0] = b, which in
general are
Cn even = ina
(7.28)
Cn odd = in+1b .
(7.29)
From this, our desired result follows straightforwardly:
BaB† = eθGae−θG
(7.30)
=
∞

n=0
θn
n! Cn
(7.31)
=

n even
(iθ)n
n! a + i

n odd
(iθ)n
n! b
(7.32)
= a cos θ −b sin θ .
(7.33)
The transform BbB† is trivially found by swapping a and b in the above solution. Note
that the beamsplitter operator arises from a deep relationship between the beamsplitter
and the algebra of SU(2), as explained in Box 7.3.
In terms of quantum logic gates, B performs a useful operation. First note that B|00⟩
= |00⟩, that is, when no photons in either input mode exist, no photons will exist in
either output mode. When one photon exists in mode a, recalling that |1⟩= a†|0⟩, we
ﬁnd that
B|01⟩= Ba†|00⟩= Ba†B†B|00⟩= (a† cos θ + b† sin θ)|00⟩= cos θ|01⟩+ sin θ|10⟩.
(7.34)
Similarly, B|10⟩= cos θ|10⟩−sin θ|01⟩. Thus, on the |0L⟩and |1L⟩manifold of states,
we may write B as
B =
 cos θ
−sin θ
sin θ
cos θ

= eiθY .
(7.35)
Phase shifters and beamsplitters together allow arbitrary single qubit operations to be
performed to our optical qubit. This a consequence of Theorem 4.1 on page 175, which
states that all single qubit operations can be generated from ˆz-axis rotations Rz(α) =
exp(−iαZ/2), and ˆy-axis rotations, Ry(α) = exp(−iαY/2). A phase shifter performs Rz
rotations, and a beamsplitter performs Ry rotations.
Exercise 7.9: (Optical Hadamard gate)
Show that the following circuit acts as a
Hadamard gate on dual-rail single photon states, that is, |01⟩→(|01⟩+ |10⟩)/
√
2
and |10⟩→(|01⟩−|10⟩)/
√
2 up to an overall phase:

Exercise 7.10: (Mach–Zehnder interferometer)
Interferometers are optical tools
used to measure small phase shifts, which are constructed from two
beamsplitters. Their basic principle of operation can be understood by this
simple exercise.
1. Note that this circuit performs the identity operation:
Optical photon quantum computer
293
Box 7.3: SU(2) Symmetry and quantum beamsplitters
There is an interesting connection between the Lie group SU(2) and the algebra of
two coupled harmonic oscillators, which is useful for understanding the quantum
beamsplitter transformation. Identify
a†a −b†b →Z
(7.36)
a†b →σ+
(7.37)
ab† →σ−,
(7.38)
where Z is the Pauli operator, and σ± = (X ± iY )/2 are raising and lowering
operators deﬁned in terms of Pauli X and Y . From the commutation relations for a,
a†, b, and b†, it is easy to verify that these deﬁnitions satisfy the usual commutation
relations for the Pauli operators, (2.40). Also note that the total number operator,
a†a + b†b, commutes with σz, σ+, and σ−, as it should, being an invariant quantity
under rotations in the SU(2) space. Using X = a†b + ab† and Y = −i(a†b −ab†)
in the traditional SU(2) rotation operator
R(ˆn, θ) = e−iθ⃗σ· ˆn/2
(7.39)
gives us the desired beamsplitter operator when ˆn is taken to be the −ˆy-axis.




2. Compute the rotation operation (on dual-rail states) which this circuit
performs, as a function of the phase shift ϕ:



Exercise 7.11:
What is B|2, 0⟩for θ = π/4?
Exercise 7.12: (Quantum beamsplitter with classical inputs)
What is B|α⟩|β⟩
where |α⟩and |β⟩are two coherent states as in Equation (7.16)? (Hint: recall
that |n⟩= (a†)n
√
n! |0⟩.)
Nonlinear Kerr media. The most important effect of a Kerr medium is the cross
phase modulation it provides between two modes of light. That is classically described
by the n2 term in (7.19), which is effectively an interaction between photons, mediated
by atoms in the Kerr medium. Quantum-mechanically, this effect is described by the
Hamiltonian
Hxpm = −χa†ab†b ,
(7.40)
where a and b describe two modes propagating through the medium, and for a crystal of
294
Quantum computers: physical realization
length L we obtain the unitary transform
K = eiχLa†ab†b .
(7.41)
χ is a coefﬁcient related to n2, and the third order nonlinear susceptibility coefﬁcient
usually denoted as χ(3). That the expected classical behavior arises from this Hamiltonian
is left as Exercise 7.14 for the reader.
By combining Kerr media with beamsplitters, a controlled-
gate can be constructed
in the following manner. For single photon states, we ﬁnd that
K|00⟩= |00⟩
(7.42)
K|01⟩= |01⟩
(7.43)
K|10⟩= |10⟩
(7.44)
K|11⟩= eiχL|11⟩,
(7.45)
and let us take χL = π, such that K|11⟩= −|11⟩. Now consider two dual-rail states,
that is, four modes of light. These live in a space spanned by the four basis states
|e00⟩= |1001⟩, |e01⟩= |1010⟩, |e10⟩= |0101⟩, |e11⟩= |0110⟩. Note that we have ﬂipped
the usual order of the two modes for the ﬁrst pair, for convenience (physically, the two
modes are easily swapped using mirrors). Now, if a Kerr medium is applied to act upon
the two middle modes, then we ﬁnd that K|ei⟩= |ei⟩for all i except K|e11⟩= −|e11⟩.
This is useful because the controlled-
operation can be factored into
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⎤
⎥⎥⎦
"
# 
!
UCN
=
1
√
2
⎡
⎢⎢⎣
1
1
0
0
1
−1
0
0
0
0
1
1
0
0
1
−1
⎤
⎥⎥⎦
"
# 
!
I ⊗H
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1
⎤
⎥⎥⎦
"
# 
!
K
1
√
2
⎡
⎢⎢⎣
1
1
0
0
1
−1
0
0
0
0
1
1
0
0
1
−1
⎤
⎥⎥⎦
"
# 
!
I ⊗H
,
(7.46)
where H is the single qubit Hadamard transform (simply implemented with beamsplitters
and phase shifters), and K is the Kerr interaction we just considered, with χL = π. Such
an apparatus has been considered before, for constructing a reversible classical optical
logic gate, as described in Box 7.4; in the single photon regime, it also functions as a
quantum logic gate.
Summarizing, the
can be constructed from Kerr media, and arbitrary single
qubit operations realized using beamsplitters and phase shifters. Single photons can be
created using attenuated lasers, and detected with photodetectors. Thus, in theory, a
quantum computer can be implemented using these optical components!
Exercise 7.13: (Optical Deutsch–Jozsa quantum circuit)
In Section 1.4.4
(page 34), we described a quantum circuit for solving the one-bit Deutsch–Jozsa
problem. Here is a version of that circuit for single photon states (in the dual-rail
representation), using beamsplitters, phase shifters, and nonlinear Kerr media:
Optical photon quantum computer
295
Box 7.4: The quantum optical Fredkin gate
An optical Fredkin gate can be built using two beamsplitters and a nonlinear Kerr
medium as shown in this schematic diagram:
Kerr
a
b
c
a'
b'
c'
This performs the unitary transform U = B†KB, where B is a 50/50 beamsplitter,
K is the Kerr cross phase modulation operator K = eiξ b†b c†c, and ξ = χL is the
product of the coupling constant and the interaction distance. This simpliﬁes to
give
U=exp

iξc†c
b† −a†
2
 b −a
2

(7.47)
=ei π
2 b†b e
ξ
2 c†c(a†b−b†a) e−i π
2 b†b ei ξ
2 a†a c†c ei ξ
2 b†b c†c .
(7.48)
The ﬁrst and third exponentials are constant phase shifts, and the last two phase
shifts come from cross phase modulation. All those effects are not fundamental,
and can be compensated for. The interesting term is the second exponential, which
is deﬁnes the quantum Fredkin operator
F(ξ) = exp
ξ
2c†c (a†b −b†a)

.
(7.49)
The usual (classical) Fredkin gate operation is obtained for ξ = π, in which case
when no photons are input at c, then a′ = a and b′ = b, but when a single photon is
input at c, then a′ = b and b′ = a. This can be understood by realizing that F(χ) is
like a controlled-beamsplitter operator, where the rotation angle is ξc†c. Note that
this description does not use the dual-rail representation; in that representation,
this Fredkin gate corresponds to a controlled-
gate.





1. Construct circuits for the four possible classical functions Uf using Fredkin
gates and beamsplitters.
2. Why are no phase shifters necessary in this construction?
3. For each Uf show explicitly how interference can be used to explain how the
quantum algorithm works.
296
Quantum computers: physical realization
4. Does this implementation work if the single photon states are replaced by
coherent states?
Exercise 7.14: (Classical cross phase modulation)
To see that the expected
classical behavior of a Kerr medium is obtained from the deﬁnition of K,
Equation (7.41), apply it to two modes, one with a coherent state and the other
in state |n⟩; that is, show that
K|α⟩|n⟩= |αeiχLn⟩|n⟩.
(7.50)
Use this to compute
ρa = Trb

K|α⟩|β⟩⟨β|⟨α|K†	
(7.51)
= e−|β|2 
m
|β|2m
m! |αeiχLm⟩⟨αeiχLm| ,
(7.52)
and show that the main contribution to the sum is for m = |β|2.
7.4.3
Drawbacks
The single photon representation of a qubit is attractive. Single photons are relatively
simple to generate and measure, and in the dual-rail representation, arbitrary single qubit
operations are possible. Unfortunately, interacting photons is difﬁcult – the best nonlin-
ear Kerr media available are very weak, and cannot provide a cross phase modulation of
π between single photon states. In fact, because a nonlinear index of refraction is usually
obtained by using a medium near an optical resonance, there is always some absorption
associated with the nonlinearity, and it can theoretically be estimated that in the best such
arrangement, approximately 50 photons must be absorbed for each photon which expe-
riences a π cross phase modulation. This means that the outlook for building quantum
computers from traditional nonlinear optics components is slim at best.
Nevertheless, from studying this optical quantum computer, we have gained some
valuable insight into the nature of the architecture and system design of a quantum
computer. We now can see what an actual quantum computer might look like in the
laboratory (if only sufﬁciently good components were available to construct it), and a
striking feature is that it is constructed nearly completely from optical interferometers.
In the apparatus, information is encoded both in the photon number and the phase of
the photon, and interferometers are used to convert between the two representations.
Although it is feasible to construct stable optical interferometers, if an alternate, massive
representation of a qubit were chosen, then it could rapidly become difﬁcult to build
stable interferometers because of the shortness of typical de Broglie wavelengths. Even
with the optical representation, the multiple interlocked interferometers which would
be needed to realize a large quantum algorithm would be a challenge to stabilize in the
laboratory.
Historically, optical classical computers were once thought to be promising replace-
ments for electronic machines, but they ultimately failed to live up to expectations when
sufﬁciently nonlinear optical materials were not discovered, and when their speed and
parallelism advantages did not sufﬁciently outweigh their alignment and power disadvan-
tages. On the other hand, optical communications is a vital and important area; one reason
for this is that for distances longer than one centimeter, the energy needed to transmit
Optical cavity quantum electrodynamics
297
a bit using a photon over a ﬁber is smaller than the energy required to charge a typical
50 ohm electronic transmission line covering the same distance. Similarly, it may be that
optical qubits may ﬁnd a natural home in communication of quantum information, such
as in quantum cryptography, rather than in computation.
Despite the drawbacks facing optical quantum computer realizations, the theoretical
formalism which describes them is absolutely fundamental in all the other realizations
we shall study in the remainder of this chapter. In fact, you may think of what we shall
turn to next as being just another kind of optical quantum computer, but with a different
(and better!) kind of nonlinear medium.
Optical photon quantum computer
• Qubit representation: Location of single photon between two modes, |01⟩and
|10⟩, or polarization.
• Unitary evolution: Arbitrary transforms are constructed from phase shifters (Rz
rotations), beamsplitters (Ry rotations), and nonlinear Kerr media, which allow
two single photons to cross phase modulate, performing exp
iχL|11⟩⟨11|
.
• Initial state preparation: Create single photon states (e.g. by attenuating laser
light).
• Readout: Detect single photons (e.g. using a photomultipler tube).
• Drawbacks: Nonlinear Kerr media with large ratio of cross phase modulation
strength to absorption loss are difﬁcult to realize.
7.5
Optical cavity quantum electrodynamics
Cavity quantum electrodynamics (QED) is a ﬁeld of study which accesses an important
regime involving coupling of single atoms to only a few optical modes. Experimentally,
this is made possible by placing single atoms within optical cavities of very high Q; because
only one or two electromagnetic modes exist within the cavity, and each of these has a very
high electric ﬁeld strength, the dipole coupling between the atom and the ﬁeld is very high.
Because of the high Q, photons within the cavity have an opportunity to interact many
times with the atoms before escaping. Theoretically, this technique presents a unique
opportunity to control and study single quantum systems, opening many opportunities
in quantum chaos, quantum feedback control, and quantum computation.
In particular, single-atom cavity QED methods offer a potential solution to the dilemma
with the optical quantum computer described in the previous section. Single photons can
be good carriers of quantum information, but they require some other medium in order
to interact with each other. Because they are bulk materials, traditional nonlinear optical
Kerr media are unsatisfactory in satisfying this need. However, well isolated single atoms
might not necessarily suffer from the same decoherence effects, and moreover, they could
also provide cross phase modulation between photons. In fact, what if the state of single
photons could be efﬁciently transfered to and from single atoms, whose interactions could
be controlled? This potential scenario is the topic of this section.
298
Quantum computers: physical realization
7.5.1
Physical apparatus
The two main experimental components of a cavity QED system are the electromagnetic
cavity and the atom. We begin by describing the basic physics of cavity modes, and then
summarize basic ideas about atomic structure and the interaction of atoms with light.
Fabry–Perot cavity
The main interaction involved in cavity QED is the dipolar interaction ⃗d · ⃗E between an
electric dipole moment ⃗d and an electric ﬁeld ⃗E. How large can this interaction be? It
is difﬁcult in practice to change the size of ⃗d; however, |⃗E| is experimentally accessible,
and one of the most important tools for realizing a very large electric ﬁeld in a narrow
band of frequencies and in a small volume of space, is the Fabry–Perot cavity.
In the approximation that the electric ﬁeld is monochromatic and occupies a single
spatial mode, it can be given a very simple quantum-mechanical description:
⃗E(r) = i⃗ϵ E0

aeikr −a†e−ikr	
.
(7.56)
As described in Box 7.5, these approximations are appropriate for the ﬁeld in a Fabry-
Perot cavity. Here, k = ω/c is the spatial frequency of the light, E0 is the ﬁeld strength,
⃗ϵ is the polarization, and r is the position at which the ﬁeld is desired. a and a† are
creation and annihilation operators for photons in the mode, and behave as described
in Section 7.4.2. Note that the Hamiltonian governing the evolution of the ﬁeld in the
cavity is simply
Hﬁeld = ℏωa†a ,
(7.57)
and this is consistent with the semiclassical notion that the energy is the volume integral
of |⃗E|2 in the cavity.
Exercise 7.15:
Plot (7.55) as a function of ﬁeld detuning ϕ, for R1 = R2 = 0.9.
Two-level atoms
Until this section of the chapter, we have discussed only photons, or interactions such as
the cross phase modulation between photons mediated by a semiclassical medium. Now,
let us turn our attention to atoms, their electronic structure, and their interactions with
photons. This is, of course, a very deep and well-developed ﬁeld of study; we shall only
describe a small part of it that touches upon quantum computation.
The electronic energy eigenstates of an atom can be very complicated (see Box 7.6), but
for our purposes modeling an atom as having only two states is an excellent approximation.
This two-level atom approximation can be valid because we shall be concerned with the
interaction with monochromatic light and, in this case, the only relevant energy levels
are those satisfying two conditions: their energy difference matches the energy of the
incident photons, and symmetries (‘selection rules’) do not inhibit the transition. These
conditions arise from basic conservation laws for energy, angular momentum, and parity.
Energy conservation is no more than the condition that
ℏω = E2 −E1 ,
(7.58)
where E2 and E1 are two eigenenergies of the atom. Angular momentum and parity
conservation requirements can be illustrated by considering the matrix element of ˆr
between two orbital wavefunctions, ⟨l1, m1|ˆr|l2, m2⟩. Without loss of generality, we can
Optical cavity quantum electrodynamics
299
Box 7.5: The Fabry–Perot cavity
A basic component of a Fabry–Perot cavity is a partially silvered mirror, off which
incident light Ea and Eb partially reﬂect and partially transmit, producing the
output ﬁelds Ea′ and Eb′. These are related by the unitary transform
 Ea′
Eb′

=
0
√
R
√
1 −R
√
1 −R
−
√
R
1  Ea
Eb

,
(7.53)
where R is the reﬂectivity of the mirror, and the location of the ‘−’ sign is a
convention chosen as given here for convenience.












	




	


	

///


A Fabry–Perot cavity is made from two plane parallel mirrors of reﬂectivities R1
and R2, upon which light Ein is incident from the outside, as shown in the ﬁgure.
Inside the cavity, light bounces back and forth between the two mirrors, such that
the ﬁeld acquires a phase shift eiϕ on each round-trip; ϕ is a function of the path
length and the frequency of the light. Thus, using (7.53), we ﬁnd the cavity internal
ﬁeld to be
Ecav =

k
Ek =
√1 −R1Ein
1 + eiϕ√R1R2
,
(7.54)
where E0 = √1 −R1Ein, and Ek = −eiϕ√R1R2Ek−1. Similarly, we ﬁnd Eout =
eiϕ/2√1 −R2, and Ereﬂ= √R1Ein + √1 −R1
√R2eiϕEcav.
One of the most important characteristics of a Fabry–Perot cavity for our purpose
is the power in the cavity internal ﬁeld as a function of the input power and ﬁeld
frequency,
Pcav
Pin
=
++++
Ecav
Ein
++++
2
=
1 −R1
|1 + eiϕ√R1R2|2 .
(7.55)
Two aspects are noteworthy. First, frequency selectivity is given by the fact that
ϕ = ωd/c, where d is the mirror separation, c is the speed of light, and ω is
the frequency of the ﬁeld. Physically, it comes about because of constructive and
destructive interference between the cavity ﬁeld and the front surface reﬂected
light. And second, on resonance, the cavity ﬁeld achieves a maximum value which
is approximately 1/(1 −R) times the incident ﬁeld. This property is invaluable for
cavity QED.
300
Quantum computers: physical realization
take ˆr to be in the ˆx −ˆy plane, such that it can be expressed in terms of spherical
harmonics (Box 7.6) as
ˆr =
)
3
8π

(−rx + iry)Y1,+1 + (rx + iry)Y1,−1
	
(7.59)
In this basis, the relevant terms in ⟨l, m1|ˆr|l, m2⟩are
-
Y ∗
l1m1Y1mYl2m2 dΩ .
(7.60)
Recall that m = ±1; this integral is non-zero only when m2−m1 = ±1 and Δl = ±1. The
ﬁrst condition is the conservation of angular momentum, and the second, parity, under
the dipole approximation where ⟨l1, m1|ˆr|l2, m2⟩becomes relevant. These conditions are
selection rules which are important in the two-level atom approximation.
Exercise 7.16: (Electric dipole selection rules)
Show that (7.60) is non-zero only
when m2 −m1 = ±1 and Δl = ±1.
In reality, light is never perfectly monochromatic; it is generated from some source
such as a laser, in which longitudinal modes, pump noise, and other sources give rise to a
ﬁnite linewidth. Also, an atom coupled to the external world never has perfectly deﬁned
energy eigenstates; small perturbations such as nearby ﬂuctuating electric potentials, or
even interaction with the vacuum, cause each energy level to be smeared out and become
a distribution with ﬁnite width.
Nevertheless, by choosing an atom and excitation energy carefully, and by taking
advantage of the selection rules, it is possible to arrange circumstances such that the
two-level atom approximation is superb. The whole point of this procedure is that in this
approximation, if |ψ1⟩and |ψ2⟩are the two selected levels, then the matrix elements of
the ˆr are
rij = ⟨ψi|ˆr|ψj⟩≈r0Y ,
(7.65)
where r0 is some constant, and Y is a Pauli operator (Section 2.1.3; that we obtain Y as
opposed to X doesn’t really matter – it is a matter of convention, and convenience, for
later calculations). This will be relevant in describing interactions between the atom and
incident electric ﬁelds. The Hamiltonian of the atom itself, in this two-level subspace, is
simply
Hatom = ℏω0
2 Z ,
(7.66)
where ℏω0 is the difference of the energies of the two levels, since the two states are
energy eigenstates.
7.5.2
The Hamiltonian
The ⃗d · ⃗E interaction between an atom and a cavity conﬁned electric ﬁeld can be ap-
proximated quite well by a much simpler model, in the two-level approximation of the
atom, using the quantization of the ﬁeld in the cavity, and the minute size of the electron
compared to the wavelength of the ﬁeld. Using the fact that ⃗d ∝ˆr (the electric dipole
size is charge times distance), we can combine (7.56) with (7.65) to obtain the interaction
HI = −igY (a −a†) ,
(7.67)
Optical cavity quantum electrodynamics
301
Box 7.6: Energy levels of an atom
The electrons of an atom behave like particles in a three dimensional box, with a
Hamiltonian of the form
HA =

k
|⃗pk|2
2m −Ze2
rk
+ Hrel + Hee + Hso + Hhf ,
(7.61)
where the ﬁrst two terms describe the balance of the electrons’ kinetic energy with
the Coulomb attraction of the negatively charged electrons to the positively charged
nucleus, Hrel is a relativistic correction term, Hee describes electron–electron cou-
plings and contributions from the fermionic nature of the electrons, Hso is the spin
orbit interaction, which can be interpreted as the spin of the electron interacting
with a magnetic ﬁeld generated by its orbit around the atom, and Hhf is the hy-
perﬁne interaction: the electron spin interacting with the magnetic ﬁeld generated
by the nucleus. The energy eigenstates of HA are generally pretty well categorized
according to three integers or half-integers (quantum numbers): n, the principle
quantum number; l, the orbital angular momentum; and m, its ˆz component. In
addition, S, the total electron spin, and I, the nuclear spin, are often important.
The eigenvalues of H are roughly determined to order α2 by n, to slightly smaller
order by Hee, to order α4 by Hrel and Hso, and to order ≈10−3α4 by Hhf, where
α = 1/137 is the dimensionless ﬁne structure constant.
The derivation of n is simple and follows the usual one-dimensional Schr¨odinger
equation solutions for a particle in a box, since the Coulomb conﬁning potential
is dependent on radial distance only. However, orbital angular momentum is a
feature of being in three dimensions which deserves some explanation. The essential
properties arise from the angular dependence of the coordinate representation of
HA, in which ⃗p becomes the Laplacian operator ⃗∇2, giving the Schr¨odinger equation
Φ(ϕ)
sin θ
d
dθ

sin θdΘ
dθ

+ Θ(θ)
sin2 θ
dΦ(ϕ)
dϕ2
+ l(l + 1)Θ(θ)Φ(ϕ) = 0 ,
(7.62)
where θ and ϕ are the usual spherical coordinates, and Φ and Θ are the eigen-
functions we desire. The solutions Ylm(θ, ϕ) = Θlm(θ)Φm(ϕ) are the spherical
harmonics
Ylm(θ, ϕ) ≡(−1)m
9
2l + 1
4π
(l −m)!
(l + m)! Plm(cos θ)eimϕ ,
(7.63)
where Plm are the usual Legendre functions
Plm(x) = (1 −x2)m/2
2ll!
dm+l
dxm+l (x2 −1)l .
(7.64)
In these equations, −l ≤m ≤l, and it can be shown that m and l must be either
integer or half-integer. l is known as the orbital angular momentum, and m is its
component along the ˆz axis. Similarly, the electron spins S and the nuclear spin I
have components ms and mi. As you can see, the description of the energy states
of an atom can be quite complicated! Summarizing: for our purposes we may think
of the eigenenergies of an atom as being determined by seven numbers: n, l, m, S,
ms, I, and mi.
302
Quantum computers: physical realization
where we have chosen r = 0 as the point to place the atom (and thus evaluate ⃗E), and
also oriented the atom such that ˆr is aligned properly with the electric ﬁeld vector. g is
some constant (we need not be concerned about the speciﬁc values here, just the forms)
which describes the strength of the interaction. The i is present simply to allow g to
be real, since HI must be Hermitian. HI can be simpliﬁed further, by recognizing that
it contains terms which are generally small; to see these, it is useful to deﬁne the Pauli
raising and lowering operators,
σ± = X ± iY
2
,
(7.68)
such that we can re-express HI as
HI = g(σ+ −σ−)(a −a†) .
(7.69)
The terms containing σ+a† and σ−a oscillate at twice the frequencies of interest, which
are ω and ω0, and dropping them is a fairly good approximation (the rotating wave
approximation) which leads us to the total Hamiltonian H = Hatom + Hﬁeld + HI,
H = ℏω0
2 Z + ℏωa†a + g(a†σ−+ aσ+) .
(7.70)
where, again, just to recap: the Pauli operators act on the two-level atom, a†, a are raising
and lowering operators on the single mode ﬁeld, ω is the frequency of the ﬁeld, ω0 is
the frequency of the atom, and g is the coupling constant for the interaction between
atom and ﬁeld. This is the fundamental theoretical tool in the study of cavity QED, the
Jaynes–Cummings Hamiltonian, which describes interactions between two-level atoms
and an electromagnetic ﬁeld.
This Hamiltonian can be written in another convenient form by noting that N =
a†a + Z/2 is a constant of the motion, that is [H, N] = 0, so that we ﬁnd
H = ℏωN + δZ + g(a†σ−+ aσ+) ,
(7.71)
where δ = (ω0 −ω)/2 is known as the detuning – the frequency difference between
the ﬁeld and atomic resonance. This Hamiltonian, the Jaynes–Cummings Hamiltonian,
is very important, and we shall be spending nearly all of the rest of the chapter studying
its properties and guises in different physical systems.
Exercise 7.17: (Eigenstates of the Jaynes–Cummings Hamiltonian)
Show that
|χn⟩=
1
√
2

|n, 1⟩+ |n + 1, 0⟩
	
(7.72)
|χn⟩=
1
√
2

|n, 1⟩−|n + 1, 0⟩
	
(7.73)
are eigenstates of the Jaynes–Cummings Hamiltonian (7.71) for ω = δ = 0, with
the eigenvalues
H|χn⟩= g
√
n + 1|χn⟩
(7.74)
H|χn⟩= −g
√
n + 1|χn⟩,
(7.75)
where the labels in the ket are |ﬁeld, atom⟩.
Optical cavity quantum electrodynamics
303
7.5.3
Single-photon single-atom absorption and refraction
The most interesting regime in cavity QED, for our purposes, is that in which single pho-
tons interact with single atoms. This is an unusual regime, in which traditional concepts
(such as index of refraction and permittivity) in classical theories of electromagnetism
break down. In particular, we would like to utilize a single atom to obtain a nonlinear
interaction between photons.
Let us begin by showing one striking and general characteristic of the atom–ﬁeld
system known as Rabi oscillations. Without loss of generality we may neglect N, since it
only contributes a ﬁxed phase. Recalling that time evolution is given by U = e−iHt (here
and in the following, it will often be convenient to drop ℏ, and we shall do so freely),
and focusing on the case of at most a single excitation in the ﬁeld mode, where
H = −
⎡
⎣
δ
0
0
0
δ
g
0
g
−δ
⎤
⎦,
(7.76)
(the basis states are |00⟩, |01⟩, |10⟩, from left to right and top to bottom, where the left
label corresponds to the ﬁeld, and the right one to the atom), we ﬁnd that
U = e−iδt|00⟩⟨00|
+ (cos Ωt + i δ
Ω sin Ωt)|01⟩⟨01|
+ (cos Ωt −i δ
Ω sin Ωt)|10⟩⟨10|
−i g
Ω sin Ωt
'
|01⟩⟨10| + |10⟩⟨01|
(
.
(7.77)
The interesting behavior is in the last line of this equation, which shows that the atom
and ﬁeld oscillate back and forth exchanging a quantum of energy, at the Rabi frequency
Ω =

g2 + δ2.
Exercise 7.18: (Rabi oscillations)
Show that (7.77) is correct by using
ei⃗n·⃗σ = sin |n| + iˆn · ⃗σ cos |n|
(7.78)
to exponentiate H. This is an unusually simple derivation of the Rabi oscillations
and the Rabi frequency; ordinarily, one solves coupled differential equations to
obtain Ω, but here we obtain the essential dynamics just by focusing on the
single-atom, single-photon subspace!
The transformation of the photon, in interacting with a single atom, can be obtained
by tracing over the atom’s state (Section 2.4.3). The probability that an initial photon |1⟩
is absorbed by the atom (which we assume starts in its ground state, |0⟩) is simply
χr =

k
|⟨0k|U|10⟩|2 =
g2
g2 + δ2 sin2 Ωt .
(7.79)
This has the usual Lorentzian proﬁle expected for absorption as a function of detuning
δ from resonance.
The refractive index (of the single atom!) is given by the matrix elements of U in
which the atom stays in the ground state. The phase shift experienced by the photon is
304
Quantum computers: physical realization
the difference in the angle of rotation experienced by the |1⟩and the |0⟩states of the
ﬁeld, tracing over the atom. This is found to be
χi = arg

eiδt

cos Ωt −i δ
Ω sin Ωt

.
(7.80)
For ﬁxed non-zero δ, as the coupling g is decreased, the absorption probability χr de-
creases as g2, but the phase shift χi remains nearly constant. This is the origin of materials
which can perform phase shifts without scattering much light.
Exercise 7.19: (Lorentzian absorption proﬁle)
Plot (7.79) for t = 1 and g = 1.2,
as a function of the detuning δ, and (if you know it) the corresponding classical
result. What are the oscillations due to?
Exercise 7.20: (Single photon phase shift)
Derive (7.80) from U, and plot it for
t = 1 and g = 1.2, as a function of the detuning δ. Compare with δ/Ω2.


2
 
 
Figure 7.4. Three level atom (with levels 0, 1, and 2) interacting with two orthogonal polarizations of light,
described by the operators a and b. The atom–photon couplings are respectively ga and gb. The energy
differences between 0 and 1, and between 0 and 2 are assumed to be nearly equal.
A natural application of the atom–photon interaction is to study what happens when
two different photon modes (each containing at most one photon) interact with the same
atom. This can give rise to a nonlinear interaction between the two modes. Recall from
Section 7.4.2 that nonlinear Kerr media can be described phenomenologically as media
which induce a cross phase modulation with Hamiltonian of the form H = χa†ab†b.
There, we did not see how that effect arises from fundamental interactions. Using the
present formalism, the origin of the Kerr effect can be illustrated using a simple model, in
which two polarizations of light interact with a three-level atom, as shown in Figure 7.4.
This is described by a modiﬁed version of the Jaynes–Cummings Hamiltonian,
H = δ
⎡
⎣
−1
0
0
0
1
0
0
0
1
⎤
⎦+ ga
⎛
⎝a
⎡
⎣
0
0
0
1
0
0
0
0
0
⎤
⎦+ a†
⎡
⎣
0
1
0
0
0
0
0
0
0
⎤
⎦
⎞
⎠
+ gb
⎛
⎝b
⎡
⎣
0
0
0
0
0
0
1
0
0
⎤
⎦+ b†
⎡
⎣
0
0
1
0
0
0
0
0
0
⎤
⎦
⎞
⎠,
(7.81)
where the basis elements for the 3×3 atom operators are |0⟩, |1⟩, and |2⟩. In matrix form,
Optical cavity quantum electrodynamics
305
the relevant terms in H are found to be the block-diagonal matrix
H =
⎡
⎣
H0
0
0
0
H1
0
0
0
H2
⎤
⎦,
(7.82)
where
H0 = −δ
(7.83)
H1 =
⎡
⎢⎢⎣
−δ
ga
0
0
ga
δ
0
0
0
0
−δ
gb
0
0
gb
δ
⎤
⎥⎥⎦
(7.84)
H2 =
⎡
⎣
−δ
ga
gb
ga
δ
0
gb
0
δ
⎤
⎦.
(7.85)
in the basis |a, b, atom⟩= |000⟩for H0, |100⟩, |001⟩, |010⟩, |002⟩for H1, and |110⟩,
|011⟩, |102⟩for H2, across the columns from left to right. Exponentiating to give U =
exp(iHt) allows one to ﬁnd the single photon phase shifts ϕa = arg(⟨100|U|100⟩) −
arg(⟨000|U|000⟩) and ϕb = arg(⟨010|U|010⟩) −arg(⟨000|U|000⟩) and the two photon
phase shift ϕab = arg(⟨110|U|110⟩) −arg(⟨000|U|000⟩). For linear media, one would
expect that ϕab = ϕa + ϕb, that is, the two photon state has twice the phase shift of
the single photon state, since exp[−iω(a†a + b†b)]|11⟩= exp(−2iω)|11⟩. However, this
system behaves nonlinearly, and gives χ3 ≡ϕab −ϕa −ϕb as shown in Figure 7.5. In
this physical system, this Kerr effect arises from the slight amplitude for the atom to
exchange quanta between the two optical modes.
-2
-1
0
1
2
Detuning delta
-20
-10
0
10
20
Kerr relative phase shift [deg]
Figure 7.5. Kerr phase shift χ3 in degrees, for t = 0.98 and ga = gb = 1, plotted as a function of the detuning δ,
computed from (7.82) for single photons interacting with a single three-level atom.
Exercise 7.21:
Explicitly exponentiate (7.82) and show that
ϕab = arg

eiδt

cos Ω′t −i δ
Ω′ sin Ω′t

,
(7.86)
306
Quantum computers: physical realization
where Ω′ =

δ2 + g2
a + g2
b. Use this to compute χ3, the nonlinear Kerr phase
shift. This is a very simple way to model and understand the Kerr interaction,
which sidesteps much of the complication typically involved in classical
nonlinear optics.
Exercise 7.22:
Associated with the cross phase modulation is also a certain amount of
loss, which is given by the probability that a photon is absorbed by the atom.
Compute this probability, 1 −⟨110|U|110⟩, where U = exp(−iHt) for H as in
(7.82); compare with 1 −⟨100|U|100⟩as a function of δ, ga, gb, and t.
7.5.4
Quantum computation
Broadly speaking, cavity QED techniques can be used to perform quantum computation
in a number of different ways, two of which are the following: quantum information can be
represented by photon states, using cavities with atoms to provide nonlinear interactions
between photons; or quantum information can be represented using atoms, using photons
to communicate between the atoms. Let us now close out this subject by describing an
experiment which demonstrates the ﬁrst of these methods to realize a quantum logic gate.
As we saw in Section 7.4.2, a quantum computer can be constructed using single
photon states, phase shifters, beamsplitters, and nonlinear Kerr media, but the π cross
phase modulation required to produce a controlled-
gate is nearly infeasible with
standard bulk nonlinear optics techniques. Cavity QED can be used to implement a
Kerr interaction, as shown in Section 7.5.3; unlike for bulk media, this can have a very
strong effect even at the single photon level, because of the strong ﬁeld provided by a
Fabry–Perot type cavity.
Figure 7.6 illustrates a cavity QED experiment which was performed (see ‘History
and further reading’ at the end of the chapter) to demonstrate the potential for realizing
a logic gate with the unitary transform
⎡
⎢⎢⎣
1
0
0
0
0
eiϕa
0
0
0
0
eiϕb
0
0
0
0
ei(ϕa+ϕb+Δ)
⎤
⎥⎥⎦,
(7.87)
where Δ = 16◦, using single photons. In the experiment, two modes of light (distinguished
by a very small frequency difference) with weak coherent states are prepared, one linearly
polarized (the probe), and one circularly polarized (the pump), as input to the cavity.
This state can be expressed as
|ψin⟩= |β+⟩
|α+⟩+ |α−⟩
√
2

,
(7.88)
recalling that linearly polarized light is an equal superposition of the two possible circu-
larly polarized states, + and −. Approximating the weak coherent states as |α⟩≈|0⟩+α|1⟩
and similarly for |β⟩(and leaving out normalizations for the moment) gives
|ψin⟩≈

|0+⟩+ β|1+⟩
	 
|0+⟩+ α|1+⟩+ |0−⟩+ α|1−⟩
	
.
(7.89)
These photons pass through the optical cavity and interact with the atom, which is
modeled as causing a different phase shift to occur to states depending on the total
number of photons in each polarization (independent of which mode the photons are
Optical cavity quantum electrodynamics
307
in). Speciﬁcally, we assume that a photon in the |1+⟩state experiences a eiϕa phase shift
if it is in the probe beam, and eiϕb for the pump. In addition to this single photon
phase shift, the state |1+1+⟩experiences an additional Kerr phase shift Δ, so it becomes
ei(ϕa+ϕb+Δ)|1+1+⟩. Other states (and in particular, other polarizations) remain unchanged.
The physics which leads to this behavior is similar to that described in Section 7.5.3, and
the end effect is the same: a cross phase modulation between the pump and the probe
light. The output from the cavity is thus
|ψout⟩≈|0+⟩

|0+⟩+ αeiϕa|1+⟩+ |0−⟩+ α|1−⟩
	
+ eiϕbβ|1+⟩

|0+⟩+ αei(ϕa+Δ)|1+⟩+ |0−⟩+ α|1−⟩
	
(7.90)
≈|0+⟩|α, ϕa/2⟩+ eiϕbβ|1+⟩|α, (ϕa + Δ)/2⟩,
(7.91)
where |α, ϕa/2⟩denotes a linearly polarized probe ﬁeld rotated from the vertical by ϕa/2.
The ﬁeld polarizations are measured by the detector, giving ϕa ≈17.5◦, ϕb ≈12.5◦and
Δ ≈16◦. Since Δ is a non-trivial value, this result suggests that a universal two qubit
logic gate (Exercise 7.23) is possible using single photons, and a single atom in a cavity
as a nonlinear optical Kerr medium to interact photons.
Probe
Pump
M1
M2
Optical pumping
Local
oscillator
PBS
λ/2
Ωa
Ωb
Heterodyne
Cs beam
Figure 7.6. Schematic of an experimental apparatus used to demonstrate the possibility of using a single atom to
provide cross phase modulation between single photons, as an elementary quantum logic gate. A linearly polarized
weak probe beam of light Ωa, and a stronger circularly polarized pump beam Ωb are prepared and shone on an
optical cavity with high reﬂectivity mirrors M1 and M2. Cesium atoms prepared in the electronic state
6S1/2, F = 4, m = 4 by optical pumping fall (the ﬁgure shows the atoms upside down) such that the average
number of atoms in the cavity is around one. The light traverses the cavity, interacting with the atom; σ+ polarized
light causes strong transitions to the 6P3/2, F ′ = 5, m′ = 5 state, and the orthogonal σ−polarized light causes
weak transitions to the 6P3/2, F ′ = 5, m′ = 3 state. The polarization of the output light is then measured, using a
half wave plate, a polarizing beamsplitter (PBS), and a sensitive balanced heterodyne detector (which selectively
detects light at a speciﬁc frequency, as determined by the local oscillator). Figure courtesy of Q. Turchette.
Several important caveats must be kept in mind in interpreting these experimental
results. The incident photons are absorbed with non-trivial probability when traversing
the cavity and atom, and thus the true quantum operation performed is not unitary; this
308
Quantum computers: physical realization
problem would be aggravated if multiple gates were cascaded, which would be required,
for example, to realize a controlled-
gate (which requires Δ = π). In fact, reﬂection
losses of the cavity arrangement used in this experiment would signiﬁcantly impede
cascading; to understand how to get around this, a proper time-dependent model would
have to be developed and studied. Also, although the cross phase modulation model is
consistent with data measured, the photon–atom interaction model used is an ansatz,
and other models are not ruled out by the experiment. In fact, it would be possible in
principle to use single photon states (as opposed to attenuated coherent states) in the
experiment, and measurements of the resulting entanglement of the two modes in |ψout⟩
would be a good test. At the time this experiment was carried out, no general procedure
was known for fully characterizing a quantum operation and its suitability as a quantum
logic gate. However, a method for doing this, known as process tomography, is now well
understood (Chapter 8), and remarkably it even allows full characterization of dissipation
and other non-unitary behavior. Performing such a test would unambiguously determine
exactly the extent to which the experiment described here actually reﬂects a quantum
computation.
Despite these drawbacks, the experiment does demonstrate fundamental concepts re-
quired for quantum information processing. It certiﬁes that nonlinear optical behavior
such as the Kerr interaction really does occur at the single photon level, thus validating
the essence of the Jaynes–Cummings model. Also, this experiment is performed in what
is called the bad cavity regime, where the atom’s coherent coupling rate g2/κ to the
cavity mode dominates incoherent emission rate γ into free space, but this coupling is
weaker than the rate κ at which input photons enter and leave the cavity. The strong
coupling operating regime, in which g > κ > γ, offers an alternative in which larger
conditional phase shifts Δ may be obtained.
Most importantly, perhaps, cavity QED opens the door to a wealth of additional inter-
actions which are valuable for quantum information processing. We have also seen how
the quantum information perspective – focusing on single photons and single atoms – has
allowed us take the Jaynes–Cummings Hamiltonian, the basic cavity QED interaction,
and construct from it some of the most fundamental physics of the interaction of electro-
magnetic waves with matter. We now leave the subject of cavity QED, but as we continue
on next to ion traps, and then to magnetic resonance, we shall keep with us these no-
tions of photon–atom interactions, single atoms and photons, and the Jaynes–Cummings
Hamiltonian.
Exercise 7.23:
Show that the two qubit gate of (7.87) can be used to realize a
controlled-
gate, when augmented with arbitrary single qubit operations, for
any ϕa and ϕb, and Δ = π. It turns out that for nearly any value of Δ this gate is
universal when augmented with single qubit unitaries.
Optical cavity quantum electrodynamics
• Qubit representation: Location of single photon between two modes, |01⟩and
|10⟩, or polarization.
• Unitary evolution: Arbitrary transforms are constructed from phase shifters (Rz
Ion traps
309
rotations), beamsplitters (Ry rotations), and a cavity QED system, comprised of a
Fabry–Perot cavity containing a few atoms, to which the optical ﬁeld is coupled.
• Initial state preparation: Create single photon states (e.g. by attenuating laser
light).
• Readout: Detect single photons (e.g. using a photomultipler tube).
• Drawbacks: The coupling of two photons is mediated by an atom, and thus it is
desirable to increase the atom–ﬁeld coupling. However, coupling the photon into
and out of the cavity then becomes difﬁcult, and limits cascadibility.
7.6
Ion traps
Thus far in this chapter, we have focused mainly on representing qubits using photons.
Let us now turn to representations which use atomic and nuclear states. Speciﬁcally, as we
saw in Section 7.1, electron and nuclear spins provide potentially good representations for
qubits. Spin is a strange (but very real!) concept (Box 7.7), but since the energy difference
between different spin states is typically very small compared with other energy scales
(such as the kinetic energy of typical atoms at room temperature), the spin states of
an atom are usually difﬁcult to observe, and even more difﬁcult to control. In carefully
crafted environments, however, exquisite control is possible. Such circumstances are
provided by isolating and trapping small numbers of charged atoms in electromagnetic
traps, then cooling the atoms until their kinetic energy is much lower than the spin energy
contribution. After doing this, incident monochromatic light can be tuned to selectively
cause transitions which change certain spin states depending on other spin states. This
is the essence of how trapped ions can be made to perform quantum computation, as we
describe in this section. We begin with an overview of the experimental apparatus and
its main components, then we present a Hamiltonian modeling the system. We describe
an experiment which has been performed to demonstrate a controlled-
gate with
trapped 9Be ions, and then close with a few comments on the potential and limitations
of the method.
Exercise 7.24:
The energy of a nuclear spin in a magnetic ﬁeld is approximately μNB,
where μN = eh/4πmp ≈5×10−27 joules per tesla is the nuclear Bohr magneton.
Compute the energy of a nuclear spin in a B = 10 tesla ﬁeld, and compare with
the thermal energy kBT at T = 300 K.
7.6.1
Physical apparatus
An ion trap quantum computer has as its main components an electromagnetic trap with
lasers and photodetectors, and ions.
Trap geometry and lasers
The main experimental apparatus, an electromagnetic trap constructed from four cylin-
drical electrodes, is shown in Figure 7.7. The end segments of the electrodes are biased
at a different voltage U0 than the middle, so that the ions are axially conﬁned by a static
potential Φdc = κU0

z2 −(x2 + y2)
	
/2 along the ˆz axis (κ is a geometrical factor). How-
ever, a result known as Earnshaw’s theorem states that a charge cannot be conﬁned in
three dimensions by static potentials. Thus, to provide conﬁnement, two of the electrodes
310
Quantum computers: physical realization
Box 7.7: Spin
Spin is a strange concept. When a particle has spin, it possesses a magnetic moment
as if it were a composite particle with some current running around in a loop. But
electrons are elementary particles, and the quarks which compose a nucleon are not
known to produce spin by orbital motion. Furthermore, the spin of a particle is
only ever either integer or half-integer.
Spin is nevertheless quite real, and an important part of everyday physics. Integer
spin particles, known as bosons, include the photon. Being massless, it is somewhat
special and only has spin ±1 (and no spin zero) components; these correspond to
the two familiar orthogonal polarization states. Sunglasses made from cheap plastic
polarizers are effective when driving because sunlight becomes partially polarized
in the opposite direction after reﬂecting off of surfaces such as roadways (light
polarized with the electric ﬁeld transverse to the interface always partially reﬂects no
matter the angle of incidence, in contrast with the transverse magnetic polarization
which does not reﬂect when the angle of incidence is at Brewster’s angle). Half-
integer spin particles, known as fermions, include the electron, proton, and neutron.
These are ‘spin-1/2’ particles, in that their spin component can either be +1/2 (spin
‘up’) or −1/2 (spin ‘down’). When we say ‘spin’ often what is meant is a spin-1/2
particle.
The energy eigenstates of an atom intimately involve spin, and the combination of
multiple spins. For example, the nucleus of 9Be has spin 3/2. Spins interact with a
magnetic ﬁeld just as magnetic moments do; in a magnetic ﬁeld ⃗B, an electron with
spin ⃗S has energy ge⃗S · ⃗B, and similarly, a nucleus ⃗I has energy gn⃗I · ⃗B. Pictorially,
for example, the spin contribution to an atom’s energy levels can be viewed as:
3&2
&2
 &2
 3&2
3&2
&2
 &2
 3&2
&2
 &2
" :$"
		" :$"
		" = "	 :$"
"	;
where we have assumed a spin-1/2 electron, and a spin-3/2 nucleus. By tuning the
frequency of an incident laser just right, any of these transitions could be selected,
as long as conservation laws (Section 7.5.1) are satisﬁed. In particular, angular
momentum conversation implies that when a photon is absorbed by an atom, one
unit of angular momentum or spin must change between the initial and ﬁnal states.
These states thus must have deﬁnite values of angular momenta; this can be taken
into account.
Unlike continuous variables such as position and momentum, and other inﬁnite
Hilbert space systems which must be artiﬁcially truncated to represent quantum
bits, spin states provide good representations for quantum information because they
live in an inherently ﬁnite state space.
Ion traps
311
are grounded, while the other two are driven by a fast oscillating voltage which creates a
radiofrequency (RF) potential Φrf = (V0 cos ΩTt+Ur)(1 +(x2 −y2)/R2)/2, where R is a
geometrical factor. The segments of the electrodes are capacitively coupled such that the
RF potential is constant across them. The combination of Φdc and Φrf creates, on average
(over ΩT), a harmonic potential in x, y, and z. Together with the Coulomb repulsion of
the ions, this gives a Hamiltonian governing the motion of the N ions in the trap,
H =
N

i=1
M
2

ω2
xx2
i + ω2
yy2
i + ω2
zz2
i + |⃗pi|2
M 2

+
N

i=1

j>i
e2
4πϵ0|⃗ri −⃗rj| ,
(7.92)
where M is the mass of each ion. Typically, ωx, ωy ≫ωz by design, so that the ions all
lie generally along the ˆz axis. As the number of ions becomes large, the geometrical con-
ﬁguration of the ions can become quite complicated, forming zig-zag and other patterns,
but we shall focus on the simple case where just a few ions are trapped, in a string-like
conﬁguration.
9:	

>		:
*:
?
0
;
Figure 7.7. Schematic drawing (not to scale) of an ion trap quantum computer, depicting four ions trapped in the
center of a potential created by four cylindrical electrodes. The apparatus is typically contained in a high vacuum
(≈10−8 Pa), and the ions are loaded from a nearby oven. Modulated laser light incident on the ions through
windows in vacuum chamber perform operations on and are used to readout the atomic states.
Just as a mass on a spring can behave as a quantum system when the coupling to the
external world becomes sufﬁciently small, the motion of the electromagnetically conﬁned
ion becomes quantized when it is sufﬁciently well isolated. Let us ﬁrst understand what
the quantization means, then consider the isolation criteria. As we saw in Section 7.3, the
energy levels of a harmonic oscillator are equally spaced, in units of ℏωz. In the ion trap,
in the regime which concerns us, these energy eigenstates represent different vibrational
modes of the entire linear chain of ions moving together as one body, with mass NM.
These are called the center of mass modes. Each ℏωz quantum of vibrational energy is
called a phonon, and can be thought of as a particle, just as a quantum of electromagnetic
radiation in a cavity is a photon.
For the above phonon description to hold, certain criteria must hold. First, the cou-
pling to the environment must be sufﬁciently small such that thermalization does not
randomize the state of the system (and thus cause it to behave classically). Physically,
312
Quantum computers: physical realization
what can happen is that nearby ﬂuctuating electric and magnetic ﬁelds push on the ions,
causing their motional state to randomly transition between energy eigenstates. Such
noise sources are nearly inevitable, in a technical sense, since, for example, one cannot
drive the conﬁning electrodes from a perfect voltage source; the source will always have a
ﬁnite resistance, and this resistance gives rise to Johnson noise, which has ﬂuctuations on
time scales the ions are sensitive to. The electric ﬁeld on local patches of the electrodes
can also ﬂuctuate, randomly driving the ions’ motion. As the randomness increases, the
quantum properties of the ions’ state is lost, and their behavior becomes well described
by classical statistical averages. For example, both their momentum and position become
well deﬁned, which cannot be simultaneously true for a quantum system. Nevertheless,
in practice, most technical noise sources can indeed be controlled quite well, to the extent
that they do not heat or dephase the trapped ions too much on the time scale of most
experiments. In part, one important reason this is possible is that as long as the harmonic
approximation holds, the trapped ions are very selective about the frequency of the noise
they are sensitive to; just as transitions between atomic levels can be selected by radiation
tuned only to the correct frequency, only ﬂuctuations which have high spectral power
density around ωz will affect the ions.
It is also quite important for the ions to be sufﬁciently cool so as to make the one-
dimensional harmonic approximation valid. The true potential is non-quadratic for large
displacements along any direction away from the trap center. And higher order vibrational
modes in which the ions move relative to each other (instead of moving together) must
have energies much higher than the center of mass mode. When this holds, and the ions
are cooled to their motional ground state, their transition to the next higher energy state is
through absorption of a center of mass phonon; this process is related to the Mossba¨uer
effect, in which a photon is absorbed by atoms in a crystal without generating local
phonons because the entire crystal recoils together.
How are the ions cooled to their motional ground state? The goal is to satisfy kBT ≪
ℏωz, where T is the temperature reﬂecting the kinetic energy of the ions. Essentially, this
can be done by using the fact that photons carry not only energy, but also momentum
p = h/λ, where λ is the wavelength of the light. Just as the whistle of an approaching
train has a higher pitch than a departing train, an atom moving toward a laser beam has
transition frequencies which are slightly higher in energy than an atom moving away. If
the laser is tuned such that it is absorbed only by approaching atoms, then the atoms
slow down because the photons kick them in the opposite direction. This method is
known as Doppler cooling. Shining a properly tuned laser (which has momentum vector
components along each axis) at trapped atoms thus can cool the atoms down to the limit
kBT ≈ℏΓ/2, where Γ is the radiative width of the transition used for the cooling. To
cool beyond this limit, another method, known as sideband cooling, is then applied, as
illustrated in Figure 7.8. This allows one to reach the kBT ≪ℏωz limit.
Another criterion which must be satisﬁed is that the width of the ion oscillation in the
trap potential should be small compared to the wavelength of the incident light. This
Lamb–Dicke criterion is conveniently expressed in terms of the Lamb-Dicke parameter
η ≡2πz0/λ, where λ is the wavelength, and z0 =
ℏ/2NMω is the characteristic length
scale of the spacing between ions in the trap. The Lamb–Dicke criterion requires that
η ≪1; this does not strictly have to be met in order for ion traps to be useful for quantum
computation, but it is desired to have that η ≈1 at least, in order that the individual
Ion traps
313

	








	
		
	

	
	




Figure 7.8. Sideband cooling method, showing transitions between |0, n⟩and |1, m⟩, where 0 and 1 are two
electronic levels, and n and m are phonon levels representing motional states of the ion. Laser light is tuned to
have energy one phonon less than the electronic transition, such that, for example, the |0, 3⟩state transitions to the
|1, 2⟩state, as shown. The atom then spontaneously decays into the lower energy 0 state (wiggly lines), randomly
going to either |0, 1⟩, |0, 2⟩, or |0, 3⟩(with nearly equal probabilities). Note that the laser light actually causes all
possible transitions between |0, n⟩and |1, n −1⟩, since these all have the same energy. However, this process does
not touch the |0, 0⟩state, and eventually that is the state in which the atom will be left.
ions can be resolved by different laser beams, but without making their motional state
too difﬁcult to optically excite in order to perform logic operations.
Atomic structure
The purpose of the trap apparatus described above is to allow ions to be cooled to the
extent that their vibrational state is sufﬁciently close to having zero phonons (|0⟩), an
appropriate initial state for computation. Similarly, the internal states of the ions must
be initialized appropriately, so they may be used to store quantum information. Let us
now consider what these internal states are, and understand why they are good qubit
representations by estimating their coherent lifetime.
The internal atomic states relevant to the trapped ion we shall consider result from
the combination F of electron spin S and nuclear spin I, giving F = S + I. The formal
piece of theory which describes this – known as the addition of angular momenta –
not only describes important physics for understanding atomic structure, but also is an
interesting mechanism for quantum information. A single photon interacting with an atom
can provide or carry away one unit of angular momentum, as we saw in Section 7.5.1.
But there are numerous possible sources of angular momenta in an atom: orbital, electron
spin, and nuclear spin. Where it comes from is partly determined by the energy levels
selected by the energy of the photon, but beyond that, the photon cannot distinguish
between different sources, and to describe what happens we must select a basis in which
total angular momentum becomes a uniquely deﬁned property of the state.
Consider, for example, two spin-1/2 spins. The ‘computational’ basis for this two
qubit space is |00⟩, |01⟩, |10⟩, |11⟩, but to span the state space we could equally well
314
Quantum computers: physical realization
choose the basis
|0, 0⟩J = |01⟩−|10⟩
√
2
(7.93)
|1, −1⟩J = |00⟩
(7.94)
|1, 0⟩J = |01⟩+ |10⟩
√
2
(7.95)
|1, 1⟩J = |11⟩.
(7.96)
These basis states are special, because they are eigenstates of the total momentum oper-
ator, deﬁned by jx = (X1 + X2)/2, jy = (Y1 + Y2)/2, jz = (Z1 + Z2)/2, and
J2 = j2
x + j2
y + j2
z .
(7.97)
The states |j, mj⟩J are eigenstates of J2 with eigenvalue j(j + 1), and simultaneously
eigenstates of jz, with eigenvalue mj. These states are the natural ones selected by many
physical interactions; for example, in a ˆz oriented magnetic ﬁeld the magnetic moment
μ in the Hamiltonian μBz is proportional to mj, the component of the total angular
momentum in the ˆz direction.
The theory of addition of angular momenta is sophisticated and well developed, and
we have but scratched its surface (for the interested reader, some relevant exercises are
provided below, and pointers to the literature are given in the ‘History and further
reading’ section at the end of the chapter). Nevertheless, some interesting observations
which concern quantum information can already be drawn from the above examples.
Normally, we think of entangled states such as the Bell states (Section 1.3.6) as being
unnatural states of matter, because they have strange, non-local properties. However, the
state |0, 0⟩J is a Bell state! Why does Nature prefer this state here? It is because of a
symmetry under which the interaction involving the magnetic moment is invariant under
interchange of the two spins. Such symmetries actually occur widely in Nature, and are
potentially quite useful for performing entangling measurements and operations.
Exercise 7.25:
Show that the total angular momenta operators obey the commutation
relations for SU(2), that is, [ji, jk] = iϵikljl.
Exercise 7.26:
Verify the properties of |j, mj⟩J by explicitly writing the 4×4 matrices
J2 and jz in the basis deﬁned by |j, mj⟩J.
Exercise 7.27: (Three spin angular momenta states)
Three spin-1/2 spins can
combine together to give states of total angular momenta with j = 1/2 and
j = 3/2. Show that the states
|3/2, 3/2⟩= |111⟩
(7.98)
|3/2, 1/2⟩=
1
√
3

|011⟩+ |101⟩+ |110⟩
	
(7.99)
|3/2, −1/2⟩=
1
√
3

|100⟩+ |010⟩+ |001⟩
	
(7.100)
|3/2, −3/2⟩= |000⟩
(7.101)
|1/2, 1/2⟩1 =
1
√
2

−|001⟩+ |100⟩
	
(7.102)
Ion traps
315
|1/2, −1/2⟩1 =
1
√
2

|110⟩−|011⟩
	
(7.103)
|1/2, 1/2⟩2 =
1
√
6

|001⟩−2|010⟩+ |100⟩
	
(7.104)
|1/2, −1/2⟩2 =
1
√
6

−|110⟩+ 2|101⟩−|011⟩
	
(7.105)
form a basis for the space, satisfying J2|j, mj⟩= j(j + 1)|j, mj⟩and
jz|j, mj⟩= mj|j, mj⟩, for jz = (Z1 + Z2 + Z3)/2 (similarly for jx and jy) and
J2 = j2
x + j2
y + j2
z. There are sophisticated ways to obtain these states, but a
straightforward brute-force method is simply to simultaneously diagonalize the
8×8 matrices J2 and jz.
Exercise 7.28: (Hyperﬁne states)
We shall be taking a look at beryllium in
Section 7.6.4 – the total angular momenta states relevant there involve a nuclear
spin I = 3/2 combining with an electron spin S = 1/2 to give F = 2 or F = 1.
For a spin-3/2 particle, the angular momenta operators are
ix = 1
2
⎡
⎢⎢⎣
0
√
3
0
0
√
3
0
2
0
0
2
0
√
3
0
0
√
3
0
⎤
⎥⎥⎦
(7.106)
iy = 1
2
⎡
⎢⎢⎣
0
i
√
3
0
0
−i
√
3
0
2i
0
0
−2i
0
i
√
3
0
0
−i
√
3
0
⎤
⎥⎥⎦
(7.107)
iz = 1
2
⎡
⎢⎢⎣
−3
0
0
0
0
−1
0
0
0
0
1
0
0
0
0
3
⎤
⎥⎥⎦
(7.108)
1. Show that ix, iy, and iz satisfy SU(2) commutation rules.
2. Give 8×8 matrix representations of fz = iz ⊗I + I ⊗Z/2 (where I here
represents the identity operator on the appropriate subspace) and similarly
fx and fy, and, F 2 = f 2
x + f 2
y + f 2
z. Simultaneously diagonalize fz and F 2 to
obtain basis states |F, mF⟩for which F 2|F, mF⟩= F(F + 1)|F, mF ⟩and
fz|F, mF ⟩= mF|F, mF⟩.
How long can a superposition of different spin states exist? The limiting process, known
as spontaneous emission, occurs when an atom transitions from its excited state to its
ground state by emitting a photon. This happens at some random time, at a rate which
we shall estimate. It might seem that spontaneously emitting a photon is a strange thing
for an atom to do, if it is simply sitting in free space with nothing apparently disturbing
it. But this process is actually a very natural consequence of the coupling of the atom to
electromagnetic ﬁelds, described simply by the Jaynes–Cummings interaction,
HI = g(a†σ−+ aσ+) ,
(7.109)
as we recall from Section 7.5.2. Previously, we used this model to describe how a laser
interacts with an atom, but the model also describes what happens to an atom even
316
Quantum computers: physical realization
when no optical ﬁeld is present! Consider an atom in its excited state coupled to a
single mode which contains no photon, the state |01⟩(using |ﬁeld, atom⟩). This is not an
eigenstate of HI, and thus it cannot remain stationary as time evolves. What happens is
described by the unitary operator U in (7.77), by which we ﬁnd that there is a probability
pdecay = |⟨10|U|01⟩|2 for the atom to decay into its ground state and emit a photon, where
pdecay = g2 4 sin2 1
2(ω −ω0)2t
(ω −ω0)2
,
(7.110)
to lowest order in g, the atom–ﬁeld coupling. ω is the frequency of the photon, and ℏω0
the energy difference between the two levels of the atom. An atom sitting in free space
interacts with many different optical modes; inserting the coupling
g2 =
ω2
0
2ℏωϵ0c2 |⟨0|⃗μ|1⟩|2 ,
(7.111)
where ⃗μ is the atomic dipole operator, integrating over all the optical modes (Exer-
cise 7.29) and taking a time derivative gives the probability per second of decay,
γrad = ω3
0|⟨0|⃗μ|1⟩|2
3πℏϵ0c5
.
(7.112)
If we make the approximation that |⟨0|⃗μ|1⟩| ≈μB ≈9×10−24 J/T, the Bohr magneton,
and assume that ω0/2π ≈10 GHz, then γrad ≈10−15 sec−1, a spontaneous emission
rate of less than one decay every 3 000 000 years. This calculation is representative of
those done to estimate lifetimes of atomic states; as you can see, the hyperﬁne states can
have remarkably long coherence times in theory, and this is generally consistent with
experiments, in which lifetimes of tens of seconds to tens of hours have been observed.
Exercise 7.29: (Spontaneous emission)
The spontaneous emission rate (7.112) can
be derived from (7.110)–(7.111) by the following steps.
1. Integrate
1
(2πc)3
8π
3
- ∞
0
ω2 pdecaydω ,
(7.113)
where the 8π/3 comes from summing over polarizations and integrating over
the solid angle dΩ, and ω2/(2πc)3 comes from the mode density in
three-dimensional space. (Hint: you may want to extend the lower limit of
the integral to −∞.)
2. Differentiate the result with respect to t, to obtain γrad.
The form of g2 is a result of quantum electrodynamics; taking this for granted,
the remainder of the calculation as presented here really stems from just the
Jaynes–Cummings interaction. Again, we see how considering its properties in
the single atom, single photon regime gives us a fundamental property of atoms,
without resorting to perturbation theory!
Exercise 7.30: (Electronic state lifetimes)
A calculation similar to that for γred can
be done to estimate the lifetimes expected for electronic transitions, that is, those
which involve energy level changes Δn ̸= 0. For such transitions, the relevant
Ion traps
317
interaction couples the atom’s electric dipole moment to the electromagnetic
ﬁeld, giving
g2
ed =
ω2
0
2ℏωϵ0
|⟨0|⃗μed|1⟩|2 .
(7.114)
This gives a spontaneous emission rate
γed
red = ω3
0|⟨0|⃗μed|1⟩|2
3πℏϵ0c3
.
(7.115)
Give a value for γed
red, taking |⟨0|⃗μed|1⟩| ≈qa0, where q is the electric charge, and
a0 the Bohr radius, and assuming ω0/2π ≈1015 Hz. The result show how much
faster electronic states can decay compared with hyperﬁne states.
7.6.2
The Hamiltonian
Combining the simpliﬁed models given in the previous section for the harmonic elec-
tromagnetic trap and the atomic structure provides us with the following simpliﬁed toy
model for an ion trap quantum information processor. Imagine a single two-level spin
interacting via the usual magnetic dipole interaction HI = −⃗μ · ⃗B with an electromag-
netic ﬁeld, where the dipole moment ⃗μ = μm⃗S is proportional to the spin operator S,
and the magnetic ﬁeld is ⃗B = B1 ˆx cos(kz −ωt + ϕ), and B1 is the ﬁeld strength, k its
momentum in the ˆz direction, ω its frequency, and ϕ its phase. Note that in this section,
we shall use Sx = X/2, Sy = Y/2, and Sz = Z/2 as the spin operators; they are related
to the Pauli operators by a factor of two.
In addition to the usual electromagnetic interaction, there are interactions with the
vibrational modes. The spin is physically conﬁned within a harmonic potential of energy
scale ℏωz (Figure 7.9), such that its position becomes quantized and we must describe
it by an operator z = z0(a† + a), where a†, a are raising and lowering operators for the
vibrational modes of the particle, representing creation and annihilation of phonons.


Figure 7.9. Toy model of a trapped ion: a single particle in a harmonic potential with two internal states,
interacting with electromagnetic radiation.
Let us assume that the particle is cooled to near its lowest vibrational mode, such that
the width of its oscillation in the well is small compared to the wavelength of the incident
light, that is, the Lamb–Dicke parameter η ≡kz0 is small. Deﬁning the Rabi frequency
318
Quantum computers: physical realization
of the spin as Ω = μmB1/2ℏ, and recalling that Sx = (S+ + S−)/2, we ﬁnd that the
interaction Hamiltonian simpliﬁes in the small η limit to become
HI = −⃗μ · ⃗B
(7.116)
≈
ℏΩ
2
'
S+ei(ϕ−ωt) + S−e−i(ϕ−ωt)(
+

iηℏΩ
2
:S+a + S−a† + S+a† + S−a
; '
ei(ϕ−ωt) −e−i(ϕ−ωt)(
. (7.117)
The ﬁrst term in brackets results from the usual Jaynes–Cummings Hamiltonian as we
saw in Section 7.5.2, which occurs when the location z of the spin is a constant. However,
it is simpliﬁed and does not contain photon operators because it turns out that as long as
B1 is a strong coherent state, we can neglect its quantum properties and leave ourselves
with a Hamiltonian which describes just the evolution of the internal atomic state. It is
in fact quite remarkable that a coherent state of the ﬁeld does not become entangled with
an atom after interacting with it (to an excellent degree of approximation); this is a deep
result which you may explore further by looking at Problem 7.3 at the end of the chapter.
We shall also touch on this fact in describing resonance in Section 7.7.2.
The second term in brackets describes the coupling of the motional state of the ion to
its spin state, through the fact that the magnetic ﬁeld it sees is dependent on its position.
The four terms in braces correspond to four transitions (two up and two down) which
are known as the red and blue motional sidebands, illustrated in Figure 7.10.






%	
	
Figure 7.10. Energy levels of the toy model trapped ion showing the red and blue motional sideband transitions,
which correspond to creation or annihilation of a single phonon. There is an inﬁnite ladder of additional motional
states, which are usually not involved. The states are labeled as |n, m⟩where n represents the spin state, and m
the number of phonons.
Why these sideband transitions have frequencies ω0 ± ωz is easy to see, by including
the free particle Hamiltonian
H0 = ℏω0Sz + ℏωza†a ,
(7.118)
which causes the spin and phonon operators to evolve as
S+(t) = S+eiω0t
S−(t) = S−e−iω0t
(7.119)
a†(t) = a†eiωzt
a(t) = ae−iωzt .
(7.120)
Ion traps
319
Thus, in the frame of reference of H0, the dominant terms of H′
I = eiH0t/ℏHIe−iH0t/ℏ
are found to be
H′
I =
⎧
⎨
⎩
i ηℏΩ
2
S+a†eiϕ −S−ae−iϕ
ω = ω0 + ωz
i ηℏΩ
2
S+aeiϕ −S−a†e−iϕ
ω = ω0 −ωz
(7.121)
where the frequency of the electromagnetic ﬁeld, ω, is as shown on the right.
Extending the above model from one spin to N spins conﬁned within the same har-
monic potential is simple if we assume that they share a single center of mass vibrational
mode, whose energy is much lower than any other vibrational mode of the system. A
straightforward extension of the theory shows that the only required modiﬁcation is
replacement of Ω by Ω/
√
N, since all N particles move together collectively.
7.6.3
Quantum computation
Quantum computation with trapped ions requires one to be able to construct arbitrary
unitary transforms on the internal states of the atoms. We now show how this is done,
in three steps: we describe (1) how arbitrary single qubit operations are performed on
the internal atomic (spin) state, (2) a method for performing a controlled two qubit gate
between the spin and the phonon state, and (3) a way to swap quantum information
between the spin and the phonon. Given these building blocks, we then describe an
experiment which was performed to demonstrate a controlled-
gate, complete with
state preparation and readout.
Single qubit operations
Applying an electromagnetic ﬁeld tuned to frequency ω0 turns on the internal Hamilto-
nian term
Hinternal
I
= ℏΩ
2

S+eiϕ + S−e−iϕ
.
(7.122)
By choosing ϕ and the duration of the interaction appropriately, this allows us to per-
form rotation operations Rx(θ) = exp(−iθSx) and Ry(θ) = exp(−iθSy), which, by
Theorem 4.1 on page 175, thereby allow us to perform any single qubit operation on the
spin state. We shall denote rotations on the jth ion by a subscript, for example, Rxj(θ).
Exercise 7.31:
Construct a Hadamard gate from Ry and Rx rotations.
Controlled phase-ﬂip gate
Suppose, now, that one qubit is stored in the atom’s internal spin state, and another
qubit is stored using the |0⟩and |1⟩phonon states. If this is the case, we can perform a
controlled phase-ﬂip gate, with the unitary transform
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1
⎤
⎥⎥⎦.
(7.123)
It is easiest to explain how to do this with an atom that has a third energy level, as shown
in Figure 7.11 (the extra level is not fundamentally necessary; see Problem 7.4). A laser
320
Quantum computers: physical realization
is tuned to the frequency ωaux + ωz, to cause transitions between the |20⟩and |11⟩states;
this turns on a Hamiltonian of the form
Haux = iηℏΩ′
2

S′
+eiϕ + S′
−e−iϕ
,
(7.124)
where S′
+ and S′
−denote transitions between |20⟩and |11⟩, and we assume that higher
order motional states are unoccupied. Note that because of the uniqueness of this fre-
quency, no other transitions are excited. We apply the laser with phase and duration to
perform a 2π pulse, that is, the rotation Rx(2π) on the space spanned by |11⟩and |20⟩,
which is just the unitary transform |11⟩→−|11⟩. All the other states remain unchanged,
assuming that undesired states such as |1, 2⟩have no probability amplitude. This real-
izes the transform of (7.123), as desired. We shall write this gate as Cj(Z) (denoting a
controlled-Z operation), where j indicates which ion the gate is applied to. Note that
the same phonon is shared by all the ions, since it is a center-of-mass phonon; because
of this, adopting engineering terminology, this has been called the phonon ‘bus’ qubit in
the literature.









Figure 7.11. Energy levels of a three-level atom in an ion trap, with two phonon states each. The labels |n, m⟩
indicate the atom’s state n and the phonon state m. The |20⟩↔|11⟩transition is used to perform a controlled
phase-ﬂip gate.
Swap gate
Finally, we need some way to swap qubits between the atom’s internal spin state and the
phonon state. This can be done by tuning a laser to the frequency ω0 −ωz, and arranging
for the phase to be such that we perform the rotation Ry(π) on the subspace spanned by
|01⟩and |10⟩, which is just the unitary transform
⎡
⎢⎢⎣
1
0
0
0
0
0
1
0
0
−1
0
0
0
0
0
1
⎤
⎥⎥⎦
(7.125)
on the |00⟩, |01⟩, |10⟩, |11⟩space. If the initial state is a|00⟩+ b|10⟩(that is, the phonon
is initially |0⟩), then the state after the swap is a|00⟩+ b|01⟩, so this accomplishes the
desired swap operation. We shall write this as
j when acting on ion j; the inverse
operation
j corresponds to Ry(−π). Technically, because of the minus sign in the
|10⟩⟨01| entry of Ry(π), this is not a perfect swap operation, but it is equivalent to one
up relative phases (see Exercise 4.26). Thus, this is sometimes referred to as being a
‘mapping operation’ instead of as a swap.
Ion traps
321
Controlled-
gate
Putting these gates together allows us to construct a
gate acting on ions j (control)
and k (target) using the sequence of operations
jk = Hk
k Cj(Z)
k Hk ,
(7.126)
(time going from right to left, as usual for matrices) where Hk is a Hadamard gate (con-
structed from Ry and Rx rotations on ion k). This is very similar to how a controlled-
gate was constructed using beamsplitters and optical Kerr media, as in Equation (7.46).
7.6.4
Experiment

    
   	 
 
   	 
 
  
 
Figure 7.12. Photograph of a microfabricated elliptical electrode ion trap, in which ions have been conﬁned. The
ions in this trap are barium ions, rather than beryllium, but the basic principles are the same as described in the
text. Reproduced courtesy of R. Devoe and C. Kurtsiefer, IBM Almaden Research Center.
A controlled-
gate using a single trapped ion has been demonstrated (see ‘History
and further reading’ at the end of the chapter); precisely how this experiment is done is
insightful. In the experiment, a single ion of 9Be+ is trapped in a coaxial resonator RF
ion trap, different in geometry from the linear ion trap of Figure 7.7, but functionally
equivalent, and similar to the photograph of an actual ion trap shown in Figure 7.12.
Beryllium was chosen for its convenient hyperﬁne and electronic level structure, shown
in Figure 7.13. The 2S1/2(1, 1) and 2S1/2(2, 2) energy levels (Exercise 7.28) are used as
the atom’s internal qubit state, and the |0⟩and |1⟩phonon states as another qubit (labeled
322
Quantum computers: physical realization
  
  
  
   
   
   
  
  

 	  


  

    

  

  

    
	
   
  


  
 
  
  

  
  
  
  
Figure 7.13. Energy levels of 9Be+ used in the ion trap experiment. Figure courtesy of C. Monroe at NIST.
in the ﬁgure as n = 0 and n = 1). The ≈313 nm transition between the 2S1/2(1, 1) and
2S1/2(2, 2) levels is accomplished not by tuning a single laser to the transition frequency,
but rather two lasers whose difference frequency is that of the transition. This Raman
transition method simpliﬁes requirements for laser phase stability. The 2S1/2(2, 0) state
is used as the auxiliary level; the 2S1/2 states have different energies by virtue of a
0.18 millitesla magnetic ﬁeld applied to the system. The trapped ion has vibrational
frequencies (ωx, ωy, ωz)/2π = (11.2, 18.2, 29.8) MHz in the trap, and a ground state
nx = 0 wavefunction spread of about 7 nm, giving a Lamb–Dicke parameter of about
ηx = 0.2. The Rabi frequency of the on-resonance transition is Ω/2π = 140 kHz, the
two motional sidebands, ηxΩ/2π = 30 kHz, and the auxiliary transition ηxΩ′/2π = 12
kHz.
The state of the ion is initialized using Doppler and sideband cooling to obtain, with
approximately 95% probability, the state |00⟩= |2S1/2(2, 2)⟩|nx = 0⟩. The internal and
motional states of the ion are then prepared in one of the four basis states |00⟩, |01⟩, |10⟩,
or |11⟩using single qubit operations, then a controlled-
gate is performed using three
pulses, which implement a Ry(π/2) rotation on the internal state qubit, a controlled-Z
operation between the two qubits, then a Ry(−π/2) rotation on the internal state qubit.
It is simple to show (Exercise 7.32) that this circuit, drawn in Figure 7.14, realizes a
controlled-
gate.
Readout of the computational output is performed with two measurements. The ﬁrst
is to collect the ﬂuorescence from the ion which occurs when + circularly polarized
light tuned to the 2S1/2(2, 2) – 2P3/2(3, 3) ‘cycling’ transition is applied. The light does
not couple appreciably to the 2S1/2(1, 1) state, and thus the intensity of the observed
ﬂuorescence is proportional to the probability of the internal state qubit being in the |0⟩
Ion traps
323

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Figure 7.14. Quantum circuit modeling the ion trap controlled-
experiment. The top wire represents the
phonon state, and the bottom, the ion’s internal hyperﬁne state.
state; it is a projective measurement. This measurement technique is powerful because
the transition cycles many times – the ion absorbs a photon, jumping to the 2P3/2(3, 3)
state, then emits a photon, decaying back into the 2S1/2(2, 2) state where it started.
Thousands or more cycles are possible, allowing good statistics to be accumulated. The
second measurement is similar to the ﬁrst, but a swap pulse is applied ﬁrst to exchange
the motional and internal state qubits; this projectively measures the motional state qubit.
The experiment as performed veriﬁes the classical truth table of the controlled-
operation and, in principle, by preparing superposition input states and measuring output
density matrices, the unitary transform could be completely characterized using process
tomography (Chapter 8). The controlled-
gate requires about 50 microseconds to
perform with the optical power used in the experiment. On the other hand, the coherence
time was measured to be somewhere around hundreds to thousands of microseconds. The
dominant decoherence mechanisms included instabilities in the laser beam power and the
RF ion trap drive frequency and voltage amplitude, and ﬂuctuating external magnetic
ﬁelds. Moreover, the experiment involved only a single ion, and only two qubits, and
thus was not useful for computation; to be useful, a controlled-
gate should generally
be applied between different ions, and not just between a single ion and the motional
state.
However, the technical limitations can probably be overcome, and lifetimes can be
extended by using the short-lived motional state only intermittently, capitalizing on the
much longer coherence times of the internal atomic states. And scaling to larger numbers
of ions is conceptually viable. Shown in Figure 7.15 is a string of 40 mercury ions
which have been trapped. There are many hurdles to making such systems behave as
useful quantum information processing machines, but technological surprises are a never-
ending saga. Someday, perhaps, trapped ions such as these could be registers of qubits
in a quantum computer.
Figure 7.15. Image of ﬂuorescence from about 40 trapped mercury (199Hg+) atomic ions. The ions are spaced by
approximately 15 micrometers, and the two apparent gaps are different isotopes of mercury which do not respond
to the probe laser. Reprinted courtesy of D. Wineland, at NIST.
324
Quantum computers: physical realization
Exercise 7.32:
Show that the circuit in Figure 7.14 is equivalent (up to relative
phases) to a controlled-
gate, with the phonon state as the control qubit.
Ion trap quantum computer
• Qubit representation: Hyperﬁne (nuclear spin) state of an atom, and lowest
level vibrational modes (phonons) of trapped atoms.
• Unitary evolution: Arbitrary transforms are constructed from application of
laser pulses which externally manipulate the atomic state, via the
Jaynes–Cummings interaction. Qubits interact via a shared phonon state.
• Initial state preparation: Cool the atoms (by trapping and using optical
pumping) into their motional ground state, and hyperﬁne ground state.
• Readout: Measure population of hyperﬁne states.
• Drawbacks: Phonon lifetimes are short, and ions are difﬁcult to prepare in their
motional ground states.
7.7
Nuclear magnetic resonance
Nuclear spin systems would be nearly ideal for quantum computation if only spin–
spin couplings could be large and controllable; this is an important observation from
our study of ion traps in the last section. The principal drawback of ion trap quantum
computers is the weakness of the phonon mediated spin–spin coupling technique and its
susceptibility to decoherence. One way this limitation could be circumvented would be to
trap molecules instead of single atoms – the magnetic dipole and electron mediated Fermi
contact interactions between neighboring nuclei would provide strong natural couplings.
However, with their many vibrational modes, single molecules have been difﬁcult to
trap and cool, and thus optical manipulation and detection of nuclear spins in trapped
molecules has not been feasible except in special circumstances.
On the other hand, direct manipulation and detection of nuclear spin states using
radiofrequency electromagnetic waves is a well-developed ﬁeld known as nuclear mag-
netic resonance (NMR). These techniques are widely used in chemistry, for example, to
measure properties of liquids, solids, and gases, to determine the structure of molecules,
and to image materials and even biological systems. These many applications have lead
the technology of NMR to become quite sophisticated, allowing control and observation
of tens to hundreds and thousands of nuclei in an experiment.
However, two problems arise in using NMR for quantum computation. First, because
of the smallness of the nuclear magnetic moment, a large number (more than ≈108)
molecules must be present in order to produce a measurable induction signal. Concep-
tually, a single molecule might be a ﬁne quantum computer, but how can this be true
of an ensemble of molecules? In particular, the output of an NMR measurement is an
average over all the molecule’s signals; can the average output of an ensemble of quan-
tum computers be meaningful? Second, NMR is typically applied to physical systems
in equilibrium at room temperature, where the spin energy ℏω is much less than kBT.
This means that the initial state of the spins is nearly completely random. Traditional
quantum computation requires the system be prepared in a pure state; how can quantum
computation be performed with a system which is in a high entropy mixed state?
Nuclear magnetic resonance
325
Solutions to these two problems have made NMR a particularly attractive and insight-
ful method for implementing quantum computation, despite stringent limitations which
arise from the thermal nature of typical systems. Many lessons can be learned from NMR:
for example, techniques for controlling realistic Hamiltonians to perform arbitrary unitary
transforms, methods for characterizing and circumventing decoherence (and systematic
errors), and considerations which arise in assembling components in implementing full
quantum algorithms on entire systems. We begin with a description of the physical ap-
paratus and the main Hamiltonian involved, then we discuss how quantum information
processing with NMR is possible despite the thermal input state and ensemble problems,
concluding with some experiments which have been performed demonstrating quantum
algorithms, and the drawbacks of this method.
7.7.1
Physical apparatus
Let us begin with a general description of the apparatus, whose workings will be mathe-
matically modeled in detail later. The two main parts of a pulsed NMR system for liquid
samples, which we shall focus on here, are the sample and the NMR spectrometer. A
typical molecule which might be used would contain a number n of protons which have
spin 1/2 (other possible nuclei include 13C, 19F, 15N, and 31P), and produce an NMR
signal at about 500 MHz when placed in a magnetic ﬁeld of about 11.7 tesla. The fre-
quencies of different nuclei within a molecule can differ by a few kHz to hundreds of kHz
because of differences in the local magnetic ﬁelds due to chemical environment shielding
effects. The molecules are typically dissolved in a solvent, reducing the concentration
to the extent that inter-molecular interactions become negligible, leaving a system that
might well be described as an ensemble of n qubit quantum ‘computers.’
The spectrometer itself is constructed from radiofrequency (RF) electronics and a large
superconducting magnet, within the bore of which is held the sample in a glass tube, as
shown in Figure 7.16. There, the static ˆz oriented magnetic ﬁeld B0 is carefully trimmed
to be uniform over approximately 1 cm3 to better than one part in 109. Orthogonal saddle
or Helmholtz coils lying in the transverse plane allow small, oscillating magnetic ﬁelds
to be applied along the ˆx and ˆy directions. These ﬁelds can be rapidly pulsed on and off
to manipulate nuclear spin states. The same coils are also part of tuned circuits which
are used to pick up the RF signal generated by the precessing nuclei (much like how a
spinning magnet inductively generates an alternating current in a nearby coil).
A typical experiment begins with a long waiting period in which the nuclei are allowed
to thermalize to equilibrium; this can require several minutes for well-prepared liquid
samples. Under control of a (classical) computer, RF pulses are then applied to effect the
desired transformation on the state of the nuclei. The high power pulse ampliﬁers are
then quickly switched off and a sensitive pre-ampliﬁer is enabled, to measure the ﬁnal
state of the spins. This output, called the free induction decay, is Fourier transformed
to obtain a frequency spectrum with peaks whose areas are functions of the spin states
(Figure 7.17).
There are many important practical issues which lead to observable imperfections. For
example, spatial inhomogeneities in the static magnetic ﬁeld cause nuclei in different parts
of the ﬁelds to precess at different frequencies. This broadens lines in the spectrum. An
even more challenging problem is the homogeneity of the RF ﬁeld, which is generated
by a coil which must be orthogonal to the B0 magnet; this geometric constraint and
the requirement to simultaneously maintain high B0 homogeneity usually forces the RF
326
Quantum computers: physical realization
 
      	 
  
   
  
  
	  
      
    
     
    	 
   
   
 	 
 	   
  


 
 	 
       
   
 
   
 	  
   
   
Figure 7.16. Schematic diagram of an NMR apparatus.
ﬁeld to be inhomogeneous and generated by a small coil, leading to imperfect control
of the nuclear system. Also, pulse timing, and stability of power, phase, and frequency
are important issues; however, unlike the ion traps, because of the lower frequencies,
good control of these parameters is more tractable. We shall return to imperfections in
Section 7.7.4, after understanding the basic mathematical description of the system and
the methodology for performing quantum information processing with NMR.
7.7.2
The Hamiltonian
The basic theory of NMR can be understood from an ideal model of one and two
spins, which we describe here. The ﬁrst step is to describe how electromagnetic radiation
interacts with a single magnetic spin. We then consider the physical nature of couplings
between spins which arise in molecules. These tools enable us to model readout of the
magnetization which results from transformation of an initial state which is in thermal
equilibrium. Finally, we describe a phenomenological model of decoherence, and how its
T1 and T2 parameters can be experimentally determined.
Single spin dynamics
The magnetic interaction of a classical electromagnetic ﬁeld with a two-state spin is
described by the Hamiltonian H = −⃗μ · ⃗B, where ⃗μ is the spin, and B = B0 ˆz +
B1(ˆx cos ωt + ˆy sin ωt) is a typical applied magnetic ﬁeld. B0 is static and very large, and
B1 is usually time varying and several orders of magnitude smaller than B0 in strength,
so that perturbation theory is traditionally employed to study this system. However, the
Schr¨odinger equation for this system can be solved straightforwardly without perturbation
theory, using the Pauli matrix techniques of Chapter 2, in terms of which the Hamiltonian
can be written as
H = ω0
2 Z + g(X cos ωt + Y sin ωt) ,
(7.127)
Nuclear magnetic resonance
327
where g is related to the strength of the B1 ﬁeld, and ω0 to B0, and X, Y, Z are the Pauli
matrices as usual. Deﬁne |ϕ(t)⟩= eiωtZ/2|χ(t)⟩, such that the Schr¨odinger equation
i∂t|χ(t)⟩= H|χ(t)⟩
(7.128)
can be re-expressed as
i∂t|ϕ(t)⟩=

eiωZt/2He−iωZt/2 −ω
2 Z

|ϕ(t)⟩.
(7.129)
Since
eiωZt/2Xe−iωZt/2 = (X cos ωt −Y sin ωt) ,
(7.130)
(7.129) simpliﬁes to become
i∂t|ϕ(t)⟩=
ω0 −ω
2
Z + gX

|ϕ(t)⟩,
(7.131)
where the terms on the right multiplying the state can be identiﬁed as the effective
‘rotating frame’ Hamiltonian. The solution to this equation is
|ϕ(t)⟩= e
i

ω0−ω
2
Z+gX

t|ϕ(0)⟩.
(7.132)
The concept of resonance arises from the behavior of this solution, which can be
understood using (4.8) to be a single qubit rotation about the axis
ˆn =
ˆz +
2g
ω0−ω ˆx

1 +

2g
ω0−ω
2
(7.133)
by an angle
|⃗n| = t
ω0 −ω
2
2
+ g2 .
(7.134)
When ω is far from ω0, the spin is negligibly affected by the B1 ﬁeld; the axis of its
rotation is nearly parallel with ˆz, and its time evolution is nearly exactly that of the
free B0 Hamiltonian. On the other hand, when ω0 ≈ω, the B0 contribution becomes
negligible, and a small B1 ﬁeld can cause large changes in the state, corresponding to
rotations about the ˆx axis. The enormous effect a small perturbation can have on the
spin system, when tuned to the appropriate frequency, is responsible for the ‘resonance’
in nuclear magnetic resonance. The same effect, of course, is also at the heart of the
selectivity of two-level atoms for speciﬁcally tuned laser ﬁelds that was used (but not
explained) in Section 7.5.1.
In general, when ω = ω0, the single spin rotating frame Hamiltonian can be written as
H = g1(t)X + g2(t)Y ,
(7.135)
where g1 and g2 are functions of the applied transverse RF ﬁelds.
Exercise 7.33: (Magnetic resonance)
Show that (7.128) simpliﬁes to become
(7.129). What laboratory frame Hamiltonian gives rise to the rotating frame
Hamiltonian (7.135)?
328
Quantum computers: physical realization
Exercise 7.34: (NMR frequencies)
Starting with the nuclear Bohr magneton,
compute the precession frequency of a proton in a magnetic ﬁeld of 11.8 tesla.
How many gauss should B1 be to accomplish a 90◦rotation in 10 microseconds?
Spin–spin couplings
More than one spin is usually present in systems of interest; 1H, 13C, 19F, and 15N all
have nuclear spin 1/2. These spins interact through two dominant mechanisms: direct
dipolar coupling, and indirect through-bond electron mediated interactions. Through-
space dipolar coupling is described by an interaction Hamiltonian of the form
HD
1,2 = γ1γ2ℏ
4r3

⃗σ1 · ⃗σ2 −3(⃗σ1 · ˆn)(⃗σ2 · ˆn)

,
(7.136)
where ˆn is the unit vector in the direction joining the two nuclei, and ⃗σ is the magnetic
moment vector (times two). In a low viscosity liquid, dipolar interactions are rapidly
averaged away; mathematically this is calculated by showing that the spherical average of
HD
1,2 over ˆn goes to zero as the averaging becomes rapid compared to the dipolar coupling
energy scale.
Through-bond interactions, also known simply as ‘J-coupling,’ are indirect interac-
tions mediated by electrons shared through a chemical bond; the magnetic ﬁeld seen by
one nucleus is perturbed by the state of its electronic cloud, which interacts with another
nucleus through the overlap of the electronic wavefunction with the nucleus (a Fermi
contact interaction). This coupling has the form
HJ
1,2 = ℏJ
4 ⃗σ1 · ⃗σ2 = ℏJ
4 Z1Z2 + ℏJ
8

σ+σ−+ σ−σ+

.
(7.137)
We shall be interested in the case where J is a scalar (in general it may be a tensor),
which is an excellent approximation in liquids and when couplings are weak, or when
the interacting nuclear species have vastly different precession frequencies. This case is
described by
HJ
12 ≈ℏ
4 JZ1Z2 .
(7.138)
Exercise 7.35: (Motional narrowing)
Show that the spherical average of HD
1,2 over
ˆn is zero.
Thermal equilibrium
NMR differs signiﬁcantly from the other physical systems we have studied previously
in this chapter in that it uses an ensemble of systems, and the primary measurement is
an ensemble average. Furthermore, no extensive procedures are employed to prepare the
initial state in a special state such as the ground state; in fact, to do so is challenging with
present technology.
Rather, the initial state is the thermal equilibrium state,
ρ = e−βH
Z
,
(7.139)
where β = 1/kBT, and Z = tr e−βH is the usual partition function normalization, which
ensures that tr(ρ) = 1. Since β ≈10−4 at modest ﬁelds for typical nuclei at room
Nuclear magnetic resonance
329
temperature, the high temperature approximation
ρ ≈2−n 
1 −βH

(7.140)
is appropriate, for a system of n spins.
Since spin–spin couplings are very small compared with the precession frequencies,
the thermal state density matrix is very nearly diagonal in the Z basis, and thus it can
be interpreted as being a mixture of the pure states |00 . . . 0⟩, |00 . . . 01⟩, . . ., |11 . . . 1⟩.
What is actually the true physical state of each ensemble member is a matter of debate,
because an inﬁnite number of unravelings exist for a given density matrix. In principle,
with NMR the true physical state can be measured if the ensemble members (individual
molecules) are accessible, but this is experimentally difﬁcult.
Exercise 7.36: (Thermal equilibrium NMR state)
For n = 1 show that the
thermal equilibrium state is
ρ ≈1 −ℏω
2kBT
 1
0
0
−1

,
(7.141)
and for n = 2 (and ωA ≈4ωB),
ρ ≈1 −ℏωB
4kBT
⎡
⎢⎢⎣
5
0
0
0
0
3
0
0
0
0
−3
0
0
0
0
−5
⎤
⎥⎥⎦.
(7.142)
Magnetization readout
The principal output of an experiment is the free induction decay signal, mathematically
given as
V (t) = V0tr

e−iHtρeiHt(iXk + Yk)

,
(7.143)
where Xk and Yk operate only on the kth spin, and V0 is a constant factor dependent
on coil geometry, quality factor, and maximum magnetic ﬂux from the sample volume.
This signal originates from the pickup coils detecting the magnetization of the sample in
the ˆx −ˆy plane. In the laboratory frame, this signal will oscillate at a frequency equal
to the precession frequency ω0 of the nuclei; however, V (t) is usually mixed down with
an oscillator locked at ω0, then Fourier transformed, such that the ﬁnal signal appears as
shown in Figure 7.17.
Exercise 7.37: (NMR spectrum of coupled spins)
Calculate V (t) for H = JZ1Z2
and ρ = eiπY1/4 1
4[1 −βℏω0(Z1 + Z2)]e−iπY1/4. How many lines would there be in
the spectrum of the ﬁrst spin if the Hamiltonian were H = JZ1(Z2 + Z3 + Z4)
(with a similar initial density matrix) and what would their relative magnitudes
be?
Decoherence
A prominent characteristic of the free induction decay whose description lies outside the
simple models presented so far for NMR is the exponential decay of the magnetization
signal. One cause of this is inhomogeneity in the static magnetic ﬁeld, which leads to
330
Quantum computers: physical realization
−500
−400
−300
−200
−100
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Frequency from 125.77 MHz in Hz
Signal magnitude
Figure 7.17. Carbon spectrum of 13C labeled trichloroethylene. The four lines on the left come from the carbon
nucleus directly bound to the proton; four lines appear because of couplings to the proton and to the second carbon
nucleus, whose own signal gives the closely spaced four lines on the right. The second carbon nucleus is further
away from the proton than the ﬁrst, and thus has a much smaller coupling to it.
precessing spins in one part of the sample getting out of phase with those in another part.
Effects due to inhomogeneities are reversible in principle, but there are other sources of
phase randomization which are fundamentally irreversible, such as those originating from
spin–spin couplings. Another irreversible mechanism is the thermalization of the spins to
equilibrium at the temperature of their environment, a process which involves exchange
of energy. For a single qubit state, these effects may be phenomenologically characterized
with a density matrix transformation model,
 a
b
b∗
1 −a

→
 (a −a0)e−t/T1 + a0
be−t/T2
b∗e−t/T2
(a0 −a)e−t/T1 + 1 −a0

,
(7.144)
where T1 and T2 are known as the spin–lattice (or ‘longitudinal’)
and spin–spin (or
‘transverse’) relaxation rates, respectively, and a0 characterizes the thermal equilibrium
state. They deﬁne important time scales for the lifetimes of non-equilibrium classical
states and quantum superpositions. Theoretical tools for calculating T1 and T2 in NMR
systems are well-developed, and, in fact, measurements of these rates play an important
role in using NMR to distinguish between different chemical species.
Experimental methods for measuring T1 and T2 are well known in NMR. Let Rx =
e−iπX/4 be a 90◦pulse about the ˆx axis. To measure T1, apply R2
x, wait time τ, then
Rx. The ﬁrst pulse ﬂips the spin by 180◦, after which it relaxes for time τ back towards
equilibrium (visualize this as the Bloch vector shrinking back towards the top of the Bloch
sphere, the ground state), then the ﬁnal 90◦pulse puts the magnetization in the ˆx −ˆy
plane, where it is detected. The measured magnetization M from this inversion–recovery
experiment is found to decay exponentially with τ as M = M0

1 −2 exp(−τ/T1)

. To
Nuclear magnetic resonance
331
measure T2, one can, to ﬁrst order, simply measure the linewidth of a peak. A better
way, the Carr–Purcell–Meiboom–Gill technique, is to apply an Rx operation, followed
by k iterations of ‘wait time τ/2, apply R2
x, wait time τ/2, apply R2
x’. This train of 180◦
pulses ‘refocuses’ couplings (Section 7.7.3) and partially cancels B0 ﬁeld inhomogeneities,
so that one can better estimate the true T2 of the system. The observed magnetization
decays as M = M0e−kτ/T2.
Multiple spin Hamiltonian
Summarizing our discussion of the NMR Hamiltonian, we can write H for an n spin
coupled system as
H =

k
ωkZk +

j,k
HJ
j,k + HRF +

j,k
HD
j,k + Henv ,
(7.145)
where the ﬁrst term is the free precession of the spins in the ambient magnetic ﬁeld, HD is
the magnetic dipole coupling of (7.136), HJ is the ‘J’ coupling of (7.137), HRF describes
the effect of the externally applied radiofrequency magnetic ﬁelds of (7.135), and Henv
describes interactions with the environment which lead to decoherence, as in (7.144).
For the sake of understanding the basic principles about how this Hamiltonian can be
manipulated, we shall ﬁnd it sufﬁcient to consider the simpliﬁed Hamiltonian
H =

k
ωkZk +

j,k
Zj ⊗Zk +

k
gx
k(t)Xk + gy
k(t)Yk ,
(7.146)
in much of the following discussion. The treatment of the more general (7.145) follows
the same ideas.
7.7.3
Quantum computation
Quantum information processing requires performing unitary transforms to a system
prepared in a proper initial state. Three questions arise for the present system: First,
how can arbitrary unitary transforms be implemented in a system of n coupled spins
described by the Hamiltonian of (7.146)? And second, how can the thermal state (7.140)
of an NMR system be used as a proper initial state for computation? Third, the quantum
algorithms we have studied in the last three chapters ask for projective measurements to
obtain output results, whereas with NMR, we can only easily perform ensemble average
measurements. How can we deal with this ensemble readout problem? We answer these
questions in this section.
Refocusing
Perhaps one of the most interesting techniques available to us in performing arbitrary
unitary transforms using Hamiltonians of the sort of (7.146) is refocusing, as it is known
in the art of NMR. Consider the simple two spin Hamiltonian H = Hsys + HRF where
Hsys = aZ1 + bZ2 + cZ1Z2 .
(7.147)
As was shown in Section 7.7.2, when a large RF ﬁeld is applied at the proper frequency,
to a good approximation, we can approximate
e−iHt/ℏ≈e−iHRFt/ℏ.
(7.148)
332
Quantum computers: physical realization
This allows arbitrary single qubit operations to be performed with excellent ﬁdelity. Let
us deﬁne
Rx1 = e−iπX1/4
(7.149)
as a 90◦rotation about ˆx on spin 1, and similarly for spin 2. The 180◦rotation R2
x1 has
the special property that
R2
x1e−iaZ1tR2
x1 = eiaZ1t ,
(7.150)
as can be easily veriﬁed. This is known as refocusing, because of the way it reverses
time evolution such that different frequency spins starting together at some point on the
Bloch sphere come back to the same point on the Bloch sphere. 180◦pulses applied in
this manner are known as refocusing pulses. Note that in the above expression, a can be
an operator as well as a constant (as long as it contains no operators which act on spin
1), and thus
e−iHsyst/ℏR2
x1e−iHsyst/ℏR2
x1 = e−2ibZ2t/ℏ.
(7.151)
Using another set of refocusing pulses applied to spin 2 would remove even this remaining
term. Refocusing is thus a useful technique for removing coupled evolution between spins,
and for removing all evolution entirely.
Exercise 7.38: (Refocusing)
Explicitly show that (7.150) is true (use the
anti-commutativity of the Pauli matrices).
Exercise 7.39: (Three-dimensional refocusing)
What set of pulses can be used to
refocus evolution under any single spin Hamiltonian Hsys = 
k ckσk?
Exercise 7.40: (Refocusing dipolar interactions)
Give a sequence of pulses which
can be used to turn two spin dipolar coupling HD
1,2 into the much simpler form
of (7.138).
Controlled-
gate
Realization of a controlled-
gate is simple using refocusing pulses and single qubit
pulses. Let us show how this is done for a two spin system with the Hamiltonian of
(7.147). From the construction of (7.46), we know that being able to realize the unitary
transform
UCZ =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1
⎤
⎥⎥⎦
(7.152)
is sufﬁcient. Since
√
i eiZ1Z2π/4e−iZ1π/4e−iZ2π/4 = UCZ, getting a controlled-
from
one evolution period of time ℏπ/4c together with several single qubit pulses is straight-
forward.
Exercise 7.41: (NMR controlled-
)
Give an explicit sequence of single qubit
rotations which realize a controlled-
between two spins evolving under the
Hamiltonian of (7.147). You may start with (7.46), but the result can be
simpliﬁed to reduce the number of single qubit rotations.
Nuclear magnetic resonance
333
Temporal, spatial, and logical labeling
Being able to realize arbitrary unitary transforms on a spin system to good ﬁdelity using
RF pulses is one of the most attractive aspects of NMR for quantum computation.
However, the major drawback is the fact that the initial state is usually the thermal state
of (7.140). Despite the high entropy of this state, quantum computation can nevertheless
be done, with some cost. Two techniques for achieving this are called temporal and logical
labeling.
Temporal labeling, also sometimes called temporal averaging, is based on two impor-
tant facts: quantum operations are linear, and the observables measured in NMR are
traceless (see Section 2.2.5 for background on quantum measurements). Suppose a two
spin system starts out with the density matrix
ρ1 =
⎡
⎢⎢⎣
a
0
0
0
0
b
0
0
0
0
c
0
0
0
0
d
⎤
⎥⎥⎦,
(7.153)
where a, b, c, and d are arbitrary positive numbers satisfying a+b+c+d = 1. We can use
a circuit P constructed from controlled-
gates to obtain a state with the permuted
populations
ρ2 = Pρ1P † =
⎡
⎢⎢⎣
a
0
0
0
0
c
0
0
0
0
d
0
0
0
0
b
⎤
⎥⎥⎦,
(7.154)
and similarly,
ρ3 = P †ρ1P =
⎡
⎢⎢⎣
a
0
0
0
0
d
0
0
0
0
b
0
0
0
0
c
⎤
⎥⎥⎦.
(7.155)
A unitary quantum computation U is applied to each of these states, to obtain (in three
separate experiments, which may be performed at different times) Ck = UρkU †. By
linearity,

k=1,2,3
Ck =

k
UρkU †
(7.156)
= U

k
ρk

U †
(7.157)
= (4a −1)U
⎡
⎢⎢⎣
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦U † + (1 −a)
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎤
⎥⎥⎦. (7.158)
In NMR, observables M (such as Pauli X and Y ) for which tr(M) = 0 are the only ones
ever measured; thus,
tr

k
CkM

=

k
tr

CkM

(7.159)
334
Quantum computers: physical realization
= (4a −1) tr
⎛
⎜
⎜
⎝U
⎡
⎢⎢⎣
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦U † M
⎞
⎟
⎟
⎠
(7.160)
= (4a −1) tr
U|00⟩⟨00|U † .
(7.161)
We ﬁnd that the sum of the measured signals from the three experiments gives us a
result which is proportional to what we would have obtained had the original system
been prepared in a pure state |00⟩⟨00| instead of in the arbitrary state of (7.153). This is
always possible to accomplish for arbitrarily prepared systems of any size, given enough
experiments which are summed over, and sufﬁciently long coherence time for unitary
operations to be performed before decoherence sets in. Note that the different Ck ex-
periments can actually also be done simultaneously with three different systems, or in
different parts of a single system; this is experimentally feasible by applying gradient
magnetic ﬁelds which vary systematically over a single sample, and that variant of this
technique is called spatial labeling.
Exercise 7.42: (Permutations for temporal labeling)
Give a quantum circuit to
accomplish the permutations P and P † necessary to transform ρ1 of (7.153) to ρ2
of (7.154).
Logical labeling is based on similar observations, but does not require multiple exper-
iments to be performed. Suppose we have a system of three nearly identical spins in the
state
ρ = δI + α
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
6
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
−6
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(7.162)
≈

δ′I + α′
 2
0
0
−2
⊗3
,
(7.163)
where δI refers to a background population which is unobservable (because of traceless
measurement observables), and α ≪δ is a small constant. We may then apply a unitary
operation which performs a permutation P, giving
ρ′ = PρP † = δI + α
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
6
0
0
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
−6
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(7.164)
Nuclear magnetic resonance
335
Note that the upper 4×4 block of this matrix has the form
⎡
⎢⎢⎣
6
0
0
0
0
−2
0
0
0
0
−2
0
0
0
0
−2
⎤
⎥⎥⎦= 8|00⟩⟨00| −2I ,
(7.165)
where I here denotes the 4×4 identity matrix. Just as for temporal labeling, we ﬁnd that
if a computation is performed on such a state, in this case the |000⟩, |001⟩, |010⟩, |011⟩
manifold, then it produces a result which is proportional to what we would have obtained
had the original system been prepared in a pure state |00⟩⟨00|! Experimentally, it is
possible to perform ˜P, and also to isolate the signal from just this manifold of states.
States which are of the form ρ = 2−n(1 −ϵ)I + ϵU|00 . . . 0⟩⟨00 . . . 0|U † (where U
is any unitary operator), are called ‘effective pure states’, or ‘pseudopure’ states. n is
the number of qubits, but the dimension of the Hilbert space need not be a power of
two in general. There are many strategies for preparing such states, and in general they
all incur some cost. We shall return to discuss this later, in Section 7.7.4. Effective
pure states make it possible to observe zero temperature dynamics from a system which
equilibrates to a high temperature state, as long as the coupling of the system to its
high temperature environment is sufﬁciently small. This is the way it is used in NMR
quantum computation.
Exercise 7.43: (Permutations for logical labeling)
Give a quantum circuit to
accomplish the permutations P necessary to transform ρ of (7.163) to ρ′ of
(7.165).
Exercise 7.44: (Logical labeling for n spins)
Suppose we have a system of n
nearly identical spins of Zeeman frequency ℏω in thermal equilibrium at
temperature T with state ρ. What is the largest effective pure state that you can
construct from ρ using logical labeling? (Hint: take advantage of states whose
labels have Hamming weight of n/2.)
Ensemble readout of quantum algorithm results
We have seen how arbitrary unitary transforms can be performed on an n spin system
with a Hamiltonian of (7.146), and we have learned how to prepare from a thermal
state a well-deﬁned input which behaves as a low-entropy ground state. However, to
complete the requirements for quantum computation, we must have a way of performing
measurements on the system to read out computational results. The difﬁculty is that the
output of a typical quantum algorithm is a random number, whose distribution gives
information which allows the problem to be solved. Unfortunately, the average value of
the random variable does not necessarily give any relevant information. This would be
the output if the quantum algorithm were executed without modiﬁcation on an NMR
quantum computer, since it is performed with a large ensemble of molecules, rather than
with a single n spin molecule.
This difﬁculty is illustrated by the following example. The quantum factoring algo-
rithm produces as its output a random rational number c/r, where c is an unknown
integer, and r is the desired result (also an integer). Normally, a projective measurement
is used to obtain c/r, then a classical continued fractions algorithm is performed to obtain
c with high probability (Section 5.3.1). The answer is then checked by plugging in the
336
Quantum computers: physical realization
result to the original problem, and if it fails the entire algorithm is repeated. Unfortu-
nately, however, if only the ensemble average is available, since c is nearly uniformly
distributed the average value ⟨c/r⟩gives no meaningful information.
A simple resolution to this problem, which works for all quantum algorithms based
on the hidden subgroup problem (Chapter 5), is simply to append any required classi-
cal post-processing step to the quantum computation. This is always possible because
quantum computation subsumes classical computation. In the example given above, we
simply ask each individual quantum computer (each molecule) to perform a continued
fractions algorithm. The result is then checked on each quantum computer, then only the
computers which succeed in the veriﬁcation give an output. The ﬁnal ensemble average
measurement thus gives ⟨r⟩.
7.7.4
Experiment
One of the most exciting aspects of the NMR approach is the ready experimental real-
ization of small instances of quantum computation and quantum information tasks. In
this concluding section on NMR, we brieﬂy describe three experiments which have been
performed, demonstrating state tomography, elementary logic gates, and the quantum
search algorithm. We also summarize the drawbacks of this method.
State tomography
How does one debug a quantum computer? A classical computer is analyzed by measuring
its internal state at different points in time. Analogously, for a quantum computer, an
essential technique is the ability to measure its density matrix – this is called state
tomography.
Recall that the density matrix of a single qubit can be expressed as
ρ = 1
2

1 +

k
rkσk

,
(7.166)
where σk are the Pauli matrices and rb is a real, three-component vector. Because of the
trace orthogonality property of Pauli matrices,
tr(σkσj) = 2δkj ,
(7.167)
it follows that ρ can be reconstructed from the three measurement results
rk = ⟨σk⟩= tr(ρσk) .
(7.168)
Measurement of the usual observable in NMR, (7.143), preceded by the appropriate
single qubit pulses, allows us to determine ⟨σk⟩, and thus obtain ρ. Similar results hold
for larger numbers of spins. In practice, it is convenient to measure just the traceless
deviation of ρ from the identity; this is called the deviation density matrix. Example
results for two and three spin systems are shown in Figure 7.18.
Exercise 7.45: (State tomography with NMR)
Let the voltage measurement
Vk(t) = V0tr

e−iHtMkρM †
keiHt(iXk + Yk)

be the result of experiment k.
Show that for two spins, nine experiments, with M0 = I, M1 = Rx1, M2 = Ry1,
M3 = Rx2, M4 = Rx2Rx1, M5 = Rx2Ry1 etc. provide sufﬁcient data from which
ρ can be reconstructed.
Nuclear magnetic resonance
337
1
2
3
4
30
20
10
0
10
20
30
1
2
3
4
40
20
0
20
40
60
80
1
2
3
4
5
6
7
8
8
6
4
2
0
2
4
6
1
2
3
4
5
6
7
8
6
4
2
0
2
4
6
Figure 7.18. Experimentally measured deviation density matrices. Vertical scales are arbitrary, and only the real
part is shown; the imaginary components are all relatively small. (top left) The two qubit thermal equilibrium state
of the proton and the carbon nucleus in molecules of chloroform (13CHCl3) in a 11.78 tesla magnetic ﬁeld. The 0.5
milliliter, 200 millimolar sample was diluted in acetone-d6, degassed, and ﬂame sealed in a thin walled, 5 mm glass
tube. (top right) Two qubit effective pure state created using temporal labeling with the chloroform, as in
Equation (7.161). (bottom left) The three qubit thermal equilibrium state of three ﬂuorine nuclei in
triﬂuoroethylene. (bottom right) An effective pure state created from the three spin system using logical labeling, as
in Equation (7.164).
Exercise 7.46:
How many experiments are sufﬁcient for three spins? Necessary?
Quantum logic gates
The two qubit proton–carbon system in chloroform presents an excellent system for
demonstration of single qubit and two qubit gates, for many reasons. At ≈500 and ≈125
MHz in an ≈11.8 tesla ﬁeld, the frequencies of the two spins are well separated and
easily addressed. The 215 Hz J-coupling frequency of the two nuclei is also convenient;
it is much slower than the time scale required for single qubit RF pulses, but much faster
than the relaxation time scales. In typical experiments, the T1 of the proton and carbon
are approximately 18 and 25 seconds, respectively, while T2 are 7 and 0.3 seconds. The
carbon T2 is short because of interactions with the three quadrupolar chlorine nuclei, but
taking the product of the shortest T2 lifetime and the J-coupling indicates that roughly
60 gates should still be realizable before quantum coherence is lost.
The Hamiltonian of the two spin system is very well approximated by the expression
in (7.147), but it can be simpliﬁed signiﬁcantly using an experimental trick. By tuning
two oscillators to exactly the rotating frequencies of the proton and carbon, we obtain,
in the rotating frame deﬁned by the oscillators, the simpliﬁed Hamiltonian
H = 2πℏJZ1Z2 ,
(7.169)
338
Quantum computers: physical realization
where J = 215 Hz. This Hamiltonian makes the realization of the controlled-
gate
quite simple. A circuit which performs a
transform equivalent up to single qubit
phases is shown in Figure 7.19, as well as a circuit for creating a Bell state, and experi-
mentally measured outputs.
e−iH/2¯hJ
Rx
Ry
1
2
3
4
1
0 8
0 6
0 4
0 2
0
0 2
0 4
0 6
0 8
1
Rx
e−iH/2¯hJ
Rx
Ry
1
2
3
4
1
0 8
0 6
0 4
0 2
0
0 2
0 4
0 6
0 8
1
Figure 7.19. Quantum circuits implemented with NMR and the real part of the experimentally measured output
deviation density matrices. In these circuits, Rx and Ry denote single qubit gates which perform 90◦rotations
about ˆx and ˆy, implemented with RF pulses about 10 microseconds long, and the two qubit box with e−iH/2ℏJ is
a free evolution period of time 1/2J ≈2.3 milliseconds. (top) Controlled-
circuit, and the output measured
for a thermal state input, showing the exchange of the |10⟩and |11⟩diagonal elements, as expected from the
classical truth table for the
gate. (bottom) Circuit for creating the Bell state (|00⟩−|11⟩)/
√
2, and its
output, when a |00⟩effective pure state is prepared as an input.
Exercise 7.47: (NMR controlled-
gate)
Verify that the circuit shown in the
top left of Figure 7.19 performs a controlled-
gate, up to single qubit phases;
that is, it acts properly on classical input states, and furthermore can be turned
into a proper controlled-
gate by applying additional single qubit Rz
rotations. Give another circuit using the same building blocks to realize a proper
gate.
Exercise 7.48:
Verify that the circuit shown in the bottom left of Figure 7.19 creates
the Bell state (|00⟩−|11⟩)/
√
2 as advertised.
Exercise 7.49: (NMR swap gate)
An important chemical application of NMR is
measurement of connectivity of spins, i.e. what protons, carbons, and
phosphorus atoms are nearest neighbors in a molecule. One pulse sequence to do
this is known as INADEQUATE (incredible natural abundance double quantum
transfer experiment – the art of NMR is full of wonderfully creative acronyms).
In the language of quantum computation, it can be understood as simply trying
to apply a
between any two resonances; if the
works, the two nuclei
must be neighbors. Another building block which is used in sequences such as
Nuclear magnetic resonance
339
TOCSY (total correlation spectroscopy) is a swap operation, but not quite in the
perfect form we can describe simply with quantum gates! Construct a quantum
circuit using only e−iH/2ℏJ, Rx, and Ry operations to implement a swap gate
(you may start from the circuit in Figure 1.7).
Quantum algorithms
Grover’s quantum search algorithm provides another simple example for NMR quantum
computation. For a problem size of four elements (n = 2 qubits), we are given the set
x = {0, 1, 2, 3} for which f(x) = 0 except at one value x0, where f(x0) = 1. The goal
is to ﬁnd x0, which can be classically accomplished by evaluating f(x) an average of
2.25 times. In comparison, the quantum algorithm ﬁnds x0 by evaluating f(x) only once
(Chapter 6; see, in particular, Box 6.1).
Three operators are required: the oracle operator O (which performs a phase ﬂip based
on the function f(x)), the Hadamard operator on two qubits H⊗2, and the conditional
phase shift operator P. The oracle O ﬂips the sign of the basis element corresponding to
x0; for x0 = 3, this is
O =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1
⎤
⎥⎥⎦,
(7.170)
Denoting a t = 1/2J (2.3 millisecond for the chloroform) period evolution e−iH/2ℏJ as
τ, we ﬁnd that O = Ry1 ¯Rx1 ¯Ry1Ry2 ¯Rx2 ¯Ry2 τ (up to an irrelevant overall phase factor)
for the x0 = 3 case. H⊗2 is just two single qubit Hadamard operations, H1 ⊗H2, where
Hk = R2
xk ¯Ryk. And the operator P,
P =
⎡
⎢⎢⎣
1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1
⎤
⎥⎥⎦
(7.171)
is simply realized as P = Ry1Rx1 ¯Ry1Ry2Rx2 ¯Ry2 τ. From these, we construct the Grover
iteration G = H⊗2PH⊗2O. This operator can be simpliﬁed straightforwardly by elim-
inating unnecessary operations which obviously cancel (see Exercise 7.51). Let |ψk⟩=
Gk|00⟩be the state after k applications of the Grover iteration to the initial state. We
ﬁnd that the amplitude ⟨x0|ψk⟩≈sin((2k + 1)θ), where θ = arcsin(1/
√
2); this pe-
riodicity is a fundamental property of the quantum search algorithm, and is a natu-
ral feature to test in an experiment. For the two qubit case, and x0 = 3, we expect
|11⟩= |ψ1⟩= −|ψ4⟩= |ψ7⟩= −|ψ10⟩, a period of 3 if the overall sign is disregarded.
Exercise 7.50:
Find quantum circuits using just single qubit rotations and e−iH/2ℏJ
to implement the oracle O for x0 = 0, 1, 2.
Exercise 7.51:
Show that the Grover iteration can be simpliﬁed, by canceling adjacent
340
Quantum computers: physical realization
single qubit rotations appropriately, to obtain
G =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
¯Rx1 ¯Ry1 ¯Rx2 ¯Ry2 τ Rx1 ¯Ry1Rx2 ¯Ry2 τ (x0 = 3)
¯Rx1 ¯Ry1 ¯Rx2 ¯Ry2 τ Rx1 ¯Ry1 ¯Rx2 ¯Ry2 τ (x0 = 2)
¯Rx1 ¯Ry1 ¯Rx2 ¯Ry2 τ ¯Rx1 ¯Ry1Rx2 ¯Ry2 τ (x0 = 1)
¯Rx1 ¯Ry1 ¯Rx2 ¯Ry2 τ ¯Rx1 ¯Ry1 ¯Rx2 ¯Ry2 τ (x0 = 0)
,
(7.172)
for the four possible cases of x0.
Figure 7.20 shows the theoretical and measured deviation density matrices ρΔn =
|ψn⟩⟨ψn| −tr(|ψn⟩⟨ψn|)/4 for the ﬁrst seven iterations of U. As expected, ρΔ1 clearly
reveals the |11⟩state corresponding to x0 = 3. Analogous results were obtained for
experiments repeated for the other possible values of x0. Measuring each density matrix
required 9 × 3 = 27 experimental repetitions, 9 for the tomographic reconstruction and
3 for the pure state preparation.
The longest computation, for n = 7, took less than 35 milliseconds, which was well
within the coherence time. The periodicity of Grover’s algorithm is clearly seen in Fig-
ure 7.20, with good agreement between theory and experiment. The large signal-to-noise
(typically better than 104 to 1) was obtained with just single-shot measurements. Numer-
ical simulations indicate that the 7–44% errors are primarily due to inhomogeneity of the
magnetic ﬁeld, magnetization decay during the measurement, and imperfect calibration
of the rotations (in order of importance).
Drawbacks
The bulk-ensemble NMR implementation of quantum computation has been successful
in demonstrating quantum algorithms and quantum information tasks with systems up to
seven qubits, which is quite impressive. However, there are important limitations which
arise from the temporal, spatial, and logical labeling techniques which are at the heart of
the method.
The essential objective of these labeling techniques is to isolate the signal from the
subset of spins which happen to be in the |00 . . . 0⟩component (or any other standard,
high probability state) of the thermal equilibrium state. Temporal and spatial labeling do
this by adding up signals to cancel all but the desired state; logical labeling trades off
Hilbert space for purity. Irrespective of the method used, however, nothing can increase
the probability of the |00 . . . 0⟩component of the thermal state,
p00...0 = 1
Z ⟨00 . . . 0|e−βH|00 . . . 0⟩.
(7.173)
Taking H = 
k ωZk, we ﬁnd that p00...0 is proportional to n2−n, for an n spin molecule.
This means that the total signal decreases exponentially as the number of qubits ‘distilled’
into an effective pure state using labeling techniques, for constant initial state temperature.
Another limitation comes from using molecules as quantum computers. The structure
of the molecule plays the role of the architecture of the computer, determining what pairs
(or groups) of qubits interact with each other (analogously, the RF pulses serve as the
software). Not all qubits are necessarily well connected! This is doubly important since
interactions cannot be switched off, except by performing refocusing. Furthermore qubits
are addressed by distinguishing them in frequency, but this rapidly becomes difﬁcult as
the number of nuclei is increased. A solution to this exists, which is to use a cellular
automata style architecture, such as the one-dimensional chain X −A −B −C −A −
Nuclear magnetic resonance
341
1
2
3
4
50
0
50
1
2
3
4
50
0
50
1
2
3
4
50
0
50
1
2
3
4
50
0
50
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
1
2
3
4
50
0
50
1
2
3
4
50
0
50
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
1
2
3
4
50
0
50
1
2
3
4
40
20
0
20
40
1
2
3
4
20
10
0
10
20
1
2
3
4
20
10
0
10
20
Figure 7.20. Theoretical and experimental deviation density matrices (in arbitrary units) for seven steps of Grover’s
algorithm performed on the hydrogen and carbon spins in chloroform. Three full cycles, with a periodicity of four
iterations are clearly seen. Only the real component is plotted (the imaginary portion is theoretically zero and was
found to contribute less than 12% to the experimental results). Relative errors ||ρtheory −ρexpt||/||ρtheory|| are
shown.
B −C −· · · −A −B −C −Y , in which the ends are distinguished but the middle is
composed of a repeating regular sequence. In this architecture, only distinct letters are
addressable, and it might seem as if this is a highly restrictive model of computation.
However, it has been shown that in fact it is universal, with only polynomial slowdown.
The precise amount of slowdown required will of course be important when performing
tasks such as the quantum search algorithm, which only has a square root speedup.
Methods for circumventing the limitations of labeling techniques also exist. One pos-
sibility is to polarize the nuclei through some physical mechanism; this has been done by
using optical pumping (similar to how ions are cooled as shown in Figure 7.8) to polarize
the electronic state of rubidium atoms, which then transfer their polarization to the nuclei
of xenon atoms through formation of short-lived van der Waals molecules. This has also
been done for helium. Doing similarly for molecules is conceivable, albeit technologically
challenging. Another possibility is the use of a different labeling scheme; logical labeling
is essentially a compression algorithm, which increases the relative probability of one state
in an ensemble by discarding ensemble members. An improved version of this method
has been developed which achieves the entropic limit, giving nH(p) pure qubits from
342
Quantum computers: physical realization
n-spin molecules originally at temperature T such that p = (1−e−ΔE/kBT)/2, where ΔE
is the spin ﬂip energy. This scheme does not have any exponential cost; the compres-
sion can be achieved using only a polynomial number of basic operations. However, it
is inefﬁcient unless p is relatively small, and today p ≈0.4999 in good solenoid magnet
systems.









 


 
 
 

	 

	 





	 







  




 
 
 
  
  
  
  
  
 	 
Figure 7.21. A selection of simple molecules which have been used to demonstrate various quantum computation
and quantum information tasks with NMR. (a) Chloroform: two qubits, proton and carbon, have been used to
implement the Deutsch–Jozsa algorithm, and a two qubit quantum search. (b) Alanine: three qubits, composed of
the carbon backbone, have been used to demonstrate error-correction. Note how the three carbon nuclei have
distinguishable frequencies because their surrounding chemical environments are different (for example, the
electronegativity of the oxygen causes it to draw much of the nearby electrons away from the neighboring carbon).
(c) 2,3-dibromothiophene: two qubits, composed of the two protons, have been used to simulate four levels of a
truncated simple harmonic oscillator. Here, the two protons are at different distances from the sulphur atom, and
thus have distinguishable frequencies. (d) Triﬂuorobromoethylene: three qubits, the three ﬂuorines, have been used
to demonstrate logical labeling and the creation of a (|000⟩+ |111⟩)/
√
2 superposition state. (e) Trichloroethylene:
three qubits, the proton and two carbons, were used to demonstrate teleportation, with the proton’s state being
teleported to the rightmost carbon. (f) Sodium formate: two qubits, proton and carbon, used to demonstrate the
two qubit quantum error detection code. In this molecule, the sodium radical is used to tune the T2 times of the
two qubits to be nearly equal, by changing the ambient temperature to modify its exchange rate with the solvent.
Despite these drawbacks, NMR provides a testbed for quantum algorithms and il-
lustrates basic techniques which other realizations will have to implement to perform
quantum computation. Some of the molecules which have been used to demonstrate
quantum computation and quantum information tasks are shown in Figure 7.21. The
NMR idea is also a very rich area for innovation, combining chemistry, physics, en-
gineering, and mathematics, and undoubtedly continued innovation between ﬁelds will
further this technique.
Other implementation schemes
343
NMR quantum computer
• Qubit representation: Spin of an atomic nucleus.
• Unitary evolution: Arbitrary transforms are constructed from magnetic ﬁeld
pulses applied to spins in a strong magnetic ﬁeld. Couplings between spins are
provided by chemical bonds between neighboring atoms.
• Initial state preparation: Polarize the spins by placing them in a strong
magnetic ﬁeld, then use ‘effective pure state’ preparation techniques.
• Readout: Measure voltage signal induced by precessing magnetic moment.
• Drawbacks: Effective pure state preparation schemes reduce the signal
exponentially in the number of qubits, unless the initial polarization is sufﬁciently
high.
7.8
Other implementation schemes
In this chapter we have described but a fraction of the number of ideas people have
considered for implementation of quantum computers. Our selection illustrates basic
requirements and challenges common to all implementations: robust representation of
quantum information, application of unitary transforms, preparation of a ﬁducial input
state, and measurement of the output.
The simple harmonic oscillator example emphasizes how a digital representation is
crucial: each unit (qubit, qutrit, qudit, or whatever) of quantum information should
reside in physically separate degrees of freedom; otherwise, some resource (such as
energy) is inefﬁciently utilized. That example also provides the mathematical basis for
studying representations of qubits through the remainder of the chapter. Single photons
are nearly ideal qubits, but the nonlinear optical materials required to get them to interact
are difﬁcult to realize without causing coherence loss. Cavity-QED techniques can address
this problem by using single atoms to interact photons, but even more importantly, they
introduce the notion of two-level atoms, and the idea of guarding qubit representations
using selection rules imposed by physical symmetries such as dipole selection.
A natural extension of this idea is to represent qubits using spin-1/2 particles, which
inherently only have two states. This is the tack taken with trapped ions, which store
qubits in electron and nuclear spins; the difﬁculty with this method, though, is that
the center of mass oscillations – phonons – used to mediate interactions between spins
have short coherence times. Molecules, in which nuclear spins can be strongly coupled
by chemical bonds, can solve this difﬁculty, but the spin resonance signal from single
molecules is too small to detect with present technology. NMR quantum computing
solves this by creating ‘effective pure states’ with bulk ensembles of O(1018) molecules,
thereby demonstrating simple quantum algorithms in the laboratory. But without provid-
ing initial polarization, this capability comes at the cost of signal strength, which decreases
exponentially with the number of qubits.
As these examples demonstrate, coming up with a good physical realization for a
quantum computer is tricky business, fraught with tradeoffs. All of the above schemes
are unsatisfactory, in that none allow a large-scale quantum computer to be realized
anytime in the near future. However, that does not preclude the possibility, and in fact
344
Quantum computers: physical realization
many other implementation schemes have been proposed, some of which we brieﬂy touch
upon in this ﬁnal section here.
A good way to categorize realizations is in terms of the physical degree of freedom
used to represent qubits. Recall Figure 7.1: just about anything which comes in quantum
units could be a qubit, but, as we have seen, fundamental physical quanta such as photons
and spin are particularly good choices.
Another fundamental quantum unit which could serve as a qubit representation is
electric charge. Modern electronics provides excellent techniques to create, control, and
measure charges, even at the level of single electrons. For example, quantum dots, fabri-
cated from semiconductor materials, metals, or even small molecules, can serve as three-
dimensional boxes with electrostatic potentials which conﬁne charge quanta. This is
veriﬁed by observation of the Coulomb blockade effect, in which electrical conductance
through dots of capacitance C is found to increase in discrete steps as a function of
bias voltage across the dot, reﬂecting the e2/2C energy required to add each additional
electron. Unlike photons, (net) charge cannot be destroyed; charges can only be moved
around, and thus a charge state qubit would have to use something like the dual-rail rep-
resentation of Section 7.4.2, whereby the |0⟩and |1⟩states correspond to having charge
located in either of two dots, or two states within a single dot.
Just as for single photons, single qubit operations on charge state qubits can be per-
formed using electrostatic gates (the analogue of optical phase shifters) and either sin-
gle mode waveguide couplers (the analogue of beamsplitters) for moving electrons, or
tunnel junctions for quantum dots. Electrical charges experience long-range Coulomb
interactions with other charges (the potential created by a single charge at distance r is
V (r) = e/4πϵr), and thus charges far away can modulate the phase of a local charge, much
like the Kerr interaction between photons. Controlled Coulomb interactions can thus be
used to perform two qubit operations. Finally, single electron charges are straightforward
to measure; modern ﬁeld effect transistors easily detect movements of single charges in
their channels, and single electron transistors operating at ≈100 millikelvin temperatures
can detect charge to better than 10−4e/
√
Hz at frequencies over 200 MHz. Unfortunately,
uncontrolled distant charge motion leads to dephasing; this, and other scattering mecha-
nism such as those due to phonon interactions cause coherence times to be relatively short
for charge states, on the order of hundreds of femtoseconds to hundreds of picoseconds.
Charge carriers in superconductors have also been suggested as qubit representations.
At low temperatures in certain metals, two electrons can bind together through a phonon
interaction to form a Cooper pair, which has charge 2e. And just as electrons can be
conﬁned within quantum dots, Cooper pairs can be conﬁned within an electrostatic box,
such that the number of Cooper pairs in the box becomes a good quantum number, and
can be used to represent quantum information. Single qubit operations can be realized by
using electrostatic gates to modulate the box potential, and Josephson junctions between
coupled boxes. These junctions can also be used to couple qubits, and their strength
can be modulated using an external magnetic ﬁeld by coupling appropriately to super-
conducting interferometric loops. Finally, qubits can be measured simply by measuring
electric charge. This superconductor qubit representation is interesting because of the
relative robustness of Cooper pairs; it is estimated that the main decoherence mechanism
is spontaneous emission of electromagnetic photons, which may allow coherence times
exceeding one microsecond, long compared with typical dynamical time scales of hun-
dreds of picoseconds. Unfortunately, just as with the electronic charge representation,
Other implementation schemes
345
a ﬂuctuating background of extraneous charges (‘quasiparticles’) is highly deleterious to
qubit coherence. One means around this problem, using superconductor technology, is to
choose instead a magnetic ﬂux qubit representation, in which qubit states correspond to
left and right hand orientations of ﬂux localized through a superconducting loop device.
Here, decoherence is caused by background magnetic ﬂuctuations, which are expected to
be much quieter than the electrostatic case.
The locality of magnetic interactions is a good feature for qubit representations, and
thus we return to spin, for which schemes have also been proposed to take advantage of
solid state technology. A fairly large quantum dot, even one containing many electrons,
can behave as a spin-1/2 object, where the spin is carried by a single excess electron. This
spin state can be prepared by equilibrating in a strong magnetic ﬁeld, at low temperatures,
such that the spin ﬂip energy ΔE is much larger than kBT. Manipulating a single spin,
as we have seen in Section 7.7, can be done by applying pulsed local magnetic ﬁelds, and
coupled qubit operations can be realized with a controlled Heisenberg coupling,
H(t) = J(t)⃗S1 · ⃗S2 = 1
4

X1X2 + Y1Y2 + Z1Z2

,
(7.174)
where ⃗S are the spin operators (Pauli operators divided by two), and J(t) = 4τ 2
0 (t)/u,
u being the charging energy of the dot, and τ0(t) being the tunneling matrix element
controlled by local electrostatic gates placed between dots. This interaction is universal,
in the sense that it is equivalent to the controlled-
gate (see exercise below). Spin
states may, in theory, be measured by allowing the spin-carrying electron to tunnel
to a readout paramagnetic dot, or to spin-dependently tunnel through a ‘spin valve’
to a readout electrometer. The challenge is to realize such measurements in practice;
high ﬁdelity single spin measurement in semiconductors has not been accomplished with
present technology.
Exercise 7.52: (Universality of Heisenberg Hamiltonian)
Show that a swap
operation U can be implemented by turning on J(t) for an appropriate amount
of time in the Heisenberg coupling Hamiltonian of (7.174), to obtain
U = exp(−iπ⃗S1 · ⃗S2). The ‘√
’ gate obtained by turning on the interaction
for half this time is universal; compute this transform and show how to obtain a
controlled-
gate by composing it with single qubit operations.
Eventually, with sufﬁciently advanced technology, it may be possible to place, control,
and measure single nuclear spins in semiconductors, making possible the following vision.
Imagine being able to precisely place single phosphorus atoms (nuclear spin-1/2) within
a crystalline wafer of 28Si (nuclear spin 0), positioned beneath lithographically patterned
electrostatic gates. These gates allow manipulation of the electron cloud surrounding the
31P dopants, to perform single qubit operations via modulation of the magnetic ﬁeld seen
by the 31P nuclei. Additional gates located above the region separating the 31P dopants can
be used to artiﬁcially create electron distributions connecting adjacent 31P, much like a
chemical bond, thus allowing two qubit operations to be performed. Although fabrication
constraints of such a scheme are extremely challenging – for example, the gates should
be separated by 100 ˚A or less, and the 31P dopants must be registered precisely and in
an ordered array – at least this vision articulates a possible venue for marrying quantum
computing with more conventional computing technologies.
346
Quantum computers: physical realization
Of the schemes we have described for realizing quantum computers, the ones which
have most captivated the attention of technologists are the ones based on solid state
technologies. Of course, atomic, molecular, and optical quantum computing schemes
continue to be proposed, using systems such as optical lattices (artiﬁcial crystals made
from atoms conﬁned by crossed beams of light) and Bose condensates which are at the
forefront of those ﬁelds; someday, we may even see quantum computing proposals using
mesons, quarks, and gluons, or even black holes. But the motivation to envision some
kind of solid state quantum computer is enormous. It has been estimated that the world
has invested over US$1 trillion in silicon technology since the invention of the transistor
in the late 1940s. Condensed matter systems have also been rich in new physics, such as
superconductivity, the quantum Hall effect, and the Coulomb blockade (a classical effect
discovered long after it was widely thought that everything about classical physics was
well known!).
This chapter has concentrated mainly on the implementation of quantum computing
machines, but the basic components which were presented are also useful in many other
quantum applications. Quantum cryptography and its experimental realization are de-
scribed in Section 12.6. And pointers to experimental work on quantum teleportation
and superdense coding are given in ‘History and further reading’ at the end of this chap-
ter. The general interface between quantum communication and computation includes
challenges such as distributed quantum computation; development of new algorithms and
experimental implementations of such systems will certainly continue to be a rich area
of research for the future.
Much of the allure of quantum computing and communication machines is certainly
their potential economic ramiﬁcations, as novel information technologies. But as we have
seen in this chapter, quantum computation and quantum information also motivate new
questions about physical systems, and provide different ways to understand their proper-
ties. These new ideas embody a need to move away from traditional many-body, statistical,
and thermodynamic studies of physical systems, all the way from atoms to condensed
matter systems. They represent a new opportunity to focus instead on dynamical prop-
erties of physical systems at the single quantum level. Hopefully, by giving a ﬂavor of
the richness of this approach, this chapter will motivate you to continue ‘thinking algo-
rithmically’ about Physics.
Problem 7.1: (Efﬁcient temporal labeling)
Can you construct efﬁcient circuits
(which require only O(poly(n)) gates) to cyclically permute all diagonal elements
in a 2n×2n diagonal density matrix except the |0n⟩⟨0n| term?
Problem 7.2: (Computing with linear optics)
In performing quantum
computation with single photons, suppose that instead of the dual rail
representation of Section 7.4.1 we use a unary representation of states, where
|00 . . . 01⟩is 0, |00 . . . 010⟩is 1, |00 . . . 0100⟩is 2, and so on, up to |10 . . . 0⟩
being 2n −1.
1. Show that an arbitrary unitary transformation on these states can be
constructed completely from just beamsplitters and phase shifters (and no
nonlinear media).
Chapter problems
347
2. Construct a circuit of beamsplitters and phase shifters to perform the one
qubit Deutsch–Jozsa algorithm.
3. Construct a circuit of beamsplitters and phase shifters to perform the two
qubit quantum search algorithm.
4. Prove that an arbitrary unitary transform will, in general, require an
exponential number (in n) of components to realize.
Problem 7.3: (Control via Jaynes–Cummings interactions)
Robust and accurate
control of small quantum systems – via an external classical degree of freedom –
is important to the ability to perform quantum computation. It is quite
remarkable that atomic states can be controlled by applying optical pulses,
without causing superpositions of atomic states to decohere very much! In this
problem, we see what approximations are necessary for this to be the case. Let us
begin with the Jaynes–Cummings Hamiltonian for a single atom coupled to a
single mode of an electromagnetic ﬁeld,
H = a†σ−+ aσ+ ,
(7.175)
where σ± act on the atom, and a, a† act on the ﬁeld.
1. For U = eiθH, compute
An = ⟨n|U|α⟩,
(7.176)
where |α⟩and |n⟩are coherent states and number eigenstates of the ﬁeld,
respectively; An is an operator on atomic states, and you should obtain
An = e−|α2| |α|2
n!

cos(θ√n)
i√n
α sin(θ√n)
iα
√
n+1 sin(θ
√
n + 1)
cos(θ
√
n + 1)

.
(7.177)
The results of Exercise 7.17 may be helpful.
2. It is useful to make an approximation that α is large (without loss of
generality, we may choose α real). Consider the probability distribution
pn = e−x xn
n! ,
(7.178)
which has mean ⟨n⟩= x, and standard deviation
$
⟨n2⟩−⟨n⟩2 = √x. Now
change variables to n = x −L√x, and use Stirling’s approximation
n! ≈
√
2πn nne−n
(7.179)
to obtain
pL ≈e−L2/2
√
2π
.
(7.180)
3. The most important term is An for n = |α|2. Deﬁne n = α2 + Lα, and for
a = y

1
y2 + L
y
and
b = y

1
y2 + L
y + 1 ,
(7.181)
where y = 1/α, show that
AL ≈e−L2/4
(2π)1/4

cos aϕ
ia sin aϕ
(i/b) sin bϕ
cos bϕ

,
(7.182)
348
Quantum computers: physical realization
using θ = ϕ/α. Also verify that
% ∞
−∞
A†
LAL dL = I
(7.183)
as expected.
4. The ideal unitary transform which occurs to the atom is
U =
 cos αθ
i sin αθ
i sin αθ
cos αθ

.
(7.184)
How close is AL to U? See if you can estimate the ﬁdelity
F = min|ψ⟩
% ∞
−∞
|⟨ψ|U †AL|ψ⟩|2 dL
(7.185)
as a Taylor series in y.
Problem 7.4: (Ion trap logic with two-level atoms)
The controlled-
gate
described in Section 7.6.3 used a three-level atom for simplicity. It is possible to
do without this third level, with some extra complication, as this problem
demonstrates.
Let Υblue,j
ˆn
(θ) denote the operation accomplished by pulsing light at the blue
sideband frequency, ω = Ω + ωz, of the jth particle for time θ
√
N/ηΩ, and
similarly for the red sideband. ˆn denotes the axis of the rotation in the ˆx- ˆy
plane, controlled by setting the phase of the incident light. The superscript j
may be omitted when it is clear which ion is being addressed. Speciﬁcally,
Υblue
ˆn (θ) = exp
 
eiϕ|00⟩⟨11| + e−iϕ|11⟩⟨00|
+ eiϕ√
2|01⟩⟨12| + e−iϕ√
2|12⟩⟨01| + · · ·
 iθ
2

, (7.186)
where ˆn = ˆx cos ϕ + ˆy sin ϕ, and the two labels in the ket represent the internal
and the motional states, respectively, from left to right. The
√
2 factor comes
from the fact that a†|n⟩=
√
n + 1|n + 1⟩for bosonic states.
(1) Show that Sj = Υred,j
ˆy
(π) performs a swap between the internal and motional
states of ion j when the motional state is initially |0⟩.
(2) Find a value of θ such that Υblue
ˆn (θ) acting on any state in the computational
subspace, spanned by |00⟩, |01⟩, |10⟩, and |11⟩, leaves it in that subspace.
This should work for any axis ˆn.
(3) Show that if Υblue
ˆn (ϕ) stays within the computational subspace, then
U = Υblue
α (−β)Υblue
ˆn (θ)Υblue
α (β) also stays within the computational subspace,
for any choice of rotation angle β and axis α.
(4) Find values of α and β such that U is diagonal. Speciﬁcally, it is useful to
obtain an operator such as
⎡
⎢⎢⎣
e−iπ/
√
2
0
0
0
0
−1
0
0
0
0
1
0
0
0
0
eiπ/
√
2
⎤
⎥⎥⎦.
(7.187)
History and further reading
349
(5) Show that (7.187) describes a non-trivial gate, in that a controlled-
gate
between the internal states of two ions can be constructed from it and single
qubit operations. Can you come up with a composite pulse sequence for
performing a
without requiring the motional state to initially be |0⟩?
Summary of Chapter 7: Quantum computers: physical realization
• There are four basic requirements for implementation of quantum computa-
tion: (1) Representation of qubits, (2) Controllable unitary evolution, (3) Prepara-
tion of initial qubit states, and (4) Measurement of ﬁnal qubit states.
• Single photons can serve as good qubits, using |01⟩and |10⟩as logical 0 and 1,
but conventional nonlinear optical materials which are sufﬁciently strong to allow
single photons to interact inevitably absorb or scatter the photons.
• Cavity-QED is a technique by which single atoms can be made to interact strongly
with single photons. It provides a mechanism for using an atom to mediate inter-
actions between single photons.
• Trapped ions can be cooled to the extent that their electronic and nuclear spin
states can be controlled by applying laser pulses. By coupling spin states through
center-of-mass phonons, logic gates between different ions can be performed.
• Nuclear spins are nearly ideal qubits, and single molecules would be nearly
ideal quantum computers if their spin states could only be controlled and mea-
sured. Nuclear magnetic resonance makes this possible using large ensembles of
molecules at room temperature, but at the expense of signal loss due to an inefﬁ-
cient preparation procedure.
History and further reading
For an excellent discussion of why building a quantum computer is difﬁcult, see the
article by DiVincenzo[DiV95a], on which Figure 7.1 is based. DiVincenzo also presents
ﬁve criteria for realizing a quantum computer, which are similar to those discussed in
Section 7.2.
The quantum simple harmonic oscillator of Section 7.3 is a staple of quantum me-
chanics, and is treated in any standard textbook, such as [Sak95]. The general necessary
and sufﬁcient conditions for quantum computation given in Section 7.3.3 were discussed
by Lloyd[Llo94].
The optical quantum computer of Section 7.4 uses as its main theoretical tools the
formalism of quantum optics, which is described in many textbooks; two excellent ones
are [Lou73, Gar91]. For more on basic optics and optical technology, such as polarizers,
beamsplitters, photon detectors, and the like, see for example, the textbook [ST91]. The
beamsplitter in the single photon regime was studied by Campos, Saleh, and Tiech[CST89],
and related to that context, the elegant connection between SU(2) and two coupled har-
monic oscillators was ﬁrst described by Schwinger[Sak95]. The concept of a ‘dual-rail’ rep-
resentation of a qubit was suggested by Yurke and used by Chuang and Yamamoto[CY95]
to describe a complete quantum computer (using nonlinear Kerr media) to perform the
350
Quantum computers: physical realization
Deutsch–Jozsa algorithm (as in Exercise 7.13). The quantum-optical Fredkin gate was
ﬁrst described by Yamamoto, Kitagawa, and Igeta[YKI88] and Milburn[Mil89a]. The single
photon generation and detection technology required for an optical quantum computer
has been discussed by Imamoglu and Yamamoto[IY94] and by Kwiat, Steinberg, Chiao,
Eberhard, and Petroff[KSC+94]. An analogous mechanism using electron optics and the
Coulomb interaction in place of the Kerr interaction has been discussed by Kitagawa
and Ueda[KU91]. The fundamental limits of traditional, off-resonance nonlinear optical
materials at single quanta levels have been studied by Watanabe and Yamamoto[WY90].
Cerf, Adami, and Kwiat have studied the simulation of quantum logic using (expo-
nentially many) linear optical components[CAK98]. An inﬂuential earlier paper by Reck,
Zeilinger, Bernstein and Bertani[RZBB94] described similar constructions, but did not make
the explicit connection to quantum computation. Kwiat, Mitchell, Schwindt, and White
have constructed an optical simulation of Grover’s quantum search algorithm which uses
linear optics, but appears to require exponential resources when scaling[KMSW99]. See
Miller[Mil89b] for a discussion of the energetics of optical communication over different
distances.
Allen and Eberly[AE75] have written a beautiful treatise on two-level atoms and optical
resonance. The experiment described in Section 7.5.4 was performed by Turchette, Hood,
Lange, Mabuchi, and Kimble at Caltech in 1995[THL+95]. A detailed explanation is also
given in Turchette’s Ph.D. thesis[Tur97]. The single photons used in this experiment were
called ‘ﬂying qubits’. A different approach, in which atomic states are used as qubits
and the atoms travel through optical cavities, was proposed by Domokos, Raimond,
Brune, and Haroche[DRBH95]. It is based on the idea of using a single atom to switch
a coherent state into a cavity, as described by Davidovich, Maali, Brune, Raimond, and
Haroche[DRBH87, DMB+93].
The idea of ion trap quantum compution was proposed by Cirac and Zoller[CZ95].
Our discussion of this idea in Section 7.6.1 beneﬁted greatly from the articles by
Steane[Ste97], and by Wineland, Monroe, Itano, Leibfried, King, and Meekof[WMI+98].
Earnshaw’s theorem can be derived from Laplace’s equation, as described in his original
paper[Ear42], or a modern electromagnetics textbook such as the one by Ramo, Whin-
nery, and van Duzer[RWvD84]. Figure 7.8 is drawn after Figure 6 in [Ste97]. Figure 7.7
is drawn after [WMI+98]. The experiment described in Section 7.6.4 was performed
by Monroe, Meekhof, King, Itano, and Wineland at the National Institute of Stan-
dards and Technology in Boulder, Colorado[MMK+95]. Figure 7.15 is reprinted courtesy
of Wineland[WMI+98]. Brewer, DeVoe, and Kallenbach have envisioned using large ar-
rays of planar ion microtraps[BDK92] as a scalable quantum computer; this is the kind
of trap shown in Figure 7.12. James has extensively studied the theory of heating and
other decoherence mechanisms in ion traps[Jam98]. The impact of decoherence on ion trap
quantum computation has also been studied in some depth by Plenio and Knight, who
also consider effects such as the failure of the two-level approximation[PK96].
DiVincenzo ﬁrst suggested the use of nuclear spins in quantum computation[DiV95b],
and pointed out that the well known, and very old ENDOR (electron nucleon dou-
ble resonance) pulse sequence is essentially an instance of the controlled-
gate.
However, the problem of how to use an ensemble of nuclei at room temperature for
quantum computation was not solved until Cory, Fahmy, and Havel[CFH97], and Ger-
shenfeld and Chuang[GC97] realized that effective pure states could be prepared. It was
also necessary to realize that quantum algorithms can be modiﬁed to allow ensemble
History and further reading
351
readout; a solution to this problem was provided in [GC97], and is presented in Sec-
tion 7.7.3. Excellent textbooks on NMR have been written by Ernst, Bodenhausen, and
Wokaun[EBW87], and by Slichter[Sli96]. Warren has written a criticism of NMR quan-
tum computation[War97]; interestingly, in the same paper he advocates using electron
spin resonance (ESR) to do quantum computation. Temporal labeling was proposed
by Knill, Chuang, and Laﬂamme[KCL98]. The NMR logic gates, and Bell state prepa-
ration circuit of Section 7.7.4 were discussed by Chuang, Gershenfeld, Kubinec, and
Leung[CGKL98]. The realization of Grover’s algorithm in Section 7.7.4 was by Chuang,
Gershenfeld, and Kubinec[CGK98]; Figure 7.20 is taken from their paper. Linden, Kupce,
and Freeman note that the swap gate may be a useful contribution of quantum comput-
ing to NMR, and provides a pulse sequence for it[LKF99]. The three spin data in Fig-
ure 7.18 showing logical labeling are from the article by Vandersypen, Yannoni, Sherwood
and Chuang[VYSC99].
Yamaguchi and Yamamoto have creatively extended the NMR
idea to use a crystal lattice[YY99]. The molecules shown in Figure 7.21 are attributed
to (a) Chuang, Vandersypen, Zhou, Leung, and Lloyd[CVZ+98] (b) Cory, Mass, Price,
Knill, Laﬂamme, Zurek, and Havel[CMP+98] (c) Somaroo, Tseng, Havel, Laﬂamme, and
Cory[STH+99] (d) Vandersypen, Yannoni, Sherwood and Chuang[VYSC99]
(e) Nielsen,
Knill, and Laﬂamme[NKL98] (f) Leung, Vandersypen, Zhou, Sherwood, Yannoni, Ku-
binec and Chuang[LVZ+99]. Jones, Mosca, and Hansen have also realized various quantum
algorithms on small molecules[JM98, JMH98]. The optimal labeling scheme which achieves
the entropic limit was devised by Schulman and Vazirani[SV99].
Various criticisms of the NMR approach to quantum information processing have been
leveled. Perhaps the most comprehensive discussion is due to Schack and Caves[SC99],
building on earlier work by Braunstein, Caves, Jozsa, Linden, Popescu and Schack[BCJ+99],
whose main technical conclusions (though not the possible connection with NMR) were
obtained by Vidal and Tarrach[Vid99] and also by Zyczkowski, Horodecki, Sanpera and
Lewenstein[ZHSL99]. See also the discussion by Linden and Popescu[LP99].
There are far too many proposals for implementations of quantum computers to all
be mentioned here, so only a few are given; references in the given citations should
provide good leads to additional articles. Lloyd has envisioned many implementations
of quantum computers, including polymer systems[Llo93]. Nakamura, Pashkin, and Tsai
have demonstrated control of single Cooper-pair qubits and observation of their Rabi
oscillations[NPT99]. Mooij, Orlando, Levitov, Tian, van der Waal, and Lloyd have stud-
ied the representation of a qubit using a superconducting ﬂux representation[MOL+99].
Platzman and Dykman have creatively proposed the use of electrons bound to the sur-
face of liquid helium as qubits[PD99]. The description of the spin based quantum dot qubit
realization in Section 7.8 was proposed by Loss and DiVincenzo[LD98]; Exercise 7.52 is
due to them. An interesting entree into the literature on the coherence times of quan-
tum dots is the article by Huibers, Switkes, Marcus, Campman, and Gossard[HSM+98].
Imamoglu, Awschalom, Burkard, DiVincenzo, Loss, Sherwin, and Small have proposed
a quantum computer implementation using electron spins in quantum dots manipulated
with cavity QED techniques[IAB+99]. The silicon-based nuclear spin quantum computer
with 31P dopants was proposed by Kane[Kan98], and Vrijen, Yablonovitch, Wang, Jiang,
Balandin, Roychowdhury, Mor, and DiVincenzo[VYW+99] have extended this to use elec-
tron spins buried in a silicon-germanium heterostructure. Finally, Brennen, Caves, Jessen,
and Deutsch[BCJD99] have proposed an implementation of quantum computation using
neutral atoms trapped in a far off-resonance optical lattice.
352
Quantum computers: physical realization
Quantum teleportation has been experimentally realized using single photons and nu-
clear spins as qubits, as was described in ‘History and further reading’ at the end of
Chapter 1. One of these implementations, by Furusawa, Sørensen, Braunstein, Fuchs,
Kimble, and Polzik[FSB+98], is particularly of note in the context of this chapter, because
it eschews the use of a ﬁnite Hilbert space representation of quantum information (such
as qubits)! Instead, it utilizes inﬁnite dimensional Hilbert spaces, where continuous vari-
ables (such as position and momentum, as in Section 7.3.2) parameterize quantum states.
This approach to teleportation was originally suggested by Vaidman[Vai94], then further
developed by Braunstein and Kimble[BK98a]. The continuous variable representation has
also been extended to superdense coding by Braunstein and Kimble[BK99]; to quantum
error correction, independently by Braunstein[Bra98] and by Lloyd and Slotine[LS98]; and
to computation, by Lloyd and Braunstein[LB99].
III Quantum information
8 Quantum noise and quantum operations
Until now we have dealt almost solely with the dynamics of closed quantum systems, that
is, with quantum systems that do not suffer any unwanted interactions with the outside
world. Although fascinating conclusions can be drawn about the information processing
tasks which may be accomplished in principle in such ideal systems, these observations
are tempered by the fact that in the real world there are no perfectly closed systems, except
perhaps the universe as a whole. Real systems suffer from unwanted interactions with the
outside world. These unwanted interactions show up as noise in quantum information
processing systems. We need to understand and control such noise processes in order
to build useful quantum information processing systems. This is a central topic of the
third part of this book, which begins in this chapter with the description of the quantum
operations formalism, a powerful set of tools enabling us to describe quantum noise and
the behavior of open quantum systems.
What is the distinction between an open and a closed system? A swinging pendulum
like that found in some mechanical clocks can be a nearly ideal closed system. A pendulum
interacts only very slightly with the rest of the world – its environment – mainly through
friction. However, to properly describe the full dynamics of the pendulum and why it
eventually ceases to move one must take into account the damping effects of air friction
and imperfections in the suspension mechanism of the pendulum. Similarly, no quantum
systems are ever perfectly closed, and especially not quantum computers, which must be
delicately programmed by an external system to perform some desired set of operations.
For example, if the state of a qubit is represented by two positions of a electron, then that
electron will interact with other charged particles, which act as a source of uncontrolled
noise affecting the state of the qubit. An open system is nothing more than one which has
interactions with some other environment system, whose dynamics we wish to neglect,
or average over.
The mathematical formalism of quantum operations is the key tool for our descrip-
tion of the dynamics of open quantum systems. This tool is very powerful, in that it
simultaneously addresses a wide range of physical scenarios. It can be used to describe
not only nearly closed systems which are weakly coupled to their environments, but also
systems which are strongly coupled to their environments, and closed systems that are
opened suddenly and subject to measurement. Another advantage of quantum operations
in applications to quantum computation and quantum information is that they are espe-
cially well adapted to describe discrete state changes, that is, transformations between
an initial state ρ and ﬁnal state ρ′, without explicit reference to the passage of time. This
discrete-time analysis is rather different to the tools traditionally used by physicists for the
description of open quantum systems (such as ‘master equations’, ‘Langevin equations’,
and ‘stochastic differential equations’), which tend to be continuous-time descriptions.
354
Quantum noise and quantum operations
The chapter is structured as follows. We begin in Section 8.1 with a discussion of how
noise is described in classical systems. The intuition gained by understanding classical
noise is invaluable in learning how to think about quantum operations and quantum noise.
Section 8.2 introduces the quantum operations formalism from three different points
of view, enabling us to become thoroughly familiar with the basic theory of quantum
operations. Section 8.3 illustrates several important examples of noise using quantum
operations. These include such examples as depolarization, amplitude damping, and phase
damping. A geometric approach to understanding quantum noise on single qubits is
explained, using the Bloch sphere. Section 8.4 explains some miscellaneous applications
of quantum operations: the connection between quantum operations and other tools
conventionally used by physicists to describe quantum noise, such as master equations;
how to experimentally determine the dynamics a quantum system undergoes using a
procedure known as quantum process tomography; and an explanation of how quantum
operations can be used to understand the fact that the world around us appears to obey the
rules of classical physics, while it really follows quantum mechanical laws. The chapter
concludes in Section 8.5 with a discussion of the limitations of the quantum operations
formalism as a general approach to the description of noise in quantum systems.
8.1
Classical noise and Markov processes
To understand noise in quantum systems it is helpful to build some intuition by under-
standing noise in classical systems. How should we model noise in a classical system?
Let’s look at some simple examples to understand how this is done, and what it can teach
us about noise in quantum systems.
Imagine a bit is being stored on a hard disk drive attached to an ordinary classical
computer. The bit starts out in the state 0 or 1, but after a long time it becomes likely
that stray magnetic ﬁelds will cause the bit to become scrambled, possibly ﬂipping its
state. We can model this by a probability p for the bit to ﬂip, and a probability 1 −p for
the bit to remain the same. This process is illustrated in Figure 8.1.

NNNNNNNNNNNNNN

pppppppppppppp
Figure 8.1. After a long time a bit on a hard disk drive may ﬂip with probability p.
What is really going on, of course, is that the environment contains magnetic ﬁelds
which can cause the bit to ﬂip. To ﬁgure out the probability p for the bit to ﬂip we need
to understand two things. First, we need a model for the distribution of magnetic ﬁelds
in the environment. Assuming that the user of the hard disk drive isn’t doing anything
silly like running a strong magnet near the disk drive, we can construct a realistic model
by sampling the magnetic ﬁeld in environments similar to the one in which the drive
will be running. Second, we need a model for how magnetic ﬁelds in the environment
Classical noise and Markov processes
355
will interact with bits on the disk. Fortunately, such a model is already well known to
physicists, and goes by the name ‘Maxwell’s equations’. With these two elements in hand,
we can in principle calculate the probability p that a bit ﬂip will occur on the drive over
some prescribed period of time.
This basic procedure – ﬁnding a model for the environment and for the system–
environment interaction – is one we follow repeatedly in our study of noise, both classical
and quantum. Interactions with the environment are the fundamental source of noise in
both classical and quantum systems. It is often not easy to ﬁnd exact models for the
environment or the system–environment interaction; however, by being conservative in
our modeling and closely studying the observed properties of a system to see if it obeys
our model, it is possible to attain a high degree of accuracy in the modeling of noise in
real physical systems.
The behavior of the bit on the hard disk can be succinctly summarized in a single
equation. Suppose p0 and p1 are the initial probabilities that the bit is in the states 0
and 1, respectively. Let q0 and q1 be the corresponding probabilities after the noise has
occurred. Let X be the initial state of the bit, and Y the ﬁnal state of the bit. Then the
law of total probability (Appendix 1) states that
p(Y = y) =

x
p(Y = y|X = x)p(X = x) .
(8.1)
The conditional probabilities p(Y = y|X = x) are called transition probabilities, since
they summarize the changes that may occur in the system. Writing these equations out
explicitly for the bit on a hard disk we have
 q0
q1

=
 1 −p
p
p
1 −p
  p0
p1

.
(8.2)
Let’s look at a slightly more complicated example of noise in a classical system. Imagine
that we are trying to build a classical circuit to perform some computational task. Un-
fortunately, we’ve been given faulty components to build the circuit. Our rather artiﬁcial
circuit consists of a single input bit, X, to which are applied two consecutive (faulty)
gates, producing an intermediate bit Y , and a ﬁnal bit Z. It seems reasonable to
assume that whether the second
gate works correctly is independent of whether the
ﬁrst
gate worked correctly. This assumption – that the consecutive noise processes
act independently – is a physically reasonable assumption to make in many situations.
It results in a stochastic process X →Y →Z of a special type known as a Markov
process. Physically, this assumption of Markovicity corresponds to assuming that the
environment causing the noise in the ﬁrst
gate acts independently of the environ-
ment causing the noise in the second
gate, a reasonable assumption given that the
gates are likely to be located a considerable distance apart in space.
Summarizing, noise in classical systems can be described using the theory of stochastic
processes. Often, in the analysis of multi-stage processes it is a good assumption to use
Markov processes. For a single stage process the output probabilities ⃗q are related to the
input probabilities ⃗p by the equation
⃗q = E⃗p ,
(8.3)
where E is a matrix of transition probabilities which we shall refer to as the evolution
matrix. Thus, the ﬁnal state of the system is linearly related to the starting state. This
356
Quantum noise and quantum operations
feature of linearity is echoed in the description of quantum noise, with density matrices
replacing probability distributions.
What properties must the evolution matrix E possess? We require that if ⃗p is a valid
probability distribution, then E⃗p must also be a valid probability distribution. Satisfying
this condition turns out to be equivalent to two conditions on E. First, all the entries
of E must be non-negative, a condition known as the positivity requirement. If they
weren’t, then it would be possible to obtain negative probabilities in E⃗p. Second, all the
columns of E must sum to one, a condition known as the completeness requirement.
Suppose this weren’t true. Imagine, for example, that the ﬁrst column didn’t sum to one.
Letting ⃗p contain a one in the ﬁrst entry and zeroes everywhere else, we see that E⃗p
would not be a valid probability distribution in this case.
Summarizing, the key features of classical noise are as follows: there is a linear rela-
tionship between input and output probabilities, described by a transition matrix with
non-negative entries (positivity) and columns summing to one (completeness). Classical
noise processes involving multiple stages are described as Markov processes, provided the
noise is caused by independent environments. Each of these key features has important
analogues in the theory of quantum noise. Of course, there are also some surprising new
features of quantum noise!
8.2
Quantum operations
8.2.1
Overview
The quantum operations formalism is a general tool for describing the evolution of
quantum systems in a wide variety of circumstances, including stochastic changes to
quantum states, much as Markov processes describe stochastic changes to classical states.
Just as a classical state is described by a vector of probabilities, we shall describe quantum
states in terms of the density operator (density matrix) ρ, whose properties you may wish
to review by rereading Section 2.4, beginning on page 98, before continuing to read this
chapter. And similar to how classical states transform as described by (8.3), quantum
states transform as
ρ′ = E(ρ) .
(8.4)
The map E in this equation is a quantum operation. Two simple examples of quantum
operations which we have encountered previously, in Chapter 2, are unitary transforma-
tions and measurements, for which E(ρ) = UρU †, and Em(ρ) = MmρM †
m, respectively
(see Exercises 8.1 and 8.2, below). The quantum operation captures the dynamic change
to a state which occurs as the result of some physical process; ρ is the initial state be-
fore the process, and E(ρ) is the ﬁnal state after the process occurs, possibly up to some
normalization factor.
Over the next few sections, we develop a general theory of quantum operations in-
corporating unitary evolution, measurement, and even more general processes! We shall
develop three separate ways of understanding quantum operations, illustrated in Fig-
ure 8.2, all of which turn out to be equivalent. The ﬁrst method is based on the idea
of studying dynamics as the result of an interaction between a system and an environ-
ment, much as classical noise was described in Section 8.1. This method is concrete
and easy to relate to the real world. Unfortunately, it suffers from the drawback of not
being mathematically convenient. Our second method of understanding quantum op-
Quantum operations
357
erations, completely equivalent to the ﬁrst, overcomes this mathematical inconvenience
by providing a powerful mathematical representation for quantum operations known as
the operator-sum representation. This method is rather abstract, but is very useful for
calculations and theoretical work. Our third approach to quantum operations, equivalent
to the other two, is via a set of physically motivated axioms that we would expect a
dynamical map in quantum mechanics to satisfy. The advantage of this approach is that
it is exceedingly general – it shows that quantum dynamics will be described by quantum
operations under an amazingly wide range of circumstances. However, it does not offer
the calculational convenience of the second approach, nor the concrete nature of the ﬁrst.
Taken together, these three approaches to quantum operations offer a powerful tool with
which we can understand quantum noise and its effects.
	

	

		 

 

	
 


	
Figure 8.2. Three approaches to quantum operations which are equivalent, but offer different advantages
depending upon the intended application.
Exercise 8.1: (Unitary evolution as a quantum operation)
Pure states evolve
under unitary transforms as |ψ⟩→U|ψ⟩. Show that, equivalently, we may write
ρ →E(ρ) ≡UρU †, for ρ = |ψ⟩⟨ψ|.
Exercise 8.2: (Measurement as a quantum operation)
Recall from Section 2.2.3
(on page 84) that a quantum measurement with outcomes labeled by m is
described by a set of measurement operators Mm such that 
m M †
mMm = I.
Let the state of the system immediately before the measurement be ρ. Show that
for Em(ρ) ≡MmρM †
m, the state of the system immediately after the
measurement is
Em(ρ)
tr(Em(ρ)) .
(8.5)
Also show that the probability of obtaining this measurement result is
p(m) = tr(Em(ρ)).
8.2.2
Environments and quantum operations
The dynamics of a closed quantum system are described by a unitary transform. Con-
ceptually, we can think of the unitary transform as a box into which the input state
enters and from which the output exits, as illustrated on the left hand side of Figure 8.3.
358
Quantum noise and quantum operations
For our purposes, the interior workings of the box are not of concern to us; it could be
implemented by a quantum circuit, or by some Hamiltonian system, or anything else.
A natural way to describe the dynamics of an open quantum system is to regard it
as arising from an interaction between the system of interest, which we shall call the
principal system, and an environment, which together form a closed quantum system,
as illustrated on the right hand side of Figure 8.3. In other words, suppose we have a
system in state ρ, which is sent into a box which is coupled to an environment. In general
the ﬁnal state of the system, E(ρ), may not be related by a unitary transformation to the
initial state ρ. We assume (for now) that the system–environment input state is a product
state, ρ ⊗ρenv. After the box’s transformation U the system no longer interacts with
the environment, and thus we perform a partial trace over the environment to obtain the
reduced state of the system alone:
E(ρ) = trenv

U (ρ ⊗ρenv) U †
.
(8.6)
Of course, if U does not involve any interaction with the environment, then E(ρ) = ˜Uρ ˜U †,
where ˜U is the part of U which acts on the system alone. Equation (8.6) is our ﬁrst of
three equivalent deﬁnitions of a quantum operation.
Figure 8.3. Models of closed (left) and open (right) quantum systems. An open quantum system consists of two
parts, the principal system and an environment.
An important assumption is made in this deﬁnition – we assume that the system and
the environment start in a product state. In general, of course, this is not true. Quantum
systems interact constantly with their environments, building up correlations. One way
this expresses itself is via the exchange of heat between the system and its environment.
Left to itself a quantum system will relax to the same temperature as its environment,
which causes correlations to exist between the two. However, in many cases of practical
interest it is reasonable to assume that the system and its environment start out in a
product state. When an experimentalist prepares a quantum system in a speciﬁed state
they undo all the correlations between that system and the environment. Ideally, the
correlations will be completely destroyed, leaving the system in a pure state. Even if this
is not the case, we shall see later that the quantum operations formalism can even describe
quantum dynamics when the system and environment do not start out in a product state.
Another issue one might raise is: how can U be speciﬁed if the environment has nearly
inﬁnite degrees of freedom? It turns out, very interestingly, that in order for this model
to properly describe any possible transformation ρ →E(ρ), if the principal system has
a Hilbert space of d dimensions, then it sufﬁces to model the environment as being in a
Hilbert space of no more than d2 dimensions. It also turns out not to be necessary for the
environment to start out in a mixed state; a pure state will do. We shall return to these
points in Section 8.2.3.
Quantum operations
359
As an explicit example of the use of Equation (8.6), consider the two qubit quantum
circuit shown in Figure 8.4, in which U is a controlled-
gate, with the principal
system the control qubit, and the environment initially in the state ρenv = |0⟩⟨0| as the
target qubit. Inserting into Equation (8.6), it is easily seen that
E(ρ) = P0ρP0 + P1ρP1 ,
(8.7)
where P0 = |0⟩⟨0| and P1 = |1⟩⟨1| are projection operators. Intuitively, this dynamics
occurs because the environment stays in the |0⟩state only when the system is |0⟩; other-
wise the environment is ﬂipped to the state |1⟩. In the next section we give a derivation
of this equation as an example of the operator-sum representation.
Figure 8.4. Controlled-
gate as an elementary example of a single qubit quantum operation.
We have described quantum operations as arising from the interaction of a princi-
pal system with an environment; however, it is convenient to generalize the deﬁnition
somewhat to allow different input and output spaces. For example, imagine that a single
qubit, which we label A, is prepared in an unknown state ρ. A three-level quantum sys-
tem (‘qutrit’) labelled B is prepared in some standard state |0⟩, and then interacts with
system A via a unitary interaction U, causing the joint system to evolve into the state
U(ρ ⊗|0⟩⟨0|)U †. We then discard system A, leaving system B in some ﬁnal state ρ′. By
deﬁnition, the quantum operation E describing this process is
E(ρ) = ρ′ = trA(U(ρ ⊗|0⟩⟨0|)U †).
(8.8)
Notice that E maps density operators of the input system, A, to density operators of the
output system, B. Most of our discussion of quantum operations below is concerned with
quantum operations ‘on’ some system A, that is, they map density operators of system
A to density operators of system A. However it is occasionally useful in applications
to allow a more general deﬁnition. Such a deﬁnition is provided by deﬁning quantum
operations as the class of maps which arise as a result of the following processes: some
initial system is prepared in an unknown quantum state ρ, and then brought into contact
with other systems prepared in standard states, allowed to interact according to some
unitary interaction, and then some part of the combined system is discarded, leaving just
a ﬁnal system in some state ρ′. The quantum operation E deﬁning this process simply
maps ρ to ρ′. We will see that this extension to allow different input and output spaces gels
naturally with our treatment of quantum operations via the operator-sum representation,
and also our axiomatic study. Nevertheless, for the most part it simpliﬁes discussion if we
assume that the input and output spaces of a quantum operation are the same, using the
convenient distinction between ‘principal system’ and ‘environment’ which disappears
360
Quantum noise and quantum operations
in the general case, and giving occasional exercises to indicate the necessary extensions
when the input and output spaces are different.
8.2.3
Operator-sum representation
Quantum operations can be represented in an elegant form known as the operator-sum
representation, which is essentially a re-statement of Equation (8.6) explicitly in terms
of operators on the principal system’s Hilbert space alone. The main result is motivated
by the following simple calculation. Let |ek⟩be an orthonormal basis for the (ﬁnite
dimensional) state space of the environment, and let ρenv = |e0⟩⟨e0| be the initial state
of the environment. There is no loss of generality in assuming that the environment
starts in a pure state, since if it starts in a mixed state we are free to introduce an extra
system purifying the environment (Section 2.5). Although this extra system is ‘ﬁctitious’,
it makes no difference to the dynamics experienced by the principal system, and thus can
be used as an intermediate step in calculations. Equation (8.6) can thus be rewritten as
E(ρ) =

k
⟨ek|U

ρ ⊗|e0⟩⟨e0|

U †|ek⟩
(8.9)
=

k
EkρE†
k ,
(8.10)
where Ek ≡⟨ek|U|e0⟩is an operator on the state space of the principal system. Equa-
tion (8.10) is known as the operator-sum representation of E. The operators {Ek} are
known as operation elements for the quantum operation E. The operator-sum represen-
tation is important; it will be used repeatedly for the remainder of the book.
The operation elements satisfy an important constraint known as the completeness re-
lation, analogous to the completeness relation for evolution matrices in the description of
classical noise. In the classical case, the completeness relation arose from the requirement
that probability distributions be normalized to one. In the quantum case the completeness
relation arises from the analogous requirement that the trace of E(ρ) be equal to one,
1 = tr(E(ρ))
(8.11)
= tr

k
EkρE†
k

(8.12)
= tr

k
E†
kEkρ

.
(8.13)
Since this relationship is true for all ρ it follows that we must have

k
E†
kEk = I.
(8.14)
This equation is satisﬁed by quantum operations which are trace-preserving. There
are also non-trace-preserving quantum operations, for which 
k E†
kEk ≤I, but they
describe processes in which extra information about what occurred in the process is
obtained by measurement, as we explain in more detail shortly. Maps E of the form
of (8.10) for which 
k E†
kEk ≤I provide our second deﬁnition of a quantum operation.
We show below that this deﬁnition is essentially equivalent to the ﬁrst, Equation (8.6), and
in fact is slightly more general, since it allows for non-trace-preserving operations. We
Quantum operations
361
will often have occasion to move backwards and forwards between these two deﬁnitions; it
should be clear from context which deﬁnition we are working from at any given moment.
Exercise 8.3:
Our derivation of the operator-sum representation implicitly assumed
that the input and output spaces for the operation were the same. Suppose a
composite system AB initially in an unknown quantum state ρ is brought into
contact with a composite system CD initially in some standard state |0⟩, and the
two systems interact according to a unitary interaction U. After the interaction
we discard systems A and D, leaving a state ρ′ of system BC. Show that the
map E(ρ) = ρ′ satisﬁes
E(ρ) =

k
EkρE†
k,
(8.15)
for some set of linear operators Ek from the state space of system AB to the
state space of system BC, and such that 
k E†
kEk = I.
The operator-sum representation is important because it gives us an intrinsic means
of characterizing the dynamics of the principal system. The operator-sum representation
describes the dynamics of the principal system without having to explicitly consider prop-
erties of the environment; all that we need to know is bundled up into the operators Ek,
which act on the principal system alone. This simpliﬁes calculations and often provides
considerable theoretical insight. Furthermore, many different environmental interactions
may give rise to the same dynamics on the principal system. If it is only the dynamics of
the principal system which are of interest then it makes sense to choose a representation
of the dynamics which does not include unimportant information about other systems.
In the remainder of this section, we explore the properties of the operator-sum repre-
sentation, and in particular, three features. First, we give it a physical interpretation, in
terms of the operation elements Ek. A natural question which arises from this is how an
operator-sum representation can be determined for any open quantum system (given, for
example, the system–environment interaction or other speciﬁcation). This is answered
in the second topic addressed below, and the converse, how to construct a model open
quantum system for any operator-sum representation, concludes.
Exercise 8.4: (Measurement)
Suppose we have a single qubit principal system,
interacting with a single qubit environment through the transform
U = P0 ⊗I + P1 ⊗X ,
(8.16)
where X is the usual Pauli matrix (acting on the environment), and
P0 ≡|0⟩⟨0|, P1 ≡|1⟩⟨1| are projectors (acting on the system). Give the quantum
operation for this process, in the operator-sum representation, assuming the
environment starts in the state |0⟩.
Exercise 8.5: (Spin ﬂips)
Just as in the previous exercise, but now let
U = X
√
2
⊗I + Y
√
2
⊗X ,
(8.17)
Give the quantum operation for this process, in the operator-sum representation.
362
Quantum noise and quantum operations
Exercise 8.6: (Composition of quantum operations)
Suppose E and F are
quantum operations on the same quantum system. Show that the composition
F ◦E is a quantum operation, in the sense that it has an operator-sum
representation. State and prove an extension of this result to the case where E
and F do not necessarily have the same input and output spaces.
Physical interpretation of the operator-sum representation
There is a nice interpretation that can be given to the operator-sum representation. Imag-
ine that a measurement of the environment is performed in the basis |ek⟩after the unitary
transformation U has been applied. Applying the principle of implicit measurement, we
see that such a measurement affects only the state of the environment, and does not
change the state of the principal system. Let ρk be the state of the principal system given
that outcome k occurs, so
ρk ∝trE(|ek⟩⟨ek|U(ρ ⊗|e0⟩⟨e0|)U †|ek⟩⟨ek|) = ⟨ek|U(ρ ⊗|e0⟩⟨e0|)U †|ek⟩(8.18)
= EkρE†
k .
(8.19)
Normalizing ρk,
ρk =
EkρE†
k
tr(EkρE†
k)
,
(8.20)
we ﬁnd the probability of outcome k is given by
p(k) = tr(|ek⟩⟨ek|U(ρ ⊗|e0⟩⟨e0|)U †|ek⟩⟨ek|)
(8.21)
= tr(EkρE†
k).
(8.22)
Thus
E(ρ) =

k
p(k)ρk =

k
EkρE†
k .
(8.23)
This gives us a beautiful physical interpretation of what is going on in a quantum op-
eration with operation elements {Ek}. The action of the quantum operation is equivalent
to taking the state ρ and randomly replacing it by EkρE†
k/tr(EkρE†
k), with probability
tr(EkρE†
k). In this sense, it is very similar to the concept of noisy communication chan-
nels used in classical information theory; in this vein, we shall sometimes refer to certain
quantum operations which describe quantum noise processes as being noisy quantum
channels.

________

_ _ _ _ _ _ _ _

Figure 8.5. Controlled-
gate as an elementary model of single qubit measurement.
Quantum operations
363
A simple example, based on Figure 8.4, illustrates this interpretation of the operator-
sum representation. Suppose we choose the states |ek⟩= |0E⟩and |1E⟩, where we include
the E subscript to make it clear that the state is a state of the environment. This can
be interpreted as doing a measurement in the computational basis of the environment
qubit, as shown in Figure 8.5. Doing such a measurement does not, of course, change
the state of the principal system. Using subscripts P to denote the principal system, the
controlled-
may be expanded as
U = |0P0E⟩⟨0P0E| + |0P1E⟩⟨0P 1E| + |1P1E⟩⟨1P0E| + |1P 0E⟩⟨1P1E| .
(8.24)
Thus
E0 = ⟨0E|U|0E⟩= |0P ⟩⟨0P|
(8.25)
E1 = ⟨1E|U|0E⟩= |1P ⟩⟨1P| ,
(8.26)
and therefore
E(ρ) = E0ρE0 + E1ρE1 ,
(8.27)
in agreement with Equation (8.7).
Measurements and the operator-sum representation
Given a description of an open quantum system, how do we determine an operator-sum
representation for its dynamics? We have already found one answer: given the unitary
system–environment transformation operation U, and a basis of states |ek⟩for the envi-
ronment, the operation elements are
Ek ≡⟨ek|U|e0⟩.
(8.28)
It is possible to extend this result even further by allowing the possibility that a measure-
ment is performed on the combined system–environment after the unitary interaction,
allowing the acquisition of information about the quantum state. It turns out that this
physical possibility is naturally connected to non-trace-preserving quantum operations,
that is, maps E(ρ) = 
k EkρE†
k such that 
k E†
kEk ≤I.
Suppose the principal system is initially in a state ρ. For convenience we denote the
principal system by the letter Q. Adjoined to Q is an environment system E. We suppose
that Q and E are initially independent systems, and that E starts in some standard state,
σ. The joint state of the system is thus initially
ρQE = ρ ⊗σ .
(8.29)
We suppose that the systems interact according to some unitary interaction U. After the
unitary interaction a projective measurement is performed on the joint system, described
by projectors Pm. The case where no measurement is made corresponds to the special
case where there is only a single measurement outcome, m = 0, which corresponds to
the projector P0 ≡I.
The situation is summarized in Figure 8.6. Our aim is to determine the ﬁnal state of
Q as a function of the initial state, ρ. The ﬁnal state of QE is given by
PmU(ρ ⊗σ)U †Pm
tr(PmU(ρ ⊗σ)U †Pm) ,
(8.30)
364
Quantum noise and quantum operations
Q
-
Q′
ρQ
E
6
?
PmU
Figure 8.6. Environmental model for a quantum operation.
given that measurement outcome m occurred. Tracing out E we see that the ﬁnal state
of Q alone is
trE(PmU(ρ ⊗σ)U †Pm)
tr(PmU(ρ ⊗σ)U †Pm) .
(8.31)
This representation of the ﬁnal state involves the initial state σ of the environment, the
interaction U and the measurement operators Pm. Deﬁne a map
Em(ρ) ≡trE(PmU(ρ ⊗σ)U †Pm) ,
(8.32)
so the ﬁnal state of Q alone is Em(ρ)/tr(Em(ρ)). Note that tr[Em(ρ)] is the probability
of outcome m of the measurement occurring. Let σ = 
j qj|j⟩⟨j| be an ensemble
decomposition for σ. Introduce an orthonormal basis |ek⟩for the system E. Note that
Em(ρ) =

jk
qjtrE(|ek⟩⟨ek|PmU(ρ ⊗|j⟩⟨j|)U †Pm|ek⟩⟨ek|)
(8.33)
=

jk
EjkρE†
jk ,
(8.34)
where
Ejk ≡√qj⟨ek|PmU|j⟩.
(8.35)
This equation is a generalization of Equation (8.10), and gives an explicit means for
calculating the operators appearing in an operator-sum representation for Em, given that
the initial state σ of E is known, and the dynamics between Q and E are known. The
quantum operations Em can be thought of as deﬁning a kind of measurement process
generalizing the description of measurements given in Chapter 2.
Exercise 8.7:
Suppose that instead of doing a projective measurement on the
combined principal system and environment we had performed a general
measurement described by measurement operators {Mm}. Find operator-sum
representations for the corresponding quantum operations Em on the principal
system, and show that the respective measurement probabilities are tr[E(ρ)].
Quantum operations
365
System–environment models for any operator-sum representation
We have shown that interacting quantum systems give rise in a natural way to an operator-
sum representation for quantum operations. What about the converse problem? Given a
set of operators {Ek} is there some reasonable model environmental system and dynam-
ics which give rise to a quantum operation with those operation elements? By ‘reasonable’
we mean that the dynamics must be either a unitary evolution or a projective measure-
ment. Here, we show how to construct such a model. We will only show how to do this
for quantum operations mapping the input space to the same output space, although it
is mainly a matter of notation to generalize the construction to the more general case. In
particular, we show that for any trace-preserving or non-trace-preserving quantum op-
eration, E, with operation elements {Ek}, there exists a model environment, E, starting
in a pure state |e0⟩, and model dynamics speciﬁed by a unitary operator U and projector
P onto E such that
E(ρ) = trE(PU(ρ ⊗|e0⟩⟨e0|)U †P) .
(8.36)
To see this, suppose ﬁrst that E is a trace-preserving quantum operation, with operator-
sum representation generated by operation elements {Ek} satisfying the completeness
relation 
k E†
kEk = I, so we are only attempting to ﬁnd an appropriate unitary operator
U to model the dynamics. Let |ek⟩be an orthonormal basis set for E, in one-to-one
correspondence with the index k for the operators Ek. Note that by deﬁnition E has
such a basis; we are trying to ﬁnd a model environment giving rise to a dynamics described
by the operation elements {Ek}. Deﬁne an operator U which has the following action
on states of the form |ψ⟩|e0⟩,
U|ψ⟩|e0⟩≡

k
Ek|ψ⟩|ek⟩,
(8.37)
where |e0⟩is just some standard state of the model environment. Note that for arbitrary
states |ψ⟩and |ϕ⟩of the principal system,
⟨ψ|⟨e0|U †U|ϕ⟩|e0⟩=

k
⟨ψ|E†
kEk|ϕ⟩= ⟨ψ|ϕ⟩,
(8.38)
by the completeness relation. Thus the operator U can be extended to a unitary operator
acting on the entire state space of the joint system. It is easy to verify that
trE(U(ρ ⊗|e0⟩⟨e0|)U †) =

k
EkρE†
k ,
(8.39)
so this model provides a realization of the quantum operation E with operation elements
{Ek}. This result is illustrated in Box 8.1.
Non-trace-preserving quantum operations can easily be modeled using a construction
along the same lines (Exercise 8.8). A more interesting generalization of this construction
is the case of a set of quantum operations {Em} corresponding to possible outcomes
from a measurement, so the quantum operation 
m Em is trace-preserving, since the
probabilities of the distinct outcomes sum to one, 1 = 
m p(m) = tr

m Em
 (ρ)

for
all possible inputs ρ. See Exercise 8.9, below.
Exercise 8.8: (Non-trace-preserving quantum operations)
Explain how to
construct a unitary operator for a system–environment model of a
366
Quantum noise and quantum operations
non-trace-preserving quantum operation, by introducing an extra operator, E∞,
into the set of operation elements Ek, chosen so that when summing over the
complete set of k, including k = ∞, one obtains 
k E†
kEk = I.
Exercise 8.9: (Measurement model)
If we are given a set of quantum operations
{Em} such that 
m Em is trace-preserving, then it is possible to construct a
measurement model giving rise to this set of quantum operations. For each m,
let Emk be a set of operation elements for Em. Introduce an environmental
system, E, with an orthonormal basis |m, k⟩in one-to-one correspondence with
the set of indices for the operation elements. Analogously to the earlier
construction, deﬁne an operator U such that
U|ψ⟩|e0⟩=

mk
Emk|ψ⟩|m, k⟩.
(8.40)
Next, deﬁne projectors Pm ≡
k |m, k⟩⟨m, k| on the environmental system, E.
Show that performing U on ρ ⊗|e0⟩⟨e0|, then measuring Pm gives m with
probability tr(Em(ρ)), and the corresponding post-measurement state of the
principal system is Em(ρ)/tr(Em(ρ)).
Box 8.1: Mocking up a quantum operation
Given a trace-preserving quantum operation expressed in the operator-sum rep-
resentation, E(ρ) = 
k EkρE†
k, we can construct a physical model for it in the
following way. From (8.10), we want U to satisfy
Ek = ⟨ek|U|e0⟩,
(8.41)
where U is some unitary operator, and |ek⟩are orthonormal basis vectors for the
environment system. Such a U is conveniently represented as the block matrix
U =
⎡
⎢⎢⎢⎢⎢⎣
[E1]
·
·
·
. . .
[E2]
·
·
·
. . .
[E3]
·
·
·
. . .
[E4]
·
·
·
. . .
...
...
...
...
⎤
⎥⎥⎥⎥⎥⎦
(8.42)
in the basis |ek⟩. Note that the operation elements Ek only determine the ﬁrst
block column of this matrix (unlike elsewhere, here it is convenient to have the
ﬁrst label of the states be the environment, and the second, the principal system).
Determination of the rest of the matrix is left up to us; we simply choose the entries
such that U is unitary. Note that by the results of Chapter 4, U can be implemented
by a quantum circuit.
8.2.4
Axiomatic approach to quantum operations
Until now the main motivation for our study of quantum operations has been that they
provide an elegant way of studying systems which are interacting with an environment.
We’re going to switch to a different viewpoint now, where we try to write down physically
Quantum operations
367
motivated axioms which we expect quantum operations to obey. This viewpoint is more
abstract than our earlier approach, based on explicit system–environment models, but it
is also extremely powerful because of that abstraction.
The way we’re going to proceed is as follows. First, we’re going to forget everything
we’ve learned about quantum operations, and start over by deﬁning quantum operations
according to a set of axioms, which we’ll justify on physical grounds. That done, we’ll
prove that a map E satisﬁes these axioms if and only if it has an operator-sum represen-
tation, thus providing the missing link between the abstract axiomatic formulation, and
our earlier discussion.
We deﬁne a quantum operation E as a map from the set of density operators of the
input space Q1 to the set of density operators for the output space Q2, with the following
three axiomatic properties: (note that for notational simplicity in the proofs we take
Q1 = Q2 = Q)
• A1: First, tr[E(ρ)] is the probability that the process represented by E occurs, when
ρ is the initial state. Thus, 0 ≤tr[E(ρ)] ≤1 for any state ρ.
• A2: Second, E is a convex-linear map on the set of density matrices, that is, for
probabilities {pi},
E

i
piρi

=

i
piE(ρi) .
(8.43)
• A3: Third, E is a completely positive map. That is, if E maps density operators of
system Q1 to density operators of system Q2, then E(A) must be positive for any
positive operator A. Furthermore, if we introduce an extra system R of arbitrary
dimensionality, it must be true that (I ⊗E)(A) is positive for any positive operator
A on the combined system RQ1, where I denotes the identity map on system R.
The ﬁrst property is one of mathematical convenience. To cope with the case of
measurements, it turns out that it is extremely convenient to make the convention that
E does not necessarily preserve the trace property of density matrices, that tr(ρ) = 1.
Rather, we make the convention that E is to be deﬁned in such a way that tr[E(ρ)] is
equal to the probability of the measurement outcome described by E occurring. For
example, suppose that we are doing a projective measurement in the computational basis
of a single qubit. Then two quantum operations are used to describe this process, deﬁned
by E0(ρ) ≡|0⟩⟨0|ρ|0⟩⟨0| and E1(ρ) ≡|1⟩⟨1|ρ|1⟩⟨1|. Notice that the probabilities of the
respective outcomes are correctly given by tr[E0(ρ)] and tr[E1(ρ)]. With this convention
the correctly normalized ﬁnal quantum state is therefore
E(ρ)
tr[E(ρ)] .
(8.44)
In the case where the process is deterministic, that is, no measurement is taking place, this
reduces to the requirement that tr[E(ρ)] = 1 = tr(ρ), for all ρ. As previously discussed,
in this case, we say that the quantum operation is a trace-preserving quantum operation,
since on its own E provides a complete description of the quantum process. On the other
hand, if there is a ρ such that tr[E(ρ)] < 1, then the quantum operation is non-trace-
preserving, since on its own E does not provide a complete description of the processes
that may occur in the system. (That is, other measurement outcomes may occur, with
368
Quantum noise and quantum operations
some probability.) A physical quantum operation is one that satisﬁes the requirement
that probabilities never exceed 1, tr[E(ρ)] ≤1.
The second property stems from a physical requirement on quantum operations. Sup-
pose the input ρ to the quantum operation is obtained by randomly selecting the state
from an ensemble {pi, ρi} of quantum states, that is, ρ = 
i piρi. Then we would
expect that the resulting state, E(ρ)/tr[E(ρ)] = E(ρ)/p(E) corresponds to a random se-
lection from the ensemble {p(i|E), E(ρi)/tr[E(ρi)]}, where p(i|E) is the probability that
the state prepared was ρi, given that the process represented by E occurred. Thus, we
demand that
E(ρ) = p(E)

i
p(i|E) E(ρi)
tr[E(ρi)] ,
(8.45)
where p(E) = tr[E(ρ)] is the probability that the process described by E occurs on input
of ρ. By Bayes’ rule (Appendix 1),
p(i|E) = p(E|i) pi
p(E) = tr[E(ρi)]pi
p(E)
(8.46)
so (8.45) reduces to (8.43).
The third property also originates from an important physical requirement, that not
only must E(ρ) be a valid density matrix (up to normalization) so long as ρ is valid, but
furthermore, if ρ = ρRQ is the density matrix of some joint system of R and Q, if E acts
only on Q, then E(ρRQ) must still result in a valid density matrix (up to normalization)
of the joint system. An example is given in Box 8.2. Formally, suppose we introduce a
second (ﬁnite dimensional) system R. Let I denote the identity map on system R. Then
the map I ⊗E must take positive operators to positive operators.
It is perhaps surprising that these three axioms are sufﬁcient to deﬁne quantum op-
erations. However, the following theorem shows that they are equivalent to the earlier
system-environment models and the deﬁnition in terms of an operator-sum representa-
tion:
Theorem 8.1: The map E satisﬁes axioms A1, A2 and A3 if and only if
E(ρ) =

i
EiρE†
i ,
(8.50)
for some set of operators {Ei} which map the input Hilbert space to the output
Hilbert space, and 
i E†
i Ei ≤I.
Proof
Suppose E(ρ) = 
i EiρE†
i . E is obviously linear, so to check that E is a quantum
operation we need only prove that it is completely positive. Let A be any positive operator
acting on the state space of an extended system, RQ, and let |ψ⟩be some state of RQ.
Deﬁning |ϕi⟩≡(IR ⊗E†
i )|ψ⟩, we have
⟨ψ|(IR ⊗Ei)A(IR ⊗E†
i )|ψ⟩= ⟨ϕi|A|ϕi⟩
(8.51)
≥0,
(8.52)
by the positivity of the operator A. It follows that
⟨ψ|(I ⊗E)(A)|ψ⟩=

i
⟨ϕi|A|ϕi⟩≥0,
(8.53)
Quantum operations
369
Box 8.2: Complete positivity versus positivity
The transpose operation on a single qubit provides an example of why complete
positivity is an important requirement for quantum operations. By deﬁnition, this
map transposes the density operator in the computational basis:
 a
b
c
d

T
−→
 a
c
b
d

.
(8.47)
This map preserves positivity of a single qubit. However, suppose that qubit is part
of a two qubit system initially in the entangled state
|00⟩+ |11⟩
√
2
,
(8.48)
and the transpose operation is applied to the ﬁrst of these two qubits, while the
second qubit is subject to trivial dynamics. Then the density operator of the system
after the dynamics has been applied is
1
2
⎡
⎢⎢⎣
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
⎤
⎥⎥⎦.
(8.49)
A calculation shows that this operator has eigenvalues 1/2, 1/2, 1/2 and −1/2, so
this is not a valid density operator. Thus, the transpose operation is an example of
a positive map which is not completely positive, that is, it preserves the positivity
of operators on the principal system, but does not continue to preserve positivity
when applied to systems which contain the principal system as a subsystem.
and thus for any positive operator A, the operator (I ⊗E)(A) is also positive, as required.
The requirement 
i E†
i Ei ≤I ensures that probabilities are less than or equal to 1.
This completes the ﬁrst part of the proof.
Suppose next that E satisﬁes axioms A1, A2 and A3. Our aim will be to ﬁnd an
operator-sum representation for E. Suppose we introduce a system, R, with the same
dimension as the original quantum system, Q. Let |iR⟩and |iQ⟩be orthonormal bases
for R and Q. It will be convenient to use the same index, i, for these two bases, and this
can certainly be done as R and Q have the same dimensionality. Deﬁne a joint state |α⟩
of RQ by
|α⟩≡

i
|iR⟩|iQ⟩.
(8.54)
The state |α⟩is, up to a normalization factor, a maximally entangled state of the systems
R and Q. This interpretation of |α⟩as a maximally entangled state may help in under-
standing the following construction. Next, we deﬁne an operator σ on the state space of
RQ by
σ ≡(IR ⊗E)(|α⟩⟨α|).
(8.55)
We may think of this as the result of applying the quantum operation E to one half of
370
Quantum noise and quantum operations
a maximally entangled state of the system RQ. It is a truly remarkable fact, which we
will now demonstrate, that the operator σ completely speciﬁes the quantum operation E.
That is, to know how E acts on an arbitrary state of Q, it is sufﬁcient to know how it
acts on a single maximally entangled state of Q with another system!
The trick which allows us to recover E from σ is as follows. Let |ψ⟩= 
j ψj|jQ⟩be
any state of system Q. Deﬁne a corresponding state | ˜ψ⟩of system R by the equation
| ˜ψ⟩≡

j
ψ∗
j |jR⟩.
(8.56)
Notice that
⟨˜ψ|σ| ˜ψ⟩= ⟨˜ψ|
⎛
⎝
ij
|iR⟩⟨jR| ⊗E(|iQ⟩⟨jQ|)
⎞
⎠| ˜ψ⟩
(8.57)
=

ij
ψiψ∗
j E(|iQ⟩⟨jQ|)
(8.58)
= E(|ψ⟩⟨ψ|).
(8.59)
Let σ = 
i |si⟩⟨si| be some decomposition of σ, where the vectors |si⟩need not be
normalized. Deﬁne a map
Ei(|ψ⟩) ≡⟨˜ψ|si⟩.
(8.60)
A little thought shows that this map is a linear map, so Ei is a linear operator on the
state space of Q. Furthermore, we have

i
Ei|ψ⟩⟨ψ|E†
i =

i
⟨˜ψ|si⟩⟨si| ˜ψ⟩
(8.61)
= ⟨˜ψ|σ| ˜ψ⟩
(8.62)
= E(|ψ⟩⟨ψ|).
(8.63)
Thus
E(|ψ⟩⟨ψ|) =

i
Ei|ψ⟩⟨ψ|E†
i ,
(8.64)
for all pure states, |ψ⟩, of Q. By convex-linearity it follows that
E(ρ) =

i
EiρE†
i
(8.65)
in general. The condition 
i E†
i Ei ≤I follows immediately from axiom A1 identifying
the trace of E(ρ) with a probability.
Freedom in the operator-sum representation
We have seen that the operator-sum representation provides a very general description
of the dynamics of an open quantum system. Is it a unique description?
Consider quantum operations E and F acting on a single qubit with the operator-
sum representations E(ρ) = 
k EkρE†
k and F(ρ) = 
k FkρF †
k, where the operation
elements for E and F are deﬁned by
E1 = I
√
2
=
1
√
2
 1
0
0
1

E2 = Z
√
2
=
1
√
2
 1
0
0
−1

(8.66)
Quantum operations
371
and
F1 = |0⟩⟨0| =
 1
0
0
0

F2 = |1⟩⟨1| =
 0
0
0
1

.
(8.67)
These appear to be very different quantum operations. What is interesting is that E and
F are actually the same quantum operation. To see this, note that F1 = (E1 + E2)/
√
2
and F2 = (E1 −E2)/
√
2. Thus,
F(ρ) = (E1 + E2)ρ(E†
1 + E†
2) + (E1 −E2)ρ(E†
1 −E†
2)
2
(8.68)
= E1ρE†
1 + E2ρE†
2
(8.69)
= E(ρ) .
(8.70)
This example shows that the operation elements appearing in an operator-sum represen-
tation for a quantum operation are not unique.
The freedom in the representation is very interesting. Suppose we ﬂipped a fair coin,
and, depending on the outcome of the coin toss, applied either the unitary operator I or Z
to the quantum system. This process corresponds to the ﬁrst operator-sum representation
for E. The second operator-sum representation for E (labeled F above) corresponds to
performing a projective measurement in the {|0⟩, |1⟩} basis, with the outcome of the
measurement unknown. These two apparently very different physical processes give rise
to exactly the same dynamics for the principal system.
When do two sets of operation elements give rise to the same quantum operation? Un-
derstanding this question is important for at least two reasons. First, from a physical point
of view, understanding the freedom in the representation gives us more insight into how
different physical processes can give rise to the same system dynamics. Second, under-
standing the freedom in operator-sum representation is crucial to a good understanding
of quantum error-correction.
Intuitively, it is clear that there must be a great deal of freedom in an operator-
sum representation. Consider a trace-preserving quantum operation E which describes
the dynamics of some system such as that shown in Figure 8.3. We have shown that
the operation elements Ek = ⟨ek|U|e0⟩for E may be associated with an orthonormal
basis |ek⟩for the environment. Suppose that we supplement the interaction U with an
additional unitary action U ′ on the environment alone, as shown in Figure 8.7. Clearly this
does not change the state of the principal system. What are the corresponding operation
elements to this new process, (I ⊗U ′)U? We obtain:
Fk = ⟨ek|(I ⊗U ′) U|e0⟩
(8.71)
=

j

I ⊗⟨ek|U ′|ej⟩

⟨ej|U|e0⟩
(8.72)
=

j
U ′
kjEj ,
(8.73)
where we have used the fact that 
j |ej⟩⟨ej| = I, and U ′
kj are the matrix elements
of U ′ with respect to the basis |ek⟩. It turns out that the freedom in the operator-sum
representation yielded by this physically motivated picture captures the essence of the
complete freedom available in the operator-sum representation, as proved in the following
theorem.
372
Quantum noise and quantum operations
U
U′

________

_ _ _ _ _ _ _ _

ρ
|e0⟩⟨e0|
E(ρ)
Figure 8.7. Origin of the unitary freedom in the operator-sum representation.
Theorem 8.2: (Unitary freedom in the operator-sum representation) Suppose
{E1, . . . , Em} and {F1, . . . , Fn} are operation elements giving rise to quantum
operations E and F, respectively. By appending zero operators to the shorter list
of operation elements we may ensure that m = n. Then E = F if and only if
there exist complex numbers uij such that Ei = 
j uijFj, and uij is an m by
m unitary matrix.
Proof
The key to the proof is Theorem 2.6, on page 103. Recall that this result tells us that
two sets of vectors |ψi⟩and |ϕj⟩generate the same operator if and only if
|ψi⟩=

j
uij|ϕj⟩,
(8.74)
where uij is a unitary matrix of complex numbers, and we ‘pad’ whichever set of states
|ψi⟩or |ϕj⟩is smaller with additional states 0 so that the two sets have the same number
of elements. This result allows us to characterize the freedom in operator-sum represen-
tations. Suppose {Ei} and {Fj} are two sets of operation elements for the same quantum
operation, 
i EiρE†
i = 
j FjρF †
j for all ρ. Deﬁne
|ei⟩≡

k
|kR⟩
Ei|kQ⟩

(8.75)
|fj⟩≡

k
|kR⟩
Fj|kQ⟩
 .
(8.76)
Recall the deﬁnition of σ in Equation (8.55), from which it follows that σ = 
i |ei⟩⟨ei| =

j |fj⟩⟨fj|, and thus there exists unitary uij such that
|ei⟩=

j
uij|fj⟩.
(8.77)
But for arbitrary |ψ⟩we have
Ei|ψ⟩= ⟨˜ψ|ei⟩
(8.78)
=

j
uij⟨˜ψ|fj⟩
(8.79)
=

k
uijFj|ψ⟩.
(8.80)
Thus
Ei =

j
uijFj .
(8.81)
Examples of quantum noise and quantum operations
373
Conversely, supposing Ei and Fj are related by a unitary transformation of the form Ei =

ij uijFj, simple algebra shows that the quantum operation with operation elements
{Ei} is the same as the quantum operation with operation elements {Fj}.
Theorem 8.2 can be used to answer another interesting question: what is the maximum
size of an environment that would be needed to mock up a given quantum operation?
Theorem 8.3: All quantum operations E on a system of Hilbert space dimension d can
be generated by an operator-sum representation containing at most d2 elements,
E(ρ) =
M

k=1
EkρE†
k ,
(8.82)
where 1 ≤M ≤d2.
The proof of this theorem is simple and is left as an exercise for you.
Exercise 8.10:
Give a proof of Theorem 8.3 based on the freedom in the
operator-sum representation, as follows. Let {Ej} be a set of operation elements
for E. Deﬁne a matrix Wjk ≡tr(E†
jEk). Show that the matrix W is Hermitian
and of rank at most d2, and thus there is unitary matrix u such that uWu† is
diagonal with at most d2 non-zero entries. Use u to deﬁne a new set of at most
d2 non-zero operation elements {Fj} for E.
Exercise 8.11:
Suppose E is a quantum operation mapping a d-dimensional input
space to a d′-dimensional output space. Show that E can be described using a set
of at most dd′ operation elements {Ek}.
The freedom in the operator-sum representation is surprisingly useful. We use it, for
example, in our study of quantum error-correction in Chapter 10. In that chapter we
will see that certain sets of operators in the operator-sum representation give more useful
information about the quantum error-correction process, and it behooves us to study
quantum error-correction from that point of view. As usual, having multiple ways of
understanding a process gives us much more insight into what is going on.
8.3
Examples of quantum noise and quantum operations
In this section we examine some concrete examples of quantum noise and quantum
operations. These models illustrate the power of the quantum operations formalism we
have been developing. They are also important in understanding the practical effects
of noise on quantum systems, and how noise can be controlled by techniques such as
error-correction.
We begin in Section 8.3.1 by considering how measurement can be described as a
quantum operation, and in particular we consider the trace and partial trace operations.
After that, we turn to noise processes, beginning in Section 8.3.2 with the presentation
of a graphical method for understanding quantum operations on a single qubit. This
method is used in the remainder of the section to illustrate elementary bit and phase ﬂip
error processes (in Section 8.3.3), the depolarizing channel (in Section 8.3.4), amplitude
damping (in Section 8.3.5), and phase damping (in Section 8.3.6). Amplitude and phase
374
Quantum noise and quantum operations
damping are ideal models of noise that capture many of the most important features
of the noise occurring in quantum mechanical systems, and we not only consider their
abstract mathematical formulation, but also how the processes arise in real-world quantum
systems.
8.3.1
Trace and partial trace
One of the main uses of the quantum operations formalism is to describe the effects of
measurement. Quantum operations can be used to describe both the probability of getting
a particular outcome from a measurement on a quantum system, and also the state change
in the system effected by the measurement.
The simplest operation related to measurement is the trace map ρ →tr(ρ) – which
we can show is indeed a quantum operation, in the following way. Let HQ be any
input Hilbert space, spanned by an orthonormal basis |1⟩. . . |d⟩, and let H′
Q be a one-
dimensional output space, spanned by the state |0⟩. Deﬁne
E(ρ) ≡
d

i=1
|0⟩⟨i|ρ|i⟩⟨0| ,
(8.83)
so that E is a quantum operation, by Theorem 8.1. Note that E(ρ) = tr(ρ)|0⟩⟨0|, so that,
up to the unimportant |0⟩⟨0| multiplier, this quantum operation is identical to the trace
function.
An even more useful result is the observation that the partial trace is a quantum
operation. Suppose we have a joint system QR, and wish to trace out system R. Let |j⟩
be a basis for system R. Deﬁne a linear operator Ei : HQR →HQ by
Ei
⎛
⎝
j
λj|qj⟩|j⟩
⎞
⎠≡λi|qi⟩,
(8.84)
where λj are complex numbers, and |qj⟩are arbitrary states of system Q. Deﬁne E to be
the quantum operation with operation elements {Ei}, that is,
E(ρ) ≡

i
EiρE†
i .
(8.85)
By Theorem 8.1, this is a quantum operation from system QR to system Q. Notice that
E(ρ ⊗|j⟩⟨j′|) = ρδj,j′ = trR(ρ ⊗|j⟩⟨j′|) ,
(8.86)
where ρ is any Hermitian operator on the state space of system Q, and |j⟩and |j′⟩are
members of the orthonormal basis for system R. By linearity of E and trR, it follows that
E = trR.
8.3.2
Geometric picture of single qubit quantum operations
There is an elegant geometric method for picturing quantum operations on a single qubit.
This method allows one to get an intuitive feel for the behavior of quantum operations
in terms of their action on the Bloch sphere. Recall from Exercise 2.72 on page 105 that
the state of a single qubit can always be written in the Bloch representation,
ρ = I + ⃗r · ⃗σ
2
,
(8.87)
Examples of quantum noise and quantum operations
375
where ⃗r is a three component real vector. Explicitly, this gives us
ρ = 1
2

1 + rz
rx −iry
rx + iry
1 −rz

.
(8.88)
In this representation, it turns out that an arbitrary trace-preserving quantum operation
is equivalent to a map of the form
⃗r
E→⃗r ′ = M⃗r + ⃗c,
(8.89)
where M is a 3 × 3 real matrix, and ⃗c is a constant vector. This is an afﬁne map,
mapping the Bloch sphere into itself. To see this, suppose the operators Ei generating
the operator-sum representation for E are written in the form
Ei = αiI +
3

k=1
aikσk.
(8.90)
Then it is not difﬁcult to check that
Mjk =

l

alja∗
lk + a∗
ljalk +

|αl|2 −

p
alpa∗
lp

δjk + i

p
ϵjkp(αla∗
lp −α∗
l alp)

(8.91)
ck = 2i

l

jp
ϵjpkalja∗
lp,
(8.92)
where we have made use of the completeness relation 
i E†
i Ei = I to simplify the
expression for ⃗c.
The meaning of the afﬁne map, Equation (8.89), is made clearer by considering the
polar decomposition of the matrix M, M = U|M|, where U is unitary. Because M
is real, it follows that |M| is real and Hermitian, that is, |M| is a symmetric matrix.
Furthermore, because M is real we may assume that U is real, and is thus an orthogonal
matrix, that is, U TU = I, where T is the transpose operation. Thus we may write
M = OS ,
(8.93)
where O is a real orthogonal matrix with determinant 1, representing a proper rotation,
and S is a real symmetric matrix. Viewed this way, Equation (8.89) is just a deformation
of the Bloch sphere along principal axes determined by S, followed by a proper rotation
due to O, followed by a displacement due to ⃗c.
Exercise 8.12:
Why can we assume that O has determinant 1 in the
decomposition (8.93)?
Exercise 8.13:
Show that unitary transformations correspond to rotations of the Bloch
sphere.
Exercise 8.14:
Show that det(S) need not be positive.
376
Quantum noise and quantum operations
8.3.3
Bit ﬂip and phase ﬂip channels
The geometric picture described above can be used to visualize some important quantum
operations on single qubits, which will later be used in the theory of error-correction. The
bit ﬂip channel ﬂips the state of a qubit from |0⟩to |1⟩(and vice versa) with probability
1 −p. It has operation elements
E0 = √p I = √p
 1
0
0
1

E1 =
$
1 −p X =
$
1 −p
 0
1
1
0

.
(8.94)
The effect of the bit ﬂip channel is illustrated in Figure 8.8.
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
=⇒
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
Figure 8.8. The effect of the bit ﬂip channel on the Bloch sphere, for p = 0.3. The sphere on the left represents the
set of all pure states, and the deformed sphere on the right represents the states after going through the channel.
Note that the states on the ˆx axis are left alone, while the ˆy- ˆz plane is uniformly contracted by a factor of 1 −2p.
This geometric picture makes it very easy to verify certain facts about this quantum
operation. For example, it is easy to verify that the quantity tr(ρ2) for a single qubit is
equal to (1 + |r|2)/2, where |r| is the norm of the Bloch vector. The contraction of the
Bloch sphere illustrated in Figure 8.8 cannot increase the norm of the Bloch vector, and
therefore we can immediately conclude that tr(ρ2) can only ever decrease for the bit ﬂip
channel. This is but one example of the use of the geometric picture; once it becomes
sufﬁciently familiar it becomes a great source of insight about the properties of quantum
operations on a single qubit.
The phase ﬂip channel has operation elements
E0 = √p I = √p
 1
0
0
1

E1 =
$
1 −p Z =
$
1 −p
 1
0
0
−1

.
(8.95)
The effect of the phase ﬂip channel is illustrated in Figure 8.9. As a special case of the
phase ﬂip channel, consider the quantum operation which arises when we choose p = 1/2.
Using the freedom in the operator-sum representation this operation may be written
ρ →E(ρ) = P0ρP0 + P1ρP1 ,
(8.96)
where P0 = |0⟩⟨0|, P1 = |1⟩⟨1|, which corresponds to a measurement of the qubit in the
|0⟩, |1⟩basis, with the result of the measurement unknown. Using the above prescription
Examples of quantum noise and quantum operations
377
it is easy to see that the corresponding map on the Bloch sphere is given by
(rx, ry, rz) →(0, 0, rz) .
(8.97)
Geometrically, the Bloch vector is projected along the z axis, and the x and y components
of the Bloch vector are lost.
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
=⇒
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
Figure 8.9. The effect of the phase ﬂip channel on the Bloch sphere, for p = 0.3. Note that the states on the ˆz axis
are left alone, while the ˆx −ˆy plane is uniformly contracted by a factor of 1 −2p.
The bit–phase ﬂip channel has operation elements
E0 = √p I = √p
 1
0
0
1

E1 =
$
1 −p Y =
$
1 −p
 0
−i
i
0

.
(8.98)
As the name indicates, this is a combination of a phase ﬂip and a bit ﬂip, since Y = iXZ.
The action of the bit–phase ﬂip channel is illustrated in Figure 8.10.
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
=⇒
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
Figure 8.10. The effect of the bit–phase ﬂip channel on the Bloch sphere, for p = 0.3. Note that the states on the ˆy
axis are left alone, while the ˆx- ˆz plane is uniformly contracted by a factor of 1 −2p.
378
Quantum noise and quantum operations
Exercise 8.15:
Suppose a projective measurement is performed on a single qubit in
the basis |+⟩, |−⟩, where |±⟩≡(|0⟩± |1⟩)/
√
2. In the event that we are ignorant
of the result of the measurement, the density matrix evolves according to the
equation
ρ →E(ρ) = |+⟩⟨+|ρ|+⟩⟨+|
+
|−⟩⟨−|ρ|−⟩⟨−|.
(8.99)
Illustrate this transformation on the Bloch sphere.
Exercise 8.16:
The graphical method for understanding single qubit quantum
operations was derived for trace-preserving quantum operations. Find an explicit
example of a non-trace-preserving quantum operation which cannot be described
as a deformation of the Bloch sphere, followed by a rotation and a displacement.
8.3.4
Depolarizing channel
The depolarizing channel is an important type of quantum noise. Imagine we take a
single qubit, and with probability p that qubit is depolarized. That is, it is replaced by
the completely mixed state, I/2. With probability 1 −p the qubit is left untouched. The
state of the quantum system after this noise is
E(ρ) = pI
2 + (1 −p)ρ.
(8.100)
The effect of the depolarizing channel on the Bloch sphere is illustrated in Figure 8.11.
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
=⇒
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
Figure 8.11. The effect of the depolarizing channel on the Bloch sphere, for p = 0.5. Note how the entire sphere
contracts uniformly as a function of p.
A quantum circuit simulating the depolarizing channel is illustrated in Figure 8.12.
The top line of the circuit is the input to the depolarizing channel, while the bottom two
lines are an ‘environment’ to simulate the channel. We have used an environment with
two mixed state inputs. The idea is that the third qubit, initially a mixture of the state
|0⟩with probability 1 −p and state |1⟩with probability p acts as a control for whether or
not the completely mixed state I/2 stored in the second qubit is swapped into the ﬁrst
qubit.
Examples of quantum noise and quantum operations
379
Figure 8.12. Circuit implementation of the depolarizing channel.
The form (8.100) is not in the operator-sum representation. However, if we observe
that for arbitrary ρ,
I
2 = ρ + XρX + Y ρY + ZρZ
4
(8.101)
and then substitute for I/2 into (8.100) we arrive at the equation
E(ρ) =

1 −3p
4

ρ + p
4 (XρX + Y ρY + ZρZ) ,
(8.102)
showing that the depolarizing channel has operation elements {
$
1 −3p/4 I, √p X/2,
√p Y/2, √p Z/2}. Note, incidentally, that it is frequently convenient to parametrize the
depolarizing channel in different ways, such as
E(ρ) = (1 −p)ρ + p
3 (XρX + Y ρY + ZρZ) ,
(8.103)
which has the interpretation that the state ρ is left alone with probability 1 −p, and the
operators X, Y and Z applied each with probability p/3.
Exercise 8.17:
Verify (8.101) as follows. Deﬁne
E(A) ≡A + XAX + Y AY + ZAZ
4
,
(8.104)
and show that
E(I) = I;
E(X) = E(Y ) = E(Z) = 0.
(8.105)
Now use the Bloch sphere representation for single qubit density matrices to
verify (8.101).
The depolarizing channel can, of course, be generalized to quantum systems of di-
mension more than two. For a d-dimensional quantum system the depolarizing channel
again replaces the quantum system with the completely mixed state I/d with probability
p, and leaves the state untouched otherwise. The corresponding quantum operation is
E(ρ) = pI
d + (1 −p)ρ .
(8.106)
Exercise 8.18:
For k ≥1 show that tr(ρk) is never increased by the action of the
depolarizing channel.
Exercise 8.19:
Find an operator-sum representation for a generalized depolarizing
channel acting on a d-dimensional Hilbert space.
380
Quantum noise and quantum operations
8.3.5
Amplitude damping
An important application of quantum operations is the description of energy dissipation
– effects due to loss of energy from a quantum system. What are the dynamics of an
atom which is spontaneously emitting a photon? How does a spin system at high tem-
perature approach equilibrium with its environment? What is the state of a photon in an
interferometer or cavity when it is subject to scattering and attenuation?
Each of these processes has its own unique features, but the general behavior of all of
them is well characterized by a quantum operation known as amplitude damping, which
we can derive by considering the following scenario. Suppose we have a single optical
mode containing the quantum state a|0⟩+b|1⟩, a superposition of zero or one photons. The
scattering of a photon from this mode can be modeled by thinking of inserting a partially
silvered mirror, a beamsplitter, in the path of the photon. As we saw in Section 7.4.2, this
beamsplitter allows the photon to couple to another single optical mode (representing the
environment), according to the unitary transformation B = exp
&θ
a†b −ab†', where
a, a† and b, b† are annihilation and creation operators for photons in the two modes. The
output after the beamsplitter, assuming the environment starts out with no photons, is
simply B|0⟩(a|0⟩+b|1⟩) = a|00⟩+b(cos θ|01⟩+sin θ|10⟩), using Equation (7.34). Tracing
over the environment gives us the quantum operation
EAD(ρ) = E0ρE†
0 + E1ρE†
1 ,
(8.107)
where Ek = ⟨k|B|0⟩are
E0 =
 1
0
0
√1 −γ

E1 =
 0
√γ
0
0

,
(8.108)
the operation elements for amplitude damping. γ = sin2 θ can be thought of as the
probability of losing a photon.
Observe that no linear combination can be made of E0 and E1 to give an operation
element proportional to the identity (though compare with Exercise 8.23). The E1 oper-
ation changes a |1⟩state into a |0⟩state, corresponding to the physical process of losing a
quantum of energy to the environment. E0 leaves |0⟩unchanged, but reduces the ampli-
tude of a |1⟩state; physically, this happens because a quantum of energy was not lost to
the environment, and thus the environment now perceives it to be more likely that the
system is in the |0⟩state, rather than the |1⟩state.

________

_ _ _ _ _ _ _ _

Figure 8.13. Circuit model for amplitude damping
Exercise 8.20: (Circuit model for amplitude damping)
Show that the circuit in
Examples of quantum noise and quantum operations
381
Figure 8.13 models the amplitude damping quantum operation, with
sin2(θ/2) = γ.
Exercise 8.21: (Amplitude damping of a harmonic oscillator)
Suppose that our
principal system, a harmonic oscillator, interacts with an environment, modeled
as another harmonic oscillator, through the Hamiltonian
H = χ(a†b + b†a)
(8.109)
where a and b are the annihilation operators for the respective harmonic
oscillators, as deﬁned in Section 7.3.
(1) Using U = exp(−iHΔt), denoting the eigenstates of b†b as |kb⟩, and
selecting the vacuum state |0b⟩as the initial state of the environment, show
that the operation elements Ek = ⟨kb|U|0b⟩are found to be
Ek =

n
 n
k
 (
(1 −γ)n−kγk |n −k⟩⟨n| ,
(8.110)
where γ = 1 −cos2(χΔt) is the probability of loosing a single quantum of
energy, and states such as |n⟩are eigenstates of a†a.
(2) Show that the operation elements Ek deﬁne a trace-preserving quantum
operation.
Exercise 8.22: (Amplitude damping of single qubit density matrix)
For the
general single qubit state
ρ =
 a
b
b∗
c

(8.111)
show that amplitude damping leads to
EAD(ρ) =
 1 −(1 −γ)(1 −a)
b√1 −γ
b∗√1 −γ
c(1 −γ)

.
(8.112)
Exercise 8.23: (Amplitude damping of dual-rail qubits)
Suppose that a single
qubit state is represented by using two qubits, as
|ψ⟩= a |01⟩+ b |10⟩.
(8.113)
Show that EAD ⊗EAD applied to this state gives a process which can be described
by the operation elements
Edr
0 =
$
1 −γ I
(8.114)
Edr
1 = √γ

|00⟩⟨01| + |00⟩⟨10|

,
(8.115)
that is, either nothing (Edr
0 ) happens to the qubit, or the qubit is transformed
(Edr
1 ) into the state |00⟩, which is orthogonal to |ψ⟩. This is a simple
error-detection code, and is also the basis for the robustness of the ‘dual-rail’
qubit discussed in Section 7.4.
Exercise 8.24: (Spontaneous emission is amplitude damping)
A single atom
coupled to a single mode of electromagnetic radiation undergoes spontaneous
emission, as was described in Section 7.6.1. To see that this process is just
382
Quantum noise and quantum operations
amplitude damping, take the unitary operation resulting from the
Jaynes–Cummings interaction, Equation (7.77), with detuning δ = 0, and give
the quantum operation resulting from tracing over the ﬁeld.
A general characteristic of a quantum operation is the set of states that are left invariant
under the operation. For example, we have seen how the phase ﬂip channel leaves the ˆz
axis of the Bloch sphere unchanged; this corresponds to states of the form p|0⟩⟨0| + (1 −
p)|1⟩⟨1| for arbitrary probability p. In the case of amplitude damping, only the ground
state |0⟩is left invariant. That is a natural consequence of our modeling the environment
as starting in the |0⟩state, as if it were at zero temperature.
What quantum operation describes the effect of dissipation to an environment at ﬁnite
temperature? This process, EGAD, called generalized amplitude damping, is deﬁned for
single qubits by the operation elements
E0 = √p
 1
0
0
√1 −γ

(8.116)
E1 = √p
 0
√γ
0
0

(8.117)
E2 =
$
1 −p
 √1 −γ
0
0
1

(8.118)
E3 =
$
1 −p

0
0
√γ
0

,
(8.119)
where the stationary state
ρ∞=
 p
0
0
1 −p

,
(8.120)
satisﬁes EGAD(ρ∞) = ρ∞. Generalized amplitude damping describes the ‘T1’ relaxation
processes due to coupling of spins to their surrounding lattice, a large system which is
in thermal equilibrium at a temperature often much higher than the spin temperature.
This is the case relevant to NMR quantum computation, where some of the properties
of EGAD described in Box 8.3 become important.
Exercise 8.25:
If we deﬁne the temperature T of a qubit by assuming that in
equilibrium the probabilities of being in the |0⟩and |1⟩states satisfy a
Boltzmann distribution, that is p0 = e−E0/kBT /Z and p1 = e−E1/kBT/Z, where
E0 is the energy of the state |0⟩, E1 the energy of the state |1⟩, and
Z = e−E0/kBT + e−E1/kBT , what temperature describes the state ρ∞?
We can visualize the effect of amplitude damping in the Bloch representation as the
Bloch vector transformation
(rx, ry, rz) →

rx
$
1 −γ, ry
$
1 −γ, γ + rz(1 −γ)

.
(8.122)
When γ is replaced with a time-varying function like 1 −e−t/T1 (t is time, and T1
just some constant characterizing the speed of the process), as is often the case for real
physical processes, we can visualize the effects of amplitude damping as a ﬂow on the
Bloch sphere, which moves every point in the unit ball towards a ﬁxed point at the north
pole, where |0⟩is located. This is shown in Figure 8.14.
Examples of quantum noise and quantum operations
383
Box 8.3: Generalized amplitude damping and effective pure states
The notion of ‘effective pure states’ introduced in Section 7.7 was found to be useful
in NMR implementations of quantum computers. These states behave like pure
states under unitary evolution and measurement of traceless observables. How do
they behave under quantum operations? In general, non-unitary quantum operations
ruin the effectiveness of these states, but surprisingly, they can behave properly
under generalized amplitude damping.
Consider a single qubit effective pure state ρ = (1 −p)I + (2p −1)|0⟩⟨0|. Clearly,
traceless measurement observables acting on UρU † produce results which are pro-
portional to those on the pure state U|0⟩⟨0|U †. Suppose ρ is the stationary state of
EGAD. Interestingly, in this case,
EGAD(UρU †) = (1 −p)I + (2p −1)EAD(UρU †) .
(8.121)
That is, under generalized amplitude damping, an effective pure state can remain
such, and moreover, the ‘pure’ component of ρ behaves as if it were undergoing
amplitude damping to a reservoir at zero temperature!
Similarly, generalized amplitude damping performs the transformation
(rx, ry, rz) →

rx
$
1 −γ, ry
$
1 −γ, γ(2p −1) + rz(1 −γ)

.
(8.123)
Comparing (8.122) and (8.123), it is clear that amplitude damping and generalized am-
plitude damping differ only in the location of the ﬁxed point of the ﬂow; the ﬁnal state
is along the ˆz axis, at the point (2p −1), which is a mixed state.
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
=⇒
-1
-0.5
0
0.5
1
x
-1
-0.5
0
0.5
1
y
-1
-0.5
0
0.5
1
z
-1
-0.5
0
0.5
x
1
-0.5
0
0.5
y
Figure 8.14. The effect of the amplitude damping channel on the Bloch sphere, for p = 0.8. Note how the entire
sphere shrinks towards the north pole, the |0⟩state.
8.3.6
Phase damping
A noise process that is uniquely quantum mechanical, which describes the loss of quantum
information without loss of energy, is phase damping. Physically it describes, for example,
384
Quantum noise and quantum operations
what happens when a photon scatters randomly as it travels through a waveguide, or how
electronic states in an atom are perturbed upon interacting with distant electrical charges.
The energy eigenstates of a quantum system do not change as a function of time, but do
accumulate a phase which is proportional to the eigenvalue. When a system evolves for
an amount of time which is not precisely known, partial information about this quantum
phase – the relative phases between the energy eigenstates – is lost.
A very simple model for this kind of quantum noise is the following. Suppose that
we have a qubit |ψ⟩= a |0⟩+ b |1⟩upon which the rotation operation Rz(θ) is applied,
where the angle of rotation θ is random. The randomness could originate, for example,
from a deterministic interaction with an environment, which never again interacts with
the system and thus is implicitly measured (see Section 4.4). We shall call this random
Rz operation a phase kick. Let us assume that the phase kick angle θ is well represented
as a random variable which has a Gaussian distribution with mean 0 and variance 2λ.
The output state from this process is given by the density matrix obtained from
averaging over θ,
ρ =
1
√
4πλ
% ∞
−∞
Rz(θ)|ψ⟩⟨ψ|R†
z(θ)e−θ2/4λ dθ
(8.124)
=

|a|2
ab∗e−λ
a∗b e−λ
|b|2

.
(8.125)
The random phase kicking causes the expected value of the off-diagonal elements of the
density matrix to decay exponentially to zero with time. That is a characteristic result of
phase damping.
Another way to derive the phase damping quantum operation is to consider an inter-
action between two harmonic oscillators, in a manner similar to how amplitude damping
was derived in the last section, but this time with the interaction Hamiltonian
H = χ a†a(b + b†) ,
(8.126)
Letting U = exp(−iHΔt), considering only the |0⟩and |1⟩states of the a oscillator as
our system, and taking the environment oscillator to initially be |0⟩, we ﬁnd that tracing
over the environment gives the operation elements Ek = ⟨kb|U|0b⟩, which are
E0 =
 1
0
0
√
1 −λ

(8.127)
E1 =

0
0
0
√
λ

,
(8.128)
where λ = 1 −cos2(χΔt) can be interpreted as the probability that a photon from
the system has been scattered (without loss of energy). As was the case for amplitude
damping, E0 leaves |0⟩unchanged, but reduces the amplitude of a |1⟩state; unlike
amplitude damping, however, the E1 operation destroys |0⟩and reduces the amplitude
of the |1⟩state, and does not change it into a |0⟩.
By applying Theorem 8.2, the unitary freedom of quantum operations, we ﬁnd that
a unitary recombination of E0 and E1 gives a new set of operation elements for phase
damping,
˜E0 = √α
 1
0
0
1

(8.129)
Examples of quantum noise and quantum operations
385
˜E1 =
√
1 −α
 1
0
0
−1

,
(8.130)
where α = (1 +
√
1 −λ)/2. Thus the phase damping quantum operation is exactly the
same as the phase ﬂip channel which we encountered in Section 8.3.3!
Since phase damping is the same as the phase ﬂip channel, we have already seen how
it is visualized on the Bloch sphere, in Figure 8.9. This corresponds to the Bloch vector
transformation
(rx, ry, rz) →

rx
√
1 −λ, ry
√
1 −λ, rz

,
(8.131)
which has the effect of shrinking the sphere into ellipsoids. Phase damping is often
referred to as a ‘T2’ (or ‘spin-spin’) relaxation process, for historical reasons, where
e−t/2T2 =
√
1 −λ. As a function of time, the amount of damping increases, corre-
sponding to an inwards ﬂow of all points in the unit ball towards the ˆz axis. Note that
states along the ˆz axis remain invariant.
Historically, phase damping was a process that was almost always thought of, physi-
cally, as resulting from a random phase kick or scattering process. It was not until the
connection to the phase ﬂip channel was discovered that quantum error-correction was
developed, since it was thought that phase errors were continuous and couldn’t be de-
scribed as a discrete process! In fact, single qubit phase errors can always be thought of
as resulting from a process in which either nothing happens to a qubit, with probability
α, or with probability 1−α, the qubit is ﬂipped by the Z Pauli operation. Although this
might not be the actual microscopic physical process happening, from the standpoint of
the transformation occurring to a qubit over a discrete time interval large compared to
the underlying random process, there is no difference at all.
Phase damping is one of the most subtle and important processes in the study of
quantum computation and quantum information. It has been the subject of an immense
amount of study and speculation, particularly with regard to why the world around us
appears to be so classical, with superposition states not a part of our everyday experience!
Perhaps it is phase damping that is responsible for this absence of superposition states
from the everyday (Exercise 8.31)? The pioneering quantum physicist Schr¨odinger was
perhaps the ﬁrst to pose this problem, and he did this in a particularly stark form, as
discussed in Box 8.4.
•
ρout
Ry(θ)

________

_ _ _ _ _ _ _ _

ρin
|0⟩
Figure 8.15. Circuit model for phase damping. The upper wire carries the input qubit with an unknown state, and
the lower wire is an ancilla qubit used to model the environment.
Exercise 8.26: (Circuit model for phase damping)
Show that the circuit in
Figure 8.15 can be used to model the phase damping quantum operation,
provided θ is chosen appropriately.
Exercise 8.27: (Phase damping = phase ﬂip channel)
Give the unitary
386
Quantum noise and quantum operations
transformation which relates the operation elements of (8.127)–(8.128) to those
of (8.129)–(8.130); that is, ﬁnd u such that ˜Ek = 
j ukjEj.
Exercise 8.28: (One
phase damping model circuit)
Show that a single
controlled-
gate can be used as a model for phase damping, if we let the
initial state of the environment be a mixed state, where the amount of damping is
determined by the probability of the states in the mixture.
Exercise 8.29: (Unitality)
A quantum process E is unital if E(I) = I. Show that the
depolarizing and phase damping channels are unital, while amplitude damping is
not.
Exercise 8.30: (T2 ≤T1/2)
The T2 phase coherence relaxation rate is just the
exponential decay rate of the off-diagonal elements in the qubit density matrix,
while T1 is the decay rate of the diagonal elements (see Equation (7.144)).
Amplitude damping has both nonzero T1 and T2 rates; show that for amplitude
damping T2 = T1/2. Also show that if amplitude and phase damping are both
applied then T2 ≤T1/2.
Exercise 8.31: (Exponential sensitivity to phase damping)
Using (8.126), show
that the element ρnm = ⟨n|ρ|m⟩in the density matrix of a harmonic oscillator
decays exponentially as e−λ(n−m)2 under the effect of phase damping, for some
constant λ.
8.4
Applications of quantum operations
As beﬁts a powerful tool, the quantum operations formalism has numerous applications.
In this section we describe two of these applications. Section 8.4.1 describes the theory of
master equations, a picture of quantum noise complementary to the quantum operations
formalism. The master equation approach describes quantum noise in continuous time
using differential equations, and is the approach to quantum noise most often used by
physicists. In Section 8.4.2 we describe quantum process tomography, a procedure to
experimentally determine the dynamics of a quantum system.
8.4.1
Master equations
Open quantum systems occur in a wide range of disciplines, and many tools other than
quantum operations can be employed in their study. In this section, we brieﬂy describe
one such tool, the approach of master equations.
The dynamics of open quantum systems have been studied extensively in the ﬁeld of
quantum optics. The main objective in this context is to describe the time evolution of an
open system with a differential equation which properly describes non-unitary behavior.
This description is provided by the master equation, which can be written most generally
in the Lindblad form as
dρ
dt = −i
ℏ[H, ρ] +

j

2LjρL†
j −{L†
jLj, ρ}

,
(8.134)
where {x, y} = xy+yx denotes an anticommutator, H is the system Hamiltonian, a Her-
mitian operator representing the coherent part of the dynamics, and Lj are the Lindblad
Applications of quantum operations
387
Box 8.4: Schr¨odinger’s cat
When I hear about Schr¨odinger’s cat, I reach for my gun. – Stephen Hawking
Schr¨odinger’s infamous cat faces life or death contingent upon an automatic device
which breaks a vial of poison and kills the cat if an excited atomic state is observed
to decay, as illustrated here:



 


Schr¨odinger asked what happens when the atom is in a superposition state? Is the cat
alive or dead? Why do superposition states such as this apparently not occur in the
everyday world? This conundrum is resolved by realizing that it is very unlikely
to occur in real life, because of extreme sensitivity of macroscopic superposition
states to decoherence. Let the atom represent a single qubit. The joint system has
the initial state |alive⟩|1⟩. Suppose that after one half-life of the atom, the state
is the equal superposition |alive⟩(|0⟩+ |1⟩)/
√
2 (this represents a simpliﬁcation of
the actual physics, which are too involved to go into here). The apparatus kills
the cat if the atom is in the |0⟩state; otherwise, the cat lives. This gives the state
|ψ⟩=

|dead⟩|0⟩+ |alive⟩|1⟩

/
√
2, in which the cat’s state has become entangled
with that of the atom. This would seem to indicate the cat is simultaneously alive
and dead, but suppose we consider the density matrix of this state,
ρ = |ψ⟩⟨ψ|
(8.132)
= 1
2

|alive, 1⟩⟨alive, 1| + |dead, 0⟩⟨dead, 0|
+|alive, 1⟩⟨dead, 0| + |dead, 0⟩⟨alive, 1|

.
(8.133)
Now, in practice it is impossible to perfectly isolate the cat and the atom in their
box, and thus information about this superposition state will leak into the external
world. For example, heat from the cat’s body could permeate the wall and give some
indication of its state to the outside. Such effects may be modeled as phase damping,
which exponentially damps out the ﬁnal two (off-diagonal) terms in ρ. To a ﬁrst
approximation, we may model the cat–atom system as a simple harmonic oscillator.
An important result about the decoherence of such a system is that coherence
between states of high energy difference decays faster than between states with a
lower energy difference (Exercise 8.31). Thus ρ will quickly be transformed into
a nearly diagonal state, which represents an ensemble of cat–atom states which
correspond to either dead or alive, and are not in a superposition of the two states.
388
Quantum noise and quantum operations
operators, representing the coupling of the system to its environment. The differential
equation takes on the above form in order that the process be completely positive in a
sense similar to that described earlier for quantum operations. It is also generally as-
sumed that the system and environment begin in a product state. Furthermore, in order
to derive a master equation for a process, one usually begins with a system–environment
model Hamiltonian, and then makes the Born and Markov approximations in order to
determine Lj. Note that in the master equation approach, tr[ρ(t)] = 1 at all times.
As an example of a Lindblad equation, consider a two-level atom coupled to the
vacuum, undergoing spontaneous emission. The coherent part of the atom’s evolution is
described by the Hamiltonian H = −ℏωσz/2. ℏω is the energy difference of the atomic
levels. Spontaneous emission causes an atom in the excited (|1⟩) state to drop down into
the ground (|0⟩) state, emitting a photon in the process. This emission is described by
the Lindblad operator √γσ−, where σ−≡|0⟩⟨1| is the atomic lowering operator, and γ
is the rate of spontaneous emission. The master equation describing this process is
dρ
dt = −i
ℏ[H, ρ] + γ

2σ−ρσ+ −σ+σ−ρ −ρσ+σ−

,
(8.135)
where σ+ ≡σ†
−is the atomic raising operator.
To solve the equation it is helpful to move to the interaction picture, that is, make the
change of variables
˜ρ(t) ≡eiHtρ(t)e−iHt .
(8.136)
The equation of motion for ˜ρ is easily found to be
d ˜ρ
dt = γ

2˜σ−˜ρ˜σ+ −˜σ+ ˜σ−˜ρ −˜ρ˜σ+ ˜σ−

(8.137)
where
˜σ−≡eiHtσ−e−iHt = e−iωtσ−
(8.138)
˜σ+ ≡eiHtσ+e−iHt = eiωtσ+ .
(8.139)
Our ﬁnal equation of motion is thus
d ˜ρ
dt = γ

2σ−˜ρσ+ −σ+σ−˜ρ −˜ρσ+σ−

.
(8.140)
This equation of motion is easily solved using a Bloch vector representation for ˜ρ. The
solution is
λx = λx(0)e−γt
(8.141)
λy = λy(0)e−γt
(8.142)
λz = λz(0)e−2γt + 1 −e−2γt .
(8.143)
Deﬁning γ′ = 1 −exp(−2tγ) we can easily check that this evolution is equivalent to
˜ρ(t) = E( ˜ρ(0)) ≡E0 ˜ρ(0)E†
0 + E1 ˜ρ(0)E†
1 ,
(8.144)
where
E0 ≡
 1
0
0
√1 −γ′

(8.145)
E1 ≡
 0
√γ′
0
0

(8.146)
Applications of quantum operations
389
are the operation elements deﬁning the quantum operation E. Note that the effect of E
is amplitude damping; compare with Equation (8.108). The example we have considered
is an instance of the spin-boson model, in which a small, ﬁnite dimensional quantum
system interacts with a bath of simple harmonic oscillators. Physically, it is important in
describing the interaction of atoms with electromagnetic radiation, as in cavity QED, or
atom and ion traps.
The master equation approach is less general than the quantum operations formalism.
Solving a master equation allows one to determine the time dependence of a density
matrix. Knowing this, in turn, means that the result can be expressed as a quantum
operation in the operator-sum representation,
ρ(t) =

k
Ek(t)ρ(0)E†
k(t) ,
(8.147)
where Ek(t) are time dependent operation elements, determined from the solution to the
master equation. However, a quantum process described in terms of an operator-sum
representation cannot necessarily be written down as a master equation. For example,
quantum operations can describe non-Markovian dynamics, simply because they describe
only state changes, not continuous time evolution. Nevertheless, each approach has its
own place. In fact, even quantum operations do not provide the most general description;
we consider in Section 8.5 some processes which are not described by quantum operations.
8.4.2
Quantum process tomography
Quantum operations provide a wonderful mathematical model for open quantum sys-
tems, and are conveniently visualized (at least for qubits) – but how do they relate to
experimentally measurable quantities? What measurements should an experimentalist do
if they wish to characterize the dynamics of a quantum system? For classical systems,
this elementary task is known as system identiﬁcation. Here, we show how its analogue,
known as quantum process tomography, can be performed for ﬁnite dimensional quantum
systems.
To understand process tomography we ﬁrst need to understand another procedure
called quantum state tomography. State tomography is the procedure of experimentally
determining an unknown quantum state. Suppose we are given an unknown state, ρ, of
a single qubit. How can we experimentally determine what the state of ρ is?
If we are given just a single copy of ρ then it turns out to be impossible to characterize
ρ. The basic problem is that there is no quantum measurement which can distinguish
non-orthogonal quantum states like |0⟩and (|0⟩+ |1⟩)/
√
2 with certainty. However, it
is possible to estimate ρ if we have a large number of copies of ρ. For instance, if ρ is
the quantum state produced by some experiment, then we simply repeat the experiment
many times to produce many copies of the state ρ.
Suppose we have many copies of a single qubit density matrix, ρ. The set I/
√
2,
X/
√
2, Y/
√
2, Z/
√
2 forms an orthonormal set of matrices with respect to the Hilbert–
Schmidt inner product, so ρ may be expanded as
ρ = tr(ρ)I + tr(Xρ)X + tr(Y ρ)Y + tr(Zρ)Z
2
.
(8.148)
Recall, however, that expressions like tr(Aρ) have an interpretation as the average value of
observables. For example, to estimate tr(Zρ) we measure the observable Z a large number
of times, m, obtaining outcomes z1, z2, . . . , zm, all equal to +1 or −1. The empirical
390
Quantum noise and quantum operations
average of these quantities, 
i zi/m, is an estimate for the true value of tr(Zρ). We can
use the central limit theorem to determine how well this estimate behaves for large m,
where it becomes approximately Gaussian with mean equal to tr(Zρ) and with standard
deviation Δ(Z)/√m, where Δ(Z) is the standard deviation for a single measurement of
Z, which is upper bounded by 1, so the standard deviation in our estimate 
i zi/m is
at most 1/√m.
In a similar way we can estimate the quantities tr(Xρ) and tr(Y ρ) with a high degree
of conﬁdence in the limit of a large sample size, and thus obtain a good estimate for ρ.
Generalizing this procedure to the case of more than one qubit is not difﬁcult, at least in
principle! Similar to the single qubit case, an arbitrary density matrix on n qubits can be
expanded as
ρ =

⃗v
tr (σv1 ⊗σv2 ⊗· · · ⊗σvn ρ) σv1 ⊗σv2 ⊗· · · ⊗σvn
2n
,
(8.149)
where the sum is over vectors ⃗v = (v1, . . . , vn) with entries vi chosen from the set
0, 1, 2, 3. By performing measurements of observables which are products of Pauli ma-
trices we can estimate each term in this sum, and thus obtain an estimate for ρ.
We’ve described how to do state tomography for systems comprised of qubits. What
if non-qubit systems are involved? Not surprisingly, it is easy to generalize the above
prescription to such systems. We won’t explicitly do so here, but instead refer you to the
end of chapter ‘History and further reading’ for references.
Now that we know how to do quantum state tomography, how can we use it to do
quantum process tomography? The experimental procedure may be outlined as follows.
Suppose the state space of the system has d dimensions; for example, d = 2 for a single
qubit. We choose d2 pure quantum states |ψ1⟩, . . . , |ψd2⟩, chosen so that the correspond-
ing density matrices |ψ1⟩⟨ψ1|, . . . , |ψd2⟩⟨ψd2| form a basis set for the space of matrices.
We explain in more detail how to choose such a set below. For each state |ψj⟩we prepare
the quantum system in that state and then subject it to the process which we wish to
characterize. After the process has run to completion we use quantum state tomography
to determine the state E(|ψj⟩⟨ψj|) output from the process. From a purist’s point of view
we are now done, since in principle the quantum operation E is now determined by a
linear extension of E to all states.
In practice, of course, we would like to have a way of determining a useful represen-
tation of E from experimentally available data. We will explain a general procedure for
doing so, worked out explicitly for the case of a single qubit. Our goal is to determine a
set of operation elements {Ei} for E,
E(ρ) =

i
EiρE†
i .
(8.150)
However, experimental results involve numbers, not operators, which are a theoretical
concept. To determine the Ei from measurable parameters, it is convenient to consider
an equivalent description of E using a ﬁxed set of operators ˜Ei, which form a basis for
the set of operators on the state space, so that
Ei =

m
eim ˜Em
(8.151)
Applications of quantum operations
391
for some set of complex numbers eim. Equation (8.150) may thus be rewritten as
E(ρ) =

mn
˜Emρ ˜E†
nχmn,
(8.152)
where χmn ≡
i eime∗
in are the entries of a matrix which is positive Hermitian by
deﬁnition. This expression, known as the chi matrix representation, shows that E can
be completely described by a complex number matrix, χ, once the set of operators Ei
has been ﬁxed.
In general, χ will contain d4 −d2 independent real parameters, because a general
linear map of d by d complex matrices to d by d matrices is described by d4 independent
parameters, but there are d2 additional constraints due to the fact that ρ remains Hermitian
with trace one; that is, the completeness relation

i
E†
i Ei = I,
(8.153)
is satisﬁed, giving d2 real constraints. We will show how to determine χ experimentally,
and then show how an operator-sum representation of the form (8.150) can be recovered
once the χ matrix is known.
Let ρj, 1 ≤j ≤d2 be a ﬁxed, linearly independent basis for the space of d × d
matrices; that is, any d × d matrix can be written as a unique linear combination of the
ρj. A convenient choice is the set of operators |n⟩⟨m|. Experimentally, the output state
E(|n⟩⟨m|) may be obtained by preparing the input states |n⟩, |m⟩, |+⟩= (|n⟩+|m⟩)/
√
2,
and |−⟩= (|n⟩+ i|m⟩)/
√
2 and forming linear combinations of E(|n⟩⟨n|), E(|m⟩⟨m|),
E(|+⟩⟨+|), and E(|−⟩⟨−|), as follows:
E(|n⟩⟨m|) = E(|+⟩⟨+|) + iE(|−⟩⟨−|) −1 + i
2
E(|n⟩⟨n|) −1 + i
2
E(|m⟩⟨m|). (8.154)
Thus, it is possible to determine E(ρj) by state tomography, for each ρj.
Furthermore, each E(ρj) may be expressed as a linear combination of the basis states,
E(ρj) =

k
λjkρk,
(8.155)
and since E(ρj) is known from the state tomography, λjk can be determined by standard
linear algebraic algorithms. To proceed, we may write
˜Emρj ˜E†
n =

k
βmn
jk ρk,
(8.156)
where βmn
jk are complex numbers which can be determined by standard algorithms from
linear algebra given the ˜Em operators and the ρj operators. Combining the last two
expressions and (8.152) we have

k

mn
χmnβmn
jk ρk =

k
λjkρk.
(8.157)
From the linear independence of the ρk it follows that for each k,

mn
βmn
jk χmn = λjk.
(8.158)
This relation is a necessary and sufﬁcient condition for the matrix χ to give the correct
quantum operation E. One may think of χ and λ as vectors, and β as a d4 × d4 matrix
392
Quantum noise and quantum operations
with columns indexed by mn, and rows by jk. To show how χ may be obtained, let κ
be the generalized inverse for the matrix β, satisfying the relation
βmn
jk =

st,xy
βst
jkκxy
st βmn
xy .
(8.159)
Most computer packages for matrix manipulation are capable of ﬁnding such generalized
inverses. We now prove that χ deﬁned by
χmn ≡

jk
κmn
jk λjk
(8.160)
satisﬁes the relation (8.158).
The difﬁculty in verifying that χ deﬁned by (8.160) satisﬁes (8.158) is that, in general,
χ is not uniquely determined by Equation (8.158). For convenience we rewrite these
equations in matrix form as
β⃗χ = ⃗λ
(8.161)
⃗χ ≡κ⃗λ .
(8.162)
From the construction that led to Equation (8.152) we know there exists at least one
solution to Equation (8.161), which we shall call ⃗χ′. Thus ⃗λ = β⃗χ′. The generalized
inverse satisﬁes βκβ = β. Premultiplying the deﬁnition of ⃗χ by β gives
β⃗χ = βκ⃗λ
(8.163)
= βκβ⃗χ′
(8.164)
= β⃗χ′
(8.165)
= ⃗λ .
(8.166)
Thus χ deﬁned by (8.162) satisﬁes the Equation (8.161), as we wanted to show.
Having determined χ one immediately obtains the operator-sum representation for E
in the following manner. Let the unitary matrix U † diagonalize χ,
χmn =

xy
UmxdxδxyU ∗
ny .
(8.167)
From this it can easily be veriﬁed that
Ei =
$
di

j
UjiEj
(8.168)
are operation elements for E. Our algorithm may thus be summarized as follows: λ is
experimentally determined using state tomography, which in turn determines χ via the
equation ⃗χ = κλ, which gives us a complete description of E, including a set of operation
elements Ei.
In the case of a single qubit quantum process, only 12 parameters must be determined
(Box 8.5). The dynamics of a two qubit quantum black box E2 pose an even greater
challenge for our understanding. In this case there are 240 parameters which need to be
determined in order to completely specify the quantum operation acting on the quantum
system! Determining these would obviously be quite a considerable undertaking. How-
ever, as for the single qubit case, it is relatively straightforward to implement a numerical
routine which will automate the calculation, provided experimental state tomography and
state preparation procedures are available in the laboratory.
Applications of quantum operations
393
Box 8.5: Process tomography for a single qubit
The general method of process tomography can be simpliﬁed in the case of a one
qubit operation to provide explicit formulas which may be useful in experimen-
tal contexts. This simpliﬁcation is made possible by choosing the ﬁxed operators
˜Ei to have commutation properties which conveniently allow the χ matrix to be
determined by straightforward matrix multiplication. In the one qubit case, we use:
E0 = I
(8.169)
˜E1 = X
(8.170)
˜E2 = −iY
(8.171)
˜E3 = Z.
(8.172)
There are 12 parameters, speciﬁed by χ, which determine an arbitrary single qubit
quantum operation E.
These parameters may be measured using four sets of experiments. As a speciﬁc
example, suppose the input states |0⟩, |1⟩, |+⟩= (|0⟩+ |1⟩)/
√
2 and |−⟩= (|0⟩+
i |1⟩)/
√
2 are prepared, and the four matrices
ρ′
1 = E(|0⟩⟨0|)
(8.173)
ρ′
4 = E(|1⟩⟨1|)
(8.174)
ρ′
2 = E(|+⟩⟨+|) −iE(|−⟩⟨−|) −(1 −i)(ρ′
1 + ρ′
4)/2
(8.175)
ρ′
3 = E(|+⟩⟨+|) + iE(|−⟩⟨−|) −(1 + i)(ρ′
1 + ρ′
4)/2
(8.176)
are determined using state tomography. These correspond to ρ′
j = E(ρj), where
ρ1 =
 1
0
0
0

,
(8.177)
ρ2 = ρ1X, ρ3 = Xρ1, and ρ4 = Xρ1X. From (8.156) and Equations (8.169)–(8.172)
we may determine β, and similarly ρ′
j determines λ. However, due to the particular
choice of basis, and the Pauli matrix representation of ˜Ei, we may express the β
matrix as the Kronecker product β = Λ ⊗Λ, where
Λ = 1
2
 I
X
X
−I

,
(8.178)
so that χ may be expressed conveniently as
χ = Λ
 ρ′
1
ρ′
2
ρ′
3
ρ′
4

Λ ,
(8.179)
in terms of block matrices.
We have shown how a useful representation for the dynamics of a quantum system may
be experimentally determined using a systematic procedure. This procedure of quantum
process tomography is analogous to the system identiﬁcation step performed in classical
control theory, and plays a similar role in understanding and controlling noisy quantum
systems.
394
Quantum noise and quantum operations
Exercise 8.32:
Explain how to extend quantum process tomography to the case of
non-trace-preserving quantum operations, such as arise in the study of
measurement.
Exercise 8.33: (Specifying a quantum process)
Suppose that one wished to
completely specify an arbitrary single qubit operation E by describing how a set
of points on the Bloch sphere {⃗rk} transform under E. Prove that the set must
contain at least four points.
Exercise 8.34: (Process tomography for two qubits)
Show that the χ2 describing
the black box operations on two qubits can be expressed as
χ2 = Λ2ρ′Λ2 ,
(8.180)
where Λ2 = Λ ⊗Λ, Λ is as deﬁned in Box 8.5, and ρ′ is a block matrix of 16
measured density matrices,
ρ′ = P T
⎡
⎢⎢⎣
ρ′
11
ρ′
12
ρ′
13
ρ′
14
ρ′
21
ρ′
22
ρ′
23
ρ′
24
ρ′
31
ρ′
32
ρ′
33
ρ′
34
ρ′
41
ρ′
42
ρ′
43
ρ′
44
⎤
⎥⎥⎦P ,
(8.181)
where ρ′
nm = E(ρnm), ρnm = Tn|00⟩⟨00|Tm, T1 = I ⊗I, T2 = I ⊗X,
T3 = X ⊗I, T4 = X ⊗X, and P = I ⊗[(ρ00 + ρ12 + ρ21 + ρ33) ⊗I] is a
permutation matrix.
Exercise 8.35: (Process tomography example)
Consider a one qubit black box of
unknown dynamics E1. Suppose that the following four density matrices are
obtained from experimental measurements, performed according to
Equations (8.173)–(8.176):
ρ′
1 =
 1
0
0
0

(8.182)
ρ′
2 =
 0
√1 −γ
0
0

(8.183)
ρ′
3 =

0
0
√1 −γ
0

(8.184)
ρ′
4 =
 γ
0
0
1 −γ

,
(8.185)
where γ is a numerical parameter. From an independent study of each of these
input–output relations, one could make several important observations: the
ground state |0⟩is left invariant by E1, the excited state |1⟩partially decays to the
ground state, and superposition states are damped. Determine the χ matrix for
this process.
8.5
Limitations of the quantum operations formalism
Are there interesting quantum systems whose dynamics are not described by quantum
operations? In this section we will construct an artiﬁcial example of a system whose evo-
Chapter problems
395
lution is not described by a quantum operation, and try to understand the circumstances
under which this is likely to occur.
Suppose a single qubit is prepared in some unknown quantum state, which we denote
ρ. The preparation of this qubit involves certain procedures to be carried out in the
laboratory in which the qubit is prepared. Suppose that among the laboratory degrees of
freedom is a single qubit which, as a side effect of the state preparation procedure, is left
in the state |0⟩if ρ is a state on the bottom half of the Bloch sphere, and is left in the
state |1⟩if ρ is a state on the top half of the Bloch sphere. That is, the state of the system
after preparation is
ρ ⊗|0⟩⟨0| ⊗other degrees of freedom
(8.186)
if ρ is a state on the bottom half of the Bloch sphere, and
ρ ⊗|1⟩⟨1| ⊗other degrees of freedom
(8.187)
if ρ is a state on the top half of the Bloch sphere.
Once the state preparation is done, the system begins to interact with the environment,
in this case all the laboratory degrees of freedom. Suppose the interaction is such that
a controlled-
is performed between the principal system and the extra qubit in the
laboratory system. Thus, if the system’s Bloch vector was initially in the bottom half of
the Bloch sphere it is left invariant by the process, while if it was initially in the top half
of the Bloch sphere it is rotated into the bottom half of the Bloch sphere.
Obviously, this process is not an afﬁne map acting on the Bloch sphere, and therefore,
by the results of Section 8.3.2, it cannot be a quantum operation. The lesson to be
learned from this discussion is that a quantum system which interacts with the degrees
of freedom used to prepare that system after the preparation is complete will in general
suffer a dynamics which is not adequately described within the quantum operations
formalism. This is an important conclusion to have reached, as it indicates that there are
physically reasonable circumstances under which the quantum operations formalism may
not adequately describe the processes taking place in a quantum system. This should be
kept in mind, for example, in applications of the quantum process tomography procedure
discussed in the previous section.
For the remainder of this book we will, however, work within the quantum operations
formalism. It provides a powerful, and reasonably general tool for describing the dynamics
experienced by quantum systems. Most of all, it provides a means by which concrete
progress can be made on problems related to quantum information processing. It is an
interesting problem for further research to study quantum information processing beyond
the quantum operations formalism.
Problem 8.1: (Lindblad form to quantum operation)
In the notation of
Section 8.4.1, explicitly work through the steps to solve the differential equation
˙ρ = −λ
2 (σ+σ−ρ + ρσ+σ−−2σ−ρσ+)
(8.188)
for ρ(t). Express the map ρ(0) →ρ(t) as ρ(t) = 
k Ek(t)ρ(0)E†
k(t).
Problem 8.2: (Teleportation as a quantum operation)
Suppose Alice is in
possession of a single qubit, denoted as system 1, which she wishes to teleport to
396
Quantum noise and quantum operations
Bob. Unfortunately, she and Bob only share an imperfectly entangled pair of
qubits. Alice’s half of this pair is denoted system 2, and Bob’s half is denoted
system 3. Suppose Alice performs a measurement described by a set of quantum
operations Em with result m on systems 1 and 2. Show that this induces an
operation ˜Em relating the initial state of system 1 to the ﬁnal state of system 3,
and that teleportation is accomplished if Bob can reverse this operation using a
trace-preserving quantum operation Rm, to obtain
Rm

˜Em(ρ)
tr[ ˜Em(ρ)]

= ρ ,
(8.189)
where ρ is the initial state of system 1.
Problem 8.3: (Random unitary channels)
It is tempting to believe that all unital
channels, that is, those for which E(I) = I, result from averaging over random
unitary operations, that is, E(ρ) = 
k pkUkρU †
k, where Uk are unitary operators
and the pk form a probability distribution. Show that while this is true for single
qubits, it is untrue for larger systems.
History and further reading
397
Summary of Chapter 8: Quantum noise and quantum operations
• The operator-sum representation: The behavior of an open quantum system
can be modeled as
E(ρ) =

k
EkρE†
k ,
(8.190)
where Ek are operation elements, satisfying 
k E†
kEk = I if the quantum oper-
ation is trace-preserving.
• Environmental models for quantum operations: A trace-preserving quan-
tum operation can always be regarded as arising from the unitary interaction of
a system with an initially uncorrelated environment, and vice versa. Non-trace-
preserving quantum operations may be treated similarly, except an additional pro-
jective measurement is performed on the composite of system and environment,
with the different outcomes corresponding to different non-trace-preserving quan-
tum operations.
• Quantum process tomography: A quantum operation on a d-dimensional
quantum system can be completely determined by experimentally measuring the
output density matrices produced from d2 pure state inputs.
• Operation elements for important single qubit quantum operations:
depolarizing channel

1 −3p
4
 1
0
0
1

,
p
4
 0
−i
i
0

,
p
4
 0
1
1
0

,
p
4
 1
0
0
−1

amplitude damping
 1
0
0
√1 −γ

,
 0
√γ
0
0

phase damping
 1
0
0
√1 −γ

,
 0
0
0
√γ

phase ﬂip
√p
 1
0
0
1

,
√1 −p
 1
0
0
−1

bit ﬂip
√p
 1
0
0
1

,
√1 −p
 0
1
1
0

bit–phase ﬂip
√p
 1
0
0
1

,
√1 −p
 0
−i
i
0

History and further reading
Quantum noise is an important topic in several ﬁelds, and there is an enormous liter-
ature on the subject. We will necessarily be restricted to citing only a small sample of
398
Quantum noise and quantum operations
the resources available on the topic. An early treatise on quantum noise from a rather
mathematical perspective is due to Davies[Dav76]. Caldeira and Leggett[CL83] did some
of the ﬁrst and most complete studies of an important model known as the spin-boson
model, using an approach based upon the Feynman path integral. Gardiner[Gar91] stud-
ied quantum noise from the perspective of quantum optics. More recently, the quantum
optics community has developed what is known as the quantum trajectories approach
to quantum noise. Reviews of this subject may be found in the articles by Zoller and
Gardiner[ZG97], and Plenio and Knight[PK98].
A large literature exists on the subject of quantum operations. We mention just a few
key references, primarily the book by Kraus[Kra83], which contains references to much
earlier work on the subject. Inﬂuential early papers on the subject include those by Hell-
wig and Kraus[HK69, HK70], and by Choi[Cho75]. Lindblad[Lin76] connected the quantum
operations formalism to the theory of continuous time quantum evolution, introducing
what is now known as the Lindblad form. Schumacher[Sch96b] and Caves[Cav99] have writ-
ten excellent summaries of the quantum operations formalism from the point of view of
quantum error-correction.
Quantum state tomography was suggested by Vogel and Risken[VR89]. Leonhardt[Leo97]
has written a recent review containing references to other work. The need for quantum
process tomography was pointed out in a paper by Turchette, Hood, Lange, Mabuchi, and
Kimble[THL+95]. The theory was developed independently by Chuang and Nielsen[CN97],
and by Poyatos, Cirac and Zoller[PCZ97]. Jones[Jon94] had earlier sketched out the main
ideas of quantum process tomography.
An unfortunate confusion of terms has arisen with the word ‘decoherence’. Historically,
it has been used to refer just to a phase damping process, particularly by Zurek[Zur91].
Zurek and other researchers recognized that phase damping has a unique role in the
transition from quantum to classical physics; for certain environmental couplings, it oc-
curs on a time scale which is much faster than any amplitude damping process, and can
therefore be much more important in determining the loss of quantum coherence. The
major point of these studies has been this emergence of classicality due to environmental
interactions. However, by and large, the usage of decoherence in quantum computation
and quantum information is to refer to any noise process in quantum processing. In
this book, we prefer the more generic term ‘quantum noise’ and tend towards its usage,
although occasionally decoherence ﬁnds a proper place in the context.
A more detailed discussion of some of the limitations of the quantum operations
formalism (and in particular, the assumption of a system and environment initially in a
product state) is provided by Royer[Roy96].
Problem 8.2 is due to Nielsen and Caves[NC97]. Problem 8.3 is due to Landau and
Streater[LS93] as part of an in-depth study of the extremal points of the convex set of
doubly stochastic quantum operations.
9 Distance measures for quantum information
What does it mean to say that two items of information are similar? What does
it mean to say that information is preserved by some process? These questions are
central to a theory of quantum information processing, and the purpose of this chapter
is the development of distance measures giving quantitative answers to these questions.
Motivated by our two questions we will be concerned with two broad classes of distance
measures, static measures and dynamic measures. Static measures quantify how close
two quantum states are, while dynamic measures quantify how well information has been
preserved during a dynamic process. The strategy we take is to begin by developing good
static measures of distance, and then to use those static measures as the basis for the
development of dynamic measures of distance.
There is a certain arbitrariness in the way distance measures are deﬁned, both classically
and quantum mechanically, and the community of people studying quantum computation
and quantum information has found it convenient to use a variety of distance measures
over the years. Two of those measures, the trace distance and the ﬁdelity, have particu-
larly wide currency today, and we discuss both these measures in detail in this chapter.
For the most part the properties of both are quite similar, however for certain applications
one may be easier to deal with than the other. It is for this reason and because both are
widely used within the quantum computation and quantum information community that
we discuss both measures.
9.1
Distance measures for classical information
The idea of distinguishing probability distributions is slippery business.
– Christopher Fuchs
Let’s start out in an arena where we can easily apply our intuition – distance measures
for classical information. What are the objects to be compared in classical information
theory? We might consider comparing strings of bits like 00010 and 10011. One way
of quantifying the distance between these is the Hamming distance, deﬁned to be the
number of places at which two bit strings are not equal. For example, the bit strings 00010
and 10011 differ in the ﬁrst and last place, so the Hamming distance between them is two.
Unfortunately, the Hamming distance between two objects is simply a matter of labeling,
and a priori there aren’t any labels in the Hilbert space arena of quantum mechanics!
A much better place to launch the study of distance measures for quantum information
is with the comparison of classical probability distributions. In fact, in classical informa-
tion theory an information source is usually modeled as a random variable, that is, as a
probability distribution over some source alphabet. For example, an unknown source of
English text may be modeled as a sequence of random variables over the Roman alphabet.
400
Distance measures for quantum information
Before the text is read we can make a fair guess at the relative frequency of the letters that
appear in the text, and certain correlations among them, such as the fact that occurrences
of the pair of letters ‘th’ are much more common than the pair ‘zx’ in English text. This
characterization of information sources as probability distributions over some alphabet
encourages us to concentrate on the comparison of probability distributions in our search
for measures of distance.
What does it mean to say that two probability distributions {px} and {qx} over the
same index set, x, are similar to one another? It is difﬁcult to give an answer to this
question which is obviously the unique ‘correct’ answer, so instead we propose two
different answers, each of which is widely used by the quantum computation and quantum
information community. The ﬁrst measure is the trace distance, deﬁned by the equation:
D(px, qx) ≡1
2

x
|px −qx| .
(9.1)
This quantity is sometimes known as the L1 distance or Kolmogorov distance. We prefer
the term trace distance because it anticipates the later quantum mechanical analogue
of this quantity, which is deﬁned using the trace function. The trace distance turns
out to be a metric on probability distributions, (a metric D(x, y) must be symmetric,
D(x, y) = D(y, x), and satisfy the triangle inequality, D(x, z) ≤D(x, y) + D(y, z)) so
the use of the term ‘distance’ is justiﬁed.
Exercise 9.1:
What is the trace distance between the probability distribution (1, 0)
and the probability distribution (1/2, 1/2)? Between (1/2, 1/3, 1/6) and
(3/4, 1/8, 1/8)?
Exercise 9.2:
Show that the trace distance between probability distributions (p, 1 −p)
and (q, 1 −q) is |p −q|.
A second measure of distance between probability distributions, the ﬁdelity of the
probability distributions {px} and {qx}, is deﬁned by
F(px, qx) ≡

x
√pxqx.
(9.2)
The ﬁdelity is a very different way of measuring distance between probability distributions
than is the trace distance. To begin with, it is not a metric, although later we discuss a
metric derived from the ﬁdelity. To see that the ﬁdelity is not a metric note that when the
distributions {px} and {qx} are identical, F(px, qx) = 
x px = 1. A better geometric
understanding of the ﬁdelity is illustrated in Figure 9.1; the ﬁdelity is just the inner
product between vectors with components √px and √qx, which lie on a unit sphere.
Exercise 9.3:
What is the ﬁdelity of the probability distributions (1, 0) and (1/2, 1/2)?
Of (1/2, 1/3, 1/6) and (3/4, 1/8, 1/8)?
The trace distance and ﬁdelity are mathematically useful means of deﬁning the notion
of a distance between two probability distributions. Do these measures have physically
motivated operational meanings? In the case of the trace distance, the answer to this
Distance measures for classical information
401













Figure 9.1. Geometric interpretation of the ﬁdelity as the inner product between vectors √px and √qx lying on a
unit sphere. (Since 1 = 
x(√px)2 = 
x(√qx)2.)
question is yes. In particular, it is simple to prove that
D(px, qx) = max
S |p(S) −q(S)| = max
S
)))))

x∈S
px −

x∈S
qx
))))) ,
(9.3)
where the maximization is over all subsets S of the index set {x}. The quantity being
maximized is the difference between the probability that the event S occurs, according
to the distribution {px}, and the probability that the event S occurs, according to the
distribution {qx}. The event S is thus in some sense the optimal event to examine when
trying to distinguish the distributions {px} and {qx}, with the trace distance governing
how well it is possible to make this distinction.
Unfortunately, a similarly clear interpretation for the ﬁdelity is not known. However, in
the next section we show that the ﬁdelity is a sufﬁciently useful quantity for mathematical
purposes to justify its study, even without a clear physical interpretation. Moreover, we
cannot rule out the possibility that a clear interpretation of the ﬁdelity will be discovered
in the future. Finally, it turns out that there are close connections between the ﬁdelity and
the trace distance, so properties of one quantity can often be used to deduce properties
of the other, a fact which is useful surprisingly often.
Exercise 9.4:
Prove (9.3).
Exercise 9.5:
Show that the absolute value signs may be removed from
Equation (9.3), that is,
D(px, qx) = max
S (p(S) −q(S)) = max
S

x∈S
px −

x∈S
qx

.
(9.4)
The trace distance and ﬁdelity are static measures of distance for comparing two
ﬁxed probability distributions. There is a third notion of distance which is a dynamic
measure of distance in the sense that it measures how well information is preserved by
402
Distance measures for quantum information
some physical process. Suppose a random variable X is sent through a noisy channel,
giving as output another random variable Y , to form a Markov process X →Y . For
convenience we assume both X and Y have the same range of values, denoted by x.
Then the probability that Y is not equal to X, p(X ̸= Y ), is an obvious and important
measure of the degree to which information has been preserved by the process.
Surprisingly, this dynamic measure of distance can be understood as a special case
of the static trace distance! Imagine that the random variable X is given to you, and
you make a copy of X, creating a new random variable ˜X = X. The random variable
X now passes through the noisy channel, leaving as output the random variable Y , as
illustrated in Figure 9.2. How close is the initial perfectly correlated pair, ( ˜X, X), to the
ﬁnal pair, ( ˜X, Y )? Using the trace distance as our measure of ‘closeness’, we obtain with
some simple algebra,
D(( ˜X, X), (X, Y )) = 1
2

xx′
|δxx′p(X =x) −p( ˜X =x, Y =x′)|
(9.5)
= 1
2

x̸=x′
p( ˜X =x, Y =x′) + 1
2

x
))p(X =x) −p( ˜X =x, Y =x)
))
(9.6)
= 1
2

x̸=x′
p( ˜X =x, Y =x′) + 1
2

x
p(X =x) −p( ˜X =x, Y =x)

(9.7)
= p( ˜X̸=Y ) + 1 −p( ˜X =Y )
2
(9.8)
= p(X̸=Y ) + p( ˜X̸=Y )
2
(9.9)
= p(X̸=Y ).
(9.10)
Thus, as illustrated in Figure 9.3, the probability of an error in the channel is equal to the
trace distance between the probability distribution for ( ˜X, X) and ( ˜X, Y ). This is an im-
portant construction, since it will be the basis for analogous quantum constructions. This
is necessary because there is no direct quantum analogue of the probability p(X ̸= Y ),
since there is no notion in quantum mechanics analogous to the joint probability distri-
bution for variables X and Y that exist at different times. Instead, to deﬁne dynamic
measures of quantum distance we use an approach similar to the construction just given,
based on the idea that it is quantum entanglement, rather than classical correlation, which
is the important thing to preserve during a quantum channel’s dynamics.
Figure 9.2. Given a Markov process X →Y we may ﬁrst make a copy of X, ˜X, before subjecting X to the noise
which turns it into Y .
How close are two quantum states?
403
HHHHHHHHHHHHH
Figure 9.3. The probability of an error in the channel is equal to the trace distance between the probability
distributions for ( ˜X, X) and ( ˜X, Y ).
9.2
How close are two quantum states?
How close are two quantum states? Over the next few sections we describe quantum
generalizations of the classical notions of trace distance and ﬁdelity, and discuss in detail
the properties of these quantities.
9.2.1
Trace distance
We begin by deﬁning the trace distance between quantum states ρ and σ,
D(ρ, σ) ≡1
2tr|ρ −σ|.
(9.11)
where as per usual we deﬁne |A| ≡
√
A†A to be the positive square root of A†A. Notice
that the quantum trace distance generalizes the classical trace distance in the sense that
if ρ and σ commute then the (quantum) trace distance between ρ and σ is equal to the
classical trace distance between the eigenvalues of ρ and σ. More explicitly, if ρ and σ
commute they are diagonal in the same basis,
ρ =

i
ri|i⟩⟨i|;
σ =

i
si|i⟩⟨i|,
(9.12)
for some orthonormal basis |i⟩. Thus
D(ρ, σ) = 1
2tr
)))))

i
(ri −si)|i⟩⟨i|
)))))
(9.13)
= D(ri, si).
(9.14)
Exercise 9.6:
What is the trace distance between the density operators
3
4|0⟩⟨0| + 1
4|1⟩⟨1|;
2
3|0⟩⟨0| + 1
3|1⟩⟨1|?
(9.15)
Between:
3
4|0⟩⟨0| + 1
4|1⟩⟨1|;
2
3|+⟩⟨+| + 1
3|−⟩⟨−|?
(9.16)
(Recall that |±⟩≡(|0⟩± |1⟩)/
√
2.)
404
Distance measures for quantum information
A good way of getting a feel for the trace distance is to understand it for the special
case of a qubit, in the Bloch sphere representation. Suppose ρ and σ have respective
Bloch vectors ⃗r and ⃗s,
ρ = I + ⃗r · ⃗σ
2
;
σ = I + ⃗s · ⃗σ
2
.
(9.17)
(Recall that ⃗σ denotes a vector of Pauli matrices; it should not be confused with the state
σ.) The trace distance between ρ and σ is easily calculated:
D(ρ, σ) = 1
2tr|ρ −σ|
(9.18)
= 1
4tr |(⃗r −⃗s) · ⃗σ| .
(9.19)
(⃗r −⃗s) · ⃗σ has eigenvalues ±|⃗r −⃗s|, so the trace of |(⃗r −⃗s) · σ| is 2|⃗r −⃗s|, and we see
that
D(ρ, σ) = |⃗r −⃗s|
2
.
(9.20)
That is, the distance between two single qubit states is equal to one half the ordinary
Euclidean distance between them on the Bloch sphere!
This intuitive geometric picture of the trace distance for qubits is often useful when
trying to understand general properties of the trace distance. Conjectured properties can
be suggested, refuted, or gain plausibility by looking at simple examples on the Bloch
sphere. For example, rotations of the Bloch sphere leave the Euclidean distance invariant.
This suggests that the trace distance might be preserved under unitary transformations
in general,
D(UρU †, UσU †) = D(ρ, σ),
(9.21)
a conjecture which you can easily verify with a moment’s thought. We will come back to
the Bloch sphere picture often in our investigation of distance measures.
To understand the properties of the trace distance a good starting point is to prove a
formula for the trace distance generalizing Equation (9.3) for the classical trace distance:
D(ρ, σ) = max
P
tr(P(ρ −σ)) ,
(9.22)
where the maximization may be taken alternately over all projectors, P, or over all positive
operators P ≤I; the formula is valid in either case. This formula gives rise to an appealing
interpretation of the trace distance. Recalling that POVM elements are positive operators
P ≤I, the trace distance is therefore equal to the difference in probabilities that a
measurement outcome with POVM element P may occur, depending on whether the
state is ρ or σ, maximized over all possible POVM elements P.
We prove Equation (9.22) for the case where the maximization is over projectors; the
case of positive operators P ≤I follows the same reasoning. The proof is based on
the fact that ρ −σ can be expressed as ρ −σ = Q −S, where Q and S are positive
operators with orthogonal support (see Exercise 9.7). It implies that |ρ −σ| = Q + S,
so D(ρ, σ) = (tr(Q) + tr(S))/2. But tr(Q −S) = tr(ρ −σ) = 0, so tr(Q) = tr(S),
and therefore D(ρ, σ) = tr(Q). Let P be the projector onto the support of Q. Then
tr(P(ρ −σ)) = tr(P(Q −S)) = tr(Q) = D(ρ, σ). Conversely, let P be any projector.
How close are two quantum states?
405
Then tr(P(ρ −σ)) = tr(P(Q −S)) ≤tr(PQ) ≤tr(Q) = D(ρ, σ). This completes the
proof.
Exercise 9.7:
Show that for any states ρ and σ, one may write ρ −σ = Q −S, where
Q and S are positive operators with support on orthogonal vector spaces. (Hint:
use the spectral decomposition ρ −σ = UDU †, and split the diagonal matrix D
into positive and negative parts. This fact will continue to be useful later.)
There is a closely related way of viewing the quantum trace distance which relates it
more closely to the classical trace distance:
Theorem 9.1: Let {Em} be a POVM, with pm ≡tr(ρEm) and qm ≡tr(σEm) as the
probabilities of obtaining a measurement outcome labeled by m. Then
D(ρ, σ) = max
{Em} D(pm, qm) ,
(9.23)
where the maximization is over all POVMs {Em}.
Proof
Note that
D(pm, qm) = 1
2

m
|tr(Em(ρ −σ))| .
(9.24)
Using the spectral decomposition we may write ρ −σ = Q −S, where Q and S are
positive operators with orthogonal support. Thus |ρ −σ| = Q + S, and
|tr(Em(ρ −σ))| = |tr(Em(Q −S))|
(9.25)
≤tr(Em(Q + S))
(9.26)
≤tr(Em|ρ −σ|) .
(9.27)
Thus
D(pm, qm) ≤1
2

m
tr(Em|ρ −σ|)
(9.28)
= 1
2tr(|ρ −σ|)
(9.29)
= D(ρ, σ),
(9.30)
where we have applied the completeness relation for POVM elements, 
m Em = I.
Conversely, by choosing a measurement whose POVM elements include projectors
onto the support of Q and S, we see that there exist measurements which give rise to
probability distributions such that D(pm, qm) = D(ρ, σ).
Thus, if two density operators are close in trace distance, then any measurement
performed on those quantum states will give rise to probability distributions which are
close together in the classical sense of trace distance, giving a second interpretation of
the trace distance between two quantum states as an achievable upper bound on the
trace distance between probability distributions arising from measurements performed
on those quantum states.
406
Distance measures for quantum information
We call the trace distance a ‘distance’, so we should check to see whether it has the
property of being a metric on the space of density operators. From our geometric picture
for a single qubit this is obviously true for a single qubit; is it true more generally? It is
clear that D(ρ, σ) = 0 if and only if ρ = σ, and that D(·, ·) is a symmetric function of its
inputs. All that remains to check is that the triangle inequality holds,
D(ρ, τ) ≤D(ρ, σ) + D(σ, τ).
(9.31)
To see this, note from Equation (9.22) that there exists a projector P such that
D(ρ, τ) = tr(P(ρ −τ))
(9.32)
= tr(P(ρ −σ)) + tr(P(σ −τ))
(9.33)
≤D(ρ, σ) + D(σ, τ),
(9.34)
establishing that the trace distance is a metric.
At this stage, we don’t know a whole lot about the trace distance. However, we’re
in a good position to prove some genuinely spectacular results, useful in a wide variety
of contexts. The most interesting result is that no physical process ever increases the
distance between two quantum states, a result illustrated in Figure 9.4. We state this
more formally as a theorem:
Theorem 9.2: (Trace-preserving quantum operations are contractive) Suppose E
is a trace-preserving quantum operation. Let ρ and σ be density operators. Then
D(E(ρ), E(σ)) ≤D(ρ, σ).
(9.35)
















Figure 9.4. Trace-preserving quantum operations cause a contraction on the space of density operators.
Proof
Use the spectral decomposition to write ρ −σ = Q −S, where Q and S are positive
matrices with orthogonal support, and let P be a projector such that D(E(ρ), E(σ)) =
tr[P(E(ρ) −E(σ))]. Observe that tr(Q) −tr(S) = tr(ρ) −tr(σ) = 0, so tr(Q) = tr(S) and
How close are two quantum states?
407
thus tr(E(Q)) = tr(E(S)). Using this observation we see that
D(ρ, σ) = 1
2tr|ρ −σ|
(9.36)
= 1
2tr|Q −S|
(9.37)
= 1
2tr(Q) + 1
2tr(S)
(9.38)
= 1
2tr(E(Q)) + 1
2tr(E(S))
(9.39)
= tr(E(Q))
(9.40)
≥tr(PE(Q))
(9.41)
≥tr(P(E(Q) −E(S)))
(9.42)
= tr(P(E(ρ) −E(σ)))
(9.43)
= D(E(ρ), E(σ)),
(9.44)
which completes the proof.
There is an important special case of this result which can be understood by the
following analogy. Imagine somebody shows you two different paintings in a gallery.
Provided you have reasonably good vision, you shouldn’t have any difﬁculty telling them
apart. On the other hand, if somebody covers up most of the two paintings then you might
have more difﬁculty telling the two apart, as illustrated in Figure 9.5. Similarly, if we
‘cover up’ parts of two quantum states then we can show that the distance between those
two states is never increased. To prove this, recall from page 374 that the partial trace is
a trace-preserving quantum operation. By Theorem 9.2, if we take quantum states ρAB
and σAB of a composite quantum system AB then the distance between ρA = trB(ρAB)
and σA = trB(σAB) is never more than the distance between ρAB and σAB,
D(ρA, σA) ≤D(ρAB, σAB) .
(9.45)
Figure 9.5. Objects become less distinguishable when only partial information is available.
In many applications we want to estimate the trace distance for a mixture of inputs.
Such estimates are greatly aided by the following theorem:
Theorem 9.3: (Strong convexity of the trace distance) Let {pi} and {qi} be
probability distributions over the same index set, and ρi and σi be density
operators, also with indices from the same index set. Then
D

i
piρi,

i
qiσi

≤D(pi, qi) +

i
piD(ρi, σi),
(9.46)
408
Distance measures for quantum information
where D(pi, qi) is the classical trace distance between the probability
distributions {pi} and {qi}.
This result can be used to prove convexity results for the trace distance so we refer to
this property as the strong convexity property for trace distance.
Proof
By Equation (9.22) there exists a projector P such that
D

i
piρi,

i
qiσi

=

i
pitr(Pρi) −

i
qitr(Pσi)
(9.47)
=

i
pitr(P(ρi −σi)) +

i
(pi −qi)tr(Pσi)
(9.48)
≤

i
piD(ρi, σi) + D(pi, qi) ,
(9.49)
where D(pi, qi) is the trace distance between the probability distributions {pi} and {qi},
and we used Equation (9.22) in the last line.
As a special case of this result, we see that the trace distance is jointly convex in its
inputs,
D

i
piρi,

i
piσi

≤

i
piD(ρi, σi).
(9.50)
Exercise 9.8: (Convexity of the trace distance)
Show that the trace distance is
convex in its ﬁrst input,
D

i
piρi, σ

≤

i
piD(ρi, σ).
(9.51)
By symmetry convexity in the second entry follows from convexity in the ﬁrst.
Exercise 9.9: (Existence of ﬁxed points)
Schauder’s ﬁxed point theorem is a
classic result from mathematics that implies that any continuous map on a
convex, compact subset of a Hilbert space has a ﬁxed point. Use Schauder’s
ﬁxed point theorem to prove that any trace-preserving quantum operation E has
a ﬁxed point, that is, ρ such that E(ρ) = ρ.
Exercise 9.10:
Suppose E is a strictly contractive trace-preserving quantum
operation, that is, for any ρ and σ, D(E(ρ), E(σ)) < D(ρ, σ). Show that E has a
unique ﬁxed point.
Exercise 9.11:
Suppose E is a trace-preserving quantum operation for which there
exists a density operator ρ0 and a trace-preserving quantum operation E′ such
that
E(ρ) = pρ0 + (1 −p)E′(ρ),
(9.52)
for some p, 0 < p ≤1. Physically, this means that with probability p the input
state is thrown out and replaced with the ﬁxed state ρ0, while with probability
How close are two quantum states?
409
1 −p the operation E′ occurs. Use joint convexity to show that E is a strictly
contractive quantum operation, and thus has a unique ﬁxed point.
Exercise 9.12:
Consider the depolarizing channel introduced in Section 8.3.4 on
page 378, E(ρ) = pI/2 + (1 −p)ρ. For arbitrary ρ and σ ﬁnd D(E(ρ), E(σ))
using the Bloch representation, and prove explicitly that the map E is strictly
contractive, that is, D(E(ρ), E(σ)) < D(ρ, σ).
Exercise 9.13:
Show that the bit ﬂip channel (Section 8.3.3) is contractive but not
strictly contractive. Find the set of ﬁxed points for the bit ﬂip channel.
9.2.2
Fidelity
A second measure of distance between quantum states is the ﬁdelity. The ﬁdelity is not a
metric on density operators, but we will see that it does give rise to a useful metric. This
section reviews the deﬁnition and basic properties of the ﬁdelity. The ﬁdelity of states ρ
and σ is deﬁned to be
F(ρ, σ) ≡tr
(
ρ1/2σρ1/2.
(9.53)
It is certainly not immediately obvious that this is a useful measure of distance between ρ
and σ. It doesn’t even look symmetric! Yet we will see that the ﬁdelity is symmetric in
its inputs, and has many of the other properties we expect of a good distance measure.
There are two important special cases where it is possible to give more explicit formulae
for the ﬁdelity. The ﬁrst is when ρ and σ commute, that is, are diagonal in the same
basis,
ρ =

i
ri|i⟩⟨i|;
σ =

i
si|i⟩⟨i|,
(9.54)
for some orthonormal basis |i⟩. In this case we see that
F(ρ, σ) = tr

i
risi|i⟩⟨i|
(9.55)
= tr

i
√risi|i⟩⟨i|

(9.56)
=

i
√risi
(9.57)
= F(ri, si).
(9.58)
That is, when ρ and σ commute the quantum ﬁdelity F(ρ, σ) reduces to the classical
ﬁdelity F(ri, si) between the eigenvalue distributions ri and si of ρ and σ.
Our second example is to calculate the ﬁdelity between a pure state |ψ⟩and an arbitrary
state, ρ. From Equation (9.53) we see that
F(|ψ⟩, ρ) = tr
(
⟨ψ|ρ|ψ⟩|ψ⟩⟨ψ|
(9.59)
=
(
⟨ψ|ρ|ψ⟩.
(9.60)
That is, the ﬁdelity is equal to the square root of the overlap between |ψ⟩and ρ. This is
an important result which we will make use of often.
For the case of a qubit we were able to explicitly evaluate the trace distance between two
410
Distance measures for quantum information
states, and give it a simple geometric interpretation as half the Euclidean distance between
points on the Bloch sphere. Unfortunately, no similarly clear geometric interpretation is
known for the ﬁdelity between two states of a qubit.
However, the ﬁdelity does satisfy many of the same properties as the trace distance.
For example, it is invariant under unitary transformations:
F(UρU †, UσU †) = F(ρ, σ) .
(9.61)
Exercise 9.14: (Invariance of ﬁdelity under unitary transforms)
Prove (9.61) by
using the fact that for any positive operator A,
√
UAU † = U
√
AU †.
There is also a useful characterization of the ﬁdelity analogous to the characteriza-
tion (9.22) for the trace distance.
Theorem 9.4: (Uhlmann’s theorem) Suppose ρ and σ are states of a quantum
system Q. Introduce a second quantum system R which is a copy of Q. Then
F(ρ, σ) = max
|ψ⟩,|ϕ⟩|⟨ψ|ϕ⟩|,
(9.62)
where the maximization is over all puriﬁcations |ψ⟩of ρ and |ϕ⟩of σ into RQ.
Before giving the proof of Uhlmann’s theorem we need an easily proved lemma.
Lemma 9.5: Let A be any operator, and U unitary. Then
|tr(AU)| ≤tr|A| ,
(9.63)
with equality being attained by choosing U = V †, where A = |A|V is the polar
decomposition of A.
Proof
Equality is clearly attained under the conditions stated. Observe that
|tr(AU)| = |tr(|A|V U)| =
)))tr(|A|1/2|A|1/2V U)
))) .
(9.64)
The Cauchy–Schwarz inequality for the Hilbert–Schmidt inner product gives:
|tr(AU)| ≤
(
tr|A| tr
U †V †|A|V U
 = tr|A| ,
(9.65)
which completes the proof.
Proof
(Uhlmann’s theorem)
Fix orthonormal bases |iR⟩and |iQ⟩in systems R and Q. Because R and Q are of the
same dimension, the index i may be assumed to run over the same set of values. Deﬁne
|m⟩≡
i |iR⟩|iQ⟩. Let |ψ⟩be any puriﬁcation of ρ. Then the Schmidt decomposition
and a moment’s thought should convince you that
|ψ⟩=
UR ⊗√ρUQ
 |m⟩,
(9.66)
How close are two quantum states?
411
for some unitary operators UR and UQ on systems R and Q. Similarly, if |ϕ⟩is any
puriﬁcation of σ then there exist unitary operators VR and VQ such that
|ϕ⟩=
VR ⊗√σVQ
 |m⟩.
(9.67)
Taking the inner product gives
|⟨ψ|ϕ⟩| =
)))⟨m|

U †
RVR ⊗U †
Q
√ρ√σVQ

|m⟩
))) .
(9.68)
Using Exercise 9.16 on this page we see that
|⟨ψ|ϕ⟩| =
)))tr

V †
RURU †
Q
√ρ√σVQ
))) .
(9.69)
Setting U ≡VQV †
RURU †
Q we see that
|⟨ψ|ϕ⟩| =
))tr
√ρ√σU
)) .
(9.70)
By Lemma 9.5,
|⟨ψ|ϕ⟩| ≤tr
))√ρ√σ
)) = tr
(
ρ1/2σρ1/2.
(9.71)
To see that equality may be attained, suppose √ρ√σ = |√ρ√σ|V is the polar decom-
position of √ρ√σ. Choosing UQ = UR = VR = I and VQ = V † we see that equality is
attained.
Exercise 9.15:
Show that
F(ρ, σ) = max
|ϕ⟩|⟨ψ|ϕ⟩|,
(9.72)
where |ψ⟩is any ﬁxed puriﬁcation of ρ, and the maximization is over all
puriﬁcations of σ.
Exercise 9.16: (The Hilbert–Schmidt inner product and entanglement)
Suppose R and Q are two quantum systems with the same Hilbert space. Let
|iR⟩and |iQ⟩be orthonormal basis sets for R and Q. Let A be an operator on R
and B an operator on Q. Deﬁne |m⟩≡
i |iR⟩|iQ⟩. Show that
tr(A†B) = ⟨m|(A ⊗B)|m⟩,
(9.73)
where the multiplication on the left hand side is of matrices, and it is understood
that the matrix elements of A are taken with respect to the basis |iR⟩and those
for B with respect to the basis |iQ⟩.
Uhlmann’s formula (9.62) does not provide a calculational tool for evaluating the
ﬁdelity, as does Equation (9.53). However, in many instances, properties of the ﬁdelity
are more easily proved using Uhlmann’s formula than Equation (9.53). For example,
Uhlmann’s formula makes it clear that the ﬁdelity is symmetric in its inputs, F(ρ, σ) =
F(σ, ρ), and that the ﬁdelity is bounded between 0 and 1, 0 ≤F(ρ, σ) ≤1. If ρ = σ
then it is clear that F(ρ, σ) = 1, from Uhlmann’s formula. If ρ ̸= σ then |ψ⟩̸= |ϕ⟩for
any puriﬁcations |ψ⟩and |ϕ⟩of ρ and σ, respectively, so F(ρ, σ) < 1. On the other
hand, Equation (9.53) is sometimes useful as a means for understanding properties of the
ﬁdelity. For instance, we see that F(ρ, σ) = 0 if and only if ρ and σ have support on
orthogonal subspaces. Intuitively, when ρ and σ are supported on orthogonal subspaces
412
Distance measures for quantum information
they are perfectly distinguishable, so we should expect the ﬁdelity to be minimized at
this point. Summarizing, the ﬁdelity is symmetric in its inputs, 0 ≤F(ρ, σ) ≤1, with
equality in the ﬁrst inequality if and only if ρ and σ have orthogonal support, and equality
in the second inequality if and only if ρ = σ.
We saw that the quantum trace distance could be related to the classical trace distance
by considering the probability distributions induced by a measurement. In a similar way,
we can show that
F(ρ, σ) = min
{Em} F(pm, qm),
(9.74)
where the minimum is over all POVMs {Em}, and pm ≡tr(ρEm), qm ≡tr(σEm) are
the probability distributions for ρ and σ corresponding to the POVM {Em}. To see that
this is true, apply the polar decomposition
$
ρ1/2σρ1/2 = √ρ√σU, and note that
F(ρ, σ) = tr(√ρ√σU)
(9.75)
=

m
tr(√ρ
$
Em
$
Em
√σU).
(9.76)
The Cauchy–Schwarz inequality and some simple algebra gives
F(ρ, σ) ≤

m
$
tr(ρEm)tr(σEm)
(9.77)
= F(pm, qm),
(9.78)
which establishes that
F(ρ, σ) ≤min
{Em} F(pm, qm).
(9.79)
To see that equality can be attained in this inequality, we need to ﬁnd a POVM {Em}
such that the Cauchy–Schwarz inequality is satisﬁed with equality for each term in the
sum, that is, √Em√ρ = αm
√Em
√σU for some set of complex numbers αm. But
√ρ√σU =
$
ρ1/2σρ1/2, so for invertible ρ,
√σU = ρ−1/2
(
ρ1/2σρ1/2.
(9.80)
Substituting, we ﬁnd that the equality conditions are that
$
Em (I −αmM) = 0,
(9.81)
where M ≡ρ−1/2$
ρ1/2σρ1/2ρ−1/2. If M = 
m βm|m⟩⟨m| is a spectral decomposition
for M then we choose Em = |m⟩⟨m| and αm = 1/βm. The case of non-invertible ρ
follows from continuity.
We proved three important properties of the trace distance – the metric property,
contractivity, and strong convexity. Rather remarkably, analogous properties all hold for
the ﬁdelity. What is more, the proof techniques used for ﬁdelity differ considerably from
those used for the trace distance. For that reason it’s worth looking at these results in
some detail.
The ﬁdelity is not a metric; however, there is a simple way of turning the ﬁdelity into a
metric. The basic idea can be gleaned from Figure 9.6, by noticing that the angle between
two points on the sphere is a metric. For the quantum case, Uhlmann’s theorem tells
us that the ﬁdelity between two states is equal to the maximum inner product between
How close are two quantum states?
413
puriﬁcations of those states. This suggests that we deﬁne the angle between states ρ and
σ by
A(ρ, σ) ≡arccos F(ρ, σ).
(9.82)
Obviously the angle is non-negative, symmetric in its inputs, and is equal to zero if and
only if ρ = σ. If we can show that the angle obeys the triangle inequality then we will
have established that the angle is a metric.
We prove the triangle inequality using Uhlmann’s theorem and some obvious facts
about vectors in three dimensions. Let |ϕ⟩be a puriﬁcation of σ, and choose puriﬁcations
|ψ⟩of ρ and |γ⟩of τ such that
F(ρ, σ) = ⟨ψ|ϕ⟩
(9.83)
F(σ, τ) = ⟨ϕ|γ⟩,
(9.84)
and ⟨ψ|γ⟩is real and positive. (Note that this can always be done, by multiplying |ψ⟩, |ϕ⟩
and |γ⟩by appropriate phase factors, if necessary.) It is clear from Figure 9.6 that
arccos(⟨ψ|γ⟩) ≤A(ρ, σ) + A(σ, τ).
(9.85)
But by Uhlmann’s theorem, F(ρ, τ) ≥⟨ψ|γ⟩, so A(ρ, τ) ≤arccos(⟨ψ|γ⟩). Combining
with the previous inequality gives the triangle inequality,
A(ρ, τ) ≤A(ρ, σ) + A(σ, τ).
(9.86)
Figure 9.6. The angle between points on the surface of the unit sphere is a metric.
Exercise 9.17:
Show that 0 ≤A(ρ, σ) ≤π/2, with equality in the ﬁrst inequality if
and only if ρ = σ.
Qualitatively, the ﬁdelity behaves like an ‘upside-down’ version of the trace distance,
decreasing as two states become more distinguishable, and increasing as they become less
distinguishable. Therefore, we should not expect the contractivity or non-increasing
property of the trace distance to hold for ﬁdelity. Instead, the analogous property of
being non-decreasing does hold for ﬁdelity. We will refer to this as the monotonicity of
the ﬁdelity under quantum operations.
414
Distance measures for quantum information
Theorem 9.6: (Monotonicity of the ﬁdelity) Suppose E is a trace-preserving
quantum operation. Let ρ and σ be density operators. Show that
F(E(ρ), E(σ)) ≥F(ρ, σ).
(9.87)
Proof
Let |ψ⟩and |ϕ⟩be puriﬁcations of ρ and σ into a joint system RQ such that F(ρ, σ) =
|⟨ψ|ϕ⟩|. Introduce a model environment E for the quantum operation, E, which starts
in a pure state |0⟩, and interacts with the quantum system Q via a unitary interaction
U. Note that U|ψ⟩|0⟩is a puriﬁcation of E(ρ), and U|ϕ⟩|0⟩is a puriﬁcation of E(σ). By
Uhlmann’s theorem it follows that
F(E(ρ), E(σ)) ≥|⟨ψ|⟨0|U †U|ϕ⟩|0⟩|
(9.88)
= |⟨ψ|ϕ⟩|
(9.89)
= F(ρ, σ),
(9.90)
establishing the property we set out to prove.
Exercise 9.18: (Contractivity of the angle)
Let E be a trace-preserving quantum
operation. Show that
A(E(ρ), E(σ)) ≤A(ρ, σ).
(9.91)
We ﬁnish off our study of the elementary properties of ﬁdelity by proving a result
for the ﬁdelity analogous to the strong convexity of the trace distance, using Uhlmann’s
theorem.
Theorem 9.7: (Strong concavity of the ﬁdelity) Let pi and qi be probability
distributions over the same index set, and ρi and σi density operators also
indexed by the same index set. Then
F

i
piρi,

i
qiσi

≥

i
√piqiF(ρi, σi).
(9.92)
Not surprisingly, this result may be used to prove concavity results for the ﬁdelity, for
which reason we dub it the strong concavity property for the ﬁdelity. This property is
not strictly analogous to the strong convexity of the trace distance; however, the similarity
in spirit leads us to use similar nomenclature.
Proof
Let |ψi⟩and |ϕi⟩be puriﬁcations of ρi and σi chosen such that F(ρi, σi) = ⟨ψi|ϕi⟩.
Introduce an ancillary system which has orthonormal basis states |i⟩corresponding to
the index set i for the probability distributions. Deﬁne
|ψ⟩≡

i
√pi|ψi⟩|i⟩;
|ϕ⟩≡

i
√qi|ϕi⟩|i⟩.
(9.93)
Note that |ψ⟩is a puriﬁcation of 
i piρi and |ϕ⟩is a puriﬁcation of 
i qiσi, so by
Uhlmann’s formula,
F

i
piρi,

i
qiσi

≥|⟨ψ|ϕ⟩| =

i
√piqi⟨ψi|ϕi⟩=

i
√piqiF(ρi, σi), (9.94)
How close are two quantum states?
415
which establishes the result we set out to prove.
Exercise 9.19: (Joint concavity of ﬁdelity)
Prove that the ﬁdelity is jointly
concave,
F

i
piρi,

i
piσi

≥

i
piF(ρi, σi).
(9.95)
Exercise 9.20: (Concavity of ﬁdelity)
Prove that the ﬁdelity is concave in the ﬁrst
entry,
F

i
piρi, σ

≥

i
piF(ρi, σ).
(9.96)
By symmetry the ﬁdelity is also concave in the second entry.
9.2.3
Relationships between distance measures
The trace distance and the ﬁdelity are closely related, despite their very different forms.
Qualitatively, they may be considered to be equivalent measures of distance for many
applications. In this section we quantify more precisely the relationship between trace
distance and ﬁdelity.
In the case of pure states, the trace distance and the ﬁdelity are completely equivalent
to one another. To see this, consider the trace distance between two pure states, |a⟩and
|b⟩. Using the Gram–Schmidt procedure we may ﬁnd orthonormal states |0⟩and |1⟩such
that |a⟩= |0⟩and |b⟩= cos θ|0⟩+ sin θ|1⟩. Note that F(|a⟩, |b⟩) = | cos θ|. Furthermore,
D(|a⟩, |b⟩) = 1
2tr
))))

1 −cos2 θ
−cos θ sin θ
−cos θ sin θ
−sin2 θ
))))
(9.97)
= |sin θ|
(9.98)
=
(
1 −F(|a⟩, |b⟩)2 .
(9.99)
Thus the trace distance between two pure states is a function of the ﬁdelity of those
states, and vice versa. This relationship at the level of pure states can be used to deduce a
relationship at the level of mixed states. Let ρ and σ be any two quantum states, and let
|ψ⟩and |ϕ⟩be puriﬁcations chosen such that F(ρ, σ) = |⟨ψ|ϕ⟩| = F(|ψ⟩, |ϕ⟩). Recalling
that trace distance is non-increasing under the partial trace, we see that
D(ρ, σ) ≤D(|ψ⟩, |ϕ⟩)
(9.100)
=
$
1 −F(ρ, σ)2.
(9.101)
Thus, if the ﬁdelity between two states is close to one, it follows that the states are also
close in trace distance. The converse is also true. To see this, let {Em} be a POVM such
that
F(ρ, σ) =

m
√pmqm,
(9.102)
where pm ≡tr(ρEm), qm ≡tr(σEm) are the probabilities for obtaining outcome m for
the states ρ and σ, respectively. Observe ﬁrst that

m
(√pm −√qm)2 =

m
pm +

m
qm −2F(ρ, σ)
(9.103)
416
Distance measures for quantum information
= 2(1 −F(ρ, σ)).
(9.104)
However, it is also true that
))√pm −√qm
)) ≤
))√pm + √qm
)), so

m
(√pm −√qm)2 ≤

m
|√pm −√qm| |√pm + √qm|
(9.105)
=

m
|pm −qm|
(9.106)
= 2D(pm, qm)
(9.107)
≤2D(ρ, σ) .
(9.108)
Comparing (9.104) and (9.108) we see that
1 −F(ρ, σ) ≤D(ρ, σ) .
(9.109)
Summarizing, we have
1 −F(ρ, σ) ≤D(ρ, σ) ≤
$
1 −F(ρ, σ)2 .
(9.110)
The implication is that the trace distance and the ﬁdelity are qualitatively equivalent
measures of closeness for quantum states. Indeed, for many purposes it does not matter
whether the trace distance or the ﬁdelity is used to quantify distance, since results about
one may be used to deduce equivalent results about the other.
Exercise 9.21:
When comparing pure states and mixed states it is possible to make a
stronger statement than (9.110) about the relationship between trace distance and
ﬁdelity. Prove that
1 −F(|ψ⟩, σ)2 ≤D(|ψ⟩, σ).
(9.111)
9.3
How well does a quantum channel preserve information?
Friends come and go, but enemies accumulate
– Jones’ Law, attributed to Thomas Jones
How well does a quantum channel preserve information? More precisely, suppose a
quantum system is in the state |ψ⟩and some physical process occurs, changing the
quantum system to the state E(|ψ⟩⟨ψ|). How well has the channel E preserved the state
|ψ⟩of the quantum system? The static measures of distance discussed in previous sections
will be used in this section to develop measures of how well a quantum channel preserves
information.
This type of scenario occurs often in quantum computation and quantum information.
For example, in the memory of a quantum computer, |ψ⟩is the initial state of the
memory, and E represents the dynamics that the memory undergoes, including noise
processes arising from interaction with the environment. A second example is provided
by a quantum communication channel for transmitting the state |ψ⟩from one location
to another. No channel is ever perfect, so the action of the channel is described by a
quantum operation E.
An obvious way of quantifying how well the state |ψ⟩is preserved by the channel
is to make use of the static distance measures introduced in the previous section. For
How well does a quantum channel preserve information?
417
example, we can compute the ﬁdelity between the starting state |ψ⟩and the ending state
E(|ψ⟩⟨ψ|). For the case of the depolarizing channel, we obtain
F(|ψ⟩, E(|ψ⟩⟨ψ|)) =

⟨ψ|

pI
2 + (1 −p)|ψ⟩⟨ψ|

|ψ⟩
(9.112)
=

1 −p
2.
(9.113)
This result agrees well with our intuition – the higher the probability p of depolarizing,
the lower the ﬁdelity of the ﬁnal state with the initial state. Provided p is very small the
ﬁdelity is close to one, and the state E(ρ) is practically indistinguishable from the initial
state |ψ⟩.
There is nothing special about the use of the ﬁdelity in the above expression. We could
equally well have used the trace distance. However, for the remainder of this chapter we
are going to restrict ourselves to measures of distance based upon the ﬁdelity and derived
quantities. Using the properties of the trace distance established in the last section it
is not difﬁcult, for the most part, to give a parallel development based upon the trace
distance. However, it turns out that the ﬁdelity is an easier tool to calculate with, and for
that reason we restrict ourselves to considerations based upon the ﬁdelity.
Our prototype measure for information preservation, the ﬁdelity F(|ψ⟩, E(|ψ⟩⟨ψ|)),
has some drawbacks which need to be remedied. In a real quantum memory or quantum
communications channel, we don’t know in advance what the initial state |ψ⟩of the system
will be. However, we can quantify the worst-case behavior of the system by minimizing
over all possible initial states,
Fmin(E) ≡min
|ψ⟩F(|ψ⟩, E(|ψ⟩⟨ψ|)).
(9.114)
For example, for the p-depolarizing channel Fmin =
$
1 −p/2, as the ﬁdelity of the
channel is the same for all input states |ψ⟩. A more interesting example is the phase
damping channel,
E(ρ) = pρ + (1 −p)ZρZ.
(9.115)
For the phase damping channel the ﬁdelity is given by
F(|ψ⟩, E(|ψ⟩⟨ψ|)) =
(
⟨ψ|
p|ψ⟩⟨ψ| + (1 −p)Z|ψ⟩⟨ψ|Z
 |ψ⟩
(9.116)
=
(
p + (1 −p)⟨ψ|Z|ψ⟩2.
(9.117)
The second term under the square root sign is non-negative, and equal to zero when
|ψ⟩= (|0⟩+ |1⟩)/
√
2. Thus for the phase damping channel the minimum ﬁdelity is
Fmin(E) = √p.
(9.118)
You might wonder why we minimized over pure states in the deﬁnition of Fmin. After
all, might not the quantum system of interest start in a mixed state ρ? For example,
a quantum memory might be entangled with the rest of the quantum computer, and
therefore would start out in a mixed state. Fortunately, the joint concavity of the ﬁdelity
can be used to show that allowing mixed states does not change Fmin. To see this, suppose
418
Distance measures for quantum information
that ρ = 
i λi|i⟩⟨i| is the initial state of the quantum system. Then we have
F(ρ, E(ρ)) = F

i
λi|i⟩⟨i|,

i
λiE(|i⟩⟨i|)

(9.119)
≥

i
λiF(|i⟩, E(|i⟩⟨i|)).
(9.120)
It follows that
F(ρ, E(ρ)) ≥F(|i⟩, E(|i⟩⟨i|))
(9.121)
for at least one of the states |i⟩, and thus F(ρ, E(ρ)) ≥Fmin.
Of course, we are interested not only in protecting quantum states as they are transmit-
ted through a quantum communications channel, but also as they dynamically undergo
computation. Suppose, for example, that as part of a quantum computation we attempt
to implement a quantum gate described by the unitary operator U. As described in the
last chapter, any such attempt will inevitably encounter some (hopefully not too severe)
noise, so the correct description of the gate is using a trace-preserving quantum operation
E. A natural measure of how successful our gate has been is the gate ﬁdelity,
F(U, E) ≡min
|ψ⟩F(U|ψ⟩, E(|ψ⟩⟨ψ|)).
(9.122)
Suppose, for example, that we try to implement a
gate on a single qubit, but instead
implement the noisy operation E(ρ) = (1 −p)XρX + pZρZ, for some small noise
parameter p. Then the gate ﬁdelity for this operation is given by
F(X, E) = min
|ψ⟩
(
⟨ψ|X
(1 −p)X|ψ⟩⟨ψ|X + pZ|ψ⟩⟨ψ|Z
 X|ψ⟩
(9.123)
= min
|ψ⟩
(
(1 −p) + p⟨ψ|Y |ψ⟩2
(9.124)
=
$
(1 −p).
(9.125)
In Exercise 9.22 you will show that performing a sequence of gates each with high ﬁdelity
is sufﬁcient to ensure that the total operation has high ﬁdelity, and thus for the purposes
of quantum computation it is sufﬁcient to perform each gate in the computation with
high ﬁdelity. (Compare also the similar but less general arguments of Chapter 4 on
approximating quantum circuits.)
Exercise 9.22: (Chaining property for ﬁdelity measures)
Suppose U and V are
unitary operators, and E and F are trace-preserving quantum operations meant
to approximate U and V . Letting d(·, ·) be any metric on the space of density
matrices satisfying d(UρU †, UσU †) = d(ρ, σ) for all density matrices ρ and σ
and unitary U (such as the angle arccos(F(ρ, σ))), deﬁne the corresponding error
E(U, E) by
E(U, E) ≡max
ρ d(UρU †, E(ρ)),
(9.126)
and show that E(V U, F ◦E) ≤E(U, E) + E(V, F). Thus, to perform a quantum
computation with high ﬁdelity it sufﬁces to complete each step of the
computation with high ﬁdelity.
How well does a quantum channel preserve information?
419
Quantum sources of information and the entanglement ﬁdelity
We’ve been talking about dynamic measures of information preservation, without deﬁn-
ing exactly what we mean by a quantum source of information. We’ll now explain two
possible deﬁnitions for this notion, and use these deﬁnitions to motivate the introduc-
tion of some dynamic measures of information preservation. A priori, it is not entirely
clear how best to go about deﬁning the notion of a quantum source of information.
Classically, the best solution to this deﬁnitional problem is not at all obvious, and it is
possible to come up with inequivalent deﬁnitions, each yielding a rich and useful theory
of information. Since quantum information contains classical information as a subﬁeld,
it should not be surprising if there are even more ways of deﬁning the notion of an
information source quantum mechanically! To conclude this chapter we introduce two
possible quantum deﬁnitions for the notion of an information source, explain how they
motivate corresponding distance measures for the preservation of information, and prove
some elementary properties of these distance measures. Further discussion of quantum
sources of information is deferred until Chapter 12.
One attractive deﬁnition for quantum sources is to imagine a stream of identical quan-
tum systems (say, qubits) being produced by some physical process, where the states of
the respective systems are given by ρX1, ρX2, . . ., the Xj are independent and identically
distributed random variables, and ρj is some ﬁxed set of density operators. For example,
one might imagine a stream of qubits, each of which is prepared in the state |0⟩with
probability one half, or in the state (|0⟩+ |1⟩)/
√
2 with probability one half.
This ensemble notion of a quantum source leads naturally to a notion of ensemble
average ﬁdelity which captures the idea that the source is well-preserved under the
action of a noisy channel described by a trace-preserving quantum operation E, namely
F =

j
pjF(ρj, E(ρj))2,
(9.127)
where pj are the respective probabilities for the different possible preparations of the
system ρj. Obviously, 0 ≤F ≤1, and provided F ≈1 we can be conﬁdent that, on
average, the channel E preserves the states emitted by the source with a high degree
of accuracy. You may wonder why the ﬁdelity appearing on the right hand side of
the deﬁnition is squared. There are two answers to this question, one simple, and one
complex. The simple answer is that including this square term makes the ensemble ﬁdelity
more naturally related to the entanglement ﬁdelity, as deﬁned below. The more complex
answer is that quantum information is, at present, in a state of infancy and it is not
entirely clear what the ‘correct’ deﬁnitions for notions such as information preservation
are! Nevertheless, as we shall see in Chapter 12, the ensemble average ﬁdelity and the
entanglement ﬁdelity give rise to a rich theory of quantum information, which leads us
to believe that these measures are on the right track, even though a complete theory of
quantum information has not yet been developed.
Exercise 9.23:
Show that ¯F = 1 if and only if E(ρj) = ρj for all j such that pj > 0.
There is a second notion of a quantum source we may consider, motivated by the idea
that a channel which preserves information well is a channel that preserves entanglement
well. The basic idea comes from the discussion of the classical probability of error in
Section 9.1. As noted there, a direct analogue of the probability of error p(X ̸= Y )
420
Distance measures for quantum information
cannot be deﬁned for quantum processes, for there is no direct quantum analogue of a
probability distribution deﬁned at two times. Instead, we will use a quantum analogue
of the idea illustrated in Figure 9.7, which is that a dynamic measure of distance can
be deﬁned by ﬁrst copying the random variable X to ˜X, then subjecting X to noise to
obtain Y , and using as our measure of distance some metric quantity D[( ˜X, X), ( ˜X′, Y )]
between the joint distributions for ( ˜X, X) and ( ˜X′, Y ).
HHHHHHHHHHHHH
Figure 9.7. The probability of an error in the channel is equal to the trace distance between the probability
distributions for ( ˜X, X) and ( ˜X, Y ).
A quantum analogue of this model is as follows. A quantum system, Q, is prepared in
a state ρ. The state of Q is assumed to be entangled in some way with the external world.
This entanglement replaces the correlation between X and ˜X in the classical model. We
represent the entanglement by introducing a ﬁctitious system R, such that the joint state
of RQ is a pure state. It turns out that all results that we prove do not depend in any way
on how this puriﬁcation is performed, so we may as well suppose that this is an arbitrary
entanglement with the outside world. The system Q is then subjected to a dynamics
described by a quantum operation, E. The basic situation is illustrated in Figure 9.8.
How well is the entanglement between R and Q preserved by the quantum operation
E? We quantify this by the entanglement ﬁdelity F(ρ, E) which is a function of ρ and E
deﬁned for trace-preserving quantum operations E by
F(ρ, E) ≡F(RQ, R′Q′)2
(9.128)
= ⟨RQ|

(IR ⊗E)(|RQ⟩⟨RQ|)

|RQ⟩,
(9.129)
where the use of a prime indicates the state of a system after the quantum operation
has been applied, and the absence of a prime indicates the state of a system before the
How well does a quantum channel preserve information?
421
Q
-
Q′
R
|RQ⟩
Figure 9.8. The RQ picture of a quantum channel. The initial state of RQ is a pure state.
quantum operation has been applied. The quantity appearing on the right hand side
of this deﬁnition is the square of the static ﬁdelity between the initial and ﬁnal states
of RQ. The use of the square of the static ﬁdelity is purely a convenience, since it
simpliﬁes certain properties of the entanglement ﬁdelity. Note that the entanglement
ﬁdelity depends only upon ρ and E, and not (as it may appear) upon the details of the
puriﬁcation |RQ⟩. To see this, we use the fact, proved in Exercise 2.81, that any two
puriﬁcations |R1Q1⟩and |R2Q2⟩of ρ are related by a unitary operation, U, that acts
upon R alone, |R2Q2⟩= U|R1Q1⟩. Thus
F(|R2Q2⟩, ρR′
2Q′
2) = F(|R1Q1⟩, ρR′
1Q′
2),
(9.130)
which establishes the result. The entanglement ﬁdelity provides a measure of how well
the entanglement between R and Q is preserved by the process E, with values close to 1
indicating that the entanglement has been well preserved, and values close to 0 indicating
that most of the entanglement has been destroyed. The choice of whether to use the static
ﬁdelity squared or the static ﬁdelity is essentially arbitrary; the present deﬁnition results
in slightly more attractive mathematical properties.
One of the attractive properties of the entanglement ﬁdelity is that there is a very
simple formula which enables it to be calculated exactly. Suppose Ei is a set of operation
elements for a quantum operation E. Then
F(ρ, E) = ⟨RQ|ρR′Q′|RQ⟩=

i
|⟨RQ|Ei|RQ⟩|2 .
(9.131)
Suppose we write |RQ⟩= 
j
√pj|j⟩|j⟩, where ρ = 
j pj|j⟩⟨j|. Then
⟨RQ|Ei|RQ⟩=

jk
√pjpk⟨j|k⟩⟨j|Ei|k⟩
(9.132)
=

j
pj⟨j|Ei|j⟩
(9.133)
= tr(Eiρ) .
(9.134)
Substituting this expression into Equation (9.131) we obtain the useful computational
formula
F(ρ, E) =

i
|tr(ρEi)|2 .
(9.135)
422
Distance measures for quantum information
Thus, for example, the entanglement ﬁdelity for the phase damping channel E(ρ) =
pρ + (1 −p)ZρZ is given by
F(ρ, E) = p |tr(ρ)|2 + (1 −p) |tr(ρZ)|2 = p + (1 −p)tr(ρZ)2,
(9.136)
so we see that as p decreases, the entanglement ﬁdelity decreases, as we intuitively expect.
We’ve deﬁned two notions of a quantum information source and associated distance
measures, one based on the idea that we want to preserve an ensemble of quantum states
with high average ﬁdelity, the other based on the idea that it is entanglement between
the source and some reference system that we wish to preserve. Perhaps surprisingly,
these two deﬁnitions turn out to be closely related! The reason for this lies in two useful
properties of the entanglement ﬁdelity. First, the entanglement ﬁdelity is a lower bound
on the square of the static ﬁdelity between the input and output to the process,
F(ρ, E) ≤

F(ρ, E(ρ))
2
.
(9.137)
Intuitively, this result states that it is harder to preserve a state plus entanglement
with the outside world than it is to merely preserve the state alone. The proof is an
elementary application of the monotonicity of the static ﬁdelity under partial trace,
F(ρ, E) = F(|RQ⟩, ρR′Q′)2 ≤F(ρQ, ρQ′)2.
The second property of entanglement ﬁdelity we need to relate it to the ensemble
average deﬁnition is that it is a convex function of ρ. To see this, deﬁne f(x) ≡F(xρ1 +
(1 −x)ρ2, E), and using Equation (9.135) and elementary calculus we ﬁnd
f ′′(x) =

i
|tr((ρ1 −ρ2)Ei)|2 ,
(9.138)
so that f ′′(x) ≥0, which implies the convexity of the entanglement ﬁdelity, as required.
Combining these two results we see that
F
⎛
⎝
j
pjρj , E
⎞
⎠≤

j
pjF(ρj, E)
(9.139)
≤

j
pjF(ρj, E(ρj))2,
(9.140)
and thus
F
⎛
⎝
j
pjρj , E
⎞
⎠≤¯F
!
(9.141)
Thus, any quantum channel E which does a good job of preserving the entanglement
between a source described by a density operator ρ and a reference system will automat-
ically do a good job of preserving an ensemble source described by probabilities pj and
states ρj such that ρ = 
j pjρj. In this sense the notion of a quantum source based on
entanglement ﬁdelity is a more stringent notion than the ensemble deﬁnition, and for this
reason we will prefer the entanglement ﬁdelity based deﬁnition in our study of quantum
information theory, in Chapter 12.
We conclude the chapter with a short list of some easily proved properties of the
entanglement ﬁdelity that will be useful in later chapters:
(1) 0 ≤F(ρ, E) ≤1. Immediate from properties of the static ﬁdelity.
Chapter problems
423
(2) The entanglement ﬁdelity is linear in the quantum operation input. This is
immediate from the deﬁnition of the entanglement ﬁdelity.
(3) For pure state inputs, the entanglement ﬁdelity is equal to the static ﬁdelity squared
between input and output,
F(|ψ⟩, E) = F(|ψ⟩, E(|ψ⟩⟨ψ|))2.
(9.142)
This is immediate from the observation that the state |ψ⟩is a puriﬁcation of itself,
and the deﬁnition of the entanglement ﬁdelity.
(4) F(ρ, E) = 1 if and only if for all pure states |ψ⟩in the support of ρ,
E(|ψ⟩⟨ψ|) = |ψ⟩⟨ψ|.
(9.143)
To prove this, suppose F(ρ, E) = 1, and |ψ⟩is a pure state in the support of ρ.
Deﬁne p ≡1/⟨ψ|ρ−1|ψ⟩> 0 (compare Exercise 2.73 on page 105) and σ to be a
density operator such that (1 −p)σ = ρ −p|ψ⟩⟨ψ|. Then by convexity,
1 = F(ρ, E) ≤p
(
F(|ψ⟩, E) + (1 −p),
(9.144)
and thus F(|ψ⟩, E) = 1, establishing the result one way. The converse is a
straightforward application of the deﬁnition of the entanglement ﬁdelity.
(5) Suppose that ⟨ψ|E(|ψ⟩⟨ψ|)|ψ⟩≥1 −η for all |ψ⟩in the support of ρ, for some η.
Then F(ρ, E) ≥1 −(3η/2). (See Problem 9.3.)
Problem 9.1: (Alternate characterization of the ﬁdelity)
Show that
F(ρ, σ) = inf
P tr(ρP)tr(σP −1),
(9.145)
where the inﬁmum is taken over all invertible positive matrices P.
Problem 9.2:
Let E be a trace-preserving quantum operation. Show that for each ρ
there is a set of operation elements {Ei} for E such that
F(ρ, E) = |tr(ρE1)|2 .
(9.146)
Problem 9.3:
Prove fact (5) on this page.
Summary of Chapter 9: Distance measures for quantum information
• Trace distance: D(ρ, σ) ≡1
2tr|ρ −σ|. Doubly convex metric on density opera-
tors, contractive under quantum operations.
• Fidelity:
F(ρ, σ) ≡tr
(
ρ1/2σρ1/2 = max
|ψ⟩,|ϕ⟩|⟨ψ|ϕ⟩|.
Strongly concave, F(
i piρi, 
i qiσi) ≥
i
√piqiF(ρi, σi).
• Entanglement ﬁdelity: F(ρ, E). Measure of how well entanglement is preserved
during a quantum mechanical process, starting with the state ρ of a system Q,
which is assumed to be entangled with another quantum system, R, and applying
the quantum operation E to system Q.
424
Distance measures for quantum information
History and further reading
Readers wishing to learn more about distance measures for quantum information would
be well advised to start with Fuchs’ 1996 Ph.D. Dissertation[Fuc96]. It contains a wealth of
material on distance measures for quantum information, including a list of 528 references
on related topics, organized into subject areas. Notably, the proof of Equation (9.74)
may be found there, as well as much else of interest. Also see the article by Fuchs
and van de Graaf[FvdG99]; this paper is the origin of the inequality (9.110), and is also
a good overview of distance measures for quantum information, especially in the con-
text of quantum cryptography. The contractivity of the trace distance was proved by
Ruskai[Rus94]. The monotonicity of the ﬁdelity was proved by Barnum, Caves, Fuchs,
Jozsa and Schumacher[BCF+96]. In the literature both the quantity we call ﬁdelity and its
square have been referred to as the ﬁdelity. Uhlmann’s paper[Uhl76] in which he proves the
eponymous theorem also contains an extensive discussion of the elementary properties
of ﬁdelity. The proof of Uhlmann’s theorem given here is due to Jozsa[Joz94]. Chain-
ing properties for ﬁdelity measures and the relation to noisy quantum computation are
discussed in more detail by Aharonov, Kitaev and Nisan[AKN98]. Schumacher[Sch96b] in-
troduced the entanglement ﬁdelity and proved many elementary properties. Knill and
Laﬂamme[KL97] established the connection between subspace ﬁdelity and entanglement
ﬁdelity, Problem 9.3. A more detailed proof of this fact appeared in Barnum, Knill and
Nielsen[BKN98]. Problem 9.1 is due to Alberti[Alb83].
10 Quantum error-correction
We have learned that it is possible to ﬁght entanglement with entanglement.
– John Preskill
To be an Error and to be Cast out is part of God’s Design
– William Blake
This chapter explains how to do quantum information processing reliably in the pres-
ence of noise. The chapter covers three broad topics: the basic theory of quantum error-
correcting codes, fault-tolerant quantum computation, and the threshold theorem. We
begin by developing the basic theory of quantum error-correcting codes, which protect
quantum information against noise. These codes work by encoding quantum states in
a special way that make them resilient against the effects of noise, and then decoding
when it is wished to recover the original state. Section 10.1 explains the basic ideas of
classical error-correction, and some of the conceptual challenges that must be overcome
to make quantum error-correction possible. Section 10.2 explains a simple example of
a quantum error-correcting code, which we then generalize into a theory of quantum
error-correcting codes in Section 10.3. Section 10.4 explains some ideas from the classi-
cal theory of linear codes, and how they give rise to an interesting class of quantum codes
known as Calderbank–Shor–Steane (CSS) codes. Section 10.5 concludes our introduc-
tory survey of quantum error-correcting codes with a discussion of stabilizer codes, a
richly structured class of codes with a close connection to classical error-correcting codes.
Our discussion of quantum error-correction assumes that encoding and decoding of
quantum states can be done perfectly, without error. This is useful, for example, if we
wish to send quantum states over a noisy communication channel and can use almost-
noiseless quantum computers to perform pretty good encoding and decoding of the
states at each end of the channel. However, this assumption cannot be made if the
quantum gates used to do the encoding and decoding are themselves noisy. Fortunately,
the theory of fault-tolerant quantum computation, developed in Section 10.6, allows us
to remove the assumption of perfect encoding and decoding. Even more impressively,
fault-tolerance allow us to perform logical operations on encoded quantum states, in a
manner which tolerates faults in the underlying gate operations. The chapter culminates
in Section 10.6.4 with the threshold theorem for quantum computation: provided the
noise in individual quantum gates is below a certain constant threshold it is possible
to efﬁciently perform an arbitrarily large quantum computation. Of course there are
caveats to this result, which we spend some time discussing. Nevertheless, the threshold
theorem is a remarkable result indicating that noise likely poses no fundamental barrier
to the performance of large-scale quantum computations.
426
Quantum error-correction
10.1
Introduction
Noise is a great bane of information processing systems. Whenever possible we build our
systems to avoid noise completely, and where that is not possible, we try to protect against
the effects of noise. For example, components in modern computers are extremely reliable,
with a failure rate typically below one error in 1017 operations. For most practical purposes
we can act as if computer components are completely noiseless. On the other hand, many
systems in widespread use do suffer from a substantial noise problem. Modems and CD
players both make use of error-correcting codes to protect against the effects of noise.
The details of the techniques used to protect against noise in practice are sometimes
rather complicated, but the basic principles are easily understood. The key idea is that
if we wish to protect a message against the effects of noise, then we should encode the
message by adding some redundant information to the message. That way, even if some
of the information in the encoded message is corrupted by noise, there will be enough
redundancy in the encoded message that it is possible to recover or decode the message
so that all the information in the original message is recovered.
For example, suppose we wish to send a bit from one location to another through a
noisy classical communications channel. The effect of the noise in the channel is to ﬂip
the bit being transmitted with probability p > 0, while with probability 1 −p the bit is
transmitted without error. Such a channel is known as a binary symmetric channel, and
is illustrated in Figure 10.1. A simple means of protecting the bit against the effects of
noise in the binary symmetric channel is to replace the bit we wish to protect with three
copies of itself:
0 →000
(10.1)
1 →111.
(10.2)
The bit strings 000 and 111 are sometimes referred to as the logical 0 and logical 1,
since they play the role of 0 and 1, respectively. We now send all three bits through the
channel. At the receiver’s end of the channel three bits are output, and the receiver has to
decide what the value of the original bit was. Suppose 001 were output from the channel.
Provided the probability p of a bit ﬂip is not too high, it is very likely that the third bit
was ﬂipped by the channel, and that 0 was the bit that was sent.

NNNNNNNNNNNNNN

pppppppppppppp
Figure 10.1. Binary symmetric channel.
This type of decoding is called majority voting, since the decoded output from the
channel is whatever value, 0 or 1, appears more times in the actual channel output.
Majority voting fails if two or more of the bits sent through the channel were ﬂipped,
and succeeds otherwise. The probability that two or more of the bits are ﬂipped is
Introduction
427
3p2(1 −p) + p3, so the probability of error is pe = 3p2 −2p3. Without encoding, the
probability of an error was p, so the code makes the transmission more reliable provided
pe < p, which occurs whenever p < 1/2.
The type of code just described is called a repetition code, since we encode the message
to be sent by repeating it a number of times. A similar technique has been used for
millennia as a part of everyday conversation: if we’re having difﬁculty understanding
someone’s spoken language, perhaps because they have a foreign accent, we ask them to
repeat what they’re saying. We may not catch all the words either time, but we can put
the iterations together to comprehend a coherent message. Many clever techniques have
been developed in the theory of classical error-correcting codes; however, the key idea
is always to encode messages by adding enough redundancy that the original message is
recoverable after noise has acted on the encoded message, with the amount of redundancy
needing to be added depending on the severity of noise in the channel.
10.1.1
The three qubit bit ﬂip code
To protect quantum states against the effects of noise we would like to develop quantum
error-correcting codes based upon similar principles. There are some important differ-
ences between classical information and quantum information that require new ideas to
be introduced to make such quantum error-correcting codes possible. In particular, at a
ﬁrst glance we have three rather formidable difﬁculties to deal with:
• No cloning: One might try to implement the repetition code quantum mechanically
by duplicating the quantum state three or more times. This is forbidden by the
no-cloning theorem discussed in Box 12.1 on page 532. Even if cloning were
possible, it would not be possible to measure and compare the three quantum states
output from the channel.
• Errors are continuous: A continuum of different errors may occur on a single qubit.
Determining which error occurred in order to correct it would appear to require
inﬁnite precision, and therefore inﬁnite resources.
• Measurement destroys quantum information: In classical error-correction we
observe the output from the channel, and decide what decoding procedure to adopt.
Observation in quantum mechanics generally destroys the quantum state under
observation, and makes recovery impossible.
Fortunately, none of these problems is fatal, as we shall demonstrate. Suppose we send
qubits through a channel which leaves the qubits untouched with probability 1 −p, and
ﬂips the qubits with probability p. That is, with probability p the state |ψ⟩is taken to
the state X|ψ⟩, where X is the usual Pauli sigma x operator, or bit ﬂip operator. This
channel is called the bit ﬂip channel, and we now explain the bit ﬂip code, which may
be used to protect qubits against the effects of noise from this channel.
Suppose we encode the single qubit state a|0⟩+b|1⟩in three qubits as a|000⟩+b|111⟩.
A convenient way to write this encoding is
|0⟩→|0L⟩≡|000⟩
(10.3)
|1⟩→|1L⟩≡|111⟩,
(10.4)
where it is understood that superpositions of basis states are taken to corresponding
superpositions of encoded states. The notation |0L⟩and |1L⟩indicates that these are
428
Quantum error-correction
the logical |0⟩and logical |1⟩states, not the physical zero and one states. A circuit
performing this encoding is illustrated in Figure 10.2.
• •
⊕
⊕
|ψ⟩
|0⟩
|0⟩
Figure 10.2. Encoding circuit for the three qubit bit ﬂip code. The data to be encoded enters the circuit on the top
line.
Exercise 10.1:
Verify that the encoding circuit in Figure 10.2 works as claimed.
Suppose the initial state a|0⟩+ b|1⟩has been perfectly encoded as a|000⟩+ b|111⟩.
Each of the three qubits is passed through an independent copy of the bit ﬂip channel.
Suppose a bit ﬂip occurred on one or fewer of the qubits. There is a simple two stage
error-correction procedure which can be used to recover the correct quantum state in
this case:
(1) Error-detection or syndrome diagnosis: We perform a measurement which tells us
what error, if any, occurred on the quantum state. The measurement result is called
the error syndrome. For the bit ﬂip channel there are four error syndromes,
corresponding to the four projection operators:
P0 ≡|000⟩⟨000| + |111⟩⟨111| no error
(10.5)
P1 ≡|100⟩⟨100| + |011⟩⟨011| bit ﬂip on qubit one
(10.6)
P2 ≡|010⟩⟨010| + |101⟩⟨101| bit ﬂip on qubit two
(10.7)
P3 ≡|001⟩⟨001| + |110⟩⟨110| bit ﬂip on qubit three.
(10.8)
Suppose for example that a bit ﬂip occurs on qubit one, so the corrupted state is
a|100⟩+ b|011⟩. Notice that ⟨ψ|P1|ψ⟩= 1 in this case, so the outcome of the
measurement result (the error syndrome) is certainly 1. Furthermore, the syndrome
measurement does not cause any change to the state: it is a|100⟩+ b|011⟩both
before and after syndrome measurement. Note that the syndrome contains only
information about what error has occurred, and does not allow us to infer anything
about the value of a or b, that is, it contains no information about the state being
protected. This is a generic feature of syndrome measurements, since to obtain
information about the identity of a quantum state it is in general necessary to
perturb that state.
(2) Recovery: We use the value of the error syndrome to tell us what procedure to use
to recover the initial state. For example, if the error syndrome was 1, indicating a
bit ﬂip on the ﬁrst qubit, then we ﬂip that qubit again, recovering the original state
a|000⟩+ b|111⟩with perfect accuracy. The four possible error syndromes and the
recovery procedure in each case are: 0 (no error) – do nothing; 1 (bit ﬂip on ﬁrst
qubit) – ﬂip the ﬁrst qubit again; 2 (bit ﬂip on second qubit) – ﬂip the second qubit
Introduction
429
again; 3 (bit ﬂip on third qubit) – ﬂip the third qubit again. For each value of the
error syndrome it is easy to see that the original state is recovered with perfect
accuracy, given that the corresponding error occurred.
This error-correction procedure works perfectly, provided bit ﬂips occur on one or fewer
of the three qubits. This occurs with probability (1 −p)3 + 3p(1 −p)2 = 1 −3p2 + 2p3.
The probability of an error remaining uncorrected is therefore 3p2 −2p3, just as for the
classical repetition code we studied earlier. Once again, provided p < 1/2 the encoding
and decoding improve the reliability of storage of the quantum state.
Improving the error analysis
This error analysis is not completely adequate. The problem is that not all errors and
states in quantum mechanics are created equal: quantum states live in a continuous space,
so it is possible for some errors to corrupt a state by a tiny amount, while others mess
it up completely. An extreme example is provided by the bit ﬂip ‘error’ X, which does
not affect the state (|0⟩+ |1⟩)/
√
2 at all, but ﬂips the |0⟩state so it becomes a |1⟩. In the
former case we would not be worried about a bit ﬂip error occurring, while in the latter
case we would obviously be very worried.
To address this problem we make use of the ﬁdelity quantity introduced in Chap-
ter 9. Recall that the ﬁdelity between a pure and a mixed state is given by F(|ψ⟩, ρ) =
$
⟨ψ|ρ|ψ⟩. The object of quantum error-correction is to increase the ﬁdelity with which
quantum information is stored (or communicated) up near the maximum possible ﬁdelity
of one. Let’s compare the minimum ﬁdelity achieved by the three qubit bit ﬂip code
with the ﬁdelity when no error-correction is performed. Suppose the quantum state of
interest is |ψ⟩. Without using the error-correcting code the state of the qubit after being
sent through the channel is
ρ = (1 −p)|ψ⟩⟨ψ| + pX|ψ⟩⟨ψ|X.
(10.9)
The ﬁdelity is given by
F =
(
⟨ψ|ρ|ψ⟩=
(
(1 −p) + p⟨ψ|X|ψ⟩⟨ψ|X|ψ⟩.
(10.10)
The second term under the square root is non-negative, and equal to zero when |ψ⟩= |0⟩,
so we see that the minimum ﬁdelity is F = √1 −p. Suppose the three qubit error-
correcting code is used to protect the state |ψ⟩= a|0L⟩+ b|1L⟩. The quantum state after
both the noise and error-correction is:
ρ =
&(1 −p)3 + 3p(1 −p)2' |ψ⟩⟨ψ| + · · · .
(10.11)
The omitted terms represent contributions from bit ﬂips on two or three qubits. All the
omitted terms are positive operators, so the ﬁdelity we calculate will be a lower bound
on the true ﬁdelity. We see that F =
$
⟨ψ|ρ|ψ⟩≥
$
(1 −p)3 + 3p(1 −p)2. That is,
the ﬁdelity is at least
$
1 −3p2 + 2p3, so the ﬁdelity of storage for the quantum state is
improved provided p < 1/2, which is the same conclusion we came to earlier based on
a much cruder analysis.
Exercise 10.2:
The action of the bit ﬂip channel can be described by the quantum
operation E(ρ) = (1 −p)ρ + pXρX. Show that this may be given an alternate
operator-sum representation, as E(ρ) = (1 −2p)ρ + 2pP+ρP+ + 2pP−ρP−where
430
Quantum error-correction
P+ and P−are projectors onto the +1 and −1 eigenstates of X, (|0⟩+ |1⟩)/
√
2
and (|0⟩−|1⟩)/
√
2, respectively. This latter representation can be understood as
a model in which the qubit is left alone with probability 1 −2p, and is
‘measured’ by the environment in the |+⟩, |−⟩basis with probability 2p.
There is a different way of understanding the syndrome measurement that is useful in
generalizing the three qubit code. Suppose that instead of measuring the four projectors
P0, P1, P2, P3 we performed two measurements, the ﬁrst of the observable Z1Z2 (that
is, Z ⊗Z ⊗I), and the second of the observable Z2Z3. Each of these observables has
eigenvalues ±1, so each measurement provides a single bit of information, for a total of
two bits of information – four possible syndromes, just as in the earlier description. The
ﬁrst measurement, of Z1Z2, can be thought of as comparing the ﬁrst and second qubits to
see if they are the same. To see why this is so, note that Z1Z2 has spectral decomposition
Z1Z2 = (|00⟩⟨00| + |11⟩⟨11|) ⊗I −(|01⟩⟨01| + |10⟩⟨10|) ⊗I,
(10.12)
which corresponds to a projective measurement with projectors (|00⟩⟨00| + |11⟩⟨11|) ⊗I
and (|01⟩⟨01| + |10⟩⟨10|) ⊗I. Thus, measuring Z1Z2 can be thought of as comparing
the values of the ﬁrst and second qubits, giving +1 if they are the same, and −1 if they
are different. Similarly, measuring Z2Z3 compares the values of the second and third
qubits, giving +1 if they are the same, and −1 if they are different. Combining these
two measurement results we can determine whether a bit ﬂip occurred on one of the
qubits or not, and if so, which one: if both measurement results give +1 then with high
probability no bit ﬂip has occurred; if measuring Z1Z2 gives +1 and measuring Z2Z3
gives −1 then with high probability just the third qubit ﬂipped; if measuring Z1Z2 gives
−1 and measuring Z2Z3 gives +1 then with high probability just the ﬁrst qubit ﬂipped;
and ﬁnally if both measurements give −1 then with high probability just the second qubit
ﬂipped. What is crucial to the success of these measurements is that neither measurement
gives any information about the amplitudes a and b of the encoded quantum state, and
thus neither measurement destroys the superpositions of quantum states that we wish to
preserve using the code.
Exercise 10.3:
Show by explicit calculation that measuring Z1Z2 followed by Z2Z3 is
equivalent, up to labeling of the measurement outcomes, to measuring the four
projectors deﬁned by (10.5)–(10.8), in the sense that both procedures result in
the same measurement statistics and post-measurement states.
10.1.2
Three qubit phase ﬂip code
The bit ﬂip code is interesting, but it does not appear to be that signiﬁcant an innovation
over classical error-correcting codes, and leaves many problems open (for example, many
kinds of errors other than bit ﬂips can happen to qubits). A more interesting noisy
quantum channel is the phase ﬂip error model for a single qubit. In this error model the
qubit is left alone with probability 1 −p, and with probability p the relative phase of the
|0⟩and |1⟩states is ﬂipped. More precisely, the phase ﬂip operator Z is applied to the
qubit with probability p > 0, so the state a|0⟩+b|1⟩is taken to the state a|0⟩−b|1⟩under
the phase ﬂip. There is no classical equivalent to the phase ﬂip channel, since classical
channels don’t have any property equivalent to phase. However, there is an easy way to
turn the phase ﬂip channel into a bit ﬂip channel. Suppose we work in the qubit basis
Introduction
431
|+⟩≡(|0⟩+ |1⟩)/
√
2, |−⟩≡(|0⟩−|1⟩)/
√
2. With respect to this basis the operator Z
takes |+⟩to |−⟩and vice versa, that is, it acts just like a bit ﬂip with respect to the labels
+ and −! This suggests using the states |0L⟩≡| + ++⟩and |1L⟩≡| −−−⟩as logical
zero and one states for protection against phase ﬂip errors. All the operations needed for
error-correction – encoding, error-detection, and recovery – are performed just as for
the bit ﬂip channel, but with respect to the |+⟩, |−⟩basis instead of the |0⟩, |1⟩basis. To
accomplish this basis change we simply apply the Hadamard gate and its inverse (also
the Hadamard gate) at appropriate points in the procedure, since the Hadamard gate
accomplishes the change back and forth between the |0⟩, |1⟩basis and the |+⟩, |−⟩basis.
Figure 10.3. Encoding circuits for the phase ﬂip code.
More explicitly, the encoding for the phase ﬂip channel is performed in two steps:
ﬁrst, we encode in three qubits exactly as for the bit ﬂip channel; second, we apply a
Hadamard gate to each qubit (Figure 10.3). Error-detection is achieved by applying the
same projective measurements as before, but conjugated by Hadamard gates: Pj →P ′
j ≡
H⊗3PjH⊗3. Equivalently, syndrome measurement may be performed by measuring the
observables H⊗3Z1Z2H⊗3 = X1X2 and H⊗3Z2Z3H⊗3 = X2X3. It is interesting to
interpret these measurements along similar lines to the measurement of Z1Z2 and Z2Z3
for the bit ﬂip code. Measurement of the observables X1X2 and X2X3 corresponds
to comparing the sign of qubits one and two, and two and three, respectively, in the
sense that measurement of X1X2, for example, gives +1 for states like |+⟩|+⟩⊗(·) or
|−⟩|−⟩⊗(·), and −1 for states like |+⟩|−⟩⊗(·) or |−⟩|+⟩⊗(·). Finally, error-correction
is completed with the recovery operation, which is the Hadamard-conjugated recovery
operation from the bit ﬂip code. For example, suppose we detected a ﬂip in the sign of
the ﬁrst qubit from |+⟩to |−⟩. Then we recover by applying HX1H = Z1 to the ﬁrst
qubit. Similar procedures apply for other error syndromes.
Obviously this code for the phase ﬂip channel has the same characteristics as the code
for the bit ﬂip channel. In particular, the minimum ﬁdelity for the phase ﬂip code is the
same as that for the bit ﬂip code, and we have the same criteria for the code producing an
improvement over the case with no error-correction. We say that these two channels are
unitarily equivalent, since there is a unitary operator U (in this case the Hadamard gate)
such that the action of one channel is the same as the other, provided the ﬁrst channel is
preceded by U and followed by U †. These operations may be trivially incorporated into
the encoding and error-correction operations. For general unitary operators these ideas
are worked out in Problem 10.1 on page 495.
432
Quantum error-correction
Exercise 10.4:
Consider the three qubit bit ﬂip code. Suppose we had performed the
error syndrome measurement by measuring the eight orthogonal projectors
corresponding to projections onto the eight computational basis states.
(1) Write out the projectors corresponding to this measurement, and explain how
the measurement result can be used to diagnose the error syndrome: either
no bits ﬂipped or bit number j ﬂipped, where j is in the range one to three.
(2) Show that the recovery procedure works only for computational basis states.
(3) What is the minimum ﬁdelity for the error-correction procedure?
10.2
The Shor code
There is a simple quantum code which can protect against the effects of an arbitrary
error on a single qubit! The code is known as the Shor code, after its inventor. The code
is a combination of the three qubit phase ﬂip and bit ﬂip codes. We ﬁrst encode the qubit
using the phase ﬂip code: |0⟩→| + ++⟩, |1⟩→| −−−⟩. Next, we encode each of these
qubits using the three qubit bit ﬂip code: |+⟩is encoded as (|000⟩+ |111⟩)
√
2 and |−⟩is
encoded as (|000⟩−|111⟩)
√
2. The result is a nine qubit code, with codewords given by:
|0⟩→|0L⟩≡(|000⟩+ |111⟩)(|000⟩+ |111⟩)(|000⟩+ |111⟩)
2
√
2
|1⟩→|1L⟩≡(|000⟩−|111⟩)(|000⟩−|111⟩)(|000⟩−|111⟩)
2
√
2
.
(10.13)
Figure 10.4. Encoding circuit for the Shor nine qubit code. Some of the |0⟩states appear indented, simply to
emphasize the concatenated nature of the encoding.
.
The quantum circuit encoding the Shor code is shown in Figure 10.4. As described
above, the ﬁrst part of the circuit encodes the qubit using the three qubit phase ﬂip code;
The Shor code
433
comparison with Figure 10.3 shows that the circuits are identical. The second part of the
circuit encodes each of these three qubits using the bit ﬂip code, using three copies of the
bit ﬂip code encoding circuit, Figure 10.2. This method of encoding using a hierarchy of
levels is known as concatenation. It’s a great trick for obtaining new codes from old, and
we use it again later to prove some important results about quantum error-correction.
The Shor code is able to protect against phase ﬂip and bit ﬂip errors on any qubit. To
see this, suppose a bit ﬂip occurs on the ﬁrst qubit. As for the bit ﬂip code, we perform
a measurement of Z1Z2 comparing the ﬁrst two qubits, and ﬁnd that they are different.
We conclude that a bit ﬂip error occurred on the ﬁrst or second qubit. Next we compare
the second and third qubit by performing a measurement of Z2Z3. We ﬁnd that they are
the same, so it could not have been the second qubit which ﬂipped. We conclude that the
ﬁrst qubit must have ﬂipped, and recover from the error by ﬂipping the ﬁrst qubit again,
back to its original state. In a similar way we can detect and recover from the effects of
bit ﬂip errors on any of the nine qubits in the code.
We cope in a similar manner with phase ﬂips on the qubits. Suppose a phase ﬂip
occurs on the ﬁrst qubit. Such a phase ﬂip ﬂips the sign of the ﬁrst block of qubits,
changing |000⟩+|111⟩to |000⟩−|111⟩, and vice versa. Indeed, a phase ﬂip on any of the
ﬁrst three qubits has this effect, and the error-correction procedure we describe works
for any of these three possible errors. Syndrome measurement begins by comparing the
sign of the ﬁrst and second blocks of three qubits, just as syndrome measurement for the
phase ﬂip code began by comparing the sign of the ﬁrst and second qubits. For example,
(|000⟩−|111⟩)(|000⟩−|111⟩) has the same sign (−) in both blocks of qubits, while
(|000⟩−|111⟩)(|000⟩+ |111⟩) has different signs. When a phase ﬂip occurs on any of the
ﬁrst three qubits we ﬁnd that the signs of the ﬁrst and second blocks are different. The
second and ﬁnal stage of syndrome measurement is to compare the sign of the second
and third blocks of qubits. We ﬁnd that these are the same, and conclude that the phase
must have ﬂipped in the ﬁrst block of three qubits. We recover from this by ﬂipping the
sign in the ﬁrst block of three qubits back to its original value. We can recover from a
phase ﬂip on any of the nine qubits in a similar manner.
Exercise 10.5:
Show that the syndrome measurement for detecting phase ﬂip errors
in the Shor code corresponds to measuring the observables X1X2X3X4X5X6
and X4X5X6X7X8X9.
Exercise 10.6:
Show that recovery from a phase ﬂip on any of the ﬁrst three qubits
may be accomplished by applying the operator Z1Z2Z3.
Suppose both bit and phase ﬂip errors occur on the ﬁrst qubit, that is, the operator
Z1X1 is applied to that qubit. Then it is easy to see that the procedure for detecting a
bit ﬂip error will detect a bit ﬂip on the ﬁrst qubit, and correct it, and the procedure for
detecting a phase ﬂip error will detect a phase ﬂip on the ﬁrst block of three qubits, and
correct it. Thus, the Shor code also enables the correction of combined bit and phase ﬂip
errors on a single qubit.
Indeed, the Shor code protects against much more than just bit and phase ﬂip errors
on a single qubit – we now show that it protects against completely arbitrary errors,
provided they only affect a single qubit! The error can be tiny – a rotation about the z
axis of the Bloch sphere by π/263 radians, say – or it can be an apparently disastrous
error like removing the qubit entirely and replacing it with garbage! The interesting thing
434
Quantum error-correction
is, no additional work needs to be done in order to protect against arbitrary errors – the
procedure already described works just ﬁne. This is an example of the extraordinary
fact that the apparent continuum of errors that may occur on a single qubit can all be
corrected by correcting only a discrete subset of those errors; all other possible errors
being corrected automatically by this procedure! This discretization of the errors is central
to why quantum error-correction works, and should be regarded in contrast to classical
error-correction for analog systems, where no such discretization of errors is possible.
To simplify the analysis, suppose noise of an arbitrary type is occurring on the ﬁrst
qubit only; we’ll come back to what happens when noise is affecting other qubits as well.
Following Chapter 8 we describe the noise by a trace-preserving quantum operation E.
It is most convenient to analyze error-correction by expanding E in an operator-sum
representation with operation elements {Ei}. Supposing the state of the encoded qubit
is |ψ⟩= α|0L⟩+ β|1L⟩before the noise acts, then after the noise has acted the state
is E(|ψ⟩⟨ψ|) = 
i Ei|ψ⟩⟨ψ|E†
i . To analyze the effects of error-correction it’s easiest to
focus on the effect error-correction has on a single term in this sum, say Ei|ψ⟩⟨ψ|E†
i .
As an operator on the ﬁrst qubit alone Ei may be expanded as a linear combination of
the identity, I, the bit ﬂip, X1, the phase ﬂip, Z1, and the combined bit and phase ﬂip,
X1Z1:
Ei = ei0I + ei1X1 + ei2Z1 + ei3X1Z1 .
(10.14)
The (un-normalized) quantum state Ei|ψ⟩can thus be written as a superposition of four
terms, |ψ⟩, X1|ψ⟩, Z1|ψ⟩, X1Z1|ψ⟩. Measuring the error syndrome collapses this super-
position into one of the four states |ψ⟩, X1|ψ⟩, Z1|ψ⟩or X1Z1|ψ⟩from which recovery
may then be performed by applying the appropriate inversion operation, resulting in the
ﬁnal state |ψ⟩. The same is true for all the other operation elements Ei. Thus, error-
correction results in the original state |ψ⟩being recovered, despite the fact that the error
on the ﬁrst qubit was arbitrary. This is a fundamental and deep fact about quantum
error-correction, that by correcting just a discrete set of errors – the bit ﬂip, phase ﬂip,
and combined bit–phase ﬂip, in this example – a quantum error-correcting code is able
to automatically correct an apparently much larger (continuous!) class of errors.
What happens when noise is affecting more than just the ﬁrst qubit? Two basic ideas
are used to cope with this. First, in many situations it is a good approximation to assume
that the noise acts on qubits independently. Provided the effect of the noise on one qubit
is fairly small, we can expand the total effect of the noise as a sum over terms involving
errors on no qubits, on a single qubit, on two qubits, and so on, with the terms with
errors on no qubits and on a single qubit dominating the higher order terms. Performing
error-correction results in the zeroth and ﬁrst order terms being corrected properly, and
leaves only the much smaller second and higher order errors, achieving a net suppression
of error. A more detailed analysis of this idea will be given later. Sometimes, of course,
it is not reasonable to assume that the noise acts on qubits independently. When this
occurs we use a different idea – error-correcting codes which can correct errors on more
than a single qubit. Such codes may be constructed along similar lines to the Shor code,
and we explain the basic ideas behind how this can be done later in this chapter.
Theory of quantum error-correction
435
10.3
Theory of quantum error-correction
Can we construct a general theory of quantum error-correcting codes? This section devel-
ops a general framework for studying quantum error-correction, including the quantum
error-correction conditions, a set of equations which must be satisﬁed if quantum error-
correction is to be possible. Of course, possessing such a framework doesn’t guarantee us
that good quantum error-correcting codes exist – that topic is taken up in Section 10.4
on page 445! But it does provide background to enable us to ﬁnd good quantum error-
correcting codes.
The basic ideas of the theory of quantum error-correction generalize in a natural way
the ideas introduced by the Shor code. Quantum states are encoded by a unitary operation
into a quantum error-correcting code, formally deﬁned as a subspace C of some larger
Hilbert space. It is useful to have a notation for the projector onto the code space C,
so we use the notation P; for the three qubit bit ﬂip code P = |000⟩⟨000| + |111⟩⟨111|.
After encoding the code is subjected to noise, following which a syndrome measurement
is performed to diagnose the type of error which occurred, that is, the error syndrome.
Once this has been determined, a recovery operation is performed, to return the quantum
system to the original state of the code. The basic picture is illustrated in Figure 10.5:
different error syndromes correspond to undeformed and orthogonal subspaces of the
total Hilbert space. The subspaces must be orthogonal, otherwise they couldn’t be reliably
distinguished by the syndrome measurement. Furthermore, the different subspaces must
be undeformed versions of the original code space, in the sense that the errors mapping
to the different subspaces must take the (orthogonal) codewords to orthogonal states, in
order to be able to recover from the error. This intuitive picture is essentially the content
of the quantum error-correction conditions discussed below.
To develop a general theory of quantum error-correction it behooves us to make as few
assumptions as possible about the nature of noise and about the procedure used to do the
error-correction. That is, we won’t necessarily assume that the error-correction is done
via a two-stage detection–recovery method, and we won’t make any assumptions about
the noise occurring for qubit systems, or being weak. Instead, we just make two very
broad assumptions: the noise is described by a quantum operation E, and the complete
error-correction procedure is effected by a trace-preserving quantum operation R, which
we call the error-correction operation. This error-correction operation bundles into one
piece the two steps that we have called error-detection and recovery above. In order for
error-correction to be deemed successful, we require that for any state ρ whose support
lies in the code C,
(R ◦E) (ρ) ∝ρ.
(10.15)
You may be wondering why we wrote ∝rather than = in the last equation. If E were a
trace-preserving quantum operation then by taking traces of both sides of the equation
we see that ∝would be =. However, sometimes we may be interested in error-correcting
non-trace-preserving operations E, such as measurements, for which ∝is appropriate.
Of course, the error-correction step R must succeed with probability 1, which is why we
required R to be trace-preserving.
The quantum error-correction conditions are a simple set of equations which can
be checked to determine whether a quantum error-correcting code protects against a
particular type of noise E. We will use these conditions to construct a plethora of quantum
436
Quantum error-correction






(A)






(B)
Figure 10.5. The packing of Hilbert spaces in quantum coding: (A) bad code, with non-orthogonal, deformed
resultant spaces, and (B) good code, with orthogonal (distinguishable), undeformed spaces.
codes, and also to investigate some of the general properties of quantum error-correcting
codes.
Theorem 10.1: (Quantum error-correction conditions) Let C be a quantum code,
and let P be the projector onto C. Suppose E is a quantum operation with
operation elements {Ei}. A necessary and sufﬁcient condition for the existence
of an error-correction operation R correcting E on C is that
PE†
i EjP = αijP,
(10.16)
for some Hermitian matrix α of complex numbers.
We call the operation elements {Ei} for the noise E errors, and if such an R exists
we say that {Ei} constitutes a correctable set of errors.
Proof
We prove sufﬁciency ﬁrst, by constructing an explicit error-correction operation R when-
ever (10.16) is satisﬁed. The construction we use is of the two-part form used for the Shor
code – error-detection and then recovery – so the proof also shows that error-correction
can always be accomplished using such a two-part procedure. Suppose {Ei} is a set of
operation elements satisfying the quantum error-correction conditions, (10.16). By as-
sumption α is a Hermitian matrix, and thus can be diagonalized, d = u†αu, where u is
a unitary matrix and d is diagonal. Deﬁne operators Fk ≡
i uikEi. Recalling Theo-
Theory of quantum error-correction
437
rem 8.2, we see that {Fk} is also a set of operation elements for E. By direct substitution,
PF †
kFlP =

ij
u†
kiujlPE†
i EjP.
(10.17)
Substituting (10.16) simpliﬁes this to PF †
kFlP = 
ij u†
kiαijujlP and since d = u†αu
we obtain
PF †
kFlP = dklP,
(10.18)
which can be thought of as a simpliﬁcation of the quantum error-correction condi-
tions (10.16), because dkl is diagonal.
We use the simpliﬁed conditions (10.18) and the polar decomposition (Section 2.1.10
on page 78) to deﬁne the syndrome measurement. From the polar decomposition, we
see that FkP = Uk
(
PF †
kFkP = √dkkUkP for some unitary Uk. The effect of Fk is
therefore to rotate the coding subspace into the subspace deﬁned by the projector Pk ≡
UkPU †
k = FkPU †
k/√dkk. Equation (10.18) implies that these subspaces are orthogonal,
since when k ̸= l,
PlPk = P †
l Pk = UlPF †
l FkPU †
k
√dlldkk
= 0 .
(10.19)
The syndrome measurement is a projective measurement deﬁned by the projectors Pk,
augmented by an additional projector if necessary to satisfy the completeness relation

k Pk = I. Recovery is accomplished simply by applying U †
k. To see that this error-
correction procedure works, note that the combined detection–recovery step corresponds
to the quantum operation R(σ) = 
k U †
kPkσPkUk. For states ρ in the code, simple
algebra and the deﬁnitions show that:
U †
kPkFl
√ρ = U †
kP †
kFlP√ρ
(10.20)
= U †
kUkPF †
kFlP√ρ
√dkk
(10.21)
= δkl
$
dkkP√ρ
(10.22)
= δkl
$
dkk
√ρ .
(10.23)
Thus
R(E(ρ)) =

kl
U †
kPkFlρF †
l PkUk
(10.24)
=

kl
δkldkkρ
(10.25)
∝ρ,
(10.26)
as required.
To prove necessity of the quantum error-correction conditions (10.16), suppose {Ei}
is a set of errors which is perfectly correctable by an error-correction operation R with
operation elements {Rj}. Deﬁne a quantum operation EC by EC(ρ) ≡E(PρP). Since
PρP is in the code space for all ρ, it follows that
R(EC(ρ)) ∝PρP,
(10.27)
for all states ρ. Moreover, the proportionality factor must be a constant c, not depending
438
Quantum error-correction
on ρ, if both right and left hand sides are to be linear. Rewriting the last equation explicitly
in terms of the operation elements gives

ij
RjEiPρPE†
i R†
j = cPρP.
(10.28)
This equation holds for all ρ. It follows that the quantum operation with operation
elements {RjEi} is identical to the quantum operation with a single operation element
√cP. Theorem 8.2 implies that there exist complex numbers cki such that
RkEiP = ckiP.
(10.29)
Taking the adjoint of this equation gives PE†
i R†
k = c∗
kiP and therefore PE†
i R†
kRkEjP =
c∗
kickjP. But R is a trace-preserving operation, so 
k R†
kRk = I. Summing the equation
PE†
i R†
kRkEjP = c∗
kickjP over k we deduce that
PE†
i EjP = αijP,
(10.30)
where αij ≡
k c∗
kickj is just a Hermitian matrix of complex numbers. These are the
quantum error-correction conditions.
Direct veriﬁcation of the quantum error-correction conditions is an easy but time-
consuming task. In Sections 10.4 and 10.5 we describe a theoretical formalism which
uses the quantum error-correction conditions as a launching point for the construction of
many interesting classes of codes, and which circumvents much of the difﬁculty associated
with verifying the quantum error-correction conditions directly. For now, you should
work through the following example, which demonstrates the quantum error-correction
conditions in action:
Exercise 10.7:
Consider the three qubit bit ﬂip code of Section 10.1.1, with
corresponding projector P = |000⟩⟨000| + |111⟩⟨111|. The noise process this
code protects against has operation elements
{
$
(1 −p)3I,
$
p(1 −p)2X1,
$
p(1 −p)2X2,
$
p(1 −p)2X3}, where p is the
probability that a bit ﬂips. Note that this quantum operation is not
trace-preserving, since we have omitted operation elements corresponding to bit
ﬂips on two and three qubits. Verify the quantum error-correction conditions for
this code and noise process.
10.3.1
Discretization of the errors
We have discussed the protection of quantum information against a speciﬁc noise process
E. In general, however, we don’t know exactly what noise is afﬂicting a quantum system.
It would be useful if a speciﬁc code C and error-correction operation R could be used
to protect against an entire class of noise processes. Fortunately, the quantum error-
correction conditions are easily adapted to provide exactly this sort of protection.
Theorem 10.2: Suppose C is a quantum code and R is the error-correction operation
constructed in the proof of Theorem 10.1 to recover from a noise process E with
operation elements {Ei}. Suppose F is a quantum operation with operation
elements {Fj} which are linear combinations of the Ei, that is Fj = 
i mjiEi
for some matrix mji of complex numbers. Then the error-correction operation
R also corrects for the effects of the noise process F on the code C.
Theory of quantum error-correction
439
Box 10.1: Quantum error-correction without measurement
In the main text we have described quantum error-correction as a two stage process:
an error-detection step effected using quantum measurement, followed by a recov-
ery step effected using conditioned unitary operations. It is possible to perform
quantum error-correction using only unitary operations and ancilla systems pre-
pared in standard states. The advantage of knowing how to do this is that for some
real-world quantum systems it is very difﬁcult to perform the quantum measure-
ments needed for quantum error-correction, so an alternate procedure is needed.
The techniques we use to do that are essentially the same as those described in
Chapter 8 for mocking up an arbitrary quantum operation; we now recap the basic
idea in the context of quantum error-correction.
Suppose the syndrome measurement on the principal system – the one being error-
corrected – is described by measurement operators Mi, and the corresponding
conditional unitary operation is Ui. Introduce an ancilla system with basis states
|i⟩corresponding to the possible error syndromes. The ancilla starts in a standard
pure state |0⟩before error-correction. Deﬁne a unitary operator U on the principal
system plus ancilla by
U|ψ⟩|0⟩≡

i
(UiMi|ψ⟩)|i⟩.
(10.31)
This can be extended to a unitary operator acting on the whole space since
⟨ϕ|⟨0|U †U|ψ⟩|0⟩=

ij
⟨ϕ|M †
i Mj|ψ⟩δij
(10.32)
=

i
⟨ϕ|M †
i Mi|ψ⟩
(10.33)
= ⟨ϕ|ψ⟩.
(10.34)
That is, U preserves inner products, and can thus be extended to a unitary operator
on the entire state space. The effect of U is to effect the transformation R(σ) =

i UiMiσM †
i U †
i on the system being error-corrected, exactly the same quantum
operation as described in the main text for the performance of quantum error-
correction. Note that in order for this error-correction procedure to work it is
necessary to use a fresh ancilla each time error-correction is performed.
Proof
By Theorem 10.1 the operation elements {Ei} must satisfy the quantum error-correction
conditions, PEiE†
jP = αijP. As shown in the proof of Theorem 10.1, without loss of
generality we may assume that the operation elements for E have been chosen such that
αij = dij is diagonal with real entries. The error-correction operation R has operation
elements U †
kPk, where by Equation (10.23) the Uk and Pk are chosen such that for any
ρ in the code space:
U †
kPkEi
√ρ = δki
$
dkk
√ρ.
(10.35)
440
Quantum error-correction
Substituting Fj = 
i mjiEi gives
U †
kPkFj
√ρ =

i
mjiδki
$
dkk
√ρ
(10.36)
= mjk
$
dkk
√ρ ,
(10.37)
and thus
R(F(ρ)) =

kj
U †
kPkFjρF †
j PkUk
(10.38)
=

kj
|mjk|2dkkρ
(10.39)
∝ρ ,
(10.40)
as required.
This result enables the introduction of a more powerful language to describe quantum
error-correcting codes. Instead of talking about the class of error processes E correctable
by a code C and error-correction operation R we can talk about a set of error operators
(or simply errors) {Ei} which are correctable. By this, we mean that the quantum error-
correction conditions hold for these operators,
PEiE†
jP = αijP.
(10.41)
Theorems 10.1 and 10.2 together imply that any noise process E whose operation elements
are built from linear combinations of these error operators {Ei} will be corrected by the
recovery operation R!
Let’s look at an example of this powerful new viewpoint in action. Suppose E is a
quantum operation acting on a single qubit. Then its operation elements {Ei} can each
be written as a linear combination of the Pauli matrices σ0, σ1, σ2, σ3. Therefore, to check
that the Shor code corrects against arbitrary single qubit errors on the ﬁrst qubit it is
sufﬁcient to verify that the equations
Pσ1
iσ1
jP = αijP,
(10.42)
are satisﬁed, where σ1
i are the Pauli matrices (I, X, Y and Z) acting on the ﬁrst qubit.
Once this is done it is assured that any error process whatsoever on the ﬁrst qubit
may be corrected! (The actual calculation is quite simple, and is done as part of Ex-
ercise 10.10.) Indeed this example explains a point that can be somewhat mysterious
when ﬁrst confronted with the literature on quantum error-correction: many authors
have what appears to be a suspicious fondness for the depolarizing channel, E(ρ) =
(1 −p)ρ + p
3 (XρX + Y ρY + ZρZ). It is tempting to assume that this greatly limits the
validity of their models of error-correction, but this is not so, for as the discussion just
now implies, the ability to error-correct the depolarizing channel automatically implies
the ability to error-correct an arbitrary single qubit quantum operation.
Summarizing, we have learnt that it is possible to discretize quantum errors, that to
ﬁght the continuum of errors possible on a single qubit it is sufﬁcient merely to win the
war against a ﬁnite set of errors, the four Pauli matrices. Similar results hold for higher-
dimensional quantum systems. This stands in remarkable contrast to the theory of error-
correction for analog classical systems. Error-correction in such systems is extremely
difﬁcult because in principle there are an inﬁnite number of different error syndromes.
Theory of quantum error-correction
441
Digital error-correction for classical information processing is much more successful
because it involves only a ﬁnite number of error syndromes. The surprising thing we
have learnt is that quantum error-correction seems much more similar to classical digital
error-correction than it is to classical analog error-correction.
Exercise 10.8:
Verify that the three qubit phase ﬂip code
|0L⟩= | + ++⟩, |1L⟩= | −−−⟩satisﬁes the quantum error-correction conditions
for the set of error operators {I, Z1, Z2, Z3}.
Exercise 10.9:
Again, consider the three qubit phase ﬂip code. Let Pi and Qi be the
projectors onto the |0⟩and |1⟩states, respectively, of the ith qubit. Prove that
the three qubit phase ﬂip code protects against the error set
{I, P1, Q1, P2, Q2, P3, Q3}.
Exercise 10.10:
Explicitly verify the quantum error-correction conditions for the Shor
code, for the error set containing I and the error operators Xj, Yj, Zj for j = 1
through 9.
Exercise 10.11:
Construct operation elements for a single qubit quantum operation E
that upon input of any state ρ replaces it with the completely randomized state
I/2. It is amazing that even such noise models as this may be corrected by codes
such as the Shor code!
10.3.2
Independent error models
How can we make the connection between quantum error-correction and the criteria for
doing reliable quantum information processing introduced in Chapter 9? In this section
we explain the basic idea of how this may be done using the assumption of independent
errors on different qubits. Intuitively, if a noise process acts independently on the different
qubits in the code, then provided the noise is sufﬁciently weak error-correction should
improve the storage ﬁdelity of the encoded state over the unencoded state. To illustrate
this, we begin with the example of the depolarizing channel, which provides an especially
simple demonstration of the basic ideas, and then broaden the ideas to include other
important channels.
Recall that the depolarizing channel may be described by a single parameter, a prob-
ability p. The action of the depolarizing channel on a single qubit is deﬁned by the
equation E(ρ) = (1 −p)ρ + p/3[XρX + Y ρY + ZρZ], and can be interpreted as saying
that nothing happens to the qubit with probability 1 −p, and each of the operators X, Y
and Z is applied to the qubit with probability p/3. The depolarizing channel is especially
easy to analyze in the context of quantum error-correction because it has such a nice
interpretation in terms of the four basic errors I, X, Y and Z which are most commonly
used in the analysis of quantum codes. We’ll explain how this analysis is performed, and
then return to the question of what happens when we consider a process which doesn’t
have such a simple interpretation in terms of the I, X, Y and Z operations. A simple
calculation shows that the minimum ﬁdelity for states sent through a depolarizing channel
is given by F =
$
1 −2p/3 = 1 −p/3 + O(p2).
Exercise 10.12:
Show that the ﬁdelity between the state |0⟩and E(|0⟩⟨0|) is
442
Quantum error-correction
$
1 −2p/3, and use this to argue that the minimum ﬁdelity for the depolarizing
channel is
$
1 −2p/3.
Suppose we encode a single qubit of information in an n qubit quantum code which
corrects errors on any single qubit. Suppose the depolarizing channel with parameter p
acts independently on each of the qubits, giving rise to a joint action on all n qubits of
E⊗n(ρ) = (1 −p)nρ +
n

j=1
3

k=1
(1 −p)n−1 p
3σj
k ρ σj
k + · · · ,
(10.43)
where the ‘. . .’ indicates higher-order terms which are positive and will drop out of the
analysis. After error-correction has been performed all terms appearing in this sum will
be returned to the state ρ, provided ρ was in the code originally,
R ⊗E⊗n (ρ) =
&(1 −p)n + n(1 −p)n−1p
' ρ + · · · ,
(10.44)
so the ﬁdelity satisﬁes
F ≥
$
(1 −p)n−1(1 −p + np) = 1 −
n
2

2 p2 + O(p3) .
(10.45)
Thus, provided the probability of error p is sufﬁciently small, using the quantum error-
correcting code leads to an improvement in the ﬁdelity of the quantum states being
protected by the code.
Not all noisy channels can be interpreted so easily as a random combination of no
error, bit ﬂips, phase ﬂips and combinations of the two. Many naturally occurring quan-
tum channels have no such interpretation. Consider the example of amplitude damping
(Section 8.3.5), which has operation elements E0 and E1:
E0 =
 1
0
0
√1 −γ

;
E1 =
 0
√γ
0
0

.
(10.46)
The parameter γ is a small positive parameter characterizing the strength of the ampli-
tude damping process – as γ gets closer to zero, the strength decreases, until ultimately
we end up with an essentially noise-free quantum channel. We might reasonably guess
that the amplitude damping channel has an equivalent description in terms of a set of
operation elements including a term proportional to the identity, {f(γ)I, E′
1, E′
2, . . .},
where f(γ) →1 as γ →0. If this were the case then an analysis of error-correction
for the amplitude damping channel acting independently on multiple qubits could be
done that was similar to the analysis of error-correction performed for the depolarizing
channel. Surprisingly, it turns out that no such description is possible! This follows from
Theorem 8.2 simply because for γ > 0 no linear combination of E0 and E1 can ever
be proportional to the identity, and thus no set of operation elements for the amplitude
damping channel can ever include a term proportional to the identity.
Similarly, many other noise processes in quantum mechanics are close to the identity
in a physical sense, yet no operator-sum representation for the process contains a large
identity component. Intuitively it seems reasonable that in such a circumstance error-
correction should result in a net gain in the storage ﬁdelity for quantum information,
provided the noise is sufﬁciently weak. We will now show that this is in fact the case,
using the speciﬁc example of the amplitude damping channel for concreteness. A simple
calculation shows that the minimum ﬁdelity for the amplitude damping channel applied
Theory of quantum error-correction
443
to a single qubit is √1 −γ. Suppose the qubit is encoded in an n qubit quantum code
capable of correcting arbitrary errors on a single qubit, and that amplitude damping
channels of parameter γ act independently on each qubit. We will sketch the basic idea
showing that the effect of quantum error-correction is to change the ﬁdelity of storage
to 1 −O(γ2), so for small γ encoding the qubit in the quantum code results in a net
suppression of error.
Exercise 10.13:
Show that the minimum ﬁdelity F(|ψ⟩, E(|ψ⟩⟨ψ|)) when E is the
amplitude damping channel with parameter γ, is √1 −γ.
Using Ej,k to denote the action of Ej on the kth qubit, the effect of the noise on the
encoded qubits may be written:
E⊗n(ρ) =

E0,1 ⊗E0,2 ⊗· · · ⊗E0,n

ρ

E†
0,1 ⊗E†
0,2 ⊗· · · ⊗E†
0,n

+
n

j=1
⎡
⎢⎣E1,j ⊗
⎛
⎜
⎝
*
k ̸= j
E0,k
⎞
⎟
⎠
⎤
⎥⎦ρ
⎡
⎢⎣E†
1,j ⊗
⎛
⎜
⎝
*
k ̸= j
E†
0,k
⎞
⎟
⎠
⎤
⎥⎦
+ O(γ2) .
(10.47)
Suppose we write E0 = (1−γ/4)I+γZ/4+O(γ2), and E1 = √γ(X+iY )/2. Substituting
these expressions in (10.47) gives
E⊗n(ρ) =

1 −γ
4
2n
ρ + γ
4

1 −γ
4
2n−1
n

j=1
Zjρ + ρZj

+ γ
4

1 −γ
4
2n−2
n

j=1
Xj + iYj
 ρ
Xj −iYj
 + O(γ2) . (10.48)
Suppose ρ is a state of the code. Obviously, the effect of error-correction on ρ is to
leave it invariant! The effect of error-correction on terms like Zjρ and ρZj is most easily
understood by considering the effect on Zj|ψ⟩⟨ψ|, where |ψ⟩is a state of the code. We
suppose the code is such that the error Zj takes |ψ⟩to a subspace which is orthogonal
to the code, so that when the syndrome measurement is performed terms like Zj|ψ⟩⟨ψ|
disappear. (Note that even if this orthogonality assumption is not made a similar analysis
can still be made, by working in terms of error operators which do take the code to
orthogonal subspaces.) Thus terms like Zjρ vanish after error-correction, as do terms
like ρZj, XjρYj and YjρXj. Furthermore, error-correction takes XjρXj and YjρYj back
to ρ, since the code can correct errors on one qubit. Thus, after error-correction the state
of the system is

1 −γ
4
2n
ρ + 2nγ
4

1 −γ
4
2n−2
ρ + O(γ2) = ρ + O(γ2).
(10.49)
Thus, to order γ2 error-correction returns the quantum system to its original state ρ, and
for weak noise (small γ) error-correction results in a net suppression of errors, just as for
the depolarizing channel. Our analysis here was for the amplitude damping noise model,
but it is not difﬁcult to generalize this argument to obtain similar conclusions for other
noise models. In general, however, for the remainder of this chapter we work mainly with
noise models which can be understood as stochastic application of errors corresponding
444
Quantum error-correction
to the Pauli matrices, similar to the depolarizing channel, allowing us to do the analysis
using familiar concepts from classical probability theory. Keep in mind that the ideas we
describe can be extended beyond this simple error model to apply to a much wider range
of error models using principles similar to those we have just outlined.
10.3.3
Degenerate codes
In many respects quantum error-correcting codes are quite similar to classical codes – an
error is identiﬁed by measuring the error syndrome, and then corrected as appropriate,
just as in the classical case. There is, however, an interesting class of quantum codes known
as degenerate codes possessing a striking property unknown in classical codes. The idea
is most easily illustrated for the case of the Shor code. Consider the effect of the errors
Z1 and Z2 on the codewords for the Shor code. As we have already noted, the effect of
these errors is the same on both codewords. For classical error-correcting codes errors
on different bits necessarily lead to different corrupted codewords. The phenomenon of
degenerate quantum codes is a sort of good news–bad news situation for quantum codes.
The bad news is that some of the proof techniques used classically to prove bounds on
error-correction fall down because they can’t be applied to degenerate codes. We’ll see an
example of this in the next section and the quantum Hamming bound. The good news is
that degenerate quantum codes seem to be among the most interesting quantum codes!
In some sense they are able to ‘pack more information in’ than are classical codes, because
distinct errors do not necessarily have to take the code space to orthogonal spaces, and it
is possible (though has not yet been shown) that this extra ability may lead to degenerate
codes that can store quantum information more efﬁciently than any non-degenerate code.
10.3.4
The quantum Hamming bound
For applications we would like to make use of the ‘best’ possible quantum codes. What
‘best’ means in any given circumstance depends on the application. For this reason
we would like to have criteria for determining whether or not a code with particular
characteristics exists or not. In this section we develop the quantum Hamming bound,
a simple bound which gives some insights into the general properties of quantum codes.
Unfortunately the quantum Hamming bound only applies to non-degenerate codes, but
it gives us an idea of what more general bounds may look like. Suppose a non-degenerate
code is used to encode k qubits in n qubits in such a way that it can correct errors on
any subset of t or fewer qubits. Suppose j errors occur, where j ≤t. There are
 n
j

sets of locations where errors may occur. With each such set of locations there are three
possible errors – the three Pauli matrices X, Y, Z – that may occur in each qubit, for
a total of 3j possible errors. The total number of errors that may occur on t or fewer
qubits is therefore
t

j=0
 n
j

3j.
(10.50)
(Note that j = 0 corresponds to the case of no errors on any qubit, the ‘error’ I.) In
order to encode k qubits in a non-degenerate way each of these errors must correspond
to an orthogonal 2k-dimensional subspace. All of these subspaces must be ﬁtted into the
Constructing quantum codes
445
total 2n-dimensional space available to n qubits, leading to the inequality
t

j=0
 n
j

3j2k ≤2n ,
(10.51)
which is the quantum Hamming bound. Consider, for example, the case where we wish
to encode one qubit in n qubits in such a way that errors on one qubit are tolerated. In
this case the quantum Hamming bound reads:
2(1 + 3n) ≤2n.
(10.52)
Substitution shows that this inequality is not satisﬁed for n ≤4, while it is for values of
n ≥5. Therefore, there is no non-degenerate code encoding one qubit in fewer than ﬁve
qubits in such a way as to protect from all possible errors on a single qubit.
Of course, not all quantum codes are non-degenerate, so the quantum Hamming bound
is more useful as a rule of thumb than as a hard and fast bound on the existence of quantum
codes. (In particular, at the time of writing no codes are known that violate the quantum
Hamming bound, even allowing degenerate codes.) Later, we will have occasion to look at
some bounds on quantum codes that apply to all quantum codes, not just non-degenerate
codes. For example, in Section 12.4.3 we prove the quantum Singleton bound, which
implies that any quantum code encoding k qubits in n qubits and able to correct errors
on any t qubits must satisfy n ≥4t+k. It follows that the smallest code encoding a single
qubit and able to correct an arbitrary error on a single qubit must satisfy n ≥4 + 1 = 5,
and indeed, we will soon exhibit such a ﬁve qubit code.
10.4
Constructing quantum codes
We now have a theoretical framework for the study of quantum error-correcting codes,
but we don’t as yet have many examples of such codes! We begin to remedy this defect
by taking a brief tour into the theory of classical linear codes in Section 10.4.1, and then
in Section 10.4.2 explain how ideas from classical linear codes can be used to construct a
large class of quantum codes known as Calderbank–Shor–Steane (CSS) codes. Our job is
completed in Section 10.5 with the development of the stabilizer codes, a class of codes
even more general than the CSS codes which offers a powerful means for constructing a
wide variety of quantum codes.
10.4.1
Classical linear codes
Classical error-correcting codes have many varied and important technological applica-
tions, so it is unsurprising that a powerful theory of such codes has been developed.
Our interest in the techniques of classical error-correction is that many of these tech-
niques have important implications for quantum error-correction, especially the theory
of classical linear codes, which can be used to develop a wide variety of good quan-
tum error-correcting codes. In this section we review classical linear codes, emphasizing
especially those ideas important to quantum error-correction.
A linear code C encoding k bits of information into an n bit code space is speciﬁed
by an n by k generator matrix G whose entries are all elements of Z2, that is, zeroes and
ones. The matrix G maps messages to their encoded equivalent. Thus the k bit message
x is encoded as Gx, where the message x is treated as a column vector in the obvious
446
Quantum error-correction
way. Furthermore, the multiplication operation, and all our other arithmetic operations
in this section, are done modulo 2. As a simple example, the repetition code mapping a
single bit to three repetitions is speciﬁed by the generator matrix
G =
⎡
⎣
1
1
1
⎤
⎦,
(10.53)
since G maps the possible messages, 0 and 1, to their encoded form, G[0] = (0, 0, 0) and
G[1] = (1, 1, 1). (Recall that (a, b, . . . , z) is our shorthand notation for column vectors.)
We say that a code using n bits to encode k bits of information is an [n, k] code; this
example is therefore a [3, 1] code. A slightly more complicated example is to encode two
bits using three repetitions of each bit – a [6, 2] code. This has generator matrix
G =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
1
0
1
0
0
1
0
1
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
(10.54)
from which we see that
G(0, 0) = (0, 0, 0, 0, 0, 0);
G(0, 1) = (0, 0, 0, 1, 1, 1);
(10.55)
G(1, 0) = (1, 1, 1, 0, 0, 0);
G(1, 1) = (1, 1, 1, 1, 1, 1),
(10.56)
just as we expect. The set of possible codewords for the code corresponds to the vector
space spanned by the columns of G, so in order that all messages be uniquely encoded we
require that the columns of G be linearly independent, but otherwise place no constraints
on G.
Exercise 10.14:
Write an expression for a generator matrix encoding k bits using r
repetitions for each bit. This is an [rk, k] linear code, and should have an rk×k
generator matrix.
Exercise 10.15:
Show that adding one column of G to another results in a generator
matrix generating the same code.
A great advantage of linear codes over general error-correcting codes is their compact
speciﬁcation. A general code encoding k bits in n bits requires 2k codewords each of length
n to specify the encoding, a total of n2k bits to specify a description of the code. With a
linear code we need only specify the kn bits of the generator matrix, an exponential saving
in the amount of memory required! This compact description is mirrored in the ability to
do efﬁcient encoding and decoding, important features which classical linear codes share
with their quantum cousins, the stabilizer codes. We can already see how to perform
efﬁcient encoding of a classical linear code: one simply multiplies the k bit message by
the n by k generator matrix G to obtain the n bit encoded message, a procedure which
can be done using O(nk) operations.
One of the attractive features of the generator matrix deﬁnition of linear codes is
the transparent connection between the messages we wish to encode and how they are
encoded. It is not so clear how to perform error-correction. Error-correction for linear
Constructing quantum codes
447
codes is most easily understood by introducing an alternative (but equivalent) formulation
of linear codes in terms of parity check matrices. In this deﬁnition an [n, k] code is deﬁned
to consist of all n-element vectors x over Z2 such that
Hx = 0,
(10.57)
where H is an n −k by n matrix known as the parity check matrix, with entries all
zeroes and ones. Equivalently, but more succinctly, the code is deﬁned to be the kernel
of H. A code encoding k bits has 2k possible codewords so the kernel of H must be
k-dimensional, and therefore we require that H have linearly independent rows.
Exercise 10.16:
Show that adding one row of the parity check matrix to another does
not change the code. Using Gaussian elimination and swapping of bits it is
therefore possible to assume that the parity check matrix has the standard form
[A|In−k], where A is an (n −k)×k matrix.
To connect the parity check picture of linear codes with the generator matrix picture
we need to develop a procedure that enables us to convert back and forth between the
parity check matrix H and the generator matrix G. To go from the parity check matrix to
the generator matrix, pick k linearly independent vectors y1, . . . , yk spanning the kernel
of H, and set G to have columns y1 through yk. To go from the generator matrix to
the parity check matrix, pick n−k linearly independent vectors y1, . . . , yn−k orthogonal
to the columns of G, and set the rows of H to be yT
1 , . . . , yT
n−k. (By orthogonal, we
mean that the inner product modulo 2 must be zero.) As an example, consider the [3, 1]
repetition code deﬁned by the generator matrix (10.53). To construct H we pick out
3 −1 = 2 linearly independent vectors orthogonal to the columns of G, say (1, 1, 0) and
(0, 1, 1), and deﬁne the parity check matrix as:
H ≡
 1
1
0
0
1
1

.
(10.58)
It is easy to check that Hx = 0 only for the codewords x = (0, 0, 0) and x = (1, 1, 1).
Exercise 10.17:
Find a parity check matrix for the [6, 2] repetition code deﬁned by
the generator matrix in (10.54).
Exercise 10.18:
Show that the parity check matrix H and generator matrix G for the
same linear code satisfy HG = 0.
Exercise 10.19:
Suppose an [n, k] linear code C has a parity check matrix of the form
H = [A|In−k], for some (n −k)×k matrix A. Show that the corresponding
generator matrix is
G =

Ik
−A

.
(10.59)
(Note that −A = A since we are working modulo 2; however, this equation also
holds for linear codes over more general ﬁelds than Z2.)
The parity check matrix makes error-detection and recovery quite transparent. Sup-
pose that we encode the message x as y = Gx, but an error e due to noise corrupts y
giving the corrupted codeword y′ = y + e. (Note that + here denotes bitwise addition
448
Quantum error-correction
modulo 2.) Because Hy = 0 for all codewords, it follows that Hy′ = He. We call Hy′
the error syndrome, and it plays a role similar to the role played by the error syndrome
in quantum error-correction; it is a function Hy′ of the corrupted state y′, just as the
quantum error syndrome is determined by measuring the corrupted quantum state, and
because of the relation Hy′ = He, the error syndrome contains information about the
error that occurred that hopefully will enable recovery to the original codeword y. To
see how this is possible, suppose no errors or just one error occurred. Then the error
syndrome Hy′ is equal to 0 in the no error case and is equal to Hej when an error occurs
on the jth bit, where ej is the unit vector with component 1 in the jth component.
If we assume that errors occur on at most one bit, it is therefore possible to perform
error-correction by computing the error syndrome Hy′ and comparing it to the different
possible values of Hej to determine which (if any) bit needs to be corrected.
More generally, insight into how error-correction may be performed with a linear
code can be attained using the concept of distance. Suppose x and y are words of n bits
each. The (Hamming) distance d(x, y) between x and y is deﬁned to be the number
of places at which x and y differ. Thus d((1, 1, 0, 0), (0, 1, 0, 1)) = 2, for example. The
(Hamming) weight of a word x is deﬁned to be the distance from the string of all
zeroes, wt(x) ≡d(x, 0), that is, the number of places at which x is non-zero. Note that
d(x, y) = wt(x + y). To understand the connection with error-correction suppose we
encode x as y = Gx using a linear error-correcting code. Noise corrupts the encoded bit
string producing y′ = y+e. Provided the probability of a bit ﬂip is less than 1/2, the most
likely codeword to have been encoded is the codeword y which minimizes the number of
bit ﬂips needed to get from y to y′, that is, which minimizes wt(e) = d(y, y′). In principle,
error-correction with a linear code may be accomplished by simply replacing y′ by such
a y. In practice this may be rather inefﬁcient, since determining the minimal distance
d(y, y′) in general requires searching all 2k possible codewords y. A great deal of effort
in classical coding theory has gone into constructing codes with special structure that
enable error-correction to be performed more efﬁciently, however these constructions
are beyond the scope of this book.
The global properties of the code can also be understood using the Hamming distance.
We deﬁne the distance of a code to be the minimum distance between any two codewords,
d(C) ≡
min
x,y∈C,x ̸= y
d(x, y).
(10.60)
But d(x, y) = wt(x + y). Since the code is linear, x + y is a codeword if x and y are, so
we see that
d(C) =
min
x∈C,x ̸= 0
wt(x).
(10.61)
Setting d ≡d(C), we say that C is an [n, k, d] code. The importance of the distance is
that a code with distance at least 2t + 1 for some integer t is able to correct errors on up
to t bits, simply by decoding the corrupted encoded message y′ as the unique codeword
y satisfying d(y, y′) ≤t.
Exercise 10.20:
Let H be a parity check matrix such that any d −1 columns are
linearly independent, but there exists a set of d linearly dependent columns.
Show that the code deﬁned by H has distance d.
Constructing quantum codes
449
Exercise 10.21: (Singleton bound)
Show that an [n, k, d] code must satisfy
n −k ≥d −1.
A good illustrative class of linear error-correcting codes are the Hamming codes. Sup-
pose r ≥2 is an integer and let H be the matrix whose columns are all 2r−1 bit strings of
length r which are not identically 0. This parity check matrix deﬁnes a [2r −1, 2r −r−1]
linear code known as a Hamming code. An especially important example for quantum
error-correction is the case r = 3, which is a [7, 4] code having parity check matrix:
H =
⎡
⎣
0
0
0
1
1
1
1
0
1
1
0
0
1
1
1
0
1
0
1
0
1
⎤
⎦.
(10.62)
Any two columns of H are different, and therefore linearly independent; the ﬁrst three
columns are linearly dependent, so by Exercise 10.20 the distance of the code is 3. It
follows that this code is able to correct an error on any single bit. Indeed, the error-
correction method is very simple. Suppose an error occurs on the jth bit. Inspection
of (10.62) reveals that the syndrome Hej is just a binary representation for j, telling us
which bit to ﬂip to correct the error.
Exercise 10.22:
Show that all Hamming codes have distance 3, and thus can correct
an error on a single bit. The Hamming codes are therefore [2r −1, 2r −r −1, 3]
codes.
What can we say more generally about the properties of linear codes? In particular,
we would like conditions telling us whether or not codes with particular code parameters
exist. Not surprisingly, many techniques for proving such conditions exist. One such set
of conditions is known as the Gilbert–Varshamov bound, which states that for large n
there exists an [n, k] error-correcting code protecting against errors on t bits for some k
such that
k
n ≥1 −H
2t
n

,
(10.63)
where H(x) ≡−x log(x) −(1 −x) log(1 −x) is the binary Shannon entropy, studied
in detail in Chapter 11. The importance of the Gilbert–Varshamov bound is that it
guarantees the existence of good codes, provided one doesn’t try to encode too many bits
(k) into too small a number of bits (n). The proof of the Gilbert–Varshamov bound is
quite simple, and is left as an exercise.
Exercise 10.23:
Prove the Gilbert–Varshamov bound.
We conclude our survey of classical error-correction by explaining an important con-
struction for codes known as the dual construction. Suppose C is an [n, k] code with
generator matrix G and parity check matrix H. Then we can deﬁne another code, the
dual of C, denoted C⊥, to be the code with generator matrix HT and parity check
matrix GT. Equivalently, the dual of C consists of all codewords y such that y is or-
thogonal to all the codewords in C. A code is said to be weakly self-dual if C ⊆C⊥,
and strictly self-dual if C = C⊥. Rather remarkably, the dual construction for classical
linear codes arises naturally in the study of quantum error-correction, and is the key to
the construction of an important class of quantum codes known as CSS codes.
450
Quantum error-correction
Exercise 10.24:
Show that a code with generator matrix G is weakly self-dual if and
only if GTG = 0.
Exercise 10.25:
Let C be a linear code. Show that if x ∈C⊥then

y∈C(−1)x·y = |C|, while if x ̸∈C⊥then 
y∈C(−1)x·y = 0.
10.4.2
Calderbank–Shor–Steane codes
Our ﬁrst example of a large class of quantum error-correcting codes is the Calderbank–
Shor–Steane codes, more usually known as CSS codes, after the initials of the inventors
of this class of codes. CSS codes are an important subclass of the more general class of
stabilizer codes.
Suppose C1 and C2 are [n, k1] and [n, k2] classical linear codes such that C2 ⊂C1
and C1 and C⊥
2
both correct t errors. We will deﬁne an [n, k1 −k2] quantum code
CSS(C1, C2) capable of correcting errors on t qubits, the CSS code of C1 over C2, via
the following construction. Suppose x ∈C1 is any codeword in the code C1. Then we
deﬁne the quantum state |x + C2⟩by
|x + C2⟩≡
1
$
|C2|

y∈C2
|x + y⟩,
(10.64)
where + is bitwise addition modulo 2. Suppose x′ is an element of C1 such that x−x′ ∈
C2. Then it is easy to see that |x + C2⟩= |x′ + C2⟩, and thus the state |x + C2⟩depends
only upon the coset of C1/C2 which x is in, explaining the coset notation we have used
for |x + C2⟩. Furthermore, if x and x′ belong to different cosets of C2, then for no
y, y′ ∈C2 does x + y = x′ + y′, and therefore |x + C2⟩and |x′ + C2⟩are orthonormal
states. The quantum code CSS(C1, C2) is deﬁned to be the vector space spanned by
the states |x + C2⟩for all x ∈C1. The number of cosets of C2 in C1 is |C1|/|C2| so
the dimension of CSS(C1, C2) is |C1|/|C2| = 2k1−k2, and therefore CSS(C1, C2) is an
[n, k1 −k2] quantum code.
We can exploit the classical error-correcting properties of C1 and C⊥
2 to detect and
correct quantum errors! In fact, it is possible to error-correct up to t bit and phase ﬂip
errors on CSS(C1, C2) by making use of the error-correcting properties of C1 and C⊥
2 ,
respectively. Suppose the bit ﬂip errors are described by an n bit vector e1 with 1s where
bit ﬂips occurred, and 0s elsewhere, and the phase ﬂip errors are described by an n bit
vector e2 with 1s where phase ﬂips occurred, and 0s elsewhere. If |x+C2⟩was the original
state then the corrupted state is:
1
$
|C2|

y∈C2
(−1)(x+y)·e2|x + y + e1⟩.
(10.65)
To detect where bit ﬂips occurred it is convenient to introduce an ancilla containing
sufﬁcient qubits to store the syndrome for the code C1, and initially in the all zero state
|0⟩. We use reversible computation to apply the parity matrix H1 for the code C1, taking
|x + y + e1⟩|0⟩to |x + y + e1⟩|H1(x + y + e1)⟩= |x + y + e⟩|H1e1⟩, since (x + y) ∈C1
is annihilated by the parity check matrix. The effect of this operation is to produce the
state:
1
$
|C2|

y∈C2
(−1)(x+y)·e2|x + y + e1⟩|H1e1⟩.
(10.66)
Constructing quantum codes
451
Exercise 10.26:
Suppose H is a parity check matrix. Explain how to compute the
transformation |x⟩|0⟩→|x⟩|Hx⟩using a circuit composed entirely of
controlled-
s.
Error-detection for the bit ﬂip errors is completed by measuring the ancilla to obtain the
result H1e1 and discarding the ancilla, giving the state
1
$
|C2|

y∈C2
(−1)(x+y)·e2|x + y + e1⟩.
(10.67)
Knowing the error syndrome H1e1 we can infer the error e1 since C1 can correct up to t
errors, which completes the error-detection. Recovery is performed simply by applying
gates to the qubits at whichever positions in the error e1 a bit ﬂip occurred, removing
all the bit ﬂip errors and giving the state
1
$
|C2|

y∈C2
(−1)(x+y)·e2|x + y⟩.
(10.68)
To detect phase ﬂip errors we apply Hadamard gates to each qubit, taking the state to
1
$
|C2|2n

z

y∈C2
(−1)(x+y)·(e2+z)|z⟩,
(10.69)
where the sum is over all possible values for n bit z. Setting z′ ≡z + e2, this state may
be rewritten:
1
$
|C2|2n

z′

y∈C2
(−1)(x+y)·z′|z′ + e2⟩.
(10.70)
(The next step appeared as Exercise 10.25 on page 450.) Supposing z′ ∈C⊥
2 it is easy
to see that 
y∈C2(−1)y·z′ = |C2|, while if z′ ̸∈C⊥
2 then 
y∈C2(−1)y·z′ = 0. Thus the
state may be rewritten:
1
$
2n/|C2|

z′∈C⊥
2
(−1)x·z′|z′ + e2⟩,
(10.71)
which looks just like a bit ﬂip error described by the vector e2! As for the error-detection
for bit ﬂips we introduce an ancilla and reversibly apply the parity check matrix H2 for
C⊥
2 to obtain H2e2, and correct the ‘bit ﬂip error’ e2, obtaining the state
1
$
2n/|C2|

z′∈C⊥
2
(−1)x·z′|z′⟩.
(10.72)
The error-correction is completed by again applying Hadamard gates to each qubit; we
can either compute the result of these gates directly, or note that the effect is to apply
Hadamard gates to the state in (10.71) with e2 = 0; since the Hadamard gate is self-inverse
this takes us back to the state in (10.68) with e2 = 0:
1
$
|C2|

y∈C2
|x + y⟩,
(10.73)
which is the original encoded state!
One important application of CSS codes is to prove a quantum version of the Gilbert–
Varshamov bound, guaranteeing the existence of good quantum codes. This states that
452
Quantum error-correction
in the limit as n becomes large, an [n, k] quantum code protecting against errors on up
to t qubits exists for some k such that
k
n ≥1 −2H
2t
n

.
(10.74)
Thus, good quantum error-correcting codes exist, provided one doesn’t try to pack too
many qubits k into an n qubit code. The proof of the Gilbert–Varshamov bound for CSS
codes is rather more complex than the proof of the classical Gilbert–Varshamov bound,
due to the constraints on the classical codes C1 and C2, and is left as an end of chapter
problem.
Summarizing, suppose C1 and C2 are [n, k1] and [n, k2] classical linear codes, respec-
tively, such that C2 ⊂C1, and both C1 and C⊥
2 can correct errors on up to t bits. Then
CSS(C1, C2) is an [n, k1 −k2] quantum error-correcting code which can correct arbitrary
errors on up to t qubits. Furthermore, the error-detection and correction steps require
only the application of Hadamard and controlled-
gates, in each case a number linear
in the size of the code. Encoding and decoding can also be performed using a number of
gates linear in the size of the code, but we won’t discuss that here; it’s discussed later in
more generality in Section 10.5.8.
Exercise 10.27:
Show that the codes deﬁned by
|x + C2⟩≡
1
$
|C2|

y∈C2
(−1)u·y|x + y + v⟩
(10.75)
and parameterized by u and v are equivalent to CSS(C1, C2) in the sense that
they have the same error-correcting properties. These codes, which we’ll refer to
as CSSu,v(C1, C2), will be useful later in our study of quantum key distribution,
in Section 12.6.5.
The Steane code
An important example of a CSS code may be constructed using the [7, 4, 3] Hamming
code whose parity check matrix we recall here:
H =
⎡
⎣
0
0
0
1
1
1
1
0
1
1
0
0
1
1
1
0
1
0
1
0
1
⎤
⎦.
(10.76)
Suppose we label this code C and deﬁne C1 ≡C and C2 ≡C⊥. To use these codes to
deﬁne a CSS code we need ﬁrst to check that C2 ⊂C1. By deﬁnition the parity check
matrix of C2 = C⊥is equal to the transposed generator matrix of C1 = C:
H[C2] = G[C1]T =
⎡
⎢⎢⎣
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
0
1
0
1
1
0
0
0
0
1
1
1
1
⎤
⎥⎥⎦.
(10.77)
Exercise 10.28:
Verify that the transpose of the matrix in (10.77) is the generator of
the [7, 4, 3] Hamming code.
Stabilizer codes
453
Comparing with (10.76) we see that the span of the rows of H[C2] strictly contains the
span of the rows of H[C1], and since the corresponding codes are the kernels of H[C2]
and H[C1] we conclude that C2 ⊂C1. Furthermore, C⊥
2 = (C⊥)⊥= C, so both C1 and
C⊥
2 are distance 3 codes which can correct errors on 1 bit. Since C1 is a [7, 4] code and
C2 is a [7, 3] code it follows that CSS(C1, C2) is a [7, 1] quantum code which can correct
errors on a single qubit.
This [7, 1] quantum code has nice properties that make it easy to work with, and will
be used in many of the examples for the remainder of this chapter. It is known as the
Steane code, after its inventor. The codewords of C2 are easily determined from (10.77)
and Exercise 10.28. Rather than write them out explicitly, we write them out implicitly
as the entries in the logical |0L⟩for the Steane code, |0 + C2⟩:
|0L⟩=
1
√
8

|0000000⟩+ |1010101⟩+ |0110011⟩+ |1100110⟩
+|0001111⟩+ |1011010⟩+ |0111100⟩+ |1101001⟩

.
(10.78)
To determine the other logical codeword we need to ﬁnd an element of C1 that is not in
C2. An example of such an element is (1, 1, 1, 1, 1, 1, 1), giving:
|1L⟩=
1
√
8

|1111111⟩+ |0101010⟩+ |1001100⟩+ |0011001⟩
+|1110000⟩+ |0100101⟩+ |1000011⟩+ |0010110⟩

.
(10.79)
10.5
Stabilizer codes
We cannot clone, perforce; instead, we split
Coherence to protect it from that wrong
That would destroy our valued quantum bit
And make our computation take too long.
Correct a ﬂip and phase – that will sufﬁce.
If in our code another error’s bred,
We simply measure it, then God plays dice,
Collapsing it to X or Y or zed.
We start with noisy seven, nine, or ﬁve
And end with perfect one. To better spot
Those ﬂaws we must avoid, we ﬁrst must strive
To ﬁnd which ones commute and which do not.
With group and eigenstate, we’ve learned to ﬁx
Your quantum errors with our quantum tricks.
– ‘Quantum Error Correction Sonnet’, by Daniel Gottesman
Stabilizer codes, sometimes known as additive quantum codes, are an important class of
quantum codes whose construction is analogous to classical linear codes. In order to un-
derstand stabilizer codes it is useful to ﬁrst develop the stabilizer formalism, a powerful
method for understanding a wide class of operations in quantum mechanics. The applica-
tions of the stabilizer formalism extend far beyond quantum error-correction; however, in
454
Quantum error-correction
this book our main concern is with this speciﬁc application. After deﬁning the stabilizer
formalism, we explain how unitary gates and measurements may be described using it,
and an important theorem which quantiﬁes the limitations of stabilizer operations. We
then present stabilizer constructions for stabilizer codes, along with explicit examples, a
useful standard form, and circuits for encoding, decoding, and correction.
10.5.1
The stabilizer formalism
The central insight of the stabilizer formalism is easily illustrated by an example. Consider
the EPR state of two qubits
|ψ⟩= |00⟩+ |11⟩
√
2
.
(10.80)
It is easy to verify that this state satisﬁes the identities X1X2|ψ⟩= |ψ⟩and Z1Z2|ψ⟩=
|ψ⟩; we say that the state |ψ⟩is stabilized by the operators X1X2 and Z1Z2. A little
less obviously, the state |ψ⟩is the unique quantum state (up to a global phase) which is
stabilized by these operators X1X2 and Z1Z2. The basic idea of the stabilizer formalism
is that many quantum states can be more easily described by working with the operators
that stabilize them than by working explicitly with the state itself. This claim is perhaps
rather surprising at ﬁrst sight; nevertheless it is true. It turns out that many quantum
codes (including CSS codes and the Shor code) can be much more compactly described
using stabilizers than in the state vector description. Even more importantly, errors on
the qubits and operations such as the Hadamard gate, phase gate, and even the controlled-
gate and measurements in the computational basis are all easily described using the
stabilizer formalism!
The key to the power of the stabilizer formalism lies in the clever use of group theory,
whose basic elements are reviewed in Appendix 2. The group of principal interest is the
Pauli group Gn on n qubits. For a single qubit, the Pauli group is deﬁned to consist of
all the Pauli matrices, together with multiplicative factors ±1, ±i:
G1 ≡{±I, ±iI, ±X, ±iX, ±Y, ±iY, ±Z, ±iZ}.
(10.81)
This set of matrices forms a group under the operation of matrix multiplication. You
might wonder why we don’t omit the multiplicative factors ±1 and ±i; the reason these
are included is to ensure that G1 is closed under multiplication, and thus forms a legitimate
group. The general Pauli group on n qubits is deﬁned to consist of all n-fold tensor
products of Pauli matrices, and again we allow multiplicative factors ±1, ±i.
We can now deﬁne stabilizers a little more precisely. Suppose S is a subgroup of Gn
and deﬁne VS to be the set of n qubit states which are ﬁxed by every element of S. VS is
the vector space stabilized by S, and S is said to be the stabilizer of the space VS, since
every element of VS is stable under the action of elements in S. You should convince
yourself of the truth of the following simple exercise:
Exercise 10.29:
Show that an arbitrary linear combination of any two elements of VS
is also in VS. Therefore, VS is a subspace of the n qubit state space. Show that
VS is the intersection of the subspaces ﬁxed by each operator in S (that is, the
eigenvalue one eigenspaces of elements of S).
Let’s look at a simple example of the stabilizer formalism in action, a case with n = 3
Stabilizer codes
455
qubits and S ≡{I, Z1Z2, Z2Z3, Z1Z3}. The subspace ﬁxed by Z1Z2 is spanned by |000⟩,
|001⟩, |110⟩and |111⟩, and the subspace ﬁxed by Z2Z3 is spanned by |000⟩, |100⟩,|011⟩
and |111⟩. Note that the elements |000⟩and |111⟩are common to both these lists. With
these observations and a little thought one realizes that VS must be the subspace spanned
by the states |000⟩and |111⟩.
In this example we determined VS simply by looking at the subspaces stabilized by
two of the operators in S. This is a manifestation of an important general phenomenon –
the description of a group by its generators. As explained in Appendix 2 a set of elements
g1, . . . , gl in a group G is said to generate the group G if every element of G can be
written as a product of elements from the list g1, . . . , gl, and we write G = ⟨g1, . . . , gl⟩.
In the example S = ⟨Z1Z2, Z2Z3⟩as Z1Z3 = (Z1Z2)(Z2Z3) and I = (Z1Z2)2. The great
advantage of using generators to describe groups is that they provide a compact means of
describing the group. Indeed, in Appendix 2 we show that a group G with size |G| has a
set of at most log(|G|) generators. Furthermore, to see that a particular vector is stabilized
by a group S we need only check that the vector is stabilized by the generators, since it is
then automatically stabilized by products of the generators, making this a most convenient
representation. (The notation ⟨· · ·⟩which we use for group generators may potentially be
confused with the notation for observable averages introduced in Section 2.2.5 beginning
on page 87; however, in practice, it is always clear from context how the notation is being
used.)
Not just any subgroup S of the Pauli group can be used as the stabilizer for a non-
trivial vector space. For example, consider the subgroup of G1 consisting of {±I, ±X}.
Obviously the only solution to (−I)|ψ⟩= |ψ⟩is |ψ⟩= 0, and thus {±I, ±X} is the
stabilizer for the trivial vector space. What conditions must be satisﬁed by S in order
that it stabilize a non-trivial vector space, VS? Two conditions easily seen to be necessary
are that (a) the elements of S commute, and (b) −I is not an element of S. We don’t
yet have all the tools to prove it, but will show later that these two conditions are also
sufﬁcient for VS to be non-trivial.
Exercise 10.30:
Show that −I ̸∈S implies ±iI ̸∈S.
To see that these two conditions are necessary, suppose VS is non-trivial, so it contains
a non-zero vector |ψ⟩. Let M and N be elements of S. Then M and N are tensor
products of Pauli matrices, possibly with an overall multiplicative factor; because the
Pauli matrices all commute or anti-commute with one another, it follows that M and N
must either commute or anti-commute. To establish condition (a), that they commute,
we suppose that M and N anti-commute and show that this leads to a contradiction.
By assumption −NM = MN so we have −|ψ⟩= −NM|ψ⟩= MN|ψ⟩= |ψ⟩, where
the ﬁrst and last equalities follow from the fact that M and N stabilize |ψ⟩. So we have
−|ψ⟩= |ψ⟩, which implies that |ψ⟩is the zero vector, which is the desired contradiction.
To establish condition (b), that −I ̸∈S just note that if −I is an element of S then we
have −I|ψ⟩= |ψ⟩, which again leads to a contradiction.
Exercise 10.31:
Suppose S is a subgroup of Gn generated by elements g1, . . . , gl.
Show that all the elements of S commute if and only if gi and gj commute for
each pair i, j.
A beautiful example of the stabilizer formalism is provided by the seven qubit Steane
456
Quantum error-correction
Name
Operator
g1
I I I XXXX
g2
I XX I I XX
g3
X I X I X I X
g4
I I I ZZZZ
g5
I ZZ I I ZZ
g6
Z I Z I Z I Z
Figure 10.6. Stabilizer generators for the Steane seven qubit code. The entries represent tensor products on the
respective qubits; for example, ZIZIZIZ = Z ⊗I ⊗Z ⊗I ⊗Z ⊗I ⊗Z = Z1Z3Z5Z7.
code. It turns out that the six generators g1 through g6 listed in Figure 10.6 generate
a stabilizer for the code space of the Steane code. Observe how clean and compact this
description is when compared with the rather messy speciﬁcation in terms of state vec-
tors, (10.78) and (10.79); even further advantages will manifest when we examine quantum
error-correction from this viewpoint. Note also the similarity in structure between the
generators in Figure 10.6 and the parity check matrices for the classical linear codes C1
and C⊥
2 used in the construction of the Steane code. (Recall that for the Steane code
C1 = C⊥
2 is the Hamming [7, 4, 3] code with parity check matrix given by (10.76).) The
ﬁrst three generators of the stabilizer have Xs in locations corresponding to the locations
of the 1s in the parity check matrix for C1, while the ﬁnal three generators g4 through g6
have Zs in locations corresponding to the locations of the 1s in the parity check matrix
for C⊥
2 . With these observations in hand the solution to the following exercise becomes
almost self-evident.
Exercise 10.32:
Verify that the generators in Figure 10.6 stabilize the codewords for
the Steane code, as described in Section 10.4.2.
This use of the stabilizer formalism to describe a quantum code foreshadows our later
use of stabilizers to describe a wide class of quantum codes, but for now it is important to
appreciate that there is nothing special about the Steane code’s status as a quantum code
– it is merely a subspace of a vector space which happens to have a description using
stabilizers.
In practice, we want our generators g1, . . . , gl to be independent in the sense that
removing any generator gi makes the group generated smaller,
⟨g1, . . . , gi−1, gi+1, . . . , gl⟩̸= ⟨g1, . . . , gl⟩.
(10.82)
Determining whether a particular set of generators is independent or not is rather time-
consuming with our current understanding; fortunately, there is a simple way this can
be done based on an idea known as the check matrix, so-named because it plays a role
in the theory of stabilizer codes analogous to the parity check matrix in classical linear
codes.
Suppose S = ⟨g1, . . . , gl⟩. There is an extremely useful way of presenting the gen-
erators g1, . . . , gl of S using the check matrix. This is an l×2n matrix whose rows
correspond to the generators g1 through gl; the left hand side of the matrix contains 1s
to indicate which generators contain Xs, and the right hand side contains 1s to indicate
Stabilizer codes
457
which generators contain Zs; the presence of a 1 on both sides indicates a Y in the
generator. More explicitly, the ith row is constructed as follows. If gi contains an I on
the jth qubit then the jth and n + jth column elements are 0; if it contains an X on the
jth qubit then the jth column element is a 1 and the n + jth column element is a 0; if it
contains a Z on the jth qubit then the jth column element is 0 and the n + jth column
element is 1; if it contains a Y on the jth qubit then both the jth and n + jth columns
are 1. In the case of the Steane seven qubit code we can read the check matrix off from
Figure 10.6:
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
1
1
1
1
0
0
0
0
0
0
0
0
1
1
0
0
1
1
0
0
0
0
0
0
0
1
0
1
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
0
0
0
0
0
0
0
0
1
1
0
0
1
1
0
0
0
0
0
0
0
1
0
1
0
1
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(10.83)
The check matrix doesn’t contain any information about the multiplicative factors out
the front of the generators, but it does contain much other useful information, so much
so that we use r(g) to denote the 2n-dimensional row vector representation of an element
g of the Pauli group. Suppose we deﬁne a 2n×2n matrix Λ by
Λ =
 0
I
I
0

(10.84)
where the I matrices on the off-diagonals are n×n. Elements g and g′ of the Pauli group
are easily seen to commute if and only if r(g)Λr(g′)T = 0; the formula xΛyT deﬁnes
a sort of ‘twisted’ inner product between row matrices x and y expressing whether the
elements of the Pauli group corresponding to x and y commute or not.
Exercise 10.33:
Show that g and g′ commute if and only if r(g)Λr(g′)T = 0. (In the
check matrix representation, arithmetic is done modulo two.)
Exercise 10.34:
Let S = ⟨g1, . . . , gl⟩. Show that −I is not an element of S if and
only if g2
j = I for all j, and gj ̸= −I for all j.
Exercise 10.35:
Let S be a subgroup of Gn such that −I is not an element of S.
Show that g2 = I for all g ∈S, and thus g† = g.
A useful connection between independence of the generators and the check matrix is
established by means of the following proposition:
Proposition 10.3: Let S = ⟨g1, . . . , gl⟩be such that −I is not an element of S. The
generators g1 through gl are independent if and only if the rows of the
corresponding check matrix are linearly independent.
Proof
We prove the contrapositive. Note ﬁrst that g2
i must equal I for all i, by Exercise 10.35.
Observe that r(g) + r(g′) = r(gg′), so addition in the row vector representation corre-
sponds to multiplication of group elements. Thus the rows of the check matrix are linearly
dependent, 
i air(gi) = 0, and aj ̸= 0 for some j, if and only if +
i gai
i
is equal to the
458
Quantum error-correction
identity, up to an overall multiplicative factor. But −I ̸∈S so the multiplicative factor
must be 1, and the last condition corresponds to the condition gj = g−1
j
= +
i ̸= j gai
i ,
and thus g1, . . . , gl are not independent generators.
The following innocuous looking proposition is surprisingly useful, and can be imme-
diately leveraged into a proof that VS is of dimension 2k when S is generated by l = n−k
independent commuting generators, and −I ̸∈S. We will use the proposition repeatedly
throughout the remainder of this chapter. Once again the tool of choice in the proof is
the check matrix representation.
Proposition 10.4: Let S = ⟨g1, . . . , gl⟩be generated by l independent generators and
satisfy −I ̸∈S. Fix i in the range 1, . . . , l. Then there exists g ∈Gn such that
ggig† = −gi and ggjg† = gj for all j ̸= i.
Proof
Let G be the check matrix associated to g1, . . . , gl. The rows of G are linearly independent
by Proposition 10.3, so there exists a 2n-dimensional vector x such that GΛx = ei, where
ei is the l-dimensional vector with a 1 in the ith position and 0s elsewhere. Let g be
such that r(g) = xT. Then by deﬁnition of x we have r(gj)Λr(g)T = 0 for j ̸= i and
r(gi)Λr(g)T = 1, and thus ggig† = −gi and ggjg† = gj for j ̸= i.
We conclude our look at the most basic elements of the stabilizer formalism by ful-
ﬁlling our earlier promise that VS is non-trivial provided S is generated by independent
commuting generators and −I ̸∈S. Indeed, if there are l = n −k generators, then it is
at least plausible (and we will prove) that VS is 2k-dimensional, based on the intuitive
argument that each additional generator for the stabilizer cuts the dimension of VS by a
factor of 1/2, as we might naively expect because the +1 and −1 eigenspaces for a ten-
sor product of Pauli matrices divide the total Hilbert space into two subspaces of equal
dimension.
Proposition 10.5: Let S = ⟨g1 . . . , gn−k⟩be generated by n −k independent and
commuting elements from Gn, and such that −I ̸∈S. Then VS is a
2k-dimensional vector space.
In all our later discussion of the stabilizer formalism we use the convention that sta-
bilizers are always described in terms of independent commuting generators such that
−I ̸∈S.
Proof
Let x = (x1, . . . , xn−k) be a vector of n −k elements of Z2. Deﬁne
P x
S ≡
+n−k
j=1 (I + (−1)xjgj)
2n−k
.
(10.85)
Because (I + gj)/2 is the projector onto the +1 eigenspace of gj, it is easy to see that
P (0,...,0)
S
must be the projector onto VS. By Proposition 10.4 for each x there exists gx in
Gn such that gxP (0,...,0)
S
(gx)† = P x
S , and therefore the dimension of P x
S is the same as the
dimension of VS. Furthermore, for distinct x the P x
S are easily seen to be orthogonal.
Stabilizer codes
459
The proof is concluded with the algebraic observation that
I =

x
P x
S .
(10.86)
The left hand side is a projector onto a 2n-dimensional space, while the right hand side
is a sum over 2n−k orthogonal projectors of the same dimension as VS, and thus the
dimension of VS must be 2k.
10.5.2
Unitary gates and the stabilizer formalism
We have been discussing the use of the stabilizer formalism to describe vector spaces.
The formalism can also be used to describe the dynamics of those vector spaces in the
larger state space, under a variety of interesting quantum operations. Aside from the
intrinsic interest of understanding quantum dynamical operations, this goal is especially
relevant because we will describe quantum error-correcting codes using the stabilizer
formalism, and would like an elegant means for understanding the effects of noise and
other dynamical processes on those codes. Suppose we apply a unitary operation U to a
vector space VS stabilized by the group S. Let |ψ⟩be any element of VS. Then for any
element g of S,
U|ψ⟩= Ug|ψ⟩= UgU †U|ψ⟩,
(10.87)
and thus the state U|ψ⟩is stabilized by UgU †, from which we deduce that the vector
space UVS is stabilized by the group USU † ≡{UgU †|g ∈S}. Furthermore, if g1, . . . , gl
generate S, then Ug1U †, . . . , UglU † generate USU †, so to compute the change in the
stabilizer we need only compute how it affects the generators of the stabilizer.
The great advantage of this approach to dynamics is that for certain special unitary
operations U this transformation of the generators takes on a particularly appealing form.
Suppose, for example, that we apply a Hadamard gate to a single qubit. Note that
HXH† = Z;
HY H† = −Y ;
HZH† = X.
(10.88)
As a consequence we correctly deduce that after a Hadamard gate is applied to the
quantum state stabilized by Z (|0⟩), the resulting state will be stabilized by X (|+⟩).
Not very impressive, you may think! Imagine though that we had n qubits in a state
whose stabilizer is ⟨Z1, Z2, . . . , Zn⟩. It is easy to see that this is the state |0⟩⊗n. Applying
the Hadamard gate to each of the n qubits we see that the state afterwards has a stabilizer
⟨X1, X2, . . . , Xn⟩; again it is easy to see that this is just the familiar state which is an equal
superposition of all computational basis states. What is remarkable about this example
is that the usual (state vector) description of the ﬁnal state requires 2n amplitudes to
be speciﬁed, compared with the description provided by the generators: ⟨X1, . . . , Xn⟩,
which is linear in n! Still, you might say, after applying the Hadamard gate to each
of the n qubits there is no entanglement in the quantum computer, so it is not so
surprising that a compact description can be obtained. But much more is possible within
the stabilizer formalism, including an efﬁcient description of the controlled-
, which
together with the Hadamard gate is capable of generating entanglement. To understand
how this works, consider how the operators X1, X2, Z1 and Z2 behave under conjugation
by the controlled-
. Denoting by U the controlled-
gate with qubit 1 as control
460
Quantum error-correction
Operation
Input
Output
controlled-
X1
X2
Z1
Z2
X1X2
X2
Z1
Z1Z2
H
X
Z
Z
X
S
X
Z
Y
Z
X
X
Z
X
−Z
Y
X
Z
−X
−Z
Z
X
Z
−X
Z
Figure 10.7. Transformation properties of elements of the Pauli group under conjugation by various common
operations. The controlled-
has qubit 1 as the control and qubit 2 as the target.
and qubit 2 as the target, we have
UX1U † =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0
⎤
⎥⎥⎦
= X1X2 .
(10.89)
Similar calculations show that UX2U † = X2, UZ1U † = Z1 and UZ2U † = Z1Z2. To
see how U conjugates other operators in the two qubit Pauli group we need only take
products of the facts we already know. For example, to calculate UX1X2U † we observe
that UX1X2U † = UX1U †UX2U † = (X1X2)X2 = X1. The Y Pauli matrices may
be dealt with similarly, as for example UY2U † = iUX2Z2U † = iUX2U †UZ2U † =
iX2(Z1Z2) = Z1Y2.
Exercise 10.36:
Explicitly verify that UX1U † = X1X2, UX2U † = X2, UZ1U † = Z1,
and UZ2U † = Z1Z2. These and other useful conjugation relations for the
Hadamard, phase, and Pauli gates are summarized in Figure 10.7.
Exercise 10.37:
What is UY1U †?
As an example of using the stabilizer formalism to understand unitary dynamics,
consider the swap circuit introduced in Section 1.3.4 on page 23; for convenience, the
circuit is illustrated in Figure 10.8. Consider the way the operators Z1 and Z2 transform
Stabilizer codes
461
by conjugation by the gates in the circuit. The operator Z1 transforms through the
sequence Z1 →Z1 →Z1Z2 →Z2 and the operator Z2 transforms through the sequence
Z2 →Z1Z2 →Z1 →Z1. Similarly, X1 →X2 and X2 →X1 under the circuit. Of
course, if we take U to be the swap operator then it is obvious that UZ1U † = Z2 and
UZ2U † = Z1, and similarly for X1 and X2, just as for the circuit in Figure 10.8. Proving
that this implies that the circuit implements U is left as an exercise:
Exercise 10.38:
Suppose U and V are unitary operators on two qubits which
transform Z1, Z2, X1, and X2 by conjugation in the same way. Show this implies
that U = V .
Figure 10.8. Circuit swapping two qubits.
The example of the swap circuit is interesting but doesn’t do justice to the feature
of the stabilizer formalism which makes it truly useful – the ability to describe certain
types of quantum entanglement. We’ve already seen that the stabilizer formalism can be
used to describe Hadamard gates and controlled-
gates, and of course these gates
together can be used to create entangled states (compare Section 1.3.6). We will see that
the stabilizer formalism can in fact be used to describe a wide class of entangled states,
including many quantum error-correcting codes.
What gates other than the Hadamard and controlled-
gates can be described within
the stabilizer formalism? The most important addition to this set is the phase gate, a single
qubit gate whose deﬁnition we now recall,
S =
 1
0
0
i

.
(10.90)
The action of the phase gate by conjugation on the Pauli matrices is easily computed:
SXS† = Y ;
SZS† = Z .
(10.91)
Exercise 10.39:
Verify (10.91).
Indeed, it turns out that any unitary operation taking elements of Gn to elements of
Gn under conjugation can be composed from the Hadamard, phase and controlled-
gates. By deﬁnition, we say the set of U such that UGnU † = Gn is the normalizer of Gn,
and denote it by N(Gn), so we are claiming that the normalizer of Gn is generated by the
Hadamard, phase and controlled-
gates, in view of which the Hadamard, phase and
controlled-
gates are sometimes referred to simply as the normalizer gates. The proof
of this result is simple but instructive, and you will work through it in Exercise 10.40 on
page 462.
Theorem 10.6: Suppose U is any unitary operator on n qubits with the property that
462
Quantum error-correction
if g ∈Gn then UgU † ∈Gn. Then up to a global phase U may be composed
from O(n2) Hadamard, phase and controlled-
gates.
Exercise 10.40:
Provide an inductive proof of Theorem 10.6 as follows.
(1) Prove that the Hadamard and phase gates can be used to perform any
normalizer operation on a single qubit.
(2) Suppose U is an n + 1 qubit gate in N(Gn+1) such that UZ1U † = X1 ⊗g
and UX1U † = Z1 ⊗g′ for some g, g′ ∈Gn. Deﬁne U ′ on n qubits by
U ′|ψ⟩≡
√
2⟨0|U(|0⟩⊗|ψ⟩). Use the inductive hypothesis to show that the
construction for U in Figure 10.9 may be implemented using O(n2)
Hadamard, phase and controlled-
gates.
(3) Show that any gate U ∈N(Gn+1) may be implemented using O(n2)
Hadamard, phase and controlled-
gates.
Figure 10.9. Construction used to prove that the Hadamard, phase and controlled-
gates generate the
normalizer N(Gn).
We’ve seen that many interesting quantum gates are in the normalizer N(Gn); are there
any gates which aren’t? It turns out that most quantum gates are outside the normalizer.
Two gates of particular interest which are not in the normalizer are the π/8 and Toffoli
gates. Letting U denote the Toffoli gate with qubits 1 and 2 as controls and qubit 3 as
the target, and recalling that T denotes the π/8 gate, we can easily calculate the action
by conjugation of the π/8 and Toffoli gates on Pauli matrices as
TZT † = Z
TXT † = X + Y
√
2
(10.92)
and
UZ1U † = Z1
UX1U † = X1 ⊗I + Z2 + X3 −Z2X3
2
(10.93)
UZ2U † = Z2
UX2U † = X2 ⊗I + Z1 + X3 −Z1X3
2
(10.94)
UX3U † = X3
UZ3U † = Z3 ⊗I + Z1 + Z2 −Z1Z2
2
.
(10.95)
Unfortunately, this makes analyzing quantum circuits including π/8 and Toffoli gates via
the stabilizer formalism much less convenient than circuits which only contain Hadamard,
phase and controlled-
gates. Fortunately, encoding, decoding, error-detection and
recovery for stabilizer quantum codes can all be accomplished using only such normalizer
gates, so the stabilizer formalism is very convenient for the analysis of such codes!
Exercise 10.41:
Verify Equations (10.92) through (10.95).
Stabilizer codes
463
10.5.3
Measurement in the stabilizer formalism
We have explained how a limited class of unitary gates may be conveniently described
within the stabilizer formalism, but even more is true! Measurements in the computational
basis may also be easily described within the stabilizer formalism. To understand how
this works, imagine we make a measurement of g ∈Gn (recall that g is a Hermitian
operator, and can thus be regarded as an observable in the sense of Section 2.2.5). For
convenience we assume without loss of generality that g is a product of Pauli matrices
with no multiplicative factor of −1 or ±i out the front. The system is assumed to be
in a state |ψ⟩with stabilizer ⟨g1, . . . , gn⟩. How does the stabilizer of the state transform
under this measurement? There are two possibilities:
• g commutes with all the generators of the stabilizer.
• g anti-commutes with one or more of the generators of the stabilizer. Suppose the
stabilizer has generators g1, . . . , gn, and that g anti-commutes with g1. Without loss
of generality we may assume that g commutes with g2, . . . , gn, since if it does not
commute with one of these elements (say g2) then it is easy to verify that g does
commute with g1g2, and we simply replace the generator g2 by g1g2 in our list of
generators for the stabilizer.
In the ﬁrst instance it follows that either g or −g is an element of the stabilizer by the
following argument. Since gjg|ψ⟩= ggj|ψ⟩= g|ψ⟩for each stabilizer generator, g|ψ⟩is
in VS and is thus a multiple of |ψ⟩. Because g2 = I it follows that g|ψ⟩= ±|ψ⟩, whence
either g or −g must be in the stabilizer. We assume that g is in the stabilizer, with
the discussion for −g proceeding analogously. In this instance g|ψ⟩= |ψ⟩and thus a
measurement of g yields +1 with probability one, and the measurement does not disturb
the state of the system, and thus leaves the stabilizer invariant.
What about the second instance, when g anti-commutes with g1 and commutes with
all the other generators of the stabilizer? Note that g has eigenvalues ±1 and so the
projectors for the measurement outcomes ±1 are given by (I ± g)/2, respectively, and
thus the measurement probabilities are given by
p(+1) = tr
I + g
2
|ψ⟩⟨ψ|

(10.96)
p(−1) = tr
I −g
2
|ψ⟩⟨ψ|

.
(10.97)
Using the facts that g1|ψ⟩= |ψ⟩and gg1 = −g1g gives
p(+1) = tr
I + g
2
g1|ψ⟩⟨ψ|

(10.98)
= tr

g1
I −g
2
|ψ⟩⟨ψ|

.
(10.99)
Applying the cyclic property of trace we may take g1 to the right hand end of the trace
and absorb it into ⟨ψ| using g1 = g†
1 (Exercise 10.35 on page 457), giving
p(+1) = tr
I −g
2
|ψ⟩⟨ψ|

= p(−1).
(10.100)
Since p(+1) + p(−1) = 1 we deduce that p(+1) = p(−1) = 1/2. Suppose the result +1
occurs. In this instance the new state of the system is |ψ+⟩≡(I + g)|ψ⟩/
√
2, which has
464
Quantum error-correction
stabilizer ⟨g, g2, . . . , gn⟩. Similarly, if the result −1 occurs the posterior state is stabilized
by ⟨−g, g2, . . . , gn⟩.
10.5.4
The Gottesman–Knill theorem
The results about using stabilizers to describe unitary dynamics and measurements may
be summarized in the remarkable Gottesman–Knill theorem:
Theorem 10.7: (Gottesman–Knill theorem) Suppose a quantum computation is
performed which involves only the following elements: state preparations in the
computational basis, Hadamard gates, phase gates, controlled-
gates, Pauli
gates, and measurements of observables in the Pauli group (which includes
measurement in the computational basis as a special case), together with the
possibility of classical control conditioned on the outcome of such measurements.
Such a computation may be efﬁciently simulated on a classical computer.
We have already implicitly proved the Gottesman–Knill theorem. The way the clas-
sical computer performs the simulation is simply to keep track of the generators of the
stabilizer as the various operations are being performed in the computation. For exam-
ple, to simulate a Hadamard gate we simply update each of the n generators describing
the quantum state. Similarly, simulation of the state preparation, phase gate, controlled-
gate, Pauli gates, and measurements of observables in the Pauli group may all be
done using O(n2) steps on a classical computer, so that a quantum computation involv-
ing m operations from this set can be simulated using O(n2m) operations on a classical
computer.
The Gottesman–Knill theorem highlights how subtle is the power of quantum compu-
tation. It shows that some quantum computations involving highly entangled states may
be simulated efﬁciently on classical computers. Of course, not all quantum computations
(and therefore, not all types of entanglement) can be described efﬁciently within the sta-
bilizer formalism, but an impressive variety can be. Consider that interesting quantum
information processing tasks such as quantum teleportation (Section 1.3.7) and super-
dense coding (Section 2.3) can be performed using only the Hadamard gate, controlled-
gate, and measurements in the computational basis, and can therefore be efﬁciently
simulated on a classical computer, by the Gottesman–Knill theorem. Moreover, we will
shortly see that a wide variety of quantum error-correcting codes can be described within
the stabilizer formalism. There is much more to quantum computation than just the
power bestowed by quantum entanglement!
Exercise 10.42:
Use the stabilizer formalism to verify that the circuit of Figure 1.13
on page 27 teleports qubits, as claimed. Note that the stabilizer formalism
restricts the class of states being teleported, so in some sense this is not a
complete description of teleportation, nevertheless it does allow an
understanding of the dynamics of teleportation.
10.5.5
Stabilizer code constructions
The stabilizer formalism is ideally suited to the description of quantum codes. In this
section we describe how this may be done, and use it to illustrate several important codes,
such as Shor’s nine qubit code, CSS codes, and a ﬁve qubit code which is the smallest
Stabilizer codes
465
code that can be used to protect against the effects of arbitrary errors on a single qubit.
The basic idea is very simple: an [n, k] stabilizer code is deﬁned to be the vector space
VS stabilized by a subgroup S of Gn such that −I ̸∈S and S has n −k independent
and commuting generators, S = ⟨g1, . . . , gn−k⟩. We denote this code C(S).
What are the logical basis states for the code C(S)? In principle, given n−k generators
for the stabilizer S we can choose any 2k orthonormal vectors in the code C(S) to act
as our logical computational basis states. In practice it makes a great deal more sense to
choose the states in a more systematic way. One method is as follows. First, we choose
operators ¯Z1, . . . , ¯Zk ∈Gn such that g1, . . . , gn−k, ¯Z1, . . . , ¯Zk forms an independent
and commuting set. (We explain in detail how this may be done a little later.) The ¯Zj
operator plays the role of a logical Pauli sigma z operator on logical qubit number j,
so the logical computational basis state |x1, . . . , xk⟩L is therefore deﬁned to be the state
with stabilizer
⟨g1, . . . , gn−k, (−1)x1 ¯Z1, . . . , (−1)xk ¯Zk⟩.
(10.101)
Similarly, we deﬁne ¯Xj to be that product of Pauli matrices which takes ¯Zj to −¯Zj under
conjugation, and leaves all other ¯Zi and gi alone when acting by conjugation. Clearly ¯Xj
has the effect of a quantum
gate acting on the jth encoded qubit. The operator
¯Xj satisﬁes ¯Xjgk ¯X†
j = gk, and thus commutes with all the generators of the stabilizer.
It is also easy to check that ¯Xj commutes with all the ¯Zi except for ¯Zj, with which it
anti-commutes.
How are the error-correcting properties of a stabilizer code related to the generators
of its stabilizer? Suppose we encode a state using an [n, k] stabilizer code C(S) with
stabilizer S = ⟨g1, . . . , gn−k⟩, and an error E occurs corrupting the data. In three stages
of analysis we are going to determine what types of errors can be detected using C(S)
and when recovery can be performed. First, we’ll take a look at the effect different types
of errors have on the code space, simply to gain some intuition about what types of errors
can be detected and corrected; there will be no proofs, as this stage is simply to build
intuition. The second stage is the statement and proof of a general theorem telling us
what kinds of errors can be detected and corrected by a stabilizer code, based upon the
quantum error-correction conditions. The third stage of our analysis is to give a practical
prescription for performing error-detection and recovery, using notions such as error
syndrome.
Suppose C(S) is a stabilizer code corrupted by an error E ∈Gn. What happens to
the code space when E anti-commutes with an element of the stabilizer? In this case E
takes C(S) to an orthogonal subspace, and the error can in principle be detected (and
perhaps corrected after detection) by performing an appropriate projective measurement.
If E ∈S we don’t need to worry since the ‘error’ E doesn’t corrupt the space at all.
The real danger comes when E commutes with all the elements of S but is not actually
in S, that is, Eg = gE for all g ∈S. The set of E ∈Gn such that Eg = gE for all
g ∈S is known as the centralizer of S in Gn and is denoted Z(S). In fact, for the
stabilizer groups S of concern to us, the centralizer is identical to a more familiar group,
the normalizer of S, denoted N(S), which is deﬁned to consist of all elements E of Gn
such that EgE† ∈S for all g ∈S.
Exercise 10.43:
Show that S ⊆N(S) for any subgroup S of Gn.
466
Quantum error-correction
Exercise 10.44:
Show that N(S) = Z(S) for any subgroup S of Gn not containing
−I.
These observations about the effect of various types of error operator E motivate the
statement and proof of the following theorem, which is essentially just a translation of the
quantum error-correction conditions (Theorem 10.1) into the terms of stabilizer codes.
Theorem 10.8: (Error-correction conditions for stabilizer codes) Let S be the
stabilizer for a stabilizer code C(S). Suppose {Ej} is a set of operators in Gn
such that E†
jEk ̸∈N(S) −S for all j and k. Then {Ej} is a correctable set of
errors for the code C(S).
Without loss of generality we can restrict ourselves to considering errors Ej in Gn
such that E†
j = Ej, which reduces the error-correction conditions for stabilizer codes to
having EjEk ̸∈N(S) −S for all j and k.
Proof
Let P be the projector onto the code space C(S). For given j and k there are two
possibilities: either E†
jEk in S or E†
jEk in Gn −N(S). Consider the ﬁrst case. Then
PE†
jEkP = P since P is invariant under multiplication by elements of S. Suppose
E†
jEk ∈Gn −N(S) so that E†
jEk must anticommute with some element g1 of S. Let
g1, . . . , gn−k be a set of generators of S, so that
P =
+n−k
l=1 (I + gl)
2n−k
.
(10.102)
Using the anti-commutativity gives
E†
jEkP = (I −g1)E†
jEk
+n−k
l=2 (I + gl)
2n−k
.
(10.103)
But P(I −g1) = 0 since (I + g1)(I −g1) = 0 and therefore PE†
jEkP = 0 whenever
E†
jEk ∈Gn −N(S). It follows that the set of errors {Ej} satisﬁes the quantum error-
correction conditions, and thus forms a correctable set of errors.
The statement and proof of Theorem 10.8 are wonderful theoretical results, but they
don’t explicitly tell us how to perform the error-correction operation when it is in fact
possible! To understand how this is achieved, suppose g1, . . . , gn−k is a set of generators
for the stabilizer of an [n, k] stabilizer code, and that {Ej} is a set of correctable errors
for the code. Error-detection is performed by measuring the generators of the stabilizer
g1 through gn−k in turn, to obtain the error syndrome, which consists of the results of
the measurements, β1 through βn−k. If the error Ej occurred then the error syndrome
is given by βl such that EjglE†
j = βlgl. In the case when Ej is the unique error operator
having this syndrome recovery may be achieved simply by applying E†
j. In the case
when there are two distinct errors Ej and Ej′ giving rise to the same error syndrome,
it follows that EjPE†
j = Ej′PE†
j′, where P is the projector onto the code space, so
E†
jEj′PE†
j′Ej = P, whence E†
jEj′ ∈S, and thus applying E†
j after the error Ej′ has
occurred results in a successful recovery. Thus, for each possible error syndrome we
simply pick out a single error Ej with that syndrome, and apply E†
j to achieve recovery
when that syndrome is observed.
Stabilizer codes
467
Theorem 10.8 motivates the deﬁnition of a notion of distance for a quantum code
analogous to the distance for a classical code. We deﬁne the weight of an error E ∈Gn
to be the number of terms in the tensor product which are not equal to the identity. For
example, the weight of X1Z4Y8 is three. The distance of a stabilizer code C(S) is deﬁned
to be the minimum weight of an element of N(S)−S, and if C(S) is an [n, k] code with
distance d then we say that C(S) is an [n, k, d] stabilizer code. By Theorem 10.8 a code
with distance at least 2t + 1 is able to correct arbitrary errors on any t qubits, just as in
the classical case.
Exercise 10.45: (Correcting located errors)
Suppose C(S) is an [n, k, d] stabilizer
code. Suppose k qubits are encoded in n qubits using this code, which is then
subjected to noise. Fortunately, however, we are told that only d −1 of the
qubits are affected by the noise, and moreover, we are told precisely which d −1
qubits have been affected. Show that it is possible to correct the effects of such
located errors.
10.5.6
Examples
We now give a few simple examples of stabilizer codes, including already familiar codes
such as the Shor nine qubit code and CSS codes, but from the new point of view of the
stabilizer formalism. In each case, the properties of the codes follow easily by applying
Theorem 10.8 to the generators of the stabilizer. With the examples under our belt we will
turn our attention to ﬁnding quantum circuits for performing encoding and decoding.
The three qubit bit ﬂip code
Consider the familiar three qubit bit ﬂip code spanned by the states |000⟩and |111⟩, with
stabilizer generated by Z1Z2 and Z2Z3. By inspection we see that every possible product
of two elements from the error set {I, X1, X2, X3} – I, X1, X2, X3, X1X2, X1X3, X2X3
– anti-commutes with at least one of the generators of the stabilizer (except for I, which
is in S), and thus by Theorem 10.8 the set {I, X1, X2, X3} forms a correctable set of
errors for the three qubit bit ﬂip code with stabilizer ⟨Z1Z2, Z2Z3⟩.
Error-detection for the bit ﬂip code is effected by measuring the stabilizer generators,
Z1Z2 and Z2Z3. If, for example, the error X1 occurred, then the stabilizer is transformed
to ⟨−Z1Z2, Z2Z3⟩, so the syndrome measurement gives the results −1 and +1. Similarly,
the error X2 gives error syndrome −1 and −1, the error X3 gives error syndrome +1 and
−1, and the trivial error I gives error syndrome +1 and +1. In each instance recovery
is effected in the obvious way simply by applying the inverse operation to the error
indicated by the error syndrome. The error-correction operation for the bit ﬂip code is
summarized in Figure 10.10.
Of course, the procedure we have outlined is exactly the same as that described earlier
for the three qubit bit ﬂip code! All this group-theoretic analysis would hardly be worth-
while if this was all the insight we gained. The real utility of the stabilizer formalism only
starts to become apparent as we move to more complex examples.
Exercise 10.46:
Show that the stabilizer for the three qubit phase ﬂip code is
generated by X1X2 and X2X3.
468
Quantum error-correction
Z1Z2
Z2Z3
Error type
Action
+1
+1
no error
no action
+1
−1
bit 3 ﬂipped
ﬂip bit 3
−1
+1
bit 1 ﬂipped
ﬂip bit 1
−1
−1
bit 2 ﬂipped
ﬂip bit 2
Figure 10.10. Error-correction for the three qubit bit ﬂip code in the language of stabilizer codes.
Name
Operator
g1
ZZ I I I I I I I
g2
I ZZ I I I I I I
g3
I I I ZZ I I I I
g4
I I I I ZZ I I I
g5
I I I I I I ZZ I
g6
I I I I I I I ZZ
g7
XXXXXX I I I
g8
I I I XXXXXX
¯Z
XXXXXXXXX
¯X
ZZZZZZZZZ
Figure 10.11. The eight generators for the Shor nine qubit code, and the logical Z and logical X operations. (Yes,
they really are the reverse of what one might naively expect!)
The nine qubit Shor code
The stabilizer for the Shor code has eight generators, as illustrated in Figure 10.11. It is
easy to check the conditions of Theorem 10.8 for the error set containing I and all single
qubit errors. Consider, for example, single qubit errors like X1 and Y4. The product
X1Y4 anti-commutes with Z1Z2, and thus is not in N(S). Similarly, all other products
of two errors from this error set are either in S or else anti-commute with at least one
element of S and thus are not in N(S), implying that the Shor code can be used to
correct an arbitrary single qubit error.
Exercise 10.47:
Verify that the generators of Figure 10.11 generate the two
codewords of Equation (10.13).
Exercise 10.48:
Show that the operations ¯Z = X1X2X3X4X5X6X7X8X9 and
¯X = Z1Z2Z3Z4Z5Z6Z7Z8Z9 act as logical Z and X operations on a Shor-code
encoded qubit. That is, show that this ¯Z is independent of and commutes with
the generators of the Shor code, and that ¯X is independent of and commutes
with the generators of the Shor code, and anti-commutes with ¯Z.
The ﬁve qubit code
What is the minimum size for a quantum code which encodes a single qubit so that any
error on a single qubit in the encoded state can be detected and corrected? It turns out
that the answer to this question is ﬁve qubits. (See Section 12.4.3). The stabilizer for
Stabilizer codes
469
Name
Operator
g1
XZZX I
g2
I XZZX
g3
X I XZZ
g4
ZX I XZ
¯Z
ZZZZZ
¯X
XXXXX
Figure 10.12. The four generators for the ﬁve qubit code, and the logical Z and logical X operations. Note that the
last three generators may be obtained by shifting the ﬁrst right.
the ﬁve qubit code has the generators given in Figure 10.12. Because the ﬁve qubit code
is the smallest capable of protecting against a single error it might be thought that it is
the most useful code; however, for many applications it is more transparent to use the
Steane seven qubit code.
Exercise 10.49:
Use Theorem 10.8 to verify that the ﬁve qubit code can protect
against an arbitrary single qubit error.
The logical codewords for the ﬁve qubit code are
|0L⟩= 1
4

|00000⟩+ |10010⟩+ |01001⟩+ |10100⟩
+|01010⟩−|11011⟩−|00110⟩−|11000⟩
−|11101⟩−|00011⟩−|11110⟩−|01111⟩
−|10001⟩−|01100⟩−|10111⟩+ |00101⟩

(10.104)
|1L⟩= 1
4

|11111⟩+ |01101⟩+ |10110⟩+ |01011⟩
+|10101⟩−|00100⟩−|11001⟩−|00111⟩
−|00010⟩−|11100⟩−|00001⟩−|10000⟩
−|01110⟩−|10011⟩−|01000⟩+ |11010⟩

(10.105)
Exercise 10.50:
Show that the ﬁve qubit code saturates the quantum Hamming
bound, that is, it satisﬁes the inequality of (10.51) with equality.
CSS codes and the seven qubit code
The CSS codes are an excellent example of a class of stabilizer codes, demonstrating
beautifully how easy the stabilizer formalism makes it to understand quantum code con-
struction. Suppose C1 and C2 are [n, k1] and [n, k2] classical linear codes such that
C2 ⊂C1 and C1 and C⊥
2 both correct t errors. Deﬁne a check matrix with the form
 H(C⊥
2 )
0
0
H(C1)

.
(10.106)
To see that this deﬁnes a stabilizer code, we need the check matrix to satisfy the commuta-
tivity condition H(C⊥
2 )H(C1)T = 0. But we have H(C⊥
2 )H(C1)T = [H(C1)G(C2)]T = 0
470
Quantum error-correction
because of the assumption C2 ⊂C1. Indeed, it’s an easy exercise to see that this code is
exactly CSS(C1, C2), and that it is capable of correcting arbitrary errors on any t qubits.
The seven qubit Steane code is an example of a CSS code, whose check matrix we
have already seen, in Equation (10.83). Encoded Z and X operators may be deﬁned for
the Steane code as
¯Z ≡Z1Z2Z3Z4Z5Z6Z7;
¯X ≡X1X2X3X4X5X6X7 .
(10.107)
Exercise 10.51:
Verify that the check matrix deﬁned in (10.106) corresponds to the
stabilizer of the CSS code CSS(C1, C2), and use Theorem 10.8 to show that
arbitrary errors on up to t qubits may be corrected by this code.
Exercise 10.52:
Verify by direct operation on the codewords that the operators of
(10.107) act appropriately, as logical Z and X.
10.5.7
Standard form for a stabilizer code
The construction of the logical Z and X operators for a stabilizer code is made much
easier to understand if we put the code into standard form. To understand what the
standard form is, consider the check matrix for an [n, k] stabilizer code C:
G =
&G1|G2
' .
(10.108)
This matrix has n −k rows. Swapping rows of this matrix corresponds to relabeling
generators, swapping corresponding columns on both sides of the matrix to relabeling
qubits, and adding two rows corresponds to multiplying generators; it is easy to see that
we may always replace a generator gi by gigj when i ̸= j. Thus there is an equivalent code
with a different set of generators whose corresponding check matrix corresponds to the
matrix G where Gaussian elimination has been done on G1, swapping qubits as necessary:
r{
n −k −r{

r
,-./
I
n−r
,-./
A
r
,-./
B
n−r
,-./
C
0
0
D
E

,
(10.109)
where r is the rank of G1. Next, swapping qubits as necessary we perform a Gaussian
elimination on E to obtain
r{
n −k −r −s{
s{
⎡
⎣
r
,-./
I
n−k−r−s
,-./
A1
k+s
,-./
A2
r
,-./
B
n−k−r−s
,-./
C1
k+s
,-./
C2
0
0
0
D1
I
E2
0
0
0
D2
0
0
⎤
⎦. (10.110)
The last s generators cannot commute with the ﬁrst r generators unless D2 = 0, and thus
we may assume that s = 0. Furthermore, we may also set C1 = 0 by taking appropriate
linear combinations of rows, so our check matrix has the form:
r{
n −k −r{

r
,-./
I
n−k−r
,-./
A1
k
,-./
A2
r
,-./
B
n−k−r
,-./
0
k
,-./
C
0
0
0
D
I
E

,
(10.111)
where we have relabeled E2 as E and D1 as D. It is not difﬁcult to see that this procedure
is not unique; however, we will say that any code with check matrix in the form (10.111)
is in standard form.
Given the standard form for a quantum code it is easy to deﬁne encoded Z operations
Stabilizer codes
471
for the code. That is, we have to pick k operators independent of the generators of
the stabilizer and of one another, yet commuting with one another, and also with the
generators of the stabilizer. Suppose we write the check matrix for these k encoded Z
operators in the form Gz = [F1F2F3|F4F5F6], where all the matrices have k rows, and
the respective column sizes are r, n −k −r, k, r, n −k −r and k. We choose these
matrices such that Gz = [000|AT
2 0I]. The commutativity of these encoded Z operations
with the elements of the stabilizer follows from the equation I × (AT
2 )T + A2 = 0, and it
is clear that the encoded Z operations commute with one another since they only contain
products of Z operators. The independence of the encoded Z operators from the ﬁrst r
generators of the stabilizer follows from the fact that no X terms appear in the deﬁnition
of the encoded Z operators, the independence from the set of n−k−r generators follows
from the (n−k −r)×(n−k −r) identity matrix appearing in the check matrix for those
generators, and the lack of any corresponding terms in the check matrix for the encoded
Z operators. In a similar way we may pick the encoded X operators, with k×2n check
matrix [0ETI|CT 00].
Exercise 10.53:
Prove that the encoded Z operators are independent of one another.
Exercise 10.54:
Prove that with the check matrix for the encoded X operators deﬁned
as above, the encoded X operators are independent of one another and of the
generators, commute with the generators of the stabilizer, with each other, and
¯Xj commutes with all the ¯Zk except ¯Zj, with which it anti-commutes.
As an example we bring the check matrix for the Steane code (Equation 10.83) into
standard form. We have n = 7 and k = 1 for this code, and inspection of the check
matrix shows that the rank of the σx part is r = 3. The matrix may be brought into
standard form by swapping qubits 1 and 4, 3 and 4, and 6 and 7, then by adding row 6
to row 4, then row 6 to row 5, and ﬁnally adding rows 4 and 5 to row 6. The resulting
standard form is:
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
1
1
1
0
0
0
0
0
0
0
0
1
0
1
0
1
1
0
0
0
0
0
0
0
0
0
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
0
0
1
0
0
0
0
0
0
0
0
1
1
0
1
0
1
0
0
0
0
0
0
0
1
1
1
0
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(10.112)
We read off A2 = (1, 1, 0) and thus the encoded Z has check matrix [0000000|1100001],
which corresponds to ¯Z = Z1Z2Z7. Recalling that qubits 1 and 4, 3 and 4, 6 and 7
were swapped, this corresponds to an encoded Z of ¯Z = Z2Z4Z6 in the original code.
This may appear rather puzzling in the light of Equation (10.107), which states that the
encoded Z is ¯Z = Z1Z2Z3Z4Z5Z6Z7; however, the puzzle is resolved by noting that
the two ‘different’ encoded Z operations differ only by a factor Z1Z3Z5Z7, which is an
element of the stabilizer of the Steane code, and thus both have the same effect on Steane
code states.
Exercise 10.55:
Find the ¯X operator for the standard form of the Steane code.
Exercise 10.56:
Show that replacing an encoded X or Z operator by g times that
472
Quantum error-correction
operator, where g is an element of the stabilizer, does not change the action of
the operator on the code.
Exercise 10.57:
Give the check matrices for the ﬁve and nine qubit codes in standard
form.
10.5.8
Quantum circuits for encoding, decoding, and correction
One of the features of stabilizer codes is that their structure enables systematic construc-
tion of procedures for encoding, decoding, and error-correction. We describe the general
method ﬁrst, then present some explicit circuit constructions as examples. Let us begin
with a general case, an [n, k] stabilizer code with generators g1, . . . , gn−k, and logical Z
operators Z1, . . . , ¯Zk.
Preparing an encoded |0⟩⊗k state, which is the standard state for beginning a quantum
computation, is quite simple. To do this we can start with any easily-prepared state –
say, the state |0⟩⊗n – and measure each of the observables g1, . . . , gn−k, Z1, . . . , Zk in
turn. Depending on the measurement outcomes the resulting quantum state will have
stabilizer ⟨±g1, . . . , ±gn−k, ±Z1, . . . , ±Zk⟩, with the various signs being determined by
the respective measurement outcomes. The signs of all the stabilizer generators and the
¯Zj can then be ﬁxed up by applying products of Pauli operators, as described in the proof
of Proposition 10.4, resulting in a state with stabilizer ⟨g1, . . . , gn−k, ¯Z1, . . . , ¯Zk⟩, that is,
to the encoded |0⟩⊗k. Once this state is prepared it is possible to change it to an arbitrary
encoded computational basis state |x1, . . . , xk⟩by applying the appropriate operators
from the set ¯X1, . . . , ¯Xk. Of course, this approach to encoding has the disadvantage that
it is not unitary. To obtain fully unitary encoding a different approach based upon the
standard form of the check matrix may be used; this approach is outlined in Problem 10.3.
Also, if you wish to encode an unknown state, this can also be done systematically, starting
from an encoded |0⟩⊗k state, as explained in Problem 10.4. For our purposes it will be
sufﬁcient to prepare encoded |0⟩⊗k states.
Decoding quantum codes is also quite simple, however it is worth explaining why,
for many purposes, a full decoding is not necessary. It turns out that the techniques of
fault-tolerant quantum computation can be used to perform logical operations directly
on encoded data, without the need to decode the data. Furthermore, the output of a
computation performed in this way can be directly determined simply by measuring the
logical Z operators, without the need to decode and measure in the computational basis.
Thus, doing a fully unitary decoding which preserves the encoded quantum information
is not so important for our purposes. If such a decoding procedure is desired for some
reason – perhaps one is using quantum error-correcting codes to transmit information
over a noisy communication channel – then it may be achieved by running the unitary
encoding circuit found in Problem 10.3 backwards.
The error-correction procedure for a stabilizer code has already been described in
Section 10.5.5, and is much like the encoding procedure: simply measure each of the
generators g1, . . . gn−k in turn, obtaining the error syndrome β1, . . . , βn−k. Classical
computation is then used to determine from βj the required recovery operations E†
j.
The key to constructing encoding, decoding, and error-correction circuits in each of
the above descriptions is understanding how to measure operators. Recall that this is
a generalization of the normal projective measurements we have widely used, in which
the objective is to project a state into an eigenstate of the operator and to obtain both
Stabilizer codes
473
the projected state, and an indicator of the eigenvalue. If this reminds you of the phase
estimation algorithm of Chapter 5, it’s no coincidence! Recall from that chapter, and
from Exercise 4.34 on page 188, that the circuit shown in Figure 10.13 can be used to
measure the single qubit operator M (with eigenvalues ±1), given a gate which performs
a controlled-M operation. Two useful versions of this circuit, which measure X and Z,
are given in Figures 10.14 and 10.15.

________

_ _ _ _ _ _ _ _

Figure 10.13. Quantum circuit for measuring a single qubit operator M with eigenvalues ±1. The top qubit is the
ancilla used for the measurement, and the bottom qubit is being measured.
H
•
H

________

_ _ _ _ _ _ _ _

⊕

________

_ _ _ _ _ _ _ _

X
H
•
H
=
|0⟩
|0⟩
Figure 10.14. Quantum circuit for measuring the X operator. Two equivalent circuits are given; the one on the left
is the usual construction (as in Figure 10.13), and the one on the right is a useful equivalent circuit.

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Figure 10.15. Quantum circuit for measuring the Z operator. Two equivalent circuits are given; the one on the left
is the usual construction (as in Figure 10.13), and the one on the right is a useful simpliﬁcation.
Of course, there is nothing special about the fact that M is a single qubit operator: the
circuit in Figure 10.13 works just as well if we replace the second qubit with a bundle of
qubits, and M is an arbitrary Hermitian operator with eigenvalues ±1. Such operators
include, for example, the products of Pauli operators we need to measure during the
encoding, decoding and error-correction procedures for stabilizer codes.
As a concrete example, consider the syndrome measurement and encoding procedures
for the seven qubit Steane code. A convenient starting point is the standard form of the
check matrix for the code, Equation (10.112), because we can immediately read off the
generators we need to measure directly from this matrix. Speciﬁcally, recall that the left
block corresponds to X generators, and the right, Z, so the quantum circuit shown in
Figure 10.16 immediately results. Note how the location of the zeroes and ones in the
matrix corresponds to the location of the targets for the gates in the left half (which
measure X), and the targets in the right half (which measure Z). This circuit can be
used to perform error-correction by following the measurement results with products
of Pauli operators on the code qubits to correct the errors. Or, by adding an additional
474
Quantum error-correction
measurement of ¯Z and ﬁxing the signs in the generators of the stabilizer, as described
earlier, the circuit can be used to prepare the encoded logical state |0L⟩.
H
•
H

________

_ _ _ _ _ _ _ _

H
•
H

________

_ _ _ _ _ _ _ _

H
•
H

________

_ _ _ _ _ _ _ _

H
•
H

________

_ _ _ _ _ _ _ _

H
•
H

________

_ _ _ _ _ _ _ _

H
•
H

________

_ _ _ _ _ _ _ _

X
Z
Z
X
Z
Z
X
Z
Z
Z
X
X
Z
X
X
Z
X
X
X
Z
X
X
Z
Z
|0⟩
|0⟩
|0⟩
|0⟩
|0⟩
|0⟩
Figure 10.16. Quantum circuit for measuring the generators of the Steane code, to give the error syndrome. The
top six qubits are the ancilla used for the measurement, and the bottom seven are the code qubits.
Exercise 10.58:
Verify that the circuits in Figures 10.13–10.15 work as described, and
check the claimed circuit equivalences.
Exercise 10.59:
Show that by using the identities of Figures 10.14 and 10.15, the
syndrome circuit of Figure 10.16 can be replaced with the circuit of Figure 10.17
Exercise 10.60:
Construct a syndrome measuring circuit analogous to that in
Figure 10.16, but for the nine and ﬁve qubit codes.
Exercise 10.61:
Describe explicit recovery operations E†
j corresponding to the
different possible error syndromes that may be measured using the circuit in
Figure 10.16.
10.6
Fault-tolerant quantum computation
One of the most powerful applications of quantum error-correction is not merely the
protection of stored or transmitted quantum information, but the protection of quantum
information as it dynamically undergoes computation. Remarkably, it turns out that
arbitrarily good quantum computation can be achieved even with faulty logic gates, pro-
vided only that the error probability per gate is below a certain constant threshold. Over
Fault-tolerant quantum computation
475
⊕
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
H
•
|0⟩
|0⟩
|0⟩
|0⟩
|0⟩
|0⟩
Figure 10.17. Quantum circuit equivalent to the one in Figure 10.16.
the next few sections we explain the techniques of fault-tolerant quantum computation
which are used to achieve this remarkable result. We look ﬁrst at the big picture in Sec-
tion 10.6.1, before examining in detail the various elements of fault-tolerant quantum
computation in Sections 10.6.2 and 10.6.3, and conclude in Section 10.6.4 with a discus-
sion of some of the limitations and possible extensions of the fault-tolerant constructions.
Note that a rigorous discussion of the many subtleties of fault-tolerant quantum com-
putation lies somewhat beyond our scope; the interested reader is referred to the end of
chapter ‘History and further reading’.
10.6.1
Fault-tolerance: the big picture
The theory of fault-tolerant quantum computation integrates many different ideas en
route to the threshold condition. We now describe each of these ideas in turn. We begin
with the notion of computing on encoded data, and explain how the fundamental problems
of error propagation and error accumulation require that our circuits for computing on
encoded data satisfy certain fault-tolerance criteria. We then introduce our fundamental
noise model for quantum circuits, which allows us to give more precise deﬁnitions for
the notion of a fault-tolerant operation. We work through a speciﬁc example of a fault-
tolerant operation in action – the fault-tolerant controlled-
– explaining how it can be
used to prevent the propagation and accumulation of errors. We conclude by explaining
how fault-tolerant operations can be combined with a procedure known as concatenation
to obtain the threshold theorem for quantum computation, and give a simple estimate
for the threshold.
476
Quantum error-correction
Fundamental issues
The basic idea of fault-tolerant quantum computation is to compute directly on encoded
quantum states in such a manner that decoding is never required. Suppose we are given
a simple quantum circuit such as that shown in Figure 10.18. Unfortunately, noise afﬂicts
each of the elements used to build this circuit – the state preparation procedures, quantum
logic gates, measurement of the output, and even the simple transmission of quantum
information along the quantum wires. To combat the effect of this noise we replace each
qubit in the original circuit with an encoded block of qubits, using an error-correcting
code such as the seven qubit Steane code, and replace each gate in the original circuit
with a procedure for performing an encoded gate acting on the encoded state, as shown
in Figure 10.19. By performing error-correction periodically on the encoded state we
prevent accumulation of errors in the state. Of course, merely performing error-correction
periodically is not sufﬁcient to prevent the build-up of errors, even if it is applied after
every encoded gate. The reasons for this are two-fold. First, and most importantly, the
encoded gates can cause errors to propagate. For example, the encoded controlled-
illustrated in Figure 10.20 may cause an error on the encoded control qubit to propagate
to the encoded target qubit. Thus errors in the qubits forming the encoded control
qubit can propagate to become errors in the encoded target qubit. Encoded gates should
therefore be designed very carefully so that a failure anywhere during the procedure for
performing the encoded gate can only propagate to a small number of qubits in each
block of the encoded data, in order that error-correction will be effective at removing the
errors. Such procedures for performing encoded gates are referred to as fault-tolerant
procedures, and we will show that it is possible to perform a universal set of logical
operations – the Hadamard, phase, controlled-
and π/8 gates – using fault-tolerant
procedures. The second issue that must be addressed is that error-correction itself can
introduce errors on the encoded qubits, so we must be careful to design error-correction
procedures that do not introduce too many errors into the encoded data. This can be
achieved using techniques similar to those used to prevent propagation of errors by
encoded gates, by taking care to ensure that failures during the procedure for error-
correction do not propagate to cause too many errors in the encoded data.

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Figure 10.18. A simple quantum circuit. If each component in the circuit fails with probability p then the
probability of an error at the output is O(p).
Fault-tolerant operations: deﬁnitions
Let’s pin down a little more precisely what it means for a particular procedure imple-
menting an encoded quantum gate to be fault-tolerant. We deﬁne the fault-tolerance of
a procedure to be the property that if only one component in the procedure fails then
the failure causes at most one error in each encoded block of qubits output from the
procedure. For example, the failure of a single component in a fault-tolerant recovery
Fault-tolerant quantum computation
477
Figure 10.19. A simulation of the circuit in Figure 10.18, using encoded qubits and encoded logical operations. If
fault-tolerant procedures are used to perform all the operations then the probability of error at the output is O(p2),
where p is the probability for any individual component in the circuit to fail. An interesting feature is the second
error-correction step performed on the second qubit. The ‘operation’ being error-corrected by this step appears to
be trivial: nothing happens to the qubit at all! Nevertheless, simply storing qubits for a period of time introduces
errors into the qubits, and should be periodically error-corrected in order to prevent the accumulation of errors.
× •
⊕
−→
• ×
⊕×
Figure 10.20. A controlled-
gate can cause an error to propagate so that instead of affecting one qubit, it
affects two. This is also true when encoded qubits are used, and an encoded controlled-
is implemented, as
discussed in the text.
procedure for quantum error-correction results in the recovery procedure being per-
formed correctly, up to an error on a single qubit of the output. By ‘component’ we
mean any of the elementary operations used in the encoded gate, which might include
noisy gates, noisy measurements, noisy quantum wires, and noisy state preparations. This
deﬁnition of a fault-tolerant procedure for a quantum gate is sometimes generalized in
the literature to cope with some of the more subtle issues that arise in the theory of
fault-tolerant computation, but at our level of detail it is sufﬁcient.
Of course, encoded quantum gates aren’t all we wish to perform during our quantum
computations. It is also useful to deﬁne the notions of a fault-tolerant measurement
procedure, and fault-tolerant state preparation. A procedure for measuring an observable
on a set of encoded qubits is said to be fault-tolerant if the failure of any single component
in the procedure results in an error in at most one qubit in each encoded block of qubits
at the output of the procedure. Furthermore, we require that if only one component fails
then the measurement result reported must have probability of error of O(p2), where p is
the (maximum) probability of a failure in any one of the components used to implement
the measurement procedure. A procedure for preparing a ﬁxed encoded state is said to
be fault-tolerant if, given that a single component failed during the procedure, there is at
most a single qubit in error in each encoded block of qubits output from the procedure.
To make these notions of fault-tolerance more precise, we need to be a little more
speciﬁc about our error model. One of the major simpliﬁcations we are going to make
in our analysis is to describe errors on qubits as being of one of four types: I, X, Y
or Z, occurring stochastically with appropriate probabilities. We allow correlated errors
to occur on two qubits, when performing gates such as the controlled-
, but again
will assume that they are of the form of a tensor product of Pauli matrices, occurring
with some probability. This probabilistic analysis enables us to use familiar concepts
from classical probability theory to determine the total probability that the output from
a circuit is correct or not. In more sophisticated presentations of fault-tolerance (see
478
Quantum error-correction
‘History and further reading’) much more general error models can be considered, for
example, allowing correlated errors of arbitrary type across several qubits. Nevertheless,
the techniques used in those more sophisticated analyses are essentially generalizations
of those we describe, combined with the deep insight we obtained earlier in the chapter
that to perform error-correction of a continuum of possible errors it sufﬁces to correct a
discrete set of errors.
With our noise model at hand, we can be more precise about what we mean when we
say that an error ‘propagates’ through the circuit. Consider, for example, the
gate
in Figure 10.20. Imagine that an X error on the ﬁrst qubit occurs just before the
is
applied. If the unitary operator for the
gate is denoted U, then the effective action
of the circuit is UX1 = UX1U †U = X1X2U, that is, it is as though the controlled-
was applied correctly, but an X error occurred on both qubits after the
. Over the
remainder of this chapter we repeatedly use this trick of conjugating errors through gates
to study how errors propagate through our circuits. A slightly more challenging example
of error propagation is to suppose it is the
gate itself which fails. What happens
then? Suppose our noisy
gate implements the quantum operation E. Then this
may be rewritten as E = E ◦U −1 ◦U, where U is the quantum operation implementing a
perfect
gate. Thus, the noisy
gate is equivalent to applying a perfect
followed by the operation E ◦U −1, which is approximately the identity if our noisy
is reasonably good, and can be understood in our usual error model of tensor products
such as X ⊗Z occurring on the two qubits with some small probability p.
Over the next few sections we explain in detail procedures for performing each class of
fault-tolerant operations we have described – fault-tolerant quantum logic with a universal
set of gates, fault-tolerant measurement, and fault-tolerant state preparation. The actual
constructions we describe are for the Steane code, but they generalize fairly easily to the
case of more general stabilizer codes. For now, however, we imagine that we have all of
these procedures at our disposal. How can we put them together to perform quantum
computation?
Example: fault-tolerant controlled-
Let’s examine a procedure for implementing a fault-tolerant controlled-
gate, fol-
lowed by a fault-tolerant error-correction step, as illustrated in Figure 10.21. Our analysis
of this circuit proceeds in 4 steps. Step 1 is the point of entry into the circuit, step 2 is
after the encoded
has been performed, step 3 is after the syndrome measurement,
and step 4 is after the recovery operation has been applied. Our goal is to show that
the probability that this circuit introduces two or more errors in the ﬁrst encoded block
behaves as O(p2), where p is the probability of failure of individual components in the
circuit. Because a (hypothetical) perfect decoding of the ﬁrst block of qubits only fails if
there are two or more errors in the block, it follows that the probability that a perfectly
decoded state contains errors is at most O(p2) larger after the action of this circuit than
it was before.
To show that this procedure introduces two errors into the ﬁrst encoded block with
probability O(p2), let’s identify all the possible ways this circuit can introduce two or
more errors into the ﬁrst encoded block of qubits at the output:
(1) There is a single pre-existing error entering the circuit at step 1 in each encoded
block of qubits. This can cause two errors in the output from the ﬁrst block because,
Fault-tolerant quantum computation
479
•
Syndrome
measurement
Recovery
⊕
Syndrome
measurement
Recovery
↑
1
↑
2
↑
3
↑
4
/
/
Figure 10.21. Block diagram of the fault-tolerant procedure construction, including error-correction.
for example, the error on the second block may propagate through the encoded
circuit to cause an error on the ﬁrst block of qubits. Provided the operations
up to this stage have been done fault-tolerantly, we can argue that the probability of
such an error entering on the ﬁrst block is at most c0p for some constant c0, since
such an error must have occurred during either the syndrome measurement or
recovery stages in the previous stage of the quantum circuit. c0 is the total number
of places at which a failure may occur during syndrome measurement or recovery in
the previous stage of the circuit. If we assume for simplicity that the probability of a
single pre-existing error entering at step 1 on the second block is also c0p, and that
these two errors occur independently, then the probability of this event is at most
c2
0p2. For the Steane code construction described below, there are contributions to
c0 from the six separate syndrome measurements, each of which has approximately
101 locations at which a failure may occur, together with a recovery operation
involving seven components, for a total of approximately c0 ≈70.
(2) A single pre-existing error enters in step 1 on either the ﬁrst or second block of
qubits, and a single failure occurs during the fault-tolerant controlled-
. The
probability of this is c1p2, where c1 is a constant deﬁned to be the number of pairs
of points at which a failure may occur. For the Steane code construction described
below, we argued previously that there are roughly 70 locations times two blocks
where a failure may have occurred causing an error to enter the circuit, for a total
of 140 locations. There are a further 7 locations at which a failure may occur during
the circuit, for a total of c1 ≈7 × 140 ≈103 locations at which a pair of failures
may occur.
(3) Two failures occur during the fault-tolerant
. This happens with probability
at most c2p2, where c2 is the number of pairs of points at which a failure may
occur. For the Steane code, c2 ≈102.
(4) A failure occurs during the
and during the syndrome measurement. The only
way two or more errors can occur at the output is if the syndrome measurement
gives the incorrect result, which occurs with probability c3p2 for some constant c3
(for the Steane code, c3 ≈102). Another case which appears to be of interest, but
which does not actually matter, is when the syndrome measurement gives the
correct result, in which case the error introduced by the
is correctly
diagnosed and corrected by recovery, leaving only a single error at the output,
introduced during syndrome measurement.
(5) Two or more failures occur during the syndrome measurement. This happens with
480
Quantum error-correction
probability at most c4p2, where c4 is the number of pairs of points at which a failure
may occur. For the Steane code, c4 ≈702 ≈5 × 103.
(6) A failure occurs during syndrome measurement and a failure during recovery. This
happens with probability at most c5p2, where c5 is the number of pairs of points at
which a failure may occur. For the Steane code, c5 ≈70 × 7 ≈500.
(7) Two or more failures occur during recovery. This happens with probability at most
c6p2, where c6 is the number of pairs of points at which a failure may occur. For
the Steane code, c6 ≈72 ≈50.
Thus, the probability that this circuit introduces two or more errors into the encoded
ﬁrst block of qubits is at most cp2 for some constant c = c2
0 + c1 + c2 + c3 + c4 + c5 + c6
which is approximately equal to 104 for the Steane code. If a perfect decoding were to be
performed at the end of the circuit, the probability of an error would therefore be at most
cp2. This is a truly remarkable result: we have managed to ﬁnd an implementation for
the
with the property that individual components may fail with probability p, but
the encoded procedure succeeds with probability 1 −cp2, and thus, provided p is small
enough, in the example, p < 10−4, there is a net gain to be had by using the encoded
procedure! Similar conclusions can be drawn for all the other operations one might wish to
do during a quantum computation, so that by doing any of our operations fault-tolerantly
we can reduce our probability of a failure from p to cp2, for some constant c. We have
estimated c for the
, however estimates for other fault-tolerant operations do not
differ all that greatly, and we will continue to use c ≈104 in our numerical estimates.
Concatenated codes and the threshold theorem
There is a beautiful construction based on concatenated codes which can be used to
reduce the effective error rate achieved by the computation even further! The idea is to
recursively apply the scheme described above for simulating a circuit using an encoded
circuit, constructing a hierarchy of quantum circuits C0 (the original circuit we wish to
simulate), C1, C2, . . .. In the ﬁrst stage of this construction, each qubit in the original
circuit is encoded in a quantum code whose qubits are themselves encoded in a quantum
code, whose own qubits are encoded yet again, and so forth ad inﬁnitum, as illustrated
in Figure 10.22. In the second stage of this construction, any given gate in the original
circuit C0, such as a Hadamard gate, is replaced in the circuit C1 by a fault-tolerant
procedure implementing an encoded Hadamard gate and error-correction. Each of the
components used in the circuit C1 is then replaced in the circuit C2 by a fault-tolerant
procedure implementing an encoded version of the component and error-correction, and
so on, ad inﬁnitum. Suppose we do two levels of concatenation, as illustrated. If the
failure probability of components at the lowest level of the code – the actual physical
qubits – is p, then the failure probability at the middle level (one level of encoding) is at
most cp2, and at the highest level (two levels of encoding) – the level at which the circuit
must function correctly if the computation is to produce the correct output – is c(cp2)2.
Thus, if we concatenate k times, the failure probability for a procedure at the highest
level is (cp)2k/c, while the size of the simulating circuit goes as dk times the size of the
original circuit, where d is a constant representing the maximum number of operations
used in a fault-tolerant procedure to do an encoded gate and error-correction.
Suppose then that we wish to simulate a circuit containing p(n) gates, where n speciﬁes
the size of some problem, and p(n) is a polynomial function in n. This might be, for
Fault-tolerant quantum computation
481

	






	






	






	




Figure 10.22. A two level concatenated code, encoding a single qubit in nine qubits. We use a three qubit code
merely to keep the ﬁgure simple; in practice a code such as the Steane code which can correct arbitrary errors on
one or more qubits would be used.
example, the circuit for the quantum factoring algorithm. Suppose we wish to achieve a
ﬁnal accuracy of ϵ in our simulation of this algorithm. To do so our simulation of each
gate in the algorithm must be accurate to ϵ/p(n), so we must concatenate a number of
times k such that
(cp)2k
c
≤
ϵ
p(n).
(10.113)
Provided p < pth ≡1/c such a k can be found. This condition – that p < pth – is known
as the threshold condition for quantum computation, since provided it is satisﬁed we can
achieve arbitrary accuracy in our quantum computations. How large a simulating circuit
is required to achieve this level of accuracy? Note that we have
dk =
log(p(n)/cϵ)
log(1/pc)
log d
= O
poly(log p(n)/ϵ)
 ,
(10.114)
where poly indicates a polynomial of ﬁxed degree, and thus the simulating circuit contains
O(poly(log p(n)/ϵ)p(n))
(10.115)
gates, which is only polylogarithmically larger than the size of the original circuit. Sum-
marizing, we have the threshold theorem for quantum computation:
Threshold theorem for quantum computation: A quantum circuit containing
p(n) gates may be simulated with probability of error at most ϵ using
O(poly(log p(n)/ϵ)p(n))
(10.116)
gates on hardware whose components fail with probability at most p, provided p is
below some constant threshold, p < pth, and given reasonable assumptions about
the noise in the underlying hardware.
482
Quantum error-correction
What is the value of pth? For the Steane code, c ≈104 according to our counting, so a
very rough estimate gives pth ≈10−4. It needs to be emphasized that our estimates are
(very) far from rigorous, however much more sophisticated calculations for the threshold
have typically yielded values in the range 10−5–10−6. Note that the precise value of the
threshold depends greatly on the assumptions made about the computational capabili-
ties! For example, if parallel operations are not possible, then the threshold condition
is impossible to achieve, because errors accumulate in the circuit too quickly for error-
correction to cope with. Classical computation is also required in addition to the quantum
operations, to process the measured syndromes and determine what quantum gates to
apply to correct errors. Some discussion of limitations on estimates of the threshold are
given in Section 10.6.4.
Exercise 10.62:
Show by explicit construction of generators for the stabilizer that
concatenating an [n1, 1] stabilizer code with an [n2, 1] stabilizer code gives an
[n1n2, 1] stabilizer code.
10.6.2
Fault-tolerant quantum logic
A key technique in the construction of fault-tolerant quantum circuits is the method of
constructing fault-tolerant operations to do logic on encoded states. In Section 4.5.3 of
Chapter 4 we learned that the Hadamard, phase, controlled-
and π/8 gates form a
universal set in terms of which any quantum computation may be expressed. We now
explain how each of these gates can be implemented fault-tolerantly.
Normalizer operations
We begin with fault-tolerant constructions for the normalizer operations – the Hadamard,
phase and controlled-
gates – for the speciﬁc case of the Steane code. By understand-
ing the basic principles underlying the constructions for this concrete example it is easy
to generalize them to any stabilizer code. Recall from Equation (10.107) that for the
Steane code the Pauli ¯Z and ¯X operators on the encoded states can be written in terms
of operators on the unencoded qubits, as
Z = Z1Z2Z3Z4Z5Z6Z7;
¯X = X1X2X3X4X5X6X7 .
(10.117)
An encoded Hadamard gate ¯H should interchange ¯Z and ¯X under conjugation, just as
the Hadamard gate interchanges Z and X under conjugation. ¯H = H1H2H3H4H5H6H7
accomplishes this task, so that a Hadamard on the encoded qubit can be implemented as
shown in Figure 10.23.
Exercise 10.63:
Suppose U is any unitary operation mapping the Steane code into
itself, and such that U ¯ZU † = ¯X and U ¯XU † = ¯Z. Prove that up to a global
phase the action of U on the encoded states |0L⟩and |1L⟩is
|0L⟩→(|0L⟩+ |1L⟩)/
√
2 and |1L⟩→(|0L⟩−|1L⟩)/
√
2.
This is a good ﬁrst step, but just doing logic on the encoded state is not sufﬁcient
to make this operation fault-tolerant! We also need to understand how errors propagate.
Because the circuit implementing ¯H = H⊗7 does not involve more than one qubit in the
encoded block in interaction, it seems physically reasonable to assume that the failure of a
single component in the circuit can cause at most one error in the block of qubits output
Fault-tolerant quantum computation
483
H
H
H
H
H
H
H
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
a|0L⟩+ b|1L⟩
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
a+b
√
2 |0L⟩+ a−b
√
2 |1L⟩
Figure 10.23. Transversal Hadamard gate on a qubit encoded in the Steane code.
from the procedure. To see that this is true, imagine an error on the ﬁrst qubit occurred
just before the encoded H gate was applied. For the sake of deﬁniteness, suppose that
the error is a Z error, so the combined operation on the qubit is HZ. As in our earlier
analysis of error propagation for the
gate, inserting the identity H†H = I gives
HZ = HZH†H = XH, so such an error is equivalent to ﬁrst applying H then the error
X occurring. Similarly, a failure during the gate operation itself is equivalent to a perfect
gate, followed by a small amount of noise acting on the qubit, which we can think of in
terms of our usual model of X, Y and Z all occurring with some small probability. The
circuit in Figure 10.23 thus really is a fault-tolerant operation, because a single failure
occurring anywhere in the procedure doesn’t propagate to affect other qubits, and thus
causes at most one error in the block of qubits output from the procedure.
Are there any general principles we can distill from the circuit in Figure 10.23? One
useful observation is that encoded gates are automatically fault-tolerant if they can be im-
plemented in a bitwise fashion, since that property ensures that a single failure anywhere
in the encoded gate introduces at most one error per block of the code, and thus error
probabilities do not grow out of the control of the error-correcting code. This property,
that an encoded gate can be implemented in a bitwise fashion, is known as the transver-
sality property of an encoded quantum gate. Transversality is interesting because it offers
a general design principle for ﬁnding fault-tolerant quantum circuits, and we see below
that many gates other than the Hadamard gate can be given transversal implementations.
Keep in mind, though, that it is possible to ﬁnd fault-tolerant constructions which aren’t
transversal, as we’ll see below with the example of the fault-tolerant π/8 gate.
Using the Steane code, many gates other than the Hadamard gate are easily given
transversal (and thus fault-tolerant) implementations. Three of the most interesting, in
addition to the Hadamard, are the phase gate and the Pauli X and Z gates. Suppose
we apply the X gates bitwise to each of the seven qubits of the Steane code. This
transforms each Z operator to −Z under conjugation, so Z →(−1)7Z = −Z and
¯X →¯X under conjugation by the bitwise application of X, and thus this circuit effects
an encoded X operation on the states of the Steane code. This circuit is transversal, and
thus is automatically fault-tolerant. In a similar way, applying Z bitwise to the states of
the Steane code gives a fault-tolerant implementation of an encoded Z. The transversal
484
Quantum error-correction
implementation of the phase gate is a little more challenging. Under conjugation, ¯S
must take ¯Z to ¯Z and ¯X to ¯Y = i ¯X ¯Z. However, applying the obvious guess ¯S =
S1S2S3S4S5S6S7 takes ¯Z to ¯Z under conjugation, and ¯X to −¯Y . The minus sign in
front of the −¯Y may be ﬁxed up by applying ¯Z. Thus, applying the operation ZS
to each qubit in the code effects an encoded phase gate, which is transversal and thus
fault-tolerant.
In contrast to the Hadamard, Pauli, and phase gates, implementing the controlled-
fault-tolerantly appears at ﬁrst to be a challenge, because it involves two separate
code blocks of seven qubits. How can we realize a
which does not introduce more
than one error per block of the code? Fortunately, this turns out to be very simple when
using the Steane code, as illustrated in Figure 10.24: it is easily seen to be effected by
seven
gates applied pairwise between the seven qubits in the two blocks! You
might worry that this transversal construction violates our own rules; after all, can’t the
controlled-
gates we are doing cause errors to propagate beyond a single qubit? This
is correct, but there is no problem, because the error propagation only ever affects at most
one qubit in another block; it does not adversely affect qubits within the same block.
Remember that affecting qubits in other blocks is okay because each block can handle
errors on single qubits!
More precisely, suppose that an X error on the ﬁrst qubit occurs just before the
controlled-
between the ﬁrst qubit of each block, which we’ll label qubits 1 and 8. If
this controlled-
gate is denoted U, then the effective action is UX1 = UX1U †U =
X1X8U, that is, it is as though the controlled-
was applied correctly, but an X error
occurred on the ﬁrst qubit of both blocks of encoded qubits! Slightly more challenging,
suppose one of the
gates fails. What happens then? Suppose our noisy
gate
implements the quantum operation E. Then this may be rewritten as E = E ◦U −1 ◦U,
where U is the quantum operation implementing a perfect
gate. Thus, the noisy
gate is equivalent to applying a perfect
followed by the operation E ◦U −1,
which is approximately the identity if our noisy
is reasonably good, and can be
understood in our usual error model of tensor products such as X ⊗Z occurring on the
two qubits with some small probability. Fortunately, while such errors involve two qubits,
they only involve a single qubit in each block of encoded qubits. Similar conclusions about
error propagation apply to errors at other locations. It follows that the failure of a single
component anywhere within the procedure we have described propagates to cause no
more than one error in each block of encoded qubits, and thus this implementation of
the encoded controlled-
is fault-tolerant.
Having found fault-tolerant implementations of the Hadamard, phase and controlled-
gates, it follows from Theorem 10.6 that any operation in the normalizer can be
realized fault-tolerantly. Of course, normalizer operations do not exhaust the complete
set of unitary gates required to do quantum computation, but this is a promising start!
Exercise 10.64: (Back propagation of errors)
It is clear that an X error on the
control qubit of a
gate propagates to the target qubit. In addition, it turns
out that a Z error on the target propagates back to the control! Show this using
the stabilizer formalism, and also directly using quantum circuit identities. You
may ﬁnd Exercise 4.20 on page 179 useful.
Fault-tolerant quantum computation
485
Figure 10.24. Transversal controlled-
between two qubits encoded in separate blocks with the Steane code.
Fault-tolerant π/8 gate
The one remaining gate we require to complete the standard set of gates for universal
quantum computation is the π/8 gate. Alternatively, as was noted in Section 4.5.3, adding
a fault-tolerant Toffoli gate to our current set of fault-tolerant Hadamard, phase and
controlled-
gates would also give us a universal set, allowing us to perform all the
gates required by a quantum computer in a fault-tolerant manner. It turns out that the
fault-tolerant π/8 gate is very simple to realize, and using a similar but more elaborate
construction a fault-tolerant Toffoli gate can be realized.
Our basic strategy in constructing the fault-tolerant π/8 gate is to split the construction
into three parts. The ﬁrst part of the construction is a simple circuit to simulate the π/8
gate using elements we already know how to do fault-tolerantly, such as the controlled-
, phase and X gates. There are, however, two parts of this circuit which we don’t yet
know how to make fault-tolerant. The ﬁrst is the preparation of an ancilla state for input
into the circuit. In order that this ancilla be adequate, we require that the failure of any
component during the ancilla preparation should lead to at most a single error in the block
of qubits making up the ancilla. We explain how such fault-tolerant ancilla preparation
can be done later in this section. The second operation we need is measurement. In
order to make the measurement fault-tolerant, we require that the failure of a single
component during the procedure for measurement should not affect the measurement
outcome. If it did, then the error would propagate to cause errors on many qubits in the
ﬁrst block, since whether the encoded SX operation is performed or not is determined
by the measurement result. How to do such a fault-tolerant measurement is described
in the next section. (Strictly speaking, for the fault-tolerant measurement procedure we
describe the measurement outcome may actually be incorrect with probability O(p2),
where p is the probability of failure of a single component. We will ignore this for the
purposes of the present discussion; it is easily dealt with by a slightly more sophisticated
analysis along similar lines.)
Figure 10.25 shows a circuit implementing a π/8 gate. All elements in the circuit can be
performed fault-tolerantly, except perhaps those in the dashed box and the measurement.
The circuit starts with two encoded qubits, one of which is the qubit |ψ⟩= a|0⟩+b|1⟩we
wish to operate on (let |0⟩and |1⟩denote logical states here). The other qubit is prepared
486
Quantum error-correction

________

_ _ _ _ _ _ _ _

Figure 10.25. Quantum circuit which fault-tolerantly implements a π/8 gate. The dashed box represents a
(non-fault-tolerant) preparation procedure for the ancilla state (|0⟩+ exp(iπ/4)|1⟩)/
√
2; how to do this preparation
fault-tolerantly is explained in the text. The slash on the wire denotes a bundle of seven qubits, and the double-line
wire represents the classical bit resulting from the measurement. Note that the ﬁnal SX operation is controlled by
the measurement result.
in the state
|Θ⟩= |0⟩+ exp(iπ/4) |1⟩
√
2
,
(10.118)
which is the state generated by the circuit in the dotted box in the ﬁgure. We explain how
this ancilla preparation step may be done fault-tolerantly in a moment. Next, perform a
fault-tolerant controlled-
operation, giving
1
√
2

|0⟩

a|0⟩+ b|1⟩

+ exp(iπ/4) |1⟩

a|1⟩+ b|0⟩

=
1
√
2

a|0⟩+ b exp(iπ/4) |1⟩

|0⟩+

b|0⟩+ a exp(iπ/4) |1⟩

|1⟩

. (10.119)
Finally, measure the second qubit, and if it is 0 then we are done. Otherwise, perform
the operation
SX =
 1
0
0
i
  0
1
1
0

(10.120)
to the remaining qubit. Either way, we are left with the state a|0⟩+ b exp(iπ/4) |1⟩up to
an irrelevant global phase, as required for a π/8 gate. This wonderful result may seem to
have come out of nowhere, but in fact it is the result of a systematic construction which is
explained in the exercises below. The same construction is used to realize a fault-tolerant
Toffoli gate, as shown in Exercise 10.68.
The construction of the fault-tolerant π/8 gate requires a fault-tolerant method to
produce the ancilla state |Θ⟩. This preparation can be achieved using the techniques
for fault-tolerant measurements, as explained in detail in the next section. For now
we explain the connection to fault-tolerant measurement. As shown in Figure 10.25,
|Θ⟩may be produced by applying a Hadamard gate and then a π/8 gate to the state
|0⟩. The state |0⟩is a +1 eigenstate of Z, so it follows that |Θ⟩is a +1 eigenstate of
THZHT † = TXT † = e−iπ/4SX. |Θ⟩can therefore be prepared by ﬁrst preparing an
encoded |0⟩, and then fault-tolerantly measuring e−iπ/4SX. If the result +1 is obtained
we conclude that the state has been correctly prepared. If the result −1 is obtained, we
have one of two options. We can either start over, repeating the procedure until the
fault-tolerant measurement of e−iπ/4SX gives the result +1, or we can use the more
elegant and efﬁcient observation that since Z SX Z = −SX, applying a fault-tolerant Z
operation changes the state from the −1 eigenstate of e−iπ/4SX to the +1 eigenstate of
Fault-tolerant quantum computation
487
e−iπ/4SX, |Θ⟩. Whichever procedure is used, a single failure anywhere in the procedure
produces an error in at most one qubit in the ancilla state |Θ⟩.
It is not difﬁcult to see that the procedure we have described is fault-tolerant as a
whole; however, it may be useful to look at an explicit example to see this. Suppose a
single component failure occurred during ancilla construction, leading to an error on a
single qubit in the ancilla. This propagates through the encoded controlled-
gate to
cause one error in each of the ﬁrst and second blocks of qubits. Fortunately, an error on
a single qubit in the second encoded qubit doesn’t affect the result of our fault-tolerant
measurement procedure, so SX is applied or not applied as appropriate, and thus the
error on the ﬁrst block of qubits propagates through to cause a single error in the output
from the encoded gate. Similarly, it is not difﬁcult to convince yourself that a single
failure anywhere else in this procedure for an encoded π/8 gate leads to an error on only
a single qubit in the output block of encoded qubits.
Exercise 10.65:
An unknown qubit in the state |ψ⟩can be swapped with a second
qubit which is prepared in the state |0⟩using only two controlled-
gates,
with the circuit
Show that the two circuits below, which use only a single
gate, with
measurement and a classically controlled single qubit operation, also accomplish
the same task:

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

Exercise 10.66: (Fault-tolerant π/8 gate construction)
One way to implement a
π/8 gate is to ﬁrst swap the qubit state |ψ⟩you wish to transform with some
known state |0⟩, then to apply a π/8 gate to the resulting qubit. Here is a
quantum circuit which does that:

________

_ _ _ _ _ _ _ _

Doing this does not seem particularly useful, but actually it leads to something
which is! Show that by using the relations TX = exp(−iπ/4)SX and TU = UT
(U is the controlled-
gate, and T acts on the control qubit) we may obtain
the circuit of Figure 10.25.
Exercise 10.67:
Show that the following circuit identities hold:
488
Quantum error-correction
(a)
(b)
Exercise 10.68: (Fault-tolerant Toffoli gate construction)
A procedure similar
to the above sequence of exercises for the π/8 gate gives a fault-tolerant Toffoli
gate.
(1) First, swap the three qubit state |xyz⟩you wish to transform with some
known state |000⟩, then apply a Toffoli gate to the resulting qubits. Show
that the following circuit accomplishes this task:

________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _


________

_ _ _ _ _ _ _ _

(2) Using the commutation rules from Exercise 10.67, show that moving the
ﬁnal Toffoli gate all the way back to the left side gives the circuit
H
•
•
•
•
X
H
•
•
Z
X
•
⊕
⊕
Z
⊕
⊕
⊕

________

_ _ _ _ _ _ _ _

•
⊕

________

_ _ _ _ _ _ _ _

•
•
H

________

_ _ _ _ _ _ _ _

|0⟩
|0⟩
|0⟩
|x⟩
|y⟩
|z⟩
|x⟩
|y⟩
|z ⊕xy⟩
(3) Assuming the ancilla preparation shown in the leftmost dotted box can be
done fault-tolerantly, show that this circuit can be used to give a
fault-tolerant implementation of the Toffoli gate using the Steane code.
Fault-tolerant quantum computation
489
10.6.3
Fault-tolerant measurement
An extremely useful and important tool in the construction of fault-tolerant circuits is the
ability to measure an operator M. Measurements are used to do encoding, read out the
result of a quantum computation, to diagnose the syndrome in error-correction, and to
do ancilla state preparation in the construction of the fault-tolerant π/8 and Toffoli gates,
and thus are absolutely crucial to fault-tolerant quantum computation. In order that a
procedure for performing an encoded measurement be considered to be fault-tolerant,
we require that two things be true in order to prevent the propagation of errors. First,
a single failure anywhere in the procedure should lead to at most one error in any block
of qubits at the end of the procedure. Second, even if a single failure occurs during the
procedure, we require that the measurement result be correct with probability 1−O(p2).
This latter requirement is extremely important, since the measurement result may be
used to control other operations in the quantum computer, and if it is incorrect then it
may propagate to affect many qubits in other blocks of encoded qubits.

________

_ _ _ _ _ _ _ _

Figure 10.26. Quantum circuit for measuring a single qubit operator M with eigenvalues ±1. The top qubit is the
ancilla used for the measurement, and the bottom qubit is being measured.
Recall that measurement of a single qubit observable M may be performed using the
circuit shown in Figure 10.26. Suppose M can be given a transversal encoded implemen-
tation on a quantum code as bitwise application of a gate M ′ to each qubit of the code.
For example, for the Steane code, M = H can be given a transversal implementation
as bitwise application of M ′ = H, while a transversal implementation of M = S uses
bitwise application of M ′ = ZS. This suggests a possible circuit for measuring the en-
coded M on the encoded data, as shown schematically in Figure 10.27. Note that a real
quantum code, such as the Steane code, would require more qubits. Unfortunately, the
circuit in Figure 10.27 is not fault-tolerant. To see this, imagine a single failure occurs
at the very beginning of the circuit, on the ancilla qubit. This will propagate forward to
affect all the encoded qubits, so the circuit is not fault-tolerant.
H
•
•
•
H

________

_ _ _ _ _ _ _ _

M ′
M ′
M ′
|0⟩
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
Encoded
Data
Figure 10.27. Schematic procedure for performing a measurement of an encoded observable M with a transversal
implementation as bitwise application of M′. The circuit is not fault-tolerant. Note that a real code would require
more than three qubits.
490
Quantum error-correction
A nice way to make the measurement circuit fault-tolerant is schematically illustrated
in Figure 10.28. For simplicity, this ﬁgure shows the data to be measured encoded in
only three qubits; in practice more qubits will be used, such as the seven qubit Steane
code, and for concreteness we imagine that it is the Steane code which is being used
here. In addition to the encoded data, the circuit introduces one ancilla qubit for each
data qubit, initially each in the state |0⟩. The ﬁrst step is to prepare the ancilla in a
‘cat’ state, |00 . . . 0⟩+ |11 . . . 1⟩. (Note that the cat state is not encoded in any code.)
The circuit used to do this preparation is not itself fault-tolerant, because a single failure
during the circuit can cause errors on multiple qubits in the cat state. Nevertheless, this
does not affect the fault-tolerance of the entire procedure, because we follow the ancilla
preparation by several veriﬁcation steps (only a single veriﬁcation step is shown in the
Figure).
H
•
•
•
H

________

_ _ _ _ _ _ _ _

⊕
•
•
⊕
⊕•
•
⊕
⊕⊕
M ′
M ′
M ′
|0⟩
|0⟩
|0⟩
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
Encoded
Data
|0⟩
Prepare Verify
Controlled-M
Decode
Figure 10.28. Schematic procedure for fault-tolerant measurement of an observable M, which is performed on
encoded data. This procedure is repeated three times and a majority vote of the measurement outcomes taken, with
the result that the measurement result is wrong with probability O(p2), where p is the failure probability of any
individual component, and a single error anywhere in the circuit produces at most one error in the data.
The veriﬁcation works as follows. The basic idea is that to verify that the state is a
cat state it is sufﬁcient to show that measurement of ZiZj for all pairs of qubits i and
j in the cat state will give 1; that is, the parity of any pair of qubits in the cat state is
even. To verify this for a particular pair ZiZj (Z2Z3 in the example) we introduce an
extra qubit, initially in the state |0⟩. We compute the parity of two of the qubits in the
ancilla by implementing two controlled-
s with the ancilla qubits as controls, and the
extra qubit as the target, before measuring the extra qubit. If the measured parity is 1
then we know that the ancilla is not in the cat state, discard it, and start again. Suppose a
single component failure occurs somewhere during this sequence of parity checks. This
procedure is not fault-tolerant, because it is easy to show that there are single component
failures which lead to more than one phase ﬂip in the ancilla state. For example, if there
is a Z error on the extra qubit between the
gates then this can propagate forward
to cause Z errors on two of the ancilla qubits. Fortunately, it is easy to show that multiple
Z errors in the ancilla qubits do not propagate to the encoded data, although they may
cause the ﬁnal measurement result to be incorrect. To cope with this problem, and as
described in more detail below, we repeat this procedure for measurement three times
Fault-tolerant quantum computation
491
and take a majority vote, so the probability that the measurement will be wrong two or
more times in this way is at most O(p2), where p is the probability of failure for a single
component. What about X or Y errors? Well, these can propagate to cause errors in the
encoded data, but it is a fortunate fact that a single failure during the cat state preparation
and veriﬁcation can cause at most one X or Y error in the ancilla after veriﬁcation, and
thus at most one error in the encoded data, ensuring fault-tolerance!
Exercise 10.69:
Show that a single failure anywhere in the ancilla preparation and
veriﬁcation can lead to at most one X or Y error in the ancilla output.
Exercise 10.70:
Show that Z errors in the ancilla do not propagate to affect the
encoded data, but result in an incorrect measurement result being observed.
After the cat state has been veriﬁed, controlled-M ′ gates are performed between pairs
of ancilla and data qubits, with no ancilla qubit being used more than once. Thus, if
the ancilla is in the state |00 . . . 0⟩this results in nothing being done to the encoded
data, while if the ancilla is in the state |11 . . . 1⟩the encoded M operation is applied to
the data. The value of the cat state is that it ensures errors do not propagate from one
controlled-M ′ gate to another, so a single error in the veriﬁcation stage or the sequence
of controlled-M ′ gates results in at most a single error in the encoded data. Finally, the
measurement result is obtained by decoding the cat state with a series of
gates and
a Hadamard; the resulting qubit is 0 or 1 depending on the eigenvalue of the state of
the data. These ﬁnal gates do not involve the data, and thus an error in these gates does
not propagate to affect the data at all. But what if an error in this ﬁnal sequence of gates
results in an incorrect measurement result? By repeating the measurement procedure
three times and taking a majority vote of the results, we can ensure that the probability
of an error in the measurement result is O(p2), where p is the probability of failure in an
individual component.
We have described a method for performing fault-tolerant measurements such that
the measurement gives an erroneous result with probability O(p2), where p is the failure
probability for the individual components, and a single failure anywhere in the procedure
results in an error on at most one qubit in the encoded data. The construction can be
applied for any single qubit observable M which can be implemented in a transversal
fashion. For the Steane code, this includes the Hadamard, phase and Pauli gates, and
with a slight modiﬁcation, the observable M = e−iπ/4SX. To perform the controlled-
M operation on the Steane code for this choice of M we apply controlled-ZSX gates
transversally for each pair of qubits in the ancilla and the code, followed by seven T gates
applied transversally to the ancilla qubits. As described in Section 10.6.2, a fault-tolerant
measurement of this observable can be used to create the ancilla used in the fault-tolerant
circuit for the π/8 gate.
Exercise 10.71:
Verify that when M = e−iπ/4SX the procedure we have described
gives a fault-tolerant method for measuring M.
Exercise 10.72: (Fault-tolerant Toffoli ancilla state construction)
Show how to
fault-tolerantly prepare the state created by the circuit in the dotted box of
492
Quantum error-correction
Exercise 10.68, that is,
|000⟩+ |010⟩+ |100⟩+ |111⟩
2
.
(10.121)
You may ﬁnd it helpful to ﬁrst give the stabilizer generators for this state.
Measurement of stabilizer generators
We have described the fault-tolerant measurement procedure when M is an encoded
observable for a single qubit, however the techniques immediately generalize to other
cases. For our purposes it is sufﬁcient to be able to measure stabilizer generators, which
take the form of a tensor product of Pauli matrices. Such measurements allow us to
perform fault-tolerant error-correction, the initial encoding for the quantum computer,
and to measure encoded Z operators for the ﬁnal readout stage of the computation.
As a simple example, suppose we wished to measure an operator like X1Z2X3 on the
ﬁrst three qubits of a block of seven qubits encoded using the Steane code. An obvi-
ous generalization of Figure 10.28 can be used to perform this measurement, as shown
in Figure 10.29. Once again, we perform veriﬁed cat state preparations before applying
transversal controlled operations on the encoded data, in order to achieve a fault-tolerant
measurement procedure for the operator X1Z2X3. With the ability to fault-tolerantly
measure such observables we automatically obtain the ability to perform the steps of
encoding, syndrome measurement and measurement in the (logical) computational basis
that are required to perform quantum computation. For the purposes of encoding, it
sufﬁces for quantum computation to prepare an encoded |0⟩state. For a stabilizer code
such as the Steane code, such a preparation can be achieved by fault-tolerantly measuring
all the stabilizer generators and the encoded ¯Z operator, and then ﬁxing the signs of the
stabilizer generators and the encoded Z by applying appropriate fault-tolerant opera-
tions, according to the prescription in the proof of Proposition 10.4 in Section 10.5.1.
An example illustrating how the Steane code encoded |0⟩state may be prepared fault-
tolerantly is explained in Exercise 10.73. Syndrome measurement for error-correction and
ﬁnal read-out in the encoded computational basis of the quantum computer are realized
fault-tolerantly along similar lines.
•
•
H

________

_ _ _ _ _ _ _ _

•
⊕
•
⊕
X
Z
X
⎧
⎪
⎪
⎨
⎪
⎪
⎩
|000⟩+ |111⟩
√
2
Figure 10.29. Schematic procedure for performing a fault-tolerant measurement of the operator XZX on three
qubits.
Fault-tolerant quantum computation
493
H
•
H

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕
|0⟩
→
•
• H

________

_ _ _ _ _ _ _ _

•
⊕

________

_ _ _ _ _ _ _ _

•
⊕

________

_ _ _ _ _ _ _ _

•
⊕

________

_ _ _ _ _ _ _ _

⊕
⊕
⊕
⊕
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
|0000⟩+ |1111⟩
√
2
Figure 10.30. One step in fault-tolerantly producing the Steane code encoded |0⟩state.
Exercise 10.73: (Fault-tolerant encoded state construction)
Show that the
Steane code encoded |0⟩state can be constructed fault-tolerantly in the following
manner.
(1) Begin with the circuit of Figure 10.16, and replace the measurement of each
generator, as shown in Figure 10.30, with each ancilla qubit becoming a cat
state |00 . . . 0⟩+ |11 . . . 1⟩, and the operations rearranged to have their
controls on different qubits, so that errors do not propagate within the code
block.
(2) Add a stage to fault-tolerantly measure Z.
(3) Calculate the error probability of this circuit, and of the circuit when the
generator measurements are repeated three times and majority voting is done.
(4) Enumerate the operations which should be performed conditioned on the
measurement results and show that they can be done fault-tolerantly.
Exercise 10.74:
Construct a quantum circuit to fault-tolerantly generate the encoded
|0⟩state for the ﬁve qubit code (Section 10.5.6).
10.6.4
Elements of resilient quantum computation
The most spectacular success of quantum error-correcting codes – the threshold for
quantum computation – is that provided the noise in individual quantum gates is be-
low a certain constant threshold it is possible to efﬁciently perform an arbitrarily large
quantum computation. Stated another way, noise is not a serious problem in principle
for quantum computation. The basic idea in the proof of the threshold, as was outlined
in Section 10.6.1, is to perform fault-tolerant operations directly on encoded states, in-
terleaved with error-correction steps, resulting in a net reduction in error probability
from p to O(p2). By concatenating our codes multiple times and creating hierarchical
fault-tolerant procedures, the error probability can be reduced further to O(p4), then
O(p8) and so on, ultimately being reduced to as low a level as desired, so long as the
the original error p is less than some threshold value pth. Using the procedures we have
described, we estimate a threshold of approximately pth ∼10−5-10−6.
A bold claim such as the threshold theorem obviously needs qualiﬁers. It is not the
case that it is possible to protect a quantum computation against the effect of completely
494
Quantum error-correction
arbitrary noise. The threshold theorem relies for its functioning on a small number
of physically reasonable assumptions about the type of noise occurring in the quantum
computer, and the quantum computer architecture available in order to reach its powerful
conclusion. The error model we have considered is rather simplistic, and it is certainly
the case that real quantum computers will experience more varied types of noise than
we have considered here. Nevertheless, it seems plausible that the techniques introduced
here when coupled with more sophisticated quantum error-correcting codes and with
more sophisticated tools for analysis, can result in a threshold for quantum computation
applicable in a much wider variety of circumstances than those we have considered.
We have not the space here to dive into a more sophisticated analysis, but several
observations are in order. First, it is interesting to note that the threshold result requires
a high degree of parallelism in our circuits. Even if all we wish to do is store quantum
information in a quantum memory this operation will require periodic error-correction
that demands a high degree of parallelism. Thus, a desirable goal for would-be designers
of quantum computers is to develop architectures which are parallelizable, in order that
the techniques of fault-tolerant quantum computation may be applied. Second, we note
that our presentation of the threshold has completely neglected the cost of the classical
computations and communication that are done during state preparation, syndrome mea-
surement, and recovery. The cost of these could potentially be quite high; for example,
to do recovery at the highest levels of the concatenated code requires communication
between all parts of the quantum system. If this communication cannot be accomplished
much faster than the time scale over which errors occur in the system then errors will
begin to creep back in, negating the effect of the error-correction. More sophisticated
analyses can deal with this problem; however, as with other complications there is a con-
comitant cost in the form of a more stringent threshold for quantum computation. Third,
our fault-tolerant constructions for measurement and the π/8 gate made use of ancilla
qubits in the state |0⟩, perhaps with some slight additional noise. It can be shown that in
fact a constant supply of such fresh ancilla qubits is necessary for the threshold theorem
to apply, and thus quantum computer designers must provide architectures which are
not only parallelizable, but which also allow fresh ancilla qubits to be brought up on a
regular basis.
Our presentation has focused on basic principles, not on optimizing the methods used,
and it is likely that in practice much more streamlined versions of our constructions would
be used. A simple but important guiding principle is to choose your codes well. We have
focused on the Steane code because it is easy to work with and demonstrates all the
fundamental principles; however, in practice other codes may work much better. For
example, it may pay handsome dividends at the ﬁrst level of concatenation to use a code
optimized to protect against the type of noise known to occur in the particular physical
system being used for implementation.
Although the theoretical ideas behind the threshold theorem can be adapted in a variety
of different ways, depending upon the noise prevalent in a speciﬁc implementation of
quantum computation, a skeptic might still claim that all such noise models for which
a threshold may be proved are overly restrictive, and will not be realized within any
real physical system. Such skepticism is, ﬁnally, only answerable in the laboratory with
a demonstration of large-scale fault-tolerant quantum computation. The marvel of the
present result is that it proves that, to the best of our current knowledge, no principle of
physics will limit quantum computers from being realized someday.
Chapter problems
495
Summarizing, in this chapter we have outlined the basic principles by which quantum
information processing may be performed in a resilient manner, focusing on the speciﬁc
example of quantum computation. The same basic techniques apply also to any other
system in which quantum information processing may be performed, such as quantum
communications channels for performing tasks such as quantum cryptography. The ex-
treme fragility of quantum information in all known systems makes it likely that some
form of quantum error-correction will need to be used in any practical quantum infor-
mation processing system, but surprisingly, these techniques work so well that arbitrarily
reliable quantum computations can be performed using noisy components, provided the
error probability in those components is less than some constant threshold.
Problem 10.1:
Channels E1 and E2 are said to be equivalent if there exist unitary
channels U and V such that E2 = U ◦E1 ◦V.
(1) Show that the relation of channel equivalence is an equivalence relation.
(2) Show how to turn an error-correcting code for E1 into an error-correcting
code for E2. Assume that the error-correction procedure for E1 is performed
as a projective measurement followed by a conditional unitary operation, and
explain how the error-correction procedure for E2 can be performed in the
same fashion.
Problem 10.2: (Gilbert–Varshamov bound)
Prove the Gilbert–Varshamov bound
for CSS codes, namely, that an [n, k] CSS code correcting t errors exists for
some k such that
k
n ≥1 −2H
2t
n

.
(10.122)
As a challenge, you may like to try proving the Gilbert–Varshamov bound for a
general stabilizer code, namely, that there exists an [n, k] stabilizer code
correcting errors on t qubits, with
k
n ≥1 −2 log(3)t
n
−H
2t
n

.
(10.123)
Problem 10.3: (Encoding stabilizer codes)
Suppose we assume that the generators
for the code are in standard form, and that the encoded Z and X operators have
been constructed in standard form. Find a circuit taking the n×2n check matrix
corresponding to a listing of all the generators for the code together with the
encoded Z operations from
G =
⎡
⎣
0
0
0
I
0
0
0
0
0
0
I
0
0
0
0
0
0
I
⎤
⎦
(10.124)
to the standard form
⎡
⎣
I
A1
A2
B
0
C2
0
0
0
D
I
E
0
0
0
AT
2
0
I
⎤
⎦.
(10.125)
496
Quantum error-correction
Problem 10.4: (Encoding by teleportation)
Suppose you are given a qubit |ψ⟩to
encode in a stabilizer code, but you are not told anything about how |ψ⟩was
constructed: it is an unknown state. Construct a circuit to perform the encoding
in the following manner:
(1) Explain how to fault-tolerantly construct the partially encoded state
|0⟩|0L⟩+ |1⟩|1L⟩
√
2
,
(10.126)
by writing this as a stabilizer state, so it can be prepared by measuring
stabilizer generators.
(2) Show how to fault-tolerantly perform a Bell basis measurement with |ψ⟩and
the unencoded qubit from this state.
(3) Give the Pauli operations which you need to ﬁx up the remaining encoded
qubit after this measurement, so that it becomes |ψ⟩, as in the usual
quantum teleportation scheme.
Compute the probability of error of this circuit. Also show how to modify the
circuit to perform fault-tolerant decoding.
Problem 10.5:
Suppose C(S) is an [n, 1] stabilizer code capable of correcting errors
on a single qubit. Explain how a fault-tolerant implementation of the
controlled-
gate may be implemented between two logical qubits encoded
using this code, using only fault-tolerant stabilizer state preparation,
fault-tolerant measurement of elements of the stabilizer, and normalizer gates
applied transversally.
History and further reading
497
Summary of Chapter 10: Quantum error-correction
• Quantum error-correcting code: An [n, k, d] quantum error-correcting code
uses n qubits to encode k qubits, with distance d.
• Quantum error-correction conditions: Let C be a quantum error-correcting
code and P be the projector onto C. Then the code can correct the error set {Ei}
if and only if
PE†
i EjP = αijP,
(10.127)
for some Hermitian matrix α of complex numbers.
• Stabilizer codes: Let S be the stabilizer for a stabilizer code C(S) and suppose
{Ej} is a set of errors in the Pauli group such that E†
jEk ̸∈N(S) −S for all j
and k. Then {Ej} is a correctable set of errors for the code C(S).
• Fault-tolerant quantum computation: A universal set of logical operations
on encoded quantum states can be performed in such a way that the effective
failure probability in the encoded states scales like O(p2), where p is the failure
probability in the underlying gates.
• The threshold theorem: Provided the noise in individual quantum gates is below
a certain constant threshold and obeys certain physically reasonable assumptions,
it is possible to reliably perform an arbitrarily long quantum computation, with
only a small overhead in the size of the circuit necessary to ensure reliability.
History and further reading
There are many excellent texts on error-correcting codes in classical information theory.
We especially recommend the wonderful text of MacWilliams and Sloane[MS77]. This
begins at a very elementary level, but quickly and smoothly moves into more advanced
topics, covering an enormous range of material. A more recent introduction, also very
good, is the text by Welsh[Wel88].
Quantum error-correction was independently discovered by Shor[Sho95], who found
the nine qubit code presented in Section 10.2, and by Steane[Ste96a], who used a different
approach, in which he studied the interference properties of multiple particle entangled
states. The quantum error-correction conditions were proved independently by Bennett,
DiVincenzo, Smolin and Wootters[BDSW96], and by Knill and Laﬂamme[KL97], building
upon earlier work by Ekert and Macchiavello[EM96]. The ﬁve qubit code was discovered
by Bennett, DiVincenzo, Smolin and Wootters[BDSW96], and independently by Laﬂamme,
Miquel, Paz and Zurek[LMPZ96].
Calderbank and Shor[CS96], and Steane[Ste96b] used ideas from classical error-correction
to develop the CSS (Calderbank–Shor–Steane) codes. Calderbank and Shor also stated
and proved the Gilbert–Varshamov bound for CSS codes. Gottesman[Got96] invented
the stabilizer formalism, and used it to deﬁne stabilizer codes, and investigated some
of the properties of some speciﬁc codes. Independently, Calderbank, Rains, Shor and
Sloane[CRSS97] invented an essentially equivalent approach to quantum error-correction
498
Quantum error-correction
based on ideas from classical coding theory. They were able to classify almost all known
quantum codes using a GF(4) orthogonal geometry approach[CRSS98], and also provided
the ﬁrst proof of the quantum Gilbert–Varshamov bound for general stabilizer codes,
which had earlier been stated by Ekert and Macchiavello[EM96]. The Gottesman–Knill
theorem seems to have ﬁrst been stated by Gottesman in [Got97], where he attributes
the result to Knill, with the proof based upon the stabilizer formalism Gottesman had
introduced. Gottesman has applied the stabilizer formalism to a wide variety of problems
with considerable success; see for example [Got97] for a sample and further references.
Our presentation of the stabilizer formalism is based primarily upon [Got97], wherein
may be found most of the results we describe, including the result that the Hadamard,
phase and controlled-
generate the normalizer N(Gn).
Many constructions for speciﬁc classes of quantum codes are known; we point to just a
few here. Rains, Hardin, Shor and Sloane[RHSS97] have constructed interesting examples
of quantum codes lying outside the stabilizer codes we have considered. Many people
have considered quantum codes based on systems other than qubits; we mention espe-
cially the work of Gottesman[Got98a] and Rains[Rai99b] which construct non-binary codes
and consider fault-tolerant computation with such codes. Aharonov and Ben-Or[ABO99]
construct non-binary codes using interesting techniques based on polynomials over ﬁ-
nite ﬁelds, and also investigate fault-tolerant computation with such codes. Approximate
quantum error-correction is another topic we have not touched; that approximate quan-
tum error-correction can lead to improved codes was shown by Leung, Nielsen, Chuang
and Yamamoto[LNCY97].
A large and interesting class of quantum error-correcting codes (but beyond the scope
of this chapter) are known variously by the names noiseless quantum codes and decoher-
ence free subspaces. A substantial body of work exists on these subjects (and clarifying the
connections between them). An entry into the literature may be found through the work
of Zanardi and Rasetti[ZR98, Zan99], Lidar, Chuang, and Whaley[LCW98], Bacon, Kempe,
Lidar and Whaley[BKLW99, LBW99], and of Knill, Laﬂamme and Viola[KLV99].
Many bounds on quantum error-correcting codes are known, often adapted from simi-
lar classical bounds. Ekert and Macchiavello[EM96] pointed out the possibility of proving a
quantum analogue to the Hamming bound; this construction and the role of ‘degenerate’
quantum codes was subsequently clariﬁed by Gottesman[Got96]. Shor and Laﬂamme[SL97]
proved a quantum analogue of a result in classical coding theory, the MacWilliams identi-
ties, which touched off a great deal of work studying the properties of certain polynomials
related to quantum codes (the weight enumerators) as well as more general work on the
problem of bounds for quantum codes, by Ashikhmin[Ash97], Ashikhmin and Lytsin[AL99],
and several papers on the topic by Rains[Rai98, Rai99c, Rai99a].
The theory of fault-tolerant computation for classical computers was worked out by
von Neumann[von56], and is discussed in the monograph by Winograd and Cowan[WC67].
Shor[Sho96] introduced the idea of fault-tolerance into quantum computation, and showed
how to perform all the basic fault tolerant steps (state preparation, quantum logic, error-
correction, and measurement). Kitaev[Kit97b, Kit97c] independently developed many similar
ideas, including fault-tolerant constructions for many basic quantum logic gates. Cirac,
Pellizzari and Zoller[CPZ96], and Zurek and Laﬂamme[ZL96], also took early steps toward
fault-tolerant quantum computation. DiVincenzo and Shor generalized Shor’s original
construction to show how fault-tolerant measurement of syndromes for any stabilizer
code could be performed[DS96], and Gottesman[Got98b] generalized all the fault-tolerant
History and further reading
499
constructions, showing how to perform fault-tolerant computation with any stabilizer
code. A general review of this work as well as much other survey material may be found
in [Got97]; this includes a construction to solve Problem 10.5. The fault-tolerant π/8
and Toffoli gate constructions are based on a line of thought developed by Gottesman
and Chuang[GC99], and Zhou and Chuang[ZLC00]; the circuit for the fault-tolerant Toffoli
described in Exercise 10.68 is actually Shor’s original construction[Sho96]. Steane[Ste99]
has developed many ingenious constructions for fault-tolerant procedures.
Kitaev[Kit97a, Kit97b] has introduced a beautiful set of ideas for implementing fault-
tolerance, using topological methods to assist in the performance of quantum error-
correction. The basic idea is that if information is stored in the topology of a system, then
that information will naturally be very robust against the effects of noise. These and many
other elegant ideas have been developed in further papers by Bravyi and Kitaev[BK98b]
and by Freedman and Meyer[FM98]. Preskill[Pre97] is an excellent review of the ﬁeld of
quantum error-correction as a whole, and contains an especially beautiful description
of topological quantum error-correction, as well as a provocative discussion of whether
topological error-correction can be used to gain insight into fundamental questions about
black holes and quantum gravity!
Many different groups proved threshold results for quantum computation. These
results hold for a wide variety of assumptions, giving essentially different threshold theo-
rems. Aharonov and Ben-Or[ABO97, ABO99] and Kitaev’s[Kit97c, Kit97b] threshold proofs do
not require fast or reliable classical computation. Aharonov and Ben-Or also showed that
in order for a threshold result to hold, there must be constant parallelism in the quan-
tum computer at each timestep[ABO97]. In their threshold proofs, Gottesman[Got97] and
Preskill[Pre98c, GP10] have an especially detailed optimization of the value of the threshold.
Knill, Laﬂamme and Zurek[KLZ98a, KLZ98b]’s results concentrate on proving the threshold
theorem for a wide class of error models. Aharonov, Ben-Or, Impagliazzo, and Nisan have
also shown that a supply of fresh qubits is necessary for the threshold[ABOIN96]. Further
references and historical material may be found within the cited works. In particular, each
group built on Shor’s pioneering work[Sho96] on fault-tolerant quantum computation.
Numerous excellent reviews of fault-tolerant quantum computation have been written,
developing the basic ideas in much greater detail than we have here, from a variety of
different points of view. Aharonov’s thesis[Aha99a] develops the threshold theorem and
much material of related interest in a self-contained way. Gottesman’s thesis[Got97] also
provides a review of fault-tolerant quantum computation, with more emphasis on prop-
erties of quantum codes, and developing fault-tolerant constructions for a wide variety
of different codes. Knill, Laﬂamme and Zurek have written a semi-popular overview of
the threshold result[KLZ98a]. Finally, Preskill has written two superb articles[Pre98c, Pre98a]
explaining quantum error-correction and fault-tolerant quantum computation.
11 Entropy and information
Entropy is a key concept of quantum information theory. It measures how much un-
certainty there is in the state of a physical system. In this chapter we review the basic
deﬁnitions and properties of entropy in both classical and quantum information theory.
In places the chapter contains rather detailed and lengthy mathematical arguments. On
a ﬁrst reading these sections may be read lightly and returned to later for reference
purposes.
11.1
Shannon entropy
The key concept of classical information theory is the Shannon entropy. Suppose we
learn the value of a random variable X. The Shannon entropy of X quantiﬁes how much
information we gain, on average, when we learn the value of X. An alternative view is that
the entropy of X measures the amount of uncertainty about X before we learn its value.
These two views are complementary; we can view the entropy either as a measure of our
uncertainty before we learn the value of X, or as a measure of how much information we
have gained after we learn the value of X.
Intuitively, the information content of a random variable should not depend on the
labels attached to the different values that may be taken by the random variable. For
example, we expect that a random variable taking the values ‘heads’ and ‘tails’ with re-
spective probabilities 1/4 and 3/4 contains the same amount of information as a random
variable that takes the values 0 and 1 with respective probabilities 1/4 and 3/4. For this
reason, the entropy of a random variable is deﬁned to be a function of the probabilities
of the different possible values the random variable takes, and is not inﬂuenced by the
labels used for those values. We often write the entropy as a function of a probability dis-
tribution, p1, . . . , pn. The Shannon entropy associated with this probability distribution
is deﬁned by
H(X) ≡H(p1, . . . , pn) ≡−

x
px log px.
(11.1)
We justify this deﬁnition shortly. Note that in the deﬁnition – and throughout this book –
logarithms indicated by ‘log’ are taken to base two, while ‘ln’ indicates a natural logarithm.
It is conventional to say that entropies are measured in ‘bits’ with this convention for
the logarithm. You may wonder what happens when px = 0, since log 0 is undeﬁned.
Intuitively, an event which can never occur should not contribute to the entropy, so by
convention we agree that 0 log 0 ≡0. More formally, note that limx→0 x log x = 0, which
provides further support for our convention.
Why is the entropy deﬁned in this way? Later in this section, Exercise 11.2 gives
an intuitive justiﬁcation for this deﬁnition of the entropy, based on certain ‘reasonable’
axioms which might be expected of a measure of information. This intuitive justiﬁcation
Shannon entropy
501
is reassuring but it is not the whole story. The best reason for this deﬁnition of entropy
is that it can be used to quantify the resources needed to store information. More
concretely, suppose there is some source (perhaps a radio antenna) which is producing
information of some sort, say in the form of a bit string. Let’s consider a very simple model
for a source: we model it as producing a string X1, X2, . . . of independent, identically
distributed random variables. Most real information sources don’t behave quite this way,
but it’s often a good approximation to reality. Shannon asked what minimal physical
resources are required to store the information being produced by the source, in such a
way that at a later time the information can be reconstructed? The answer to this question
turns out to be the entropy, that is, H(X) bits are required per source symbol, where
H(X) ≡H(X1) = H(X2) = . . . is the entropy of each random variable modeling the
source. This result is known as Shannon’s noiseless coding theorem, and we prove both
classical and quantum versions of it in Chapter 12.
For a concrete example of Shannon’s noiseless channel coding theorem, suppose an
information source produces one of four symbols, 1, 2, 3 or 4. Without compression two
bits of storage space corresponding to the four possible outputs are consumed for each
use of the source. Suppose however that the symbol 1 is produced by the source with
probability 1/2, the symbol 2 with probability 1/4, and the symbols 3 and 4 both with
probability 1/8. We can make use of the bias between the source outputs to compress the
source, using fewer bits to store commonly occurring symbols such as 1, and more bits to
store rarely occurring symbols like 3 and 4. One possible compression scheme is to encode
1 as the bit string 0, 2 as the bit string 10, 3 as the bit string 110, and 4 as the bit string 111.
Notice that the average length of the compressed string is 1
2 ·1+ 1
4 ·2+ 1
8 ·3+ 1
8 ·3 = 7/4 bits
of information per use of the source. This is less than is required in the naive approach
to storing this source! Amazingly, this matches the entropy of the source, H(X) =
−1/2 log(1/2) −1/4 log(1/4) −1/8 log(1/8) −1/8 log(1/8) = 7/4! Moreover, it turns
out that any attempt to compress the source further results in data being irretrievably
lost; the entropy quantiﬁes the optimal compression that may be achieved.
This operational motivation for the deﬁnition of entropy in terms of data compression
expresses a key philosophy of information theory, both quantum and classical: funda-
mental measures of information arise as the answers to fundamental questions about
the physical resources required to solve some information processing problem.
Exercise 11.1: (Simple calculations of entropy)
What is the entropy associated
with the toss of a fair coin? With the roll of a fair die? How would the entropy
behave if the coin or die were unfair?
Exercise 11.2: (Intuitive justiﬁcation for the deﬁnition of entropy)
Suppose we
are trying to quantify how much information is provided by an event E which
may occur in a probabilistic experiment. We do this using an ‘information
function’ I(E) whose value is determined by the event E. Suppose we make the
following assumptions about this function:
(1) I(E) is a function only of the probability of the event E, so we may write
I = I(p), where p is a probability in the range 0 to 1.
(2) I is a smooth function of probability.
(3) I(pq) = I(p) + I(q) when p, q > 0. (Interpretation: The information gained
502
Entropy and information
when two independent events occur with individual probabilities p and q is
the sum of the information gained from each event alone.)
Show that I(p) = k log p, for some constant, k. It follows that the average
information gain when one of a mutually exclusive set of events with
probabilities p1, . . . , pn occurs is k 
i pi log pi, which is just the Shannon
entropy, up to a constant factor.
11.2
Basic properties of entropy
11.2.1
The binary entropy
The entropy of a two-outcome random variable is so useful that we give it a special name,
the binary entropy, deﬁned as
Hbin(p) ≡−p log p −(1 −p) log(1 −p),
(11.8)
where p and 1 −p are the probabilities of the two outcomes. Where context makes the
meaning clear we write H(p) rather than Hbin(p). The binary entropy function is plotted
in Figure 11.1. Notice that H(p) = H(1 −p) and that H(p) attains its maximum value
of 1 at p = 1/2.
The binary entropy is an excellent testing ground for the understanding of more
general properties of entropy. One property of particular interest is how the entropy
behaves when we mix two or more probability distributions. Imagine, for example, that
Alice has in her possession two coins, one a quarter from the US, the other a dollar
coin from Australia. Both coins have been altered to exhibit bias, with the probability
of heads on the US coin being pU, and the probability of heads on the Australian coin
being pA. Suppose Alice ﬂips the US coin with probability q and the Australian coin
with probability 1 −q, telling Bob whether the result was heads or tails. How much
information does Bob gain on average? Intuitively it is clear that Bob should gain at least
as much information as the average of the information he would have gained from a US
coin ﬂip or an Australian coin ﬂip. As an equation this intuition may be expressed as:
H(qpU + (1 −q)pA) ≥qH(pU) + (1 −q)H(pA).
(11.9)
Sometimes the inequality can be strict, because Bob gains information not only about the
value (heads or tails) of the coin, but also some additional information about the identity
of the coin. For instance, if pU = 1/3 and pA = 5/6, and heads comes up, then Bob has
received a pretty good indicator that the coin was Australian.
Equation (11.9) is easily shown to be correct. It is an example of a broader concept,
that of concavity, which we met in Chapter 9 when discussing distance measures. Recall
that a real-valued function f is said to be concave if for any p in the range 0 to 1,
f(px + (1 −p)y) ≥pf(x) + (1 −p)f(y).
(11.10)
The binary entropy is easily seen to be concave, as can be grasped visually by examining
Figure 11.1 and observing that the graph of the binary entropy goes above any line cutting
the graph. We will be much concerned with concavity properties of the entropy, both
classical and quantum. Don’t let the simplicity of the above intuitive argument beguile
you into a false complacency: many of the deepest results in quantum information have
their roots in skilful application of concavity properties of classical or quantum entropies.
Basic properties of entropy
503
Box 11.1: Entropic quantum uncertainty principle
There is an elegant entropic way of reformulating the uncertainty principle of
quantum mechanics. Recall the Heisenberg uncertainty principle from Box 2.4 on
page 89. This states that the standard deviations Δ(C) and Δ(D) for observables C
and D must satisfy the relation
Δ(C)Δ(D) ≥|⟨ψ|[C, D]|ψ⟩|
2
,
(11.2)
for a quantum system in the state |ψ⟩.
Let C = 
c c|c⟩⟨c| and D = 
d d|d⟩⟨d| be spectral decompositions for C and
D. Deﬁne f(C, D) ≡maxc,d |⟨c|d⟩| to be the maximum ﬁdelity between any two
eigenvectors of |c⟩and |d⟩. For example, f(X, Z) = 1/
√
2 for the Pauli matrices
X and Z.
Suppose the quantum system is prepared in a state |ψ⟩, and let p(c) be the prob-
ability distribution associated with a measurement of C, with associated entropy
H(C), and q(d) the probability distribution associated with a measurement of D,
with associated entropy H(D). The entropic uncertainty principle states that
H(C) + H(D) ≥2 log

1
f(C, D)

.
(11.3)
A full proof of this result would take us too far aﬁeld (see ‘History and further
reading’ for references); however, we can give a simple proof of the weaker result:
H(C) + H(D) ≥−2 log 1 + f(C, D)
2
.
(11.4)
To prove this, note that
H(C) + H(D) = −

cd
p(c)q(d) log (p(c)q(d)) .
(11.5)
We aim to bound p(c)q(d) = |⟨c|ψ⟩⟨ψ|d⟩|2 from above. To do this, let | ˜ψ⟩be the
projection of |ψ⟩into the plane spanned by |c⟩and |d⟩, so | ˜ψ⟩has norm λ less than
or equal to one. If θ is the angle |d⟩makes with |c⟩in this plane, and ϕ is the angle
| ˜ψ⟩makes with |d⟩, then we see p(c)q(d) = |⟨c| ˜ψ⟩⟨˜ψ|d⟩|2 = λ2 cos2(θ −ϕ) cos2(ϕ).
Calculus implies that the maximum is reached when λ = 1 and ϕ = θ/2, at which
point p(c)q(d) = cos4(θ/2), which can be put in the form
p(c)q(d) =
1 + |⟨c|d⟩|
2
2
.
(11.6)
Combining this with Equation (11.5) gives
H(C) + H(D) ≥−2 log 1 + f(C, D)
2
,
(11.7)
as claimed.
Moreover, for quantum entropies it is sometimes rather difﬁcult to justify our intuitive
beliefs about what concavity properties entropy ought to have.
504
Entropy and information
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 11.1. Binary entropy function H(p).
Exercise 11.3:
Prove that the binary entropy Hbin(p) attains its maximum value of one
at p = 1/2.
Exercise 11.4: (Concavity of the binary entropy)
From Figure 11.1 it appears
that the binary entropy is a concave function. Prove that this is so, that is:
Hbin(px1 + (1 −p)x2) ≥pHbin(x1) + (1 −p)Hbin(x2),
(11.11)
where 0 ≤p, x1, x2 ≤1. Prove in addition that the binary entropy is strictly
concave, that is, the above inequality is an equality only for the trivial cases
x1 = x2, or p = 0, or p = 1.
11.2.2
The relative entropy
The relative entropy
is a very useful entropy-like measure of the closeness of two
probability distributions, p(x) and q(x), over the same index set, x. Suppose p(x) and
q(x) are two probability distributions on the same index set, x. Deﬁne the relative entropy
of p(x) to q(x) by
H(p(x)∥q(x)) ≡

x
p(x) log p(x)
q(x) ≡−H(X) −

x
p(x) log q(x).
(11.12)
We deﬁne −0 log 0 ≡0 and −p(x) log 0 ≡+∞if p(x) > 0.
It is not immediately obvious what the relative entropy is good for, or even why it is a
good measure of distance between two distributions. The following theorem gives some
motivation as to why it is regarded as being like a distance measure.
Basic properties of entropy
505
Theorem 11.1: (Non-negativity of the relative entropy) The relative entropy is
non-negative, H(p(x)∥q(x)) ≥0, with equality if and only if p(x) = q(x) for all x.
Proof
A very useful inequality in information theory is log x ln 2 = ln x ≤x−1, for all positive
x, with equality if and only if x = 1. Here we need to rearrange the result slightly, to
−log x ≥(1 −x)/ ln 2, and then note that
H(p(x)∥q(x)) = −

x
p(x) log q(x)
p(x)
(11.13)
≥
1
ln 2

x
p(x)

1 −q(x)
p(x)

(11.14)
=
1
ln 2

x
(p(x) −q(x))
(11.15)
=
1
ln 2(1 −1) = 0,
(11.16)
which is the desired inequality. The equality conditions are easily deduced by noting
that equality occurs in the second line if and only if q(x)/p(x) = 1 for all x, that is, the
distributions are identical.
The relative entropy is often useful, not in itself, but because other entropic quantities
can be regarded as special cases of the relative entropy. Results about the relative entropy
then give as special cases results about other entropic quantities. For example, we can
use the non-negativity of the relative entropy to prove the following fundamental fact
about entropies. Suppose p(x) is a probability distribution for X, over d outcomes. Let
q(x) ≡1/d be the uniform probability distribution over those outcomes. Then
H(p(x)∥q(x)) = H(p(x)∥1/d) = −H(X) −

x
p(x) log(1/d) = log d −H(X).(11.17)
From the non-negativity of the relative entropy, Theorem 11.1, we see that log d −
H(X) ≥0, with equality if and only if X is uniformly distributed. This is an elementary
fact, but so important that we restate it formally as a theorem.
Theorem 11.2: Suppose X is a random variable with d outcomes. Then H(X) ≤log d,
with equality if and only if X is uniformly distributed over those d outcomes.
We use this technique – ﬁnding expressions for entropic quantities in terms of the
relative entropy – often in the study of both classical and quantum entropies.
Exercise 11.5: (Subadditivity of the Shannon entropy)
Show that
H(p(x, y)∥p(x)p(y)) = H(p(x)) + H(p(y)) −H(p(x, y)). Deduce that
H(X, Y ) ≤H(X) + H(Y ), with equality if and only if X and Y are
independent random variables.
11.2.3
Conditional entropy and mutual information
Suppose X and Y are two random variables. How is the information content of X
related to the information content of Y ? In this section we introduce two concepts – the
506
Entropy and information
conditional entropy and the mutual information – which help answer this question. The
deﬁnitions we give for these concepts are rather formal, and at times you may be confused
as to why a particular quantity – say, the conditional entropy – is to be interpreted in
the way we indicate. Keep in mind that the ultimate justiﬁcation for these deﬁnitions is
that they answer resource questions, which we investigate in more detail in Chapter 12,
and that the interpretations given to the quantities depend on the nature of the resource
question being answered.
We already met the joint entropy of a pair of random variables implicitly in the
previous section. For clarity, we now make this deﬁnition explicit. The joint entropy of
X and Y is deﬁned in the obvious way,
H(X, Y ) ≡−

x,y
p(x, y) log p(x, y),
(11.18)
and may be extended in the obvious way to any vector of random variables. The joint
entropy measures our total uncertainty about the pair (X, Y ). Suppose we know the
value of Y , so we have acquired H(Y ) bits of information about the pair, (X, Y ). The
remaining uncertainty about the pair (X, Y ), is associated with our remaining lack of
knowledge about X, even given that we know Y . The entropy of X conditional on
knowing Y is therefore deﬁned by
H(X|Y ) ≡H(X, Y ) −H(Y ).
(11.19)
The conditional entropy is a measure of how uncertain we are, on average, about the
value of X, given that we know the value of Y .
A second quantity, the mutual information content of X and Y , measures how much
information X and Y have in common. Suppose we add the information content of X,
H(X), to the information content of Y . Information which is common to X and Y will
have been counted twice in this sum, while information which is not common will have
been counted exactly once. Subtracting off the joint information of (X, Y ), H(X, Y ), we
therefore obtain the common or mutual information of X and Y :
H(X :Y ) ≡H(X) + H(Y ) −H(X, Y ).
(11.20)
Note the useful equality H(X :Y ) = H(X) −H(X|Y ) relating the conditional entropy
and mutual information.
To get some feeling for how the Shannon entropy behaves, we now give some simple
relationships between the different entropies.
Theorem 11.3: (Basic properties of Shannon entropy)
(1) H(X, Y ) = H(Y, X), H(X :Y ) = H(Y :X).
(2) H(Y |X) ≥0 and thus H(X :Y ) ≤H(Y ), with equality if and only if Y is a
function of X, Y = f(X).
(3) H(X) ≤H(X, Y ), with equality if and only if Y is a function of X.
(4) Subadditivity: H(X, Y ) ≤H(X) + H(Y ) with equality if and only if X
and Y are independent random variables.
(5) H(Y |X) ≤H(Y ) and thus H(X :Y ) ≥0, with equality in each if and only
if X and Y are independent random variables.
(6) Strong subadditivity: H(X, Y, Z) + H(Y ) ≤H(X, Y ) + H(Y, Z), with
equality if and only if Z →Y →X forms a Markov chain.
Basic properties of entropy
507
(7) Conditioning reduces entropy: H(X|Y, Z) ≤H(X|Y ).
Most of the proofs are either obvious or easy exercises. We give a few hints below.
Proof
(1) Obvious from the relevant deﬁnitions.
(2) Since p(x, y) = p(x)p(y|x) we have
H(X, Y ) = −

xy
p(x, y) log p(x)p(y|x)
(11.21)
= −

x
p(x) log p(x) −

xy
p(x, y) log p(y|x)
(11.22)
= H(X) −

xy
p(x, y) log p(y|x).
(11.23)
Thus H(Y |X) = −
xy p(x, y) log p(y|x). But −log p(y|x) ≥0, so H(Y |X) ≥0
with equality if and only if Y is a deterministic function of X.
(3) Follows from the previous result.
(4) To prove subadditivity and, later, strong subadditivity we again use the fact that
log x ≤(x −1)/ ln 2 for all positive x, with equality if and only if x = 1. We ﬁnd
that

x,y
p(x, y) log p(x)p(y)
p(x, y) ≤
1
ln 2

x,y
p(x, y)
p(x)p(y)
p(x, y) −1

(11.24)
=
1
ln 2

x,y
p(x)p(y) −p(x, y) = 1 −1
ln 2 = 0. (11.25)
Subadditivity follows. Notice that equality is achieved if and only if
p(x, y) = p(x)p(y) for all x and y. That is, the subadditivity inequality is saturated
if and only if X and Y are independent.
(5) Follows from subadditivity and the relevant deﬁnitions.
(6) Strong subadditivity of Shannon entropy follows from the same technique used to
prove subadditivity; the difﬁculty level is only slightly higher than that proof. You
will be asked to supply this proof as Exercise 11.6.
(7) Intuitively, we expect that the uncertainty about X, given that we know the value
of Y and Z, is less than our uncertainty about X, given that we only know Y .
More formally, inserting the relevant deﬁnitions, the result that conditioning
reduces entropy is equivalent to
H(X, Y, Z) −H(Y, Z) ≤H(X, Y ) −H(Y ),
(11.26)
which is a rearranged version of the strong subadditivity inequality.
Exercise 11.6: (Proof of classical strong subadditivity)
Prove that
H(X, Y, Z) + H(Y ) ≤H(X, Y ) + H(Y, Z), with equality if and only if
Z →Y →X forms a Markov chain.
Exercise 11.7:
In Exercise 11.5 you implicitly showed that the mutual information
H(X :Y ) could be expressed as the relative entropy of two probability
distributions, H(X :Y ) = H(p(x, y)∥p(x)p(y)). Find an expression for the
508
Entropy and information
conditional entropy H(Y |X) as a relative entropy between two probability
distributions. Use this expression to deduce that H(Y |X) ≥0, and to ﬁnd the
equality conditions.
The various relationships between entropies may mostly be deduced from the ‘entropy
Venn diagram’ shown in Figure 11.2. Such ﬁgures are not completely reliable as a guide
to the properties of entropy, but they provide a useful mnemonic for remembering the
various deﬁnitions and properties of entropy.




	
Figure 11.2. Relationships between different entropies.
We conclude our study of the elementary properties of conditional entropy and mutual
information with a simple and useful chaining rule for conditional entropies.
Theorem 11.4: (Chaining rule for conditional entropies) Let X1, . . . , Xn and Y
be any set of random variables. Then
H(X1, . . . , Xn|Y ) =
n

i=1
H(Xi|Y, X1, . . . , Xi−1).
(11.27)
Proof
We prove the result for n = 2, and then induct on n. Using only the deﬁnitions and
some simple algebra we have
H(X1, X2|Y ) = H(X1, X2, Y ) −H(Y )
(11.28)
= H(X1, X2, Y ) −H(X1, Y ) + H(X1, Y ) −H(Y )
(11.29)
= H(X2|Y, X1) + H(X1|Y ),
(11.30)
which establishes the result for n = 2. Now we assume the result for general n, and show
the result holds for n + 1. Using the already established n = 2 case, we have
H(X1, . . . , Xn+1|Y ) = H(X2, . . . , Xn+1|Y, X1) + H(X1|Y ).
(11.31)
Applying the inductive hypothesis to the ﬁrst term on the right hand side gives
H(X1, . . . , Xn+1|Y ) =
n+1

i=2
H(Xi|Y, X1, . . . , Xi−1) + H(X1|Y )
(11.32)
Basic properties of entropy
509
=
n+1

i=1
H(Xi|Y, X1, . . . , Xi−1),
(11.33)
and the induction goes through.
Exercise 11.8: (Mutual information is not always subadditive)
Let X and Y be
independent identically distributed random variables taking the values 0 or 1
with probability 1/2. Let Z ≡X ⊕Y , where ⊕denotes addition modulo 2.
Show that the mutual information in this case is not subadditive,
H(X, Y :Z) ̸≤H(X :Z) + H(Y :Z).
(11.34)
Exercise 11.9: (Mutual information is not always superadditive)
Let X1 be a
random variable taking values 0 or 1 with respective probabilities of 1/2 and
X2 ≡Y1 ≡Y2 ≡X1. Show that the mutual information in this case is not
superadditive,
H(X1 :Y1) + H(X2 :Y2) ̸≤H(X1, X2 :Y1, Y2)
(11.35)
11.2.4
The data processing inequality
In many applications of interest we perform computations on the information we have
available, but that information is imperfect, as it has been subjected to noise before it
becomes available to us. A basic inequality of information theory, the data processing
inequality, states that information about the output of a source can only decrease with
time: once information has been lost, it is gone forever. Making this statement more
precise is the goal of this section.
The intuitive notion of information processing is captured in the idea of a Markov
chain of random variables. A Markov chain is a sequence X1 →X2 →· · · of random
variables such that Xn+1 is independent of X1, . . ., Xn−1, given Xn. More formally,
p(Xn+1 = xn+1|Xn = xn, . . . , X1 = x1) = p(Xn+1 = xn+1|Xn = xn).
(11.36)
Under what conditions does a Markov chain lose information about its early values, as
time progresses? The following data processing inequality gives an information-theoretic
way of answering this question.
Theorem 11.5: (Data processing inequality) Suppose X →Y →Z is a Markov
chain. Then
H(X) ≥H(X :Y ) ≥H(X :Z).
(11.37)
Moreover, the ﬁrst inequality is saturated if and only if, given Y , it is possible to
reconstruct X.
This result is intuitively plausible: it tells us that if a random variable X is subject to
noise, producing Y , then further actions on our part (‘data processing’) cannot be used
to increase the amount of mutual information between the output of the process and the
original information X.
Proof
510
Entropy and information
The ﬁrst inequality was proved in Theorem 11.3 on page 507. From the deﬁnitions we
see that H(X :Z) ≤H(X :Y ) is equivalent to H(X|Y ) ≤H(X|Z). From the fact that
X →Y →Z is a Markov chain it is easy to prove (Exercise 11.10) that Z →Y →X
is also a Markov chain, and thus H(X|Y ) = H(X|Y, Z). The problem is thus reduced
to proving that H(X, Y, Z) −H(Y, Z) = H(X|Y, Z) ≤H(X|Z) = H(X, Z) −H(Z).
This is just the already proved strong subadditivity inequality.
Suppose H(X : Y ) < H(X). Then it is not possible to reconstruct X from Y , since
if Z is the attempted reconstruction based only on knowledge of Y , then X →Y →Z
must be a Markov chain, and thus H(X) > H(X :Z) by the data processing inequality.
Thus Z ̸= X. On the other hand, if H(X :Y ) = H(X), then we have H(X|Y ) = 0 and
thus whenever p(X = x, Y = y) > 0 we have p(X = x|Y = y) = 1. That is, if Y = y
then we can infer with certainty that X was equal to x, allowing us to reconstruct X.
As noted above, if X →Y →Z is a Markov chain, then so is Z →Y →X. Thus,
as a corollary to the data processing inequality we see that if X →Y →Z is a Markov
chain then
H(Z :Y ) ≥H(Z :X) .
(11.38)
We refer to this result as the data pipelining inequality. Intuitively, it says that any
information Z shares with X must be information which Z also shares with Y ; the
information is ‘pipelined’ from X through Y to Z.
Exercise 11.10:
Show that if X →Y →Z is a Markov chain then so is
Z →Y →X.
11.3
Von Neumann entropy
The Shannon entropy measures the uncertainty associated with a classical probability
distribution. Quantum states are described in a similar fashion, with density operators
replacing probability distributions. In this section we generalize the deﬁnition of the
Shannon entropy to quantum states.
Von Neumann deﬁned the entropy of a quantum state ρ by the formula
S(ρ) ≡−tr(ρ log ρ).
(11.39)
In this formula logarithms are taken to base two, as usual. If λx are the eigenvalues of ρ
then von Neumann’s deﬁnition can be re-expressed
S(ρ) = −

x
λx log λx,
(11.40)
where we deﬁne 0 log 0 ≡0, as for the Shannon entropy. For calculations it is usually this
last formula which is most useful. For instance, the completely mixed density operator
in a d-dimensional space, I/d, has entropy log d.
From now on, when we refer to entropy, it will usually be clear from context whether
we mean the Shannon or von Neumann entropy.
Von Neumann entropy
511
Exercise 11.11: (Example calculations of entropy)
Calculate S(ρ) for
ρ =
 1
0
0
0

(11.41)
ρ = 1
2
 1
1
1
1

(11.42)
ρ = 1
3
 2
1
1
1

.
(11.43)
Exercise 11.12: (Comparison of quantum and classical entropies)
Suppose
ρ = p|0⟩⟨0| + (1 −p)(|0⟩+|1⟩)(⟨0|+⟨1|)
2
. Evaluate S(ρ). Compare the value of S(ρ) to
H(p, 1 −p).
11.3.1
Quantum relative entropy
As for the Shannon entropy, it is extremely useful to deﬁne a quantum version of the
relative entropy. Suppose ρ and σ are density operators. The relative entropy of ρ to σ
is deﬁned by
S(ρ||σ) ≡tr(ρ log ρ) −tr(ρ log σ) .
(11.50)
As with the classical relative entropy, the quantum relative entropy can sometimes be
inﬁnite. In particular, the relative entropy is deﬁned to be +∞if the kernel of σ (the vector
space spanned by the eigenvectors of σ with eigenvalue 0) has non-trivial intersection
with the support of ρ (the vector space spanned by the eigenvectors of ρ with non-zero
eigenvalue), and is ﬁnite otherwise. The quantum relative entropy is non-negative, a
result sometimes known as Klein’s inequality:
Theorem 11.7: (Klein’s inequality) The quantum relative entropy is non-negative,
S(ρ||σ) ≥0,
(11.51)
with equality if and only if ρ = σ.
Proof
Let ρ = 
i pi|i⟩⟨i| and σ = 
j qj|j⟩⟨j| be orthonormal decompositions for ρ and σ.
From the deﬁnition of the relative entropy we have
S(ρ||σ) =

i
pi log pi −

i
⟨i|ρ log σ|i⟩.
(11.52)
We substitute into this equation the equations ⟨i|ρ = pi⟨i| and
⟨i| log σ|i⟩= ⟨i|
⎛
⎝
j
log(qj)|j⟩⟨j|
⎞
⎠|i⟩=

j
log(qj)Pij,
(11.53)
where Pij ≡⟨i|j⟩⟨j|i⟩≥0, to give
S(ρ||σ) =

i
pi
⎛
⎝log pi −

j
Pij log(qj)
⎞
⎠.
(11.54)
Note that Pij satisﬁes the equations Pij ≥0, 
i Pij = 1 and 
j Pij = 1. (Regarding Pij
512
Entropy and information
Box 11.2: Continuity of the entropy
Suppose we vary ρ by a small amount. How much does S(ρ) change? Fannes’
inequality tells us that the answer is ‘not much’, and even provides a bound on
how small the change is.
Theorem 11.6: (Fannes’ inequality) Suppose ρ and σ are density matrices such
that the trace distance between them satisﬁes T(ρ, σ) ≤1/e. Then
|S(ρ) −S(σ)| ≤T(ρ, σ) log d + η(T(ρ, σ)),
(11.44)
where d is the dimension of the Hilbert space, and η(x) ≡−x log x.
Removing the restriction that T(ρ, σ) ≤1/e we can prove the weaker
inequality
|S(ρ) −S(σ)| ≤T(ρ, σ) log d + 1
e.
(11.45)
Proof
To prove Fannes’ inequality we need a simple result relating the trace distance
between two operators to their eigenvalues. Let r1 ≥r2 ≥· · · ≥rd be the eigen-
values of ρ, in descending order, and s1 ≥s2 ≥· · · ≥sd be the eigenvalues
of σ, again in descending order. By the spectral decomposition we may decom-
pose ρ −σ = Q −R, where Q and R are positive operators with orthogonal
support, so T(ρ, σ) = tr(R) + tr(Q). Deﬁning V ≡R + ρ = Q + σ, we have
T(ρ, σ) = tr(R) + tr(Q) = tr(2V ) −tr(ρ) −tr(σ). Let t1 ≥t2 ≥· · · ≥td be the
eigenvalues of T. Note that ti ≥max(ri, si), so 2ti ≥ri + si + |ri −si|, and it
follows that
T(ρ, σ) ≥

i
|ri −si|.
(11.46)
By calculus whenever |r −s| ≤1/2 it follows that |η(r) −η(s)| ≤η(|r −s|). A
moment’s thought shows that |ri −si| ≤1/2 for all i, so
|S(ρ) −S(σ)| =
+++++

i
(η(ri) −η(si))
+++++ ≤

i
η(|ri −si|).
(11.47)
Setting Δ ≡
i |ri −si| and observing that η(|ri −si|) = Δη(|ri −si|/Δ) −|ri −
si| log(Δ), we see that
|S(ρ) −S(σ)| ≤Δ

η(|ri −si|/Δ) + η(Δ) ≤Δ log d + η(Δ),
(11.48)
where we applied Theorem 11.2 to obtain the second inequality. But Δ ≤T(ρ, σ)
by (11.46), so by the monotonicity of η(·) on the interval [0, 1/e],
|S(ρ) −S(σ)| ≤T(ρ, σ) log d + η(T(ρ, σ)),
(11.49)
whenever T(ρ, σ) ≤1/e, which is Fannes’ inequality. The weaker form of Fannes’
inequality for general T(ρ, σ) follows with minor modiﬁcations.
Von Neumann entropy
513
as a matrix, this property is known as double stochasticity.) Because log(·) is a strictly
concave function it follows that 
j Pij log qj ≤log ri, where ri ≡
j Pijqj, with
equality if and only if there exists a value of j for which Pij = 1. Thus
S(ρ||σ) ≥

i
pi log pi
ri
,
(11.55)
with equality if and only if for each i there exists a value of j such that Pij = 1, that is, if
and only if Pij is a permutation matrix. This has the form of the classical relative entropy.
From the non-negativity of the classical relative entropy, Theorem 11.1, we deduce that
S(ρ||σ) ≥0,
(11.56)
with equality if and only if pi = ri for all i, and Pij is a permutation matrix. To simplify
the equality conditions further, note that by relabeling the eigenstates of σ if necessary,
we can assume that Pij is the identity matrix, and thus that ρ and σ are diagonal in the
same basis. The condition pi = ri tells us that the corresponding eigenvalues of ρ and σ
are identical, and thus ρ = σ are the equality conditions.
11.3.2
Basic properties of entropy
The von Neumann entropy has many interesting and useful properties:
Theorem 11.8: (Basic properties of von Neumann entropy)
(1) The entropy is non-negative. The entropy is zero if and only if the state is
pure.
(2) In a d-dimensional Hilbert space the entropy is at most log d. The entropy is
equal to log d if and only if the system is in the completely mixed state I/d.
(3) Suppose a composite system AB is in a pure state. Then S(A) = S(B).
(4) Suppose pi are probabilities, and the states ρi have support on orthogonal
subspaces. Then
S

i
piρi

= H(pi) +

i
piS(ρi).
(11.57)
(5) Joint entropy theorem: Suppose pi are probabilities, |i⟩are orthogonal
states for a system A, and ρi is any set of density operators for another
system, B. Then
S

i
pi|i⟩⟨i| ⊗ρi

= H(pi) +

i
piS(ρi).
(11.58)
Proof
(1) Clear from the deﬁnition.
(2) The result follows from the non-negativity of the relative entropy,
0 ≤S(ρ||I/d) = −S(ρ) + log d.
(3) From the Schmidt decomposition we know that the eigenvalues of the density
operators of systems A and B are the same. (Recall the discussion following
Theorem 2.7 on page 109.) The entropy is determined completely by the
eigenvalues, so S(A) = S(B).
514
Entropy and information
(4) Let λj
i and |ej
i⟩be the eigenvalues and corresponding eigenvectors of ρi. Observe
that piλj
i and |ej
i⟩are the eigenvalues and eigenvectors of 
i piρi, and thus
S

i
piρi

= −

ij
piλj
i log piλj
i
(11.59)
= −

i
pi log pi −

i
pi

j
λj
i log λj
i
(11.60)
= H(pi) +

i
piS(ρi),
(11.61)
as required.
(5) Immediate from the preceding result.
Exercise 11.13: (Entropy of a tensor product)
Use the joint entropy theorem to
show that S(ρ ⊗σ) = S(ρ) + S(σ). Prove this result directly from the deﬁnition
of the entropy.
By analogy with the Shannon entropies it is possible to deﬁne quantum joint and
conditional entropies and quantum mutual information, for composite quantum systems.
The joint entropy S(A, B) for a composite system with two components A and B is
deﬁned in the obvious way, S(A, B) ≡−tr(ρAB log(ρAB)), where ρAB is the density
matrix of the system AB. We deﬁne the conditional entropy and mutual information by:
S(A|B) ≡S(A, B) −S(B)
(11.62)
S(A:B) ≡S(A) + S(B) −S(A, B)
(11.63)
= S(A) −S(A|B) = S(B) −S(B|A).
(11.64)
Some properties of the Shannon entropy fail to hold for the von Neumann entropy, and
this has many interesting consequences for quantum information theory. For instance,
for random variables X and Y , the inequality H(X) ≤H(X, Y ) holds. This makes
intuitive sense: surely we cannot be more uncertain about the state of X than we are
about the joint state of X and Y . This intuition fails for quantum states. Consider a
system AB of two qubits in the entangled state (|00⟩+ |11⟩)/
√
2. This is a pure state,
so S(A, B) = 0. On the other hand, system A has density operator I/2, and thus has
entropy equal to one. Another way of stating this result is that, for this system, the
quantity S(B|A) = S(A, B) −S(A) is negative.
Exercise 11.14: (Entanglement and negative conditional entropy)
Suppose
|AB⟩is a pure state of a composite system belonging to Alice and Bob. Show
that |AB⟩is entangled if and only if S(B|A) < 0.
11.3.3
Measurements and entropy
How does the entropy of a quantum system behave when we perform a measurement on
that system? Not surprisingly, the answer to this question depends on the type of mea-
surement which we perform. Nevertheless, there are some surprisingly general assertions
we can make about how the entropy behaves.
Suppose, for example, that a projective measurement described by projectors Pi is
Von Neumann entropy
515
performed on a quantum system, but we never learn the result of the measurement. If
the state of the system before the measurement was ρ then the state after is given by
ρ′ =

i
PiρPi.
(11.65)
The following result shows that the entropy is never decreased by this procedure, and
remains constant only if the state is not changed by the measurement:
Theorem 11.9: (Projective measurements increase entropy) Suppose Pi is a
complete set of orthogonal projectors and ρ is a density operator. Then the
entropy of the state ρ′ ≡
i PiρPi of the system after the measurement is at
least as great as the original entropy,
S(ρ′) ≥S(ρ),
(11.66)
with equality if and only if ρ = ρ′.
Proof
The proof is to apply Klein’s inequality to ρ and ρ′,
0 ≤S(ρ′||ρ) = −S(ρ) −tr(ρ log ρ′).
(11.67)
The result will follow if we can prove that −tr(ρ log ρ′) = S(ρ′). To do this, we apply
the completeness relation 
i Pi = I, the relation P 2
i = Pi, and the cyclic property of
the trace, to obtain
−tr(ρ log ρ′) = −tr

i
Piρ log ρ′

(11.68)
= −tr

i
Piρ log ρ′Pi

.
(11.69)
Note that ρ′Pi = PiρPi = Piρ′. That is, Pi commutes with ρ′ and thus with log ρ′, so
−tr(ρ log ρ′) = −tr

i
PiρPi log ρ′

(11.70)
= −tr(ρ′ log ρ′) = S(ρ′).
(11.71)
This completes the proof.
Exercise 11.15: (Generalized measurements can decrease entropy)
Suppose a
qubit in the state ρ is measured using the measurement operators M1 = |0⟩⟨0|
and M2 = |0⟩⟨1|. If the result of the measurement is unknown to us then the
state of the system afterwards is M1ρM †
1 + M2ρM †
2 . Show that this procedure
can decrease the entropy of the qubit.
11.3.4
Subadditivity
Suppose distinct quantum systems A and B have a joint state ρAB. Then the joint
entropy for the two systems satisﬁes the inequalities
S(A, B) ≤S(A) + S(B)
(11.72)
S(A, B) ≥|S(A) −S(B)|.
(11.73)
516
Entropy and information
The ﬁrst of these inequalities is known as the subadditivity inequality for Von Neumann
entropy, and holds with equality if and only if systems A and B are uncorrelated, that
is, ρAB = ρA ⊗ρB. The second is called the triangle inequality, or sometimes the
Araki-Lieb inequality; it is the quantum analogue of the inequality H(X, Y ) ≥H(X)
for Shannon entropies.
The proof of subadditivity is a simple application of Klein’s inequality, S(ρ) ≤
−tr(ρ log σ). Setting ρ ≡ρAB and σ ≡ρA ⊗ρB, note that
−tr(ρ log σ) = −tr(ρAB(log ρA + log ρB))
(11.74)
= −tr(ρA log ρA) −tr(ρB log ρB)
(11.75)
= S(A) + S(B).
(11.76)
Klein’s inequality therefore gives S(A, B) ≤S(A) + S(B), as desired. The equality
conditions ρ = σ for Klein’s inequality give equality conditions ρAB = ρA ⊗ρB for
subadditivity.
To prove the triangle inequality, introduce a system R which puriﬁes systems A and
B, as in Section 2.5. Applying subadditivity we have
S(R) + S(A) ≥S(A, R).
(11.77)
Since ABR is in a pure state, S(A, R) = S(B) and S(R) = S(A, B). The previous
inequality may be rearranged to give
S(A, B) ≥S(B) −S(A).
(11.78)
The equality conditions for this inequality are not quite as easily stated as those for
subadditivity. Formally, the equality conditions are that ρAR = ρA⊗ρR. Intuitively, what
this means is that A is already as entangled as it can possibly be with the outside world,
given its existing correlations with system B. A more detailed mathematical statement of
the equality conditions is given in Exercise 11.16 on this page.
By symmetry between the systems A and B we also have S(A, B) ≥S(A) −S(B).
Combining this with S(A, B) ≥S(B) −S(A) gives the triangle inequality.
Exercise 11.16: (Equality conditions for S(A, B) ≥S(B) −S(A))
Let
ρAB = 
i λi|i⟩⟨i| is a spectral decomposition for ρAB. Show that
S(A, B) = S(B) −S(A) if and only if the operators ρA
i ≡trB(|i⟩⟨i|) have a
common eigenbasis, and the ρB
i ≡trA(|i⟩⟨i|) have orthogonal support.
Exercise 11.17:
Find an explicit non-trivial example of a mixed state ρ for AB such
that S(A, B) = S(B) −S(A).
11.3.5
Concavity of the entropy
The entropy is a concave function of its inputs. That is, given probabilities pi – non-
negative real numbers such that 
i pi = 1 – and corresponding density operators ρi, the
entropy satisﬁes the inequality:
S

i
piρi

≥

i
piS(ρi).
(11.79)
The intuition is that the 
i piρi expresses the state of a quantum system which is in an
unknown state ρi with probability pi, and our uncertainty about this mixture of states
Von Neumann entropy
517
should be higher than the average uncertainty of the states ρi, since the state 
i piρi
expresses ignorance not only due to the states ρi, but also a contribution due to our
ignorance of the index i.
Suppose the ρi are states of a system A. Introduce an auxiliary system B whose state
space has an orthonormal basis |i⟩corresponding to the index i on the density operators
ρi. Deﬁne a joint state of AB by:
ρAB ≡

i
piρi ⊗|i⟩⟨i|.
(11.80)
To prove concavity we use the subadditivity of the entropy. Note that for the density
matrix ρAB we have:
S(A) = S

i
piρi

(11.81)
S(B) = S

i
pi|i⟩⟨i|

= H(pi)
(11.82)
S(A, B) = H(pi) +

i
piS(ρi).
(11.83)
Applying the subadditivity inequality S(A, B) ≤S(A) + S(B) we obtain

i
piS(ρi) ≤S

i
piρi

,
(11.84)
which is concavity! Note that equality holds if and only if all the states ρi for which
pi > 0 are identical; that is, the entropy is a strictly concave function of its inputs.
It’s worth pausing to think about the strategy we’ve employed in the proof of concavity,
and the similar strategy used to prove the triangle inequality. We introduced an auxiliary
system B in order to prove a result about the system A. Introducing auxiliary systems is
something done often in quantum information theory, and we’ll see this trick again and
again. The intuition behind the introduction of B in this particular instance is as follows:
we want to ﬁnd a system of which part is in the state 
i piρi, where the value of i is not
known. System B effectively stores the ‘true’ value of i; if A were ‘truly’ in state ρi, the
system B would be in state |i⟩⟨i|, and observing system B in the |i⟩basis would reveal
this fact. Using auxiliary systems to encode our intuition in a rigorous way is an art, but
it is also an essential part of many proofs in quantum information theory.
Exercise 11.18:
Prove that equality holds in the concavity inequality (11.79) if and
only if all the ρis are the same.
Exercise 11.19:
Show that there exists a set of unitary matrices Uj and a probability
distribution pj such that for any matrix A,

i
piUiAU †
i = tr(A)I
d,
(11.85)
where d is the dimension of the Hilbert space A lives in. Use this observation
and the strict concavity of the entropy to give an alternate proof that the
completely mixed state I/d on a space of d dimensions is the unique state of
maximal entropy.
518
Entropy and information
Exercise 11.20:
Let P be a projector and Q = I −P the complementary projector.
Prove that there are unitary operators U1 and U2 and a probability p such that
for all ρ, PρP + QρQ = pU1ρU †
1 + (1 −p)U2ρU †
2 . Use this observation to give
an alternate proof of Theorem 11.9 based on concavity.
Exercise 11.21: (Concavity of the Shannon entropy)
Use the concavity of the
von Neumann entropy to deduce that the Shannon entropy is concave in
probability distributions.
Exercise 11.22: (Alternate proof of concavity)
Deﬁne f(p) ≡S(pρ + (1 −p)σ).
Argue that to show concavity it is sufﬁcient to prove that f′′(p) ≤0. Prove that
f ′′(p) ≤0, ﬁrst for the case where ρ and σ are invertible, and then for the case
where they are not.
11.3.6
The entropy of a mixture of quantum states
The ﬂip side of concavity is the following useful theorem which provides an upper bound
on the entropy of a mixture of quantum states. Taken together the two results imply that
for a mixture 
i piρi of quantum states ρi the following inequality holds:

i
piS(ρi) ≤S

i
piρi

≤

i
piS(ρi) + H(pi).
(11.86)
The intuition behind the upper bound on the right hand side is that our uncertainty about
the state 
i piρi is never more than our average uncertainty about the state ρi, plus an
additional contribution of H(pi) which represents the maximum possible contribution
our uncertainty about the index i contributes to our total uncertainty. We now prove this
upper bound.
Theorem 11.10: Suppose ρ = 
i piρi, where pi are some set of probabilities, and the
ρi are density operators. Then
S(ρ) ≤

i
piS(ρi) + H(pi),
(11.87)
with equality if and only if the states ρi have support on orthogonal subspaces.
Proof
We begin with the pure state case, ρi = |ψi⟩⟨ψi|. Suppose the ρi are states of a system
A, and introduce an auxiliary system B with an orthonormal basis |i⟩corresponding to
the index i on the probabilities pi. Deﬁne
|AB⟩≡

i
√pi|ψi⟩|i⟩.
(11.88)
Since |AB⟩is a pure state we have
S(B) = S(A) = S

i
pi|ψi⟩⟨ψi|

= S(ρ).
(11.89)
Suppose we perform a projective measurement on the system B in the |i⟩basis. After
the measurement the state of system B is
ρB′ =

i
pi|i⟩⟨i|.
(11.90)
Strong subadditivity
519
But by Theorem 11.9 projective measurements never decrease entropy, so S(ρ) = S(B) ≤
S(B′) = H(pi). Observing that S(ρi) = 0 for the pure state case, we have proved that
S(ρ) ≤H(pi) +

i
piS(ρi),
(11.91)
when the states ρi are pure states. Furthermore, equality holds if and only if B = B′,
which is easily seen to occur if and only if the states |ψi⟩are orthogonal.
The mixed state case is now easy. Let ρi = 
j pi
j|ei
j⟩⟨ei
j| be orthonormal decompo-
sitions for the states ρi, so ρ = 
ij pipi
j|ei
j⟩⟨ei
j|. Applying the pure state result and the
observation that 
j pi
j = 1 for each i, we have
S(ρ) ≤−

ij
pipi
j log(pipi
j)
(11.92)
= −

i
pi log pi −

i
pi

j
pi
j log pi
j
(11.93)
= H(pi) +

i
piS(ρi),
(11.94)
which is the desired result. The equality conditions for the mixed state case follow
immediately from the equality conditions for the pure state case.
11.4
Strong subadditivity
The subadditivity and triangle inequalities for two quantum systems can be extended
to three systems. The basic result is known as the strong subadditivity inequality, and
it is one of the most important and useful results in quantum information theory. The
inequality states that for a trio of quantum systems, A, B, C,
S(A, B, C) + S(B) ≤S(A, B) + S(B, C).
(11.95)
Unfortunately, unlike the classical case, all known proofs of the quantum strong subad-
ditivity inequality are quite difﬁcult. However, it is so useful in quantum information
theory that we give a full proof of this result. The basic structure of the proof is presented
in Section 11.4.1, with some of the details of the proof left to Appendix 6.
11.4.1
Proof of strong subadditivity
The proof of the strong subadditivity inequality which we shall give is based upon a deep
mathematical result known as Lieb’s theorem. We begin with a deﬁnition which allows
us to state Lieb’s theorem.
Suppose f(A, B) is a real-valued function of two matrices, A and B. Then f is said
to be jointly concave in A and B if for all 0 ≤λ ≤1,
f(λA1 + (1 −λ)A2, λB1 + (1 −λ)B2) ≥λf(A1, B1) + (1 −λ)f(A2, B2). (11.96)
Exercise 11.23: (Joint concavity implies concavity in each input)
Let f(A, B)
be a jointly concave function. Show that f(A, B) is concave in A, with B held
ﬁxed. Find a function of two variables that is concave in each of its inputs, but is
not jointly concave.
520
Entropy and information
Theorem 11.11: (Lieb’s theorem) Let X be a matrix, and 0 ≤t ≤1. Then the
function
f(A, B) ≡tr(X†AtXB1−t)
(11.97)
is jointly concave in positive matrices A and B.
Proof
See Appendix 6 for the proof of Lieb’s theorem.
Lieb’s theorem implies a sequence of results, each interesting in its own right, culmi-
nating in the proof of strong subadditivity. We begin with the convexity of the relative
entropy.
Theorem 11.12: (Convexity of the relative entropy) The relative entropy S(ρ∥σ) is
jointly convex in its arguments.
Proof
For arbitrary matrices A and X acting on the same space deﬁne
It(A, X) ≡tr(X†AtXA1−t) −tr(X†XA).
(11.98)
The ﬁrst term in this expression is concave in A, by Lieb’s theorem, and the second term
is linear in A. Thus, It(A, X) is concave in A. Deﬁne
I(A, X) ≡d
dt
++++
t=0
It(A, X) = tr(X†(log A)XA) −tr(X†X(log A)A).
(11.99)
Noting that I0(A, X) = 0 and using the concavity of It(A, X) in A we have
I(λA1 + (1 −λ)A2, X) = lim
Δ→0
IΔ(λA1 + (1 −λ)A2, X)
Δ
(11.100)
≥λ lim
Δ→0
IΔ(A1, X)
Δ
+ (1 −λ) lim
Δ→0
IΔ(A2, X)
Δ
(11.101)
= λI(A1, X) + (1 −λ)I(A2, X).
(11.102)
That is, I(A, X) is a concave function of A. Deﬁning the block matrices
A ≡
 ρ
0
0
σ

,
X ≡
 0
0
I
0

(11.103)
we can easily verify that I(A, X) = −S(ρ∥σ). The joint convexity of S(ρ∥σ) follows
from the concavity of I(A, X) in A.
Corollary 11.13: (Concavity of the quantum conditional entropy) Let AB be a
composite quantum system with components A and B. Then the conditional
entropy S(A|B) is concave in the state ρAB of AB.
Strong subadditivity
521
Proof
Let d be the dimension of system A. Note that
S

ρAB
<<<<
I
d ⊗ρB

= −S(A, B) −tr

ρAB log
I
d ⊗ρB

(11.104)
= −S(A, B) −tr(ρB log ρB) + log d
(11.105)
= −S(A|B) + log d.
(11.106)
Thus S(A|B) = log d −S(ρAB∥I/d ⊗ρB). The concavity of S(A|B) follows from the
joint convexity of the relative entropy.
Theorem 11.14: (Strong subadditivity) For any trio of quantum systems, A, B, C,
the inequalities
S(A) + S(B) ≤S(A, C) + S(B, C)
(11.107)
S(A, B, C) + S(B) ≤S(A, B) + S(B, C)
(11.108)
hold.
Proof
The two inequalities are in fact equivalent. We will use concavity of the conditional
entropy to prove the ﬁrst, and then show that the second follows. Deﬁne a function
T(ρABC) of density operators on the system ABC,
T(ρABC) ≡S(A) + S(B) −S(A, C) −S(B, C) = −S(C|A) −S(C|B).(11.109)
From the concavity of the conditional entropy we see that T(ρABC) is a convex function of
ρABC. Let ρABC = 
i pi|i⟩⟨i| be a spectral decomposition of ρABC. From the convexity
of T, T(ρABC) ≤
i piT(|i⟩⟨i|). But T(|i⟩⟨i|) = 0 as for a pure state S(A, C) = S(B)
and S(B, C) = S(A). It follows that T(ρABC) ≤0, and thus
S(A) + S(B) −S(A, C) −S(B, C) ≤0,
(11.110)
which is the ﬁrst inequality we set out to prove.
To obtain the second inequality, introduce an auxiliary system R purifying the system
ABC. Then using the just-proved inequality we have
S(R) + S(B) ≤S(R, C) + S(B, C).
(11.111)
Since ABCR is a pure state, S(R) = S(A, B, C) and S(R, C) = S(A, B), so (11.111
becomes
S(A, B, C) + S(B) ≤S(A, B) + S(B, C),
(11.112)
as we set out to show.
Exercise 11.24:
We obtained strong subadditivity as a consequence of the inequality
S(A) + S(B) ≤S(A, C) + S(B, C). Show that this inequality can be obtained as
a consequence of strong subadditivity.
Exercise 11.25:
We obtained strong subadditivity as a consequence of the concavity of
the conditional information, S(A|B). Show that the concavity of the conditional
522
Entropy and information
entropy may be deduced from strong subadditivity. (Hint: You may need to
introduce an auxiliary system into the problem.)
11.4.2
Strong subadditivity: elementary applications
Strong subadditivity and related results have many useful implications for quantum in-
formation theory. Let’s take a look at a few of the elementary consequences of these
results.
First, it is worth emphasizing how remarkable it is that the inequality S(A) + S(B) ≤
S(A, C)+S(B, C) holds. The corresponding inequality holds also for Shannon entropies,
but for different reasons. For Shannon entropies it is true that H(A) ≤H(A, C) and
H(B) ≤H(B, C), so the sum of the two inequalities must necessarily be true. In the
quantum case, it is possible to have either S(A) > S(A, C) or S(B) > S(B, C), yet
somehow Nature conspires in such a way that both of these possibilities are not true
simultaneously, in order to ensure that the condition S(A)+S(B) ≤S(A, C)+S(B, C)
is always satisﬁed. Other ways of rephrasing this are in terms of conditional entropies
and mutual informations,
0 ≤S(C|A) + S(C|B)
(11.113)
S(A:B) + S(A:C) ≤2S(A) ,
(11.114)
both of which are also remarkable inequalities, for similar reasons. Note, however, that the
inequality 0 ≤S(A|C)+S(B|C), which one might hope to be true based upon (11.114),
is not, as can be seen by choosing ABC to be a product of a pure state for A with an
EPR state for BC.
Exercise 11.26:
Prove that S(A:B) + S(A:C) ≤2S(A). Note that the corresponding
inequality for Shannon entropies holds since H(A:B) ≤H(A). Find an example
where S(A:B) > S(A).
For practical applications strong subadditivity is often most easily applied by using a
rephrasing in terms of the conditional or mutual informations. The following theorem
lists three very simple reformulations of strong subadditivity that provide a powerful
intuitive guide to the properties of quantum entropy.
Theorem 11.15:
(1) Conditioning reduces entropy: Suppose ABC is a composite quantum
system. Then S(A|B, C) ≤S(A|B).
(2) Discarding quantum systems never increases mutual information:
Suppose ABC is a composite quantum system. Then
S(A:B) ≤S(A:B, C).
(3) Quantum operations never increase mutual information: Suppose
AB is a composite quantum system and E is a trace-preserving quantum
operation on system B. Let S(A:B) denote the mutual information between
systems A and B before E is applied to system B, and S(A′ :B′) the mutual
information after E is applied to system B. Then S(A′ :B′) ≤S(A:B).
Strong subadditivity
523
Proof
(1) The proof is the same as the classical proof (part of Theorem 11.3 on page 506),
which we reproduce for convenience: S(A|B, C) ≤S(A|B) is equivalent to
S(A, B, C) −S(B, C) ≤S(A, B) −S(B), which is equivalent to
S(A, B, C) + S(B) ≤S(A, B) + S(B, C), which is strong subadditivity.
(2) S(A:B) ≤S(A:B, C) is equivalent to
S(A) + S(B) −S(A, B) ≤S(A) + S(B, C) −S(A, B, C), which is equivalent to
S(A, B, C) + S(B) ≤S(A, B) + S(B, C), which is strong subadditivity.
(3) By the constructions of Chapter 8 the action of E on B may be simulated by
introducing a third system C, initially in a pure state |0⟩, and a unitary interaction
U between B and C. The action of E on B is equivalent to the action of U
followed by discarding system C. Letting primes denote the state of the systems
after U has acted we have S(A:B) = S(A:B, C) because C starts out in a product
state with AB, and clearly S(A:B, C) = S(A′ :B′, C′). Discarding systems cannot
increase mutual information, so S(A′ :B′) ≤S(A′ :B′, C′). Putting it all together
gives S(A′ :B′) ≤S(A:B), as required.
There is an interesting set of questions related to the subadditivity properties of quan-
tum conditional entropies. We saw earlier that the Shannon mutual information is not
subadditive, and thus the quantum mutual information is not subadditive either. What
about the subadditivity of the conditional entropy? That is, is it true that
S(A1, A2|B1, B2) ≤S(A1|B1) + S(A2|B2),
(11.115)
for any four quantum systems A1, A2, B1 and B2? It turns out that this inequality is
correct. What’s more, the conditional entropy is also subadditive in the ﬁrst and sec-
ond entries. Proving these facts is an instructive exercise in the application of strong
subadditivity.
Theorem 11.16: (Subadditivity of the conditional entropy) Let ABCD be a
composite of four quantum systems. Then the conditional entropy is jointly
subadditive in the ﬁrst and second entries:
S(A, B|C, D) ≤S(A|C) + S(B|D).
(11.116)
Let ABC be a composite of three quantum systems. Then the conditional
entropy is subadditive in each of the ﬁrst and second entries:
S(A, B|C) ≤S(A|C) + S(B|C)
(11.117)
S(A|B, C) ≤S(A|B) + S(A|C).
(11.118)
Proof
To prove joint subadditivity in both entries, note that by strong subadditivity
S(A, B, C, D) + S(C) ≤S(A, C) + S(B, C, D).
(11.119)
Adding S(D) to each side of this inequality, we obtain
S(A, B, C, D) + S(C) + S(D) ≤S(A, C) + S(B, C, D) + S(D).
(11.120)
524
Entropy and information
Applying strong subadditivity to the last two terms of the right hand side gives
S(A, B, C, D) + S(C) + S(D) ≤S(A, C) + S(B, D) + S(C, D).
(11.121)
Rearranging this inequality gives
S(A, B|C, D) ≤S(A|C) + S(B|D),
(11.122)
which is the joint subadditivity of the conditional entropy.
Subadditivity of the conditional entropy in the ﬁrst entry, S(A, B|C) ≤S(A|C) +
S(B|C), is trivially seen to be equivalent to strong subadditivity. Subadditivity in the
second entry is slightly more challenging. We wish to show that S(A|B, C) ≤S(A|B)+
S(A|C). Note that this is equivalent to demonstrating the inequality
S(A, B, C) + S(B) + S(C) ≤S(A, B) + S(B, C) + S(A, C).
(11.123)
To prove this, note that at least one of the inequalities S(C) ≤S(A, C) or S(B) ≤
S(A, B) must be true, as S(A|B)+S(A|C) ≥0 by the ﬁrst inequality in Theorem 11.14.
Suppose S(C) ≤S(A, C). Adding to this inequality the strong subadditivity inequality,
S(A, B, C) + S(B) ≤S(A, B) + S(B, C) gives the result. A similar proof holds in the
case when S(B) ≤S(A, B).
When we introduced the relative entropy it was described as being rather like a measure
of distance between probability distributions or between density operators. Imagine that
a quantum system consists of two parts, labeled A and B, and that we are given two
density operators ρAB and σAB. In order to fulﬁl its distance-like promise, a very desirable
property of S(·∥·) is that it decrease when part of the system is ignored, that is:
S(ρA∥σA) ≤S(ρAB∥σAB).
(11.124)
This result is known as the monotonicity of the relative entropy. Intuitively this is a
very reasonable property for a measure of distance to have; we expect that ignoring part
of a physical system makes it harder to distinguish two states of that system (compare
Section 9.2.1), and thus decrease any reasonable measure of distance between them.
Theorem 11.17: (Monotonicity of the relative entropy) Let ρAB and σAB be any
two density matrices of a composite system AB. Then
S(ρA∥σA) ≤S(ρAB∥σAB).
(11.125)
Proof
Exercise 11.19 on page 517 implies that there exist unitary transformations Uj on the
space B and probabilities pj such that
ρA ⊗I
d =

j
pjUjρABU †
j
(11.126)
for all ρAB. From the convexity of the relative entropy we obtain
S

ρA ⊗I
d
<<<< σA ⊗I
d

≤

j
pjS
'
Uj ρABU †
j
<<< UjσABU †
j
(
.
(11.127)
Chapter problems
525
But the relative entropy is invariant under unitary conjugation, so this gives
S

ρA ⊗I
d
<<<< σA ⊗I
d

≤

j
pjS

ρAB<< σAB = S

ρAB<< σAB .
(11.128)
Combining this with the easily veriﬁed observation that
S

ρA ⊗I
d
<<<< σA ⊗I
d

= S(ρA∥σA),
(11.129)
gives the monotonicity of the relative entropy.
Problem 11.1: (Generalized Klein’s inequality)
Suppose f(·) is a convex function
from real numbers to real numbers. Then f induces a natural function f(·) on
Hermitian operators, as described in Section 2.1.8 on page 75. Prove that
tr(f(A) −f(B)) ≥tr((A −B)f ′(B)) .
(11.130)
Use this result to show that the relative entropy is non-negative.
Problem 11.2: (Generalized relative entropy)
The deﬁnition of the relative
entropy may be extended to apply to any two positive operators r and s,
S(r∥s) ≡tr(r log r) −tr(r log s).
(11.131)
The earlier argument proving joint convexity of the relative entropy goes directly
through for this generalized deﬁnition:
(1) For α, β > 0 show that
S(αr∥βs) = αS(r∥s) + α tr(r) log(α/β).
(11.132)
(2) Prove that the joint convexity of the relative entropy implies the
subadditivity of the relative entropy,
S(r1 + r2∥s1 + s2) ≤S(r1∥s1) + S(r2∥s2).
(11.133)
(3) Prove that subadditivity of the relative entropy implies joint convexity of the
relative entropy.
(4) Let pi and qi be probability distributions over the same set of indices. Show
that
S

i
piri∥

i
qisi

≤

i
piS(ri∥si) +

i
pitr(ri) log(pi/qi) .
(11.134)
In the case where the ri are density operators so tr(ri) = 1, this reduces to
the pretty formula
S

i
piri∥

i
qisi

≤

i
piS(ri∥si) + H(pi||qi) ,
(11.135)
where H(·||·) is the Shannon relative entropy.
Problem 11.3: (Analogue of the triangle inequality for conditional entropy)
(1) Show that H(X, Y |Z) ≥H(X|Z).
(2) Show that it is not always true that S(A, B|C) ≥S(A|C).
526
Entropy and information
(3) Prove the conditional version of the triangle inequality,
S(A, B|C) ≥S(A|C) −S(B|C).
(11.136)
Problem 11.4: (Conditional forms of strong subadditivity)
(1) Prove that S(A, B, C|D) + S(B|D) ≤S(A, B|D) + S(B, C|D).
(2) Show by explicit example that it is not always true that
H(D|A, B, C) + H(D|B) ≤H(D|A, B) + H(D|B, C).
Problem 11.5: (Strong subadditivity – Research)
Find a simple proof of the
strong subadditivity inequality for quantum entropies.
Summary of Chapter 11: Entropy and information
• Fundamental measures of information arise as the answers to questions
about the quantity of physical resources required to solve some informa-
tion processing problem.
• Basic deﬁnitions:
(entropy)
S(A) = −tr(ρA log ρA)
(relative entropy)
S(ρ∥σ) = −S(ρ) −tr(ρ log σ)
(conditional entropy)
S(A|B) = S(A, B) −S(B)
(mutual information)
S(A:B) = S(A) + S(B) −S(A, B)
• Strong subadditivity: S(A, B, C) + S(B) ≤S(A, B) + S(B, C). The other
entropy inequalities we discussed are corollaries of this or the joint convexity of
the relative entropy.
• The relative entropy is jointly convex in its arguments.
• The relative entropy is monotonic: S(ρA∥σA) ≤S(ρAB∥σAB).
History and further reading
Historically, the concept of entropy ﬁrst arose in the study of thermodynamics and
statistical mechanics. But the modern information-theoretic foundation for the study
of entropy came in Shannon’s wonderful papers on information theory [Sha48]. A good
general reference on properties of the Shannon entropy (and much else in information
theory) are Chapters 2 and 16 of Cover and Thomas[CT91]. General references on the
von Neumann entropy are the review article by Wehrl[Weh78], and the book by Ohya and
Petz[OP93].
The entropic uncertainty principle we prove is due to Deutsch[Deu83]. Many other
people have worked on entropic uncertainty relations, and we just mention two other pa-
pers of interest. Kraus[Kra87] conjectured an entropic uncertainty relation strengthening
Deutsch’s for a particular class of measurements, and Maassen and Ufﬁnk[MU88] proved
Kraus’ conjecture. The relative entropy was introduced by Kullback and Leibler[KL51],
and its quantum generalization is due to Umegaki[Ume62]. Fannes’ inequality appeared in
[Fan73]. Klein’s inequality was proved in [Kle31]. The triangle inequality is due to Araki
History and further reading
527
and Lieb[AL70]. Strong subadditivity has an interesting history. Robinson and Ruelle[RR67]
ﬁrst noted the importance of classical strong subadditivity for statistical physics. The
quantum version was then conjectured in 1968 by Lanford and Robinson[LR68]. Obtain-
ing a proof of the result was rather difﬁcult however. Finally, in 1973 the theorem was
proved in two papers: Lieb’s eponymous theorem in [Lie73], while the surprising con-
nection to strong subadditivity was developed in Lieb and Ruskai[LR73b]; see also[LR73a].
Lieb’s theorem is a generalization of the Wigner–Yanase–Dyson conjecture made in 1963
by Wigner and Yanase[WY63] and subsequently extended by Dyson (unpublished); prior
to 1973 it was not known that the Wigner–Yanase–Dyson conjecture and strong sub-
additivity were related! See Wehrl[Weh78] for a discussion of the Wigner–Yanase–Dyson
conjecture. The proof of Lieb’s theorem we have given is due to Simon[Sim79], and is a
variant of a proof by Uhlmann[Uhl77]. Other proofs of Lieb’s theorem are also known.
See for example, Epstein[Eps73], Ando[And79], and Petz[Pet86]. Subadditivity of the relative
entropy in the ﬁrst and second entries was proved by Lieb[Lie75]. Joint subadditivity of
the quantum conditional entropy was proved by Nielsen[Nie98]. The monotonicity of the
relative entropy was ﬁrst noted by Lindblad[Lin75]. Problem 11.2 is due to Ruskai[Rus94].
12 Quantum information theory
Classical information theory is mostly concerned with the problem of sending classical
information – letters in an alphabet, speech, strings of bits – over communications chan-
nels which operate in accordance with the laws of classical physics. How does the picture
change if we can build quantum-mechanical communications channels? Can we transmit
information more efﬁciently? Can we make use of quantum mechanics to transmit secret
information without being eavesdropped on? These are just two of the questions we may
ask when communication channels are allowed to be quantum mechanical. This redeﬁni-
tion of what a channel is causes us to go back and re-examine the fundamental questions
motivating classical information theory, in the search for new answers. This chapter sur-
veys what is known about quantum information theory, including some surprising and
intriguing possibilities made possible by quantum communication channels.
Quantum information theory is motivated by the study of communications channels,
but it has a much wider domain of application, and it is a thought-provoking challenge
to capture in a verbal nutshell the goals of the ﬁeld. As described in Section 1.6, we
can identify three fundamental goals uniting work on quantum information theory: to
identify elementary classes of static resources in quantum mechanics (which we identify
as types of ‘information’); to identify elementary classes of dynamical processes in quan-
tum mechanics (identiﬁed as types of ‘information processing’); and to quantify resource
tradeoffs incurred performing elementary dynamical processes. Quantum information
theory is fundamentally richer than classical information theory, because quantum me-
chanics includes so many more elementary classes of static and dynamic resources – not
only does it support all the familiar classical types, but there are entirely new classes
such as the static resource of entanglement to make life even more interesting than it is
classically.
The title of the chapter is ‘Quantum information theory’, and you may be forgiven for
wondering how it is that we can hope to cover all aspects of quantum information theory
in a single chapter. In fact, quantum information theory contains many facets other than
those described here, including the study of quantum operations, the deﬁnition and study
of ﬁdelity measures, quantum error-correcting codes, and various notions of entropy –
all topics which we have described in detail in earlier chapters. The purpose of the
present chapter is to describe quantum information theory in its ‘purest’ form; those
other chapters are focused on developing speciﬁc tools, while we are concerned here with
the grand sweep of things, with the most general statements one can make about the
properties of quantum information.
We begin in Section 12.1 with a discussion of some of the unique properties of quantum
states when compared to classical states, in the language of information theory. Not only
are quantum states impossible to copy, generally, but also they cannot be perfectly distin-
guished! This is quantiﬁed by the Holevo bound. We then consider, in Section 12.2, an
Distinguishing quantum states and the accessible information
529
elementary information-theoretic task, data compression, and show how quantum states
can be compressed much as classical ones are. This is done by paralleling the theorem
of typical sequences with the typical subspace theorem, to prove Schumacher’s quantum
noiseless channel coding theorem, the analogue of Shannon’s noiseless channel coding
theorem. A natural generalization of this problem is the capacity of a noisy channel for
classical information, and in Section 12.3 we deﬁne and prove the analogue to Shannon’s
noisy channel coding theorem, known as the Holevo–Schumacher–Westmoreland theo-
rem. The most difﬁcult challenge is the capacity of a noisy quantum channel for quantum
information. This is the subject of Section 12.4, in which the entropy exchange, the quan-
tum Fano inequality, and the quantum data processing inequality are deﬁned, but the
open question of the capacity is left unresolved. Two applications of the noisy channel
relations, the quantum Singleton bound, and the exorcism of Maxwell’s demon, are pre-
sented, and the ﬁrst half of this chapter is summarized. Two recurring themes throughout
this exploration of quantum information are entanglement and non-orthogonality, and
these subjects are the focus of the last two sections of the chapter. Section 12.5 describes
how entanglement can be thought of as a physical resource, and explains how it can be
transformed, distilled, and diluted. And ﬁnally, in Section 12.6, we present quantum
cryptography, a provably secure means of communication whose success arises from the
many properties of quantum information considered in this chapter.
12.1
Distinguishing quantum states and the accessible information
There is a simple game we can play to illustrate the remarkable differences between
quantum and classical information. We will describe this game using two ﬁctitious pro-
tagonists, Alice and Bob; of course the results can be rephrased in more abstract language,
but the anthropocentric language makes the results easier to think (and write!) about.
Suppose Alice has a classical information source producing symbols X = 0, . . . , n
according to a probability distribution p0, . . . , pn. The aim of the game is for Bob to
determine the value of X as best he can. To achieve this goal, Alice prepares a quantum
state ρX chosen from some ﬁxed set ρ0, . . . , ρn, and gives the state to Bob, who makes
a quantum measurement on the state he has been given, and then tries to make the best
guess he can as to the identity of X, based on his measurement result Y .
A good measure of how much information Bob has gained about X through the
measurement is the mutual information H(X : Y ) between X and the measurement
outcome Y , as deﬁned in Chapter 11. By the data processing inequality we know that
Bob can infer X from Y if and only if H(X : Y ) = H(X), and that in general H(X :
Y ) ≤H(X); we will see later that the closeness of H(X :Y ) to H(X) actually provides
a quantitative measure of how well Bob can determine X. Bob’s goal is to choose a
measurement which maximizes H(X : Y ), bringing it as close as possible to H(X).
To this end, we deﬁne Bob’s accessible information to be the maximum of the mutual
information H(X :Y ) over all possible measurement schemes. The accessible information
is a measure of how well Bob can do at inferring the state Alice prepared.
In classical information theory, the accessible information is not so interesting; while
in practice it may be difﬁcult to distinguish two classical states – consider the troubles we
have reading bad handwriting – in principle it is always possible. In contrast, in quantum
mechanics it is not always possible to distinguish distinct states, even in principle. For
example, we saw in Box 2.3 on page 87 that there is no quantum mechanical procedure to
530
Quantum information theory
reliably distinguish two non-orthogonal quantum states. Stated in terms of accessible in-
formation, if Alice prepares the state |ψ⟩with probability p, and another, non-orthogonal
state |ϕ⟩with probability 1 −p, the accessible information of this preparation is strictly
less than H(p), as it is not possible for Bob to determine the identity of the state with full
reliability. Classically, if Alice prepares one of two classical states – say a bit in the state 0
with probability p, or in the state 1 with probability 1−p – then there is no fundamental
reason of principle why Bob cannot distinguish between these two states, and thus the
accessible information is the same as the entropy of preparation, H(p).
There’s an important caveat to this discussion, a context in which the concept of
accessible information does make sense classically. The context is that of distinguishing
probability distributions. Imagine that Alice prepares the state 0 or 1 according to one of
two probability distributions, either (p, 1 −p) or (q, 1 −q). Given the state, Bob’s goal is
to identify which probability distribution Alice used to prepare the state. Clearly, it is not
always possible for Bob to perform this identiﬁcation with perfect reliability! Nevertheless,
this example (analogous to the accessible information for a quantum system prepared in
one of a set of mixed states) is of subsidiary importance. What is most important and
remarkable is that the fundamental objects in quantum mechanics – the pure quantum
states – enjoy distinguishability properties that are markedly different and much richer
than the corresponding properties for the fundamental objects of classical information
theory, such as ‘0’ or ‘1’.
The no-cloning theorem provides another perspective on the lack of accessibility suf-
fered by quantum information in comparison to classical information. Classical informa-
tion can, of course, be copied. This can be done exactly with digital information, like the
multiply backed-up LATEXﬁle being used to generate this book, or approximately, as with
the analog images appearing on each page of this book, which have been copied by print-
ing press prior to distribution. Surprisingly, the no-cloning theorem states that quantum
mechanics does not allow unknown quantum states to be copied exactly, and places severe
limitations on our ability to make approximate copies. The no-cloning theorem is proved
in Box 12.1.
At ﬁrst sight the no-cloning theorem appears rather peculiar. After all, isn’t classi-
cal physics a special case of quantum mechanics? How is it possible that we can copy
classical information if we can’t copy quantum states? The answer is that the no-cloning
theorem does not prevent all quantum states from being copied, it simply says that non-
orthogonal quantum states cannot be copied. More precisely, suppose |ψ⟩and |ϕ⟩are
two non-orthogonal quantum states. Then the no-cloning theorem implies that it is not
possible to build a quantum device that, when input with |ψ⟩or |ϕ⟩, will output two
copies of the input state, |ψ⟩|ψ⟩or |ϕ⟩|ϕ⟩. On the other hand, if |ψ⟩and |ϕ⟩are or-
thogonal, then the no-cloning theorem doesn’t prohibit their cloning. Indeed, it is rather
easy to design quantum circuits which copy such states! This observation resolves the
apparent contradiction between the no-cloning theorem and the ability to copy classical
information, for the different states of classical information can be thought of merely as
orthogonal quantum states.
Exercise 12.1:
Suppose |ψ⟩and |ϕ⟩are two orthogonal quantum states of a single
qubit. Design a quantum circuit with two input qubits (the ‘data’ and the ‘target’
qubits), with the data qubit in either the state |ψ⟩or |ϕ⟩, and the target qubit
Distinguishing quantum states and the accessible information
531
prepared in the standard state |0⟩, which produces as output |ψ⟩|ψ⟩or |ϕ⟩|ϕ⟩,
depending on whether |ψ⟩or |ϕ⟩was input to the data qubit.
What is the connection between cloning and accessible information? Suppose Alice
prepares one of two non-orthogonal quantum states |ψ⟩and |ϕ⟩with respective proba-
bilities p and 1−p. Suppose it were the case that Bob’s accessible information about these
states was H(p), that is, the laws of quantum mechanics allowed Bob to obtain enough
information by measurement to identify which of the two states |ψ⟩and |ϕ⟩Alice had
prepared. Then Bob could clone the states in a very simple manner: he would perform a
measurement to determine which of |ψ⟩and |ϕ⟩had been prepared by Alice, and once he
had made the identiﬁcation, could prepare at will multiple copies of whichever state |ψ⟩
or |ϕ⟩Alice had given him. Thus, the no-cloning theorem can be seen as a consequence of
the fact that the accessible information for these states is strictly less than H(p). It’s also
possible to turn this perspective around, and view the fact that the accessible information
is less than H(p) as a consequence of the no-cloning theorem! This is done as follows.
Imagine that it is possible to clone non-orthogonal states. After receiving the state |ψ⟩
or |ϕ⟩from Alice, Bob could repeatedly apply such a cloning device to obtain the state
|ψ⟩⊗n or |ϕ⟩⊗n. In the limit of large n these two states become very nearly orthogonal
and it is possible to distinguish them with arbitrarily high reliability by a projective mea-
surement. That is, if it were possible to clone then Bob could identify with arbitrarily
high probability of success whether the state |ψ⟩or |ϕ⟩had been prepared, and thus the
accessible information would be H(p). We can therefore view the no-cloning theorem as
being equivalent to the statement than in quantum mechanics the accessible information
for non-orthogonal states is in general less than the entropy of preparation.
As we have emphasized throughout the book, the hidden nature of quantum informa-
tion lies at the heart of the power of quantum computation and quantum information, and
the accessible information captures in a quantitative way this hidden nature of quantum
information. Unfortunately, no general method for calculating the accessible information
is known; however, a variety of important bounds can be proved, the most important of
which is the Holevo bound.
12.1.1
The Holevo bound
The Holevo bound is an exceedingly useful upper bound on the accessible information
that plays an important role in many applications of quantum information theory.
Theorem 12.1: (The Holevo bound) Suppose Alice prepares a state ρX where
X = 0, . . . , n with probabilities p0, . . . , pn. Bob performs a measurement
described by POVM elements {Ey} = {E0, . . . , Em} on that state, with
measurement outcome Y . The Holevo bound states that for any such
measurement Bob may do:
H(X :Y ) ≤S(ρ) −

x
pxS(ρx) ,
(12.6)
where ρ = 
x pxρx.
The Holevo bound is thus an upper bound on the accessible information. The quantity
appearing on the right hand side of the Holevo bound is so useful in quantum information
theory that it is given a name, the Holevo χ quantity, and is sometimes denoted χ.
532
Quantum information theory
Box 12.1: The no-cloning theorem
Is it possible to make a copy of an unknown quantum state? Surprisingly, it turns
out that the answer to this question is no. In this box we describe an elementary
proof of this fact that captures the essential reason this is not possible.
Suppose we have a quantum machine with two slots labeled A and B. Slot A, the
data slot, starts out in an unknown but pure quantum state, |ψ⟩. This is the state
which is to be copied into slot B, the target slot. We assume that the target slot
starts out in some standard pure state, |s⟩. Thus the initial state of the copying
machine is
|ψ⟩⊗|s⟩.
(12.1)
Some unitary evolution U now effects the copying procedure, ideally,
|ψ⟩⊗|s⟩
U
−→U
|ψ⟩⊗|s⟩
 = |ψ⟩⊗|ψ⟩.
(12.2)
Suppose this copying procedure works for two particular pure states, |ψ⟩and |ϕ⟩.
Then we have
U
|ψ⟩⊗|s⟩
 = |ψ⟩⊗|ψ⟩
(12.3)
U
|ϕ⟩⊗|s⟩
 = |ϕ⟩⊗|ϕ⟩.
(12.4)
Taking the inner product of these two equations gives
⟨ψ|ϕ⟩= (⟨ψ|ϕ⟩)2.
(12.5)
But x = x2 has only two solutions, x = 0 and x = 1, so either |ψ⟩= |ϕ⟩or |ψ⟩and
|ϕ⟩are orthogonal. Thus a cloning device can only clone states which are orthogonal
to one another, and therefore a general quantum cloning device is impossible. A
potential quantum cloner cannot, for example, clone the qubit states |ψ⟩= |0⟩and
|ϕ⟩= (|0⟩+ |1⟩)/
√
2, since these states are not orthogonal.
What we have shown is that it is impossible to perfectly clone an unknown quantum
state using unitary evolution. Several questions naturally arise: what if we try to
copy a mixed state? What if we allow cloning devices that are not unitary? What if
we are willing to allow imperfect copies that nevertheless are ‘good’ according to
some interesting measure of ﬁdelity? These are all good questions which have been
the subject of much investigation, as can be seen from the end of chapter ‘History
and further reading.’ The short summary of this work is that even if one allows
non-unitary cloning devices, the cloning of non-orthogonal pure states remains
impossible unless one is willing to tolerate a ﬁnite loss of ﬁdelity in the copied
states. Similar conclusions hold also for mixed states, although a somewhat more
sophisticated approach is necessary to even deﬁne what is meant by the notion of
cloning a mixed state.
Proof
The proof of the Holevo bound is via a simple and beautiful construction involving three
quantum systems, which we label P, Q and M. The system Q is the quantum system
Alice gives to Bob; P and M are ﬁctitious auxiliary systems introduced to ease the proof,
Distinguishing quantum states and the accessible information
533
just as was done in proving many of the entropic inequalities in Chapter 11. Intuitively,
P may be thought of as the ‘preparation’ system. By deﬁnition it has an orthonormal
basis |x⟩whose elements correspond to the labels 0, . . . , n on the possible preparations
for the quantum system, Q. M can be thought of intuitively as Bob’s ‘measuring device’,
and it has a basis |y⟩whose elements correspond to the possible outcomes 1, . . . , n of
Bob’s measurement. The initial state of the total system is assumed to be
ρP QM =

x
px|x⟩⟨x| ⊗ρx ⊗|0⟩⟨0|,
(12.7)
where we write the tensor product decomposition in the order PQM. Intuitively, this
state represents the situation that Alice has chosen a value of x with probability px,
prepared a corresponding ρx and given it to Bob, who is about to use his measuring
apparatus, initially in the standard state |0⟩, to perform the measurement. To describe
the measurement we introduce a quantum operation E acting on the systems Q and M
only (not P), whose action is to perform a measurement with POVM elements {Ey} on
the system Q, and to store the result of the measurement in system M:
E(σ ⊗|0⟩⟨0|) ≡

y

Eyσ

Ey ⊗|y⟩⟨y| ,
(12.8)
where σ is any state of system Q, and |0⟩is the initial state of the measuring apparatus.
In the following exercise you show that E is a trace-preserving quantum operation.
Exercise 12.2:
Deﬁne Uy to be the unitary operator acting on system M whose action
on a basis is Uy|y′⟩≡|y′ + y⟩, where the addition is done modulo n + 1. Show
that {
Ey ⊗Uy} is a set of operation elements deﬁning a trace-preserving
quantum operation E whose action on states of the form σ ⊗|0⟩⟨0| agrees
with (12.8).
The proof of Holevo’s bound now proceeds as follows. Using primes to denote states of
PQM after application of E, and unprimed states to indicate the time prior to application
of E we have S(P : Q) = S(P : Q, M), since M is initially uncorrelated with P and Q,
and S(P :Q, M) ≥S(P ′ :Q′, M ′), since applying the quantum operation E to QM can’t
increase the mutual information between P and QM (Theorem 11.15 on page 522),
and ﬁnally S(P ′ : Q′, M ′) ≥S(P ′ : M ′), since discarding systems can’t increase mutual
information (also Theorem 11.15). Putting these results together gives
S(P ′ :M ′) ≤S(P :Q) .
(12.9)
This result, with a little simple algebra, is easily understood to be the Holevo bound!
Let’s ﬁrst calculate the quantity on the right hand side. Note that
ρP Q =

x
px|x⟩⟨x| ⊗ρx ,
(12.10)
from which it follows that S(P) = H(px), S(Q) = S(ρ), and S(P, Q) = H(px) +

x pxS(ρx) (by Theorem 11.10 on page 518), whence
S(P :Q) = S(P) + S(Q) −S(P, Q) = S(ρ) −

x
pxS(ρx),
(12.11)
which is exactly what we want on the right hand side of the Holevo bound! To calculate
534
Quantum information theory
the quantity on the left hand side of (12.9), note that
ρP ′Q′M′ =

xy
px|x⟩⟨x| ⊗

Eyρx

Ey ⊗|y⟩⟨y|.
(12.12)
Tracing out system Q′ and using the observation that the joint distribution p(x, y) for
the pair (X, Y ) satisﬁes p(x, y) = pxp(y|x) = pxtr(ρxEy) = pxtr(
Eyρx
Ey), gives
ρP ′M′ =

xy
p(x, y)|x⟩⟨x| ⊗|y⟩⟨y| ,
(12.13)
whence S(P ′ :M ′) = H(X :Y ), which is exactly what we want on the left hand side of
the Holevo bound! This completes the proof of the Holevo bound.
12.1.2
Example applications of the Holevo bound
The Holevo bound is a keystone in the proof of many results in quantum information
theory. For now, we’ll give just a taste of how this important result may be applied.
Recall Theorem 11.10, which implies that
S(ρ) −

x
pxS(ρx) ≤H(X) ,
(12.14)
with equality if and only if the states ρx have orthogonal support. Suppose that the
states ρx do not have orthogonal support, so the inequality in (12.14) is strict. Then the
Holevo bound implies that H(X :Y ) is strictly less than H(X), and thus it is impossible
for Bob to determine X with perfect reliability based on his measurement result Y ,
which generalizes our existing understanding that if the states prepared by Alice are not
orthogonal then it is not possible for Bob to determine with certainty which state Alice
prepared.
A concrete example involves Alice preparing a single qubit in one of two quantum
states according to the outcome of a fair coin toss. If the coin toss yields heads, then
Alice prepares the state |0⟩, and if the coin toss yields tails, then Alice prepares the state
cos θ|0⟩+ sin θ|1⟩, where θ is some real parameter. In the |0⟩, |1⟩basis it follows that ρ
may be written
ρ = 1
2
 1
0
0
0

+ 1
2

cos2 θ
cos θ sin θ
cos θ sin θ
sin2 θ

.
(12.15)
A simple calculation shows that the eigenvalues of ρ are (1 ± cos θ)/2, and the Holevo
bound is therefore given by the binary entropy H((1 + cos θ)/2), as illustrated in Fig-
ure 12.1. Notice that the Holevo bound is maximized when θ = π/2, attaining a value of
1 bit, corresponding to the case of Alice preparing states chosen from an orthogonal set,
at which point it is possible for Bob to determine with surety which state Alice prepared.
For other values of θ the Holevo bound is strictly less than 1 bit, and it is impossible for
Bob to determine with surety which state Alice prepared.
The Holevo bound may be given more operational meaning by making use of the
Fano inequality (see Box 12.2 on page 536 for a derivation). Suppose Bob makes a guess
˜X = f(Y ) as to which state Alice prepared, based on the outcome of his measurement
Y and some rule for making a guess, encapsulated in the function f(·). Then according
to the Fano inequality and the Holevo bound,
H(p( ˜X ̸= X)) + p( ˜X ̸= X) log(|X| −1) ≥H(X|Y )
Distinguishing quantum states and the accessible information
535
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 12.1. Plot of the Holevo bound χ as a function of θ when the states |0⟩and cos θ|0⟩+ sin θ|1⟩are prepared
with equal probability. Notice that the Holevo bound reaches a maximum when θ = π/2, corresponding to
orthogonal states. It is only at this point that it is possible for Bob to determine with certainty which state Alice
prepared.
= H(X) −H(X :Y )
≥H(X) −χ,
(12.19)
which allows us to place bounds on how well Bob may infer the value of X. Heuristically,
the smaller χ is, the harder it is for Bob to determine which state Alice prepared. This
is illustrated in Figure 12.2 for the case where Alice prepares |0⟩with probability one-
half and cos θ|0⟩+ sin θ|1⟩with probability one-half, for which the bound reduces to
H(p( ˜X ̸= X)) ≥1 −χ and χ = H((1 + cos(θ))/2) as we noted before. Notice that when
θ ̸= π/2, Bob has some ﬁnite probability of making an error in his guess. This error gets
larger as θ gets closer to zero. Finally, when θ = 0 and the two states are indistinguishable,
the lower bound tells us that Bob’s probability of error is at least one-half – he can do
no better than chance in guessing which state Alice prepared, as we would expect.
Exercise 12.3:
Use the Holevo bound to argue that n qubits can not be used to
transmit more than n bits of classical information.
Exercise 12.4:
Suppose Alice sends Bob an equal mixture of the four pure states
|X1⟩= |0⟩
(12.20)
|X2⟩=
)
1
3

|0⟩+
√
2|1⟩
	
(12.21)
|X3⟩=
)
1
3

|0⟩+
√
2e2πi/3|1⟩
	
(12.22)
|X4⟩=
)
1
3

|0⟩+
√
2e4πi/3|1⟩
	
.
(12.23)
536
Quantum information theory
Box 12.2: Fano’s inequality
Suppose we wish to infer the value of a random variable X based on knowledge
of another random variable Y . Intuitively, we expect that the conditional entropy
H(X|Y ) limits how well we may perform this inference. The Fano inequality
makes this intuition rigorous, and provides a useful bound on how well we may
infer X, given Y .
Suppose ˜X ≡f(Y ) is some function of Y which we are using as our best guess for
X. Let pe ≡p(X ̸= ˜X) be the probability that this guess is incorrect. Then the
Fano inequality states that
H(pe) + pe log(|X| −1) ≥H(X|Y ) ,
(12.16)
where H(·) is the binary entropy and |X| is the number of values X may assume.
Qualitatively, what the inequality tells us is that if H(X|Y ) is large (that is, compa-
rable in size to log(|X|−1)) then the probability pe of making an error in inference
must also be large.
To prove the Fano inequality, deﬁne an ‘error’ random variable, E ≡1 if X ̸= ˜X,
and E ≡0 if X = ˜X. Notice that H(E) = H(pe). Using the chaining rule for
conditional entropies (page 508), we have H(E, X|Y ) = H(E|X, Y ) + H(X|Y ).
But E is completely determined once X and Y are known, so H(E|X, Y ) = 0 and
thus H(E, X|Y ) = H(X|Y ). Applying the chain rule for entropies again but to
different variables, we obtain H(E, X|Y ) = H(X|E, Y ) + H(E|Y ). Conditioning
reduces entropy, so H(E|Y ) ≤H(E) = H(pe), whence H(X|Y ) = H(E, X|Y ) ≤
H(X|E, Y ) + H(pe). The proof of the Fano inequality is concluded by bounding
H(X|E, Y ) as follows (we have omitted a few simple algebraic details, which you
can easily ﬁll in):
H(X|E, Y ) = p(E = 0)H(X|E = 0, Y ) + p(E = 1)H(X|E = 1, Y )(12.17)
≤p(E = 0) × 0 + pe log(|X| −1) = pe log(|X| −1),
(12.18)
where H(X|E = 1, Y ) ≤log(|X| −1) follows from the fact that when E = 1,
X ̸= Y , and X can assume at most |X| −1 values, bounding its entropy, and thus
its conditional entropy by log(|X|−1). Substituting H(X|E, Y ) ≤pe log(|X|−1)
into H(X|Y ) ≤H(X|E, Y )+H(pe) gives the Fano inequality H(X|Y ) ≤H(pe)+
pe log(|X| −1).
Show that the maximum mutual information between Bob’s measurement and
Alice’s transmission is less than one bit. A POVM which achieves ≈0.415 bits is
known. Can you construct this or, better yet, one which achieves the Holevo
bound?
12.2
Data compression
Let’s switch tacks now and investigate an elementary dynamical process – data com-
pression – which arises in both classical and quantum information theory. In its most
general form the problem of data compression is to determine what are the minimal
Data compression
537
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Figure 12.2. A lower bound on the probability of Bob making an error in inferring whether Alice prepared the state
|0⟩or cos θ|0⟩+ sin θ|1⟩. This lower bound is obtained by combining Fano’s inequality with the Holevo bound.
Observe that the bound decreases to zero as θ gets close to π/2, where the states may be reliably distinguished.
physical requirements needed to store an information source? It is one of the fundamen-
tal problems of information theory, with implications far beyond its immediate scope.
In both classical and quantum information theory the techniques utilized in solving this
problem turn out to have a far wider range of applicability than mere data compression,
yet receive perhaps their simplest and most elegant expression in the solution of the data
compression problem. In this section we examine in detail both quantum and classical
data compression.
12.2.1
Shannon’s noiseless channel coding theorem
Shannon’s noiseless channel coding theorem quantiﬁes the extent to which we can com-
press the information being produced by a classical information source. What do we
mean by a classical information source? Many models of such a source are possible. A
simple and very fruitful model is that a source consists of a sequence of random variables
X1, X2, . . . whose values represent the output of the source. We will ﬁnd it convenient to
assume that the random variables take values from a ﬁnite alphabet of symbols, although
extensions to inﬁnite alphabets also hold. Furthermore, we assume that the different uses
of the source are independent and identically distributed; that is, the source is what is
known as an i.i.d information source. In the real world sources often don’t behave in this
way. It is easy to see, for example, that the letters in the English text in front of you don’t
occur in an independent fashion; strong correlations exist between the letters. To take a
simple example the letter ‘t’ is followed by the letter ‘h’ more frequently than one would
expect based upon the overall frequency of the letter ‘h’ in normal English text; we say
538
Quantum information theory
that occurrences of ‘t’ and ‘h’ do not occur independently, but are correlated. Neverthe-
less, for a wide variety of information sources (including English text) the assumption of
an i.i.d. source works pretty well in practice, and the ideas introduced to deal with the
special case of an i.i.d. source can be generalized to more sophisticated sources.
Before we get into the technical details of Shannon’s theorem, let’s use a simple
example to understand the intuition behind the result. Suppose an i.i.d. information
source is producing bits X1, X2, X3, . . ., each being equal to zero with probability p, and
equal to one with probability 1 −p. The key idea behind Shannon’s theorem is to divide
the possible sequences of values x1, . . . , xn for the random variables X1, . . . , Xn up into
two types – sequences which are highly likely to occur, known as typical sequences,
and sequences which occur rarely, known as atypical sequences. How is this done? As
n gets large, we expect that with high probability a fraction p of the symbols output
from the source will be equal to zero, and that a fraction 1 −p will be equal to one.
The sequences x1, . . . , xn for which this assumption is correct are known as typical
sequences. Combining this deﬁnition with the independence assumption for the source
gives
p(x1, . . . , xn) = p(x1)p(x2) . . . p(xn) ≈pnp(1 −p)(1−p)n
(12.24)
for typical sequences. Taking logarithms on both sides gives
−log p(x1, . . . , xn) ≈−np log p −n(1 −p) log(1 −p) = nH(X),
(12.25)
where X is a random variable distributed according to the source distribution and
H(X) = −p log(p) −(1 −p) log(1 −p) is the entropy of the source distribution, also
known as the entropy rate of the source. Thus p(x1, . . . , xn) ≈2−nH(X), from which we
see that there can be at most 2nH(X) typical sequences, since the total probability of all
typical sequences cannot be greater than one.
We now have the tools to understand a simple scheme for data compression. Suppose
the output from the source is x1, . . . , xn. To compress this output, we check to see
whether x1, . . . , xn is a typical sequence. If it’s not, we give up – declare an error.
Fortunately, as n becomes large this happens very rarely, since nearly all sequences are
typical in the limit of large n. If the output is a typical sequence, we record that fact.
Since there are at most 2nH(X) typical sequences, it only requires nH(X) bits to uniquely
identify a particular typical sequence. We choose some such scheme for identiﬁcation,
and compress the output from the source to the corresponding string of nH(X) bits
describing which typical sequence occurred, which can later be decompressed. As n
becomes large this scheme succeeds with probability approaching one.
Several criticisms can be made of this scheme: (a) It has a small but ﬁnite chance
of failing. Slightly more sophisticated schemes make use of similar ideas to remove the
possibility of an error occurring. (b) To do the compression we have to wait until the
source has emitted a large number, n, of symbols. Again, adaptations exist which allow the
processing to be done as the symbols are emitted by the source. (c) No explicit scheme
mapping outputs from the source to the compressed sequences has been given. Once
again, slightly more sophisticated schemes can be developed which address this issue.
(d) The exact procedure being used to do the data compression depends on the output
distribution of the source. What if this is not known? Clever universal compression
algorithms exist which can cope with this possibility. The reader who is interested in
Data compression
539
these and other issues is referred to the book of Cover and Thomas listed in the end of
chapter ‘History and further reading’.
Let’s generalize the notion of typical sequences beyond the binary case. Suppose
X1, X2, . . . is an i.i.d. information source. Typically, the frequency of occurrence of
any given letter x in the sequence output from the source will be close to the probability
p(x) of that letter occurring on any given use of the source. With this intuitive under-
standing in mind we make the following rigorous deﬁnition of the notion of a typical
sequence. Given ϵ > 0 we say that a string of source symbols x1x2 . . . xn is ϵ-typical if
2−n(H(X)+ϵ) ≤p(x1, . . . , xn) ≤2−n(H(X)−ϵ),
(12.26)
and denote the set of all such ϵ-typical sequences of length n by T(n, ϵ). A useful
equivalent reformulation of the deﬁnition is as
++++
1
n log
1
p(x1, . . . , xn) −H(X)
++++ ≤ϵ.
(12.27)
Using the law of large numbers (stated and proved in Box 12.3 on page 541) we can
prove the theorem of typical sequences, which makes rigorous the idea that in the limit
of large n most sequences output by an information source are typical.
Theorem 12.2: (Theorem of typical sequences)
(1) Fix ϵ > 0. Then for any δ > 0, for sufﬁciently large n, the probability that a
sequence is ϵ-typical is at least 1 −δ.
(2) For any ﬁxed ϵ > 0 and δ > 0, for sufﬁciently large n, the number |T(n, ϵ)|
of ϵ-typical sequences satisﬁes
(1 −δ)2n(H(X)−ϵ) ≤|T(n, ϵ)| ≤2n(H(X)+ϵ) .
(12.28)
(3) Let S(n) be a collection of size at most 2nR, of length n sequences from the
source, where R < H(X) is ﬁxed. Then for any δ > 0 and for sufﬁciently
large n,

x∈S(n)
p(x) ≤δ .
(12.29)
Proof
Part 1: A direct application of the law of large numbers. Notice that −log p(Xi) are
independent, identically distributed random variables. By the law of large numbers for
any ϵ > 0 and δ > 0 for sufﬁciently large n we have
p
+++++
n

i=1
−log p(Xi)
n
−E(−log p(X))
+++++ ≤ϵ

≥1 −δ.
(12.30)
But E (log p(X)) = −H(X) and n
i=1 log p(Xi) = log (p(X1, . . . , Xn)). Thus
p

|−log(p(X1, . . . , Xn))/n −H(X)| ≤ϵ
 ≥1 −δ.
(12.31)
That is, the probability that a sequence is ϵ-typical is at least 1 −δ.
Part 2: Follows from the deﬁnition of typicality, and the observation that the sum of
540
Quantum information theory
the probabilities of the typical sequences must lie in the range 1 −δ (by part 1) to 1 (as
probabilities cannot sum to more than 1). Thus
1 ≥

x∈T (n,ϵ)
p(x) ≥

x∈T (n,ϵ)
2−n(H(X)+ϵ) = |T(n, ϵ)|2−n(H(X)+ϵ),
(12.32)
from which we deduce |T(n, ϵ)| ≤2n(H(X)+ϵ), and
1 −δ ≤

x∈T (n,ϵ)
p(x) ≤

x∈T (n,ϵ)
2−n(H(X)−ϵ) = |T(n, ϵ)|2−n(H(X)−ϵ),
(12.33)
from which we deduce |T(n, ϵ)| ≥(1 −δ)2n(H(X)−ϵ).
Part 3: The idea is to split the sequences in S(n) up into typical and atypical sequences.
The atypical sequences have small probability in the large n limit. The number of typical
sequences in S(n) is obviously no larger than the total number of sequences in S(n),
which is at most 2nR, and each typical sequence has probability about 2−nH(X) so the
total probability of the typical sequences in S(n) scales like 2n(R−H(X)), which goes to
zero when R < H(X).
More rigorously, choose ϵ so R < H(X) −δ and 0 < ϵ < δ/2. Split the sequences
in S(n) up into the ϵ-typical sequences, and the ϵ-atypical sequences. By part 1, for n
sufﬁciently large the total probability of the atypical sequences can be made less than
δ/2. There are at most 2nR typical sequences in S(n), each with probability at most
2−n(H(X)−ϵ), so the probability of the typical sequences is at most 2−n(H(X)−ϵ−R), which
goes to zero as n goes to inﬁnity. Thus the total probability of the sequences in S(n) is
less than δ for n sufﬁciently large.
Shannon’s noiseless channel coding theorem is an easy application of the theorem of
typical sequences. We give here a very simple version of the noiseless channel coding the-
orem; more sophisticated versions are left to the exercises and the end of chapter ‘History
and further reading’. The basic setting is to suppose that X1, X2, . . . is an i.i.d. classi-
cal information source over some ﬁnite alphabet containing d symbols. A compression
scheme of rate R maps possible sequences x = (x1, . . . , xn) to a bit string of length nR
which we denote by Cn(x) = Cn(x1, . . . , xn). (Note that nR may not be an integer; our
notation is simpliﬁed by agreeing that in this case nR = ⌊nR⌋.) The matching decom-
pression scheme takes the nR compressed bits and maps them back to a string of n letters
from the alphabet, Dn(Cn(x)). A compression–decompression scheme (Cn, Dn) is said
to be reliable if the probability that Dn(Cn(x)) = x approaches one as n approaches
∞. Shannon’s noiseless channel coding theorem speciﬁes for what values of the rate R a
reliable compression scheme exists, revealing a remarkable operational interpretation for
the entropy rate H(X): it is just the minimal physical resources necessary and sufﬁcient
to reliably store the output from the source.
Theorem 12.4: (Shannon’s noiseless channel coding theorem) Suppose {Xi} is an
i.i.d. information source with entropy rate H(X). Suppose R > H(X). Then
there exists a reliable compression scheme of rate R for the source. Conversely,
if R < H(X) then any compression scheme will not be reliable.
Proof
Suppose R > H(X). Choose ϵ > 0 such that H(X) + ϵ < R. Consider the set T(n, ϵ)
Data compression
541
Box 12.3: The law of large numbers
Suppose we repeat an experiment a large number of times, each time measuring the
value of some parameter, X. We label the results of the experiments X1, X2, . . ..
Assuming that the results of the experiments are independent, we intuitively expect
that the value of the estimator Sn ≡n
i=1 Xi/n of the average E(X), should
approach E(X) as n →∞. The law of large numbers is a rigorous statement of
this intuition.
Theorem 12.3: (Law of large numbers) Suppose X1, X2, . . . are independent
random variables all having the same distribution as a random variable X
with ﬁnite ﬁrst and second moments, |E(X)| < ∞and E(X2) < ∞. Then
for any ϵ > 0, p(|Sn −E(X)| > ϵ) →0 as n →∞.
Proof
To begin we assume that E(X) = 0 and discuss what happens when E(X) ̸= 0 upon
completion of the proof. Since the random variables are independent with mean
zero, it follows that E(XiXj) = E(Xi)E(Xj) = 0 when i ̸= j, and thus
E(S2
n) =
n
i,j=1 E(XiXj)
n2
=
n
i=1 E(X2
i )
n2
= E(X2)
n
,
(12.34)
where the ﬁnal equality follows from the fact that X1, . . . , Xn are identically dis-
tributed to X. By the same token, from the deﬁnition of the expectation we have
E(S2
n) =
-
dP S2
n,
(12.35)
where dP is the underlying probability measure. It is clear that either |Sn| ≤ϵ or
|Sn| > ϵ, so we can split this integral into two pieces, and then drop one of these
pieces with the justiﬁcation that it is non-negative,
E(S2
n) =
-
|Sn|≤ϵ
dP S2
n +
-
|Sn|>ϵ
dP S2
n ≥
-
|Sn|>ϵ
dP S2
n.
(12.36)
In the region of integration S2
n > ϵ2, and thus
E(S2
n) ≥ϵ2
-
|Sn|>ϵ
dP = ϵ2p(|Sn| > ϵ).
(12.37)
Comparing this inequality with (12.34) we see that
p(|Sn| > ϵ) ≤E(X2)
nϵ2 .
(12.38)
Letting n →∞completes the proof. In the case when E(X) ̸= 0, it is easy to
obtain the result, by deﬁning
Yi ≡Xi −E(X) ,
Y ≡X −E(X) .
(12.39)
Y and Y1, Y2, . . . are a sequence of independent, identically distributed random
variables with E(Y ) = 0 and E(Y 2) < ∞. The result follows from the earlier
reasoning.
542
Quantum information theory
of ϵ-typical sequences. For any δ > 0 and for sufﬁciently large n, there are at most
2n(H(x)+ϵ) < 2nR such sequences, and the probability of the source producing such a
sequence is at least 1−δ. The method of compression therefore is simply to examine the
output of the source to see if it is ϵ-typical. If it is not, then compress to some ﬁxed nR
bit string which indicates failure; the decompression operation simply outputs a random
sequence x1, . . . , xn as a guess to the information produced by the source; effectively
we give up on compression in this case. If the output of the source is typical then we
compress the output simply by storing an index for the particular sequence using nR
bits in the obvious way, allowing later recovery.
Suppose R < H(X). The combined compression–decompression operation has at
most 2nR possible outputs, so at most 2nR of the sequences output from the source
can be compressed and decompressed without an error occurring. By the theorem of
typical sequences, for sufﬁciently large n the probability of a sequence output from the
source lying in a subset of 2nR sequences goes to zero, for R < H(X). Thus any such
compression scheme cannot be reliable.
Exercise 12.5: (Variable-length zero error data compression)
Consider the
following heuristic for a variable length data compression scheme. Let x1, . . . , xn
be the output from n uses of an i.i.d. source with entropy rate H(X). If
x1, . . . , xn is typical, then send a H(X) bit index indicating which typical
sequence it is. If x1, . . . , xn is atypical, send an uncompressed log dn bit index
for the sequence (recall that d is the alphabet size). Turn this heuristic into a
rigorous argument that the source can be compressed to an average of R bits per
source symbol, for any R > H(X), with zero probability of error.
12.2.2
Schumacher’s quantum noiseless channel coding theorem
A great conceptual breakthrough of quantum information theory is to realize that we can
treat quantum states as if they were information, and ask information-theoretic questions
about those quantum states. In this section we deﬁne the notion of a quantum source
of information, and study the question: to what extent can the ‘information’ – quantum
states – being produced by that source be compressed?
How might we deﬁne the notion of a quantum information source? As with the deﬁ-
nition of a classical information source it is by no means obvious what the best means of
making this deﬁnition is, and it is possible to come up with several different deﬁnitions,
not all of which are equivalent. The deﬁnition we are going to use is based on the idea
that entanglement is what we are trying to compress and decompress. More formally,
an (i.i.d) quantum source will be described by a Hilbert space H, and a density matrix
ρ on that Hilbert space. We imagine that the state ρ of the system is merely part of a
larger system which is in a pure state, and the mixed nature of ρ is due to entanglement
between H and the remainder of the system. A compression scheme of rate R for this
source consists of two families of quantum operations Cn and Dn, analogous to the com-
pression and decompression schemes used in the classical case. Cn is the compression
operation, taking states in H⊗n to states in a 2nR-dimensional state space, the compressed
space. We may regard the compressed space as representing nR qubits. The operation
Dn is a decompression operation, which takes states in the compressed space to states in
the original state space. The combined compression–decompression operation is therefore
Dn◦Cn. Our criterion for reliability is that in the limit of large n the entanglement ﬁdelity
Data compression
543
F(ρ⊗n, Dn ◦Cn) should tend towards one. The basic idea of quantum data compression
is illustrated in Figure 12.3.
ρ
n log d
qubits
-
Cn
ρ′
nS(ρ)
qubits
-
Dn
ρ′′
n log d
qubits
Figure 12.3. Quantum data compression. The compression operation Cn compresses a quantum source ρ stored in
n log d qubits into nS(ρ) qubits. The source is accurately recovered via the decompression operation Dn.
The key technical idea making the quantum noiseless channel coding theorem possible
is a quantum version of the idea of typical sequences. Suppose the density operator ρ
associated with a quantum source has orthonormal decomposition
ρ =

x
p(x)|x⟩⟨x|,
(12.40)
where |x⟩is an orthonormal set, and p(x) are the eigenvalues of ρ. The eigenvalues p(x)
of ρ obey the same rules as a probability distribution: they are non-negative and sum
to one. Furthermore, H(p(x)) = S(ρ). Therefore, it makes sense to talk of an ϵ-typical
sequence, x1, . . . , xn for which
++++
1
n log

1
p(x1)p(x2) . . . p(xn)

−S(ρ)
++++ ≤ϵ,
(12.41)
in exactly the same fashion as for the classical deﬁnition. An ϵ-typical state is a state
|x1⟩|x2⟩. . . |xn⟩for which the sequence x1, x2, . . . , xn is ϵ-typical. Deﬁne the ϵ-typical
subspace to be the subspace spanned by all ϵ-typical states, |x1⟩. . . |xn⟩. We’ll denote the
ϵ-typical subspace by T(n, ϵ), and the projector onto the ϵ-typical subspace by P(n, ϵ).
Notice that
P(n, ϵ) =

x ϵ−typical
|x1⟩⟨x1| ⊗|x2⟩⟨x2| ⊗. . . |xn⟩⟨xn|.
(12.42)
The theorem of typical sequences may now be translated into an equivalent quantum
form, the typical subspace theorem.
Theorem 12.5: (Typical subspace theorem)
(1) Fix ϵ > 0. Then for any δ > 0, for sufﬁciently large n,
tr(P(n, ϵ)ρ⊗n) ≥1 −δ .
(12.43)
(2) For any ﬁxed ϵ > 0 and δ > 0, for sufﬁciently large n, the dimension
|T(n, ϵ)| = tr(P(n, ϵ)) of T(n, ϵ) satisﬁes:
(1 −δ)2n(S(ρ)−ϵ) ≤|T(n, ϵ)| ≤2n(S(ρ)+ϵ) .
(12.44)
(3) Let S(n) be a projector onto any subspace of H⊗n of dimension at most 2nR,
where R < S(ρ) is ﬁxed. Then for any δ > 0, and for sufﬁciently large n,
tr(S(n)ρ⊗n) ≤δ .
(12.45)
544
Quantum information theory
In each case the result can be obtained directly using the law of large numbers, but
we prefer to use the theorem of typical sequences to emphasize the close connection to
the techniques used in the proof of Shannon’s noiseless channel coding theorem.
Proof
Part 1: Observe that
tr(P(n, ϵ)ρ⊗n) =

x ϵ−typical
p(x1)p(x2) . . . p(xn).
(12.46)
The result follows immediately from part 1 of the theorem of typical sequences.
Part 2: Follows immediately from part 2 of the theorem of typical sequences.
Part 3: We split the trace up into a trace over the typical subspace and the atypical
subspace,
tr(S(n)ρ⊗n) = tr(S(n)ρ⊗nP(n, ϵ)) + tr(S(n)ρ⊗n(I −P(n, ϵ))),
(12.47)
and bound each term separately. For the ﬁrst term observe that
ρ⊗nP(n, ϵ) = P(n, ϵ)ρ⊗nP(n, ϵ) ,
(12.48)
since P(n, ϵ) is a projector which commutes with ρ⊗n. But
tr(S(n)P(n, ϵ)ρ⊗nP(n, ϵ)) ≤2nR2−n(S(ρ)−ϵ),
(12.49)
since the eigenvalues of P(n, ϵ)ρ⊗nP(n, ϵ) are bounded above by 2−n(S(ρ)−ϵ). Letting
n →∞we see the ﬁrst term tends to zero. For the second term note that S(n) ≤I. Since
S(n) and ρ⊗(I −P(n, ϵ)) are both positive operators it follows that 0 ≤tr(S(n)ρ⊗n(I −
P(n, ϵ)) ≤tr(ρ⊗n(I −P(n, ϵ)) →0 as n →∞, so the second term also tends to zero as
n becomes large, giving the result.
With the typical subspace theorem under our belts it is not difﬁcult to prove a quantum
analogue of Shannon’s noiseless channel coding theorem. The main ideas of the proof
are analogous, but the technical analysis is made a little more difﬁcult by the appearance
of non-commuting operators in the proof, which have no classical analogue.
Theorem 12.6: (Schumacher’s noiseless channel coding theorem) Let {H, ρ} be
an i.i.d. quantum source. If R > S(ρ) then there exists a reliable compression
scheme of rate R for the source {H, ρ}. If R < S(ρ) then any compression
scheme of rate R is not reliable.
Proof
Suppose R > S(ρ) and let ϵ > 0 be such that S(ρ) + ϵ ≤R. By the typical subspace
theorem, for any δ > 0 and for all n sufﬁciently large, tr(ρ⊗nP(n, ϵ)) ≥1 −δ, and
dim(T(n, ϵ)) ≤2nR. Let Hn
c be any 2nR-dimensional Hilbert space containing T(n, ϵ).
The encoding is done in the following fashion. First a measurement is made, described
by the complete set of orthogonal projectors P(n, ϵ), I −P(n, ϵ), with corresponding
outcomes we denote 0 and 1. If outcome 0 occurs nothing more is done and the state
is left in the typical subspace. If outcome 1 occurs then we replace the state of the
system with some standard state ‘|0⟩’ chosen from the typical subspace; it doesn’t matter
Data compression
545
what state is used. It follows that the encoding is a map Cn : H⊗n →Hn
c into the
2nR-dimensional subspace Hn
c , with operator-sum representation
Cn(σ) ≡P(n, ϵ)σP(n, ϵ) +

i
AiσA†
i ,
(12.50)
where Ai ≡|0⟩⟨i| and |i⟩is an orthonormal basis for the orthocomplement of the typical
subspace.
The decoding operation Dn : Hn
c →H⊗n is deﬁned to be the identity on Hn
c ,
Dn(σ) = σ. With these deﬁnitions for the encoding and decoding it follows that
F(ρ⊗n, Dn ◦Cn) = |tr(ρ⊗nP(n, ϵ))|2 +

i
|tr(ρ⊗nAi)|2
(12.51)
≥|tr(ρ⊗nP(n, ϵ))|2
(12.52)
≥|1 −δ|2 ≥1 −2δ ,
(12.53)
where the last line follows from the typical subspace theorem. But δ can be made arbitrar-
ily small for sufﬁciently large n, and thus it follows that there exists a reliable compression
scheme {Cn, Dn} of rate R whenever S(ρ) < R.
To prove the converse, suppose R < S(ρ). Without loss of generality we suppose that
the compression operation maps from H⊗n to a 2nR-dimensional subspace with corre-
sponding projector S(n). Let Cj be operation elements for the compression operation
Cn, and Dk operation elements for the decompression operation Dn. Then we have
F(ρ⊗n, Dn ◦Cn) =

jk
++tr(DkCjρ⊗n)
++2 .
(12.54)
Each of the operators Cj maps to within the subspace with projector S(n) so Cj =
S(n)Cj. Let Sk(n) be the projector onto the subspace to which the subspace S(n) is
mapped by Dk, so we have Sk(n)DkS(n) = DkS(n) and thus DkCj = DkS(n)Cj =
Sk(n)DkS(n)Cj = Sk(n)DkCj, whence
F(ρ⊗n, Dn ◦Cn) =

jk
++tr(DkCjρ⊗nSk(n))
++2 .
(12.55)
Applying the Cauchy–Schwarz inequality gives
F(ρ⊗n, Dn ◦Cn) ≤

jk
tr(DkCjρ⊗nC†
j D†
k)tr(Sk(n)ρ⊗n) .
(12.56)
Applying part 3 of the typical subspace theorem we see that for any δ > 0 and for
sufﬁciently large n, tr(Sk(n)ρ⊗n) ≤δ. Moreover, the proof of the typical subspace
theorem implies that the size n needs to be for this to hold does not depend on k. Thus
F(ρ⊗n, Dn ◦Cn) ≤δ

jk
tr(DkCjρ⊗nC†
j D†
k)
(12.57)
= δ ,
(12.58)
since Cn and Dn are trace-preserving. Since δ was arbitrary it follows that F(ρ⊗n, Dn ◦
Cn) →0 as n →∞, and thus the compression scheme is not reliable.
Schumacher’s theorem not only discusses the existence of a reliable compression
scheme, but it also gives clues as to how to actually construct one. The key is to be
546
Quantum information theory
able to efﬁciently perform the mapping Cn : H⊗n →Hn
c into the 2nR-dimensional typi-
cal subspace Hn
c . Classical compression techniques such as enumerative coding, Huffman
coding, and arithmetic coding can be applied but with one strong restriction: the encoding
circuit must be completely reversible, and also entirely erase the original state in the pro-
cess of creating the compressed one! After all, by the no-cloning theorem, it cannot copy
the original state, so it cannot leave it behind as normal classical compression schemes
typically do. A simple example illustrating how quantum compression works is given in
Box 12.4.
Exercise 12.6:
In the notation of Box 12.4, give an explicit expression for CX in
terms of X. Also, describe how to construct a quantum circuit to perform Un for
arbitrary n. How many elementary operations do you require, as a function of n?
Exercise 12.7: (Data compression circuit)
Outline the construction of a circuit to
reliably compress a qubit source with ρ = p|0⟩⟨0| + (1 −p)|1⟩⟨1| into nR qubits
for any R > S(ρ) = H(p).
Exercise 12.8: (Compression of an ensemble of quantum states)
Suppose that
instead of adopting the deﬁnition of a quantum source based on a single density
matrix ρ and the entanglement ﬁdelity, we instead adopted the following
ensemble deﬁnition, that an (i.i.d.) quantum source is speciﬁed by an ensemble
{pj, |ψj⟩} of quantum states, and that consecutive uses of the source are
independent and produce a state |ψj⟩with probability pj. A
compression–decompression scheme (Cn, Dn) is said to be reliable in this
deﬁnition if the ensemble average ﬁdelity approaches 1 as n →∞:
¯F ≡

J
pj1 . . . pjnF(ρJ, (Dn ◦Cn)(ρJ))2 ,
(12.61)
where J = (j1, . . . , jn) and ρJ ≡|ψj1⟩⟨ψj1| ⊗· · · ⊗|ψjn⟩⟨ψjn|. Deﬁne
ρ ≡
j pj|ψj⟩⟨ψj| and show that provided R > S(ρ) there exists a reliable
compression scheme of rate R with respect to this deﬁnition of ﬁdelity.
12.3
Classical information over noisy quantum channels
Anything that can go wrong, will
– Attributed to Edward A. Murphy, Jr.
We all have difﬁculty talking on the telephone from time to time. We say we have a ‘bad
line’ when we have exceptional difﬁculty understanding the person on the other end of
the line. This is an example of the general phenomenon of noise which is present to some
extent in all information processing systems. As described in Chapter 10 error-correcting
codes can be used to combat the effects of noise, allowing reliable communication and
computation to take place even in the presence of quite severe noise. Given a particular
noisy communications channel N an interesting question is how much information can be
transmitted reliably through that channel. For example, it might be possible that 1000 uses
of the channel can be used to transmit 500 bits of information using an appropriate error-
correcting code, with high probability of recovery from any errors the channel introduces.
Classical information over noisy quantum channels
547
Box 12.4: Schumacher compression
Consider an i.i.d. quantum source characterized by the single qubit density matrix
ρ = 1
4
 3
1
1
1

.
(12.59)
This could originate, for example, as a small part of a of much larger entangled
system. An alternate way of viewing this source (compare Section 9.3) is that it is
producing the state |ψ0⟩= |0⟩or |ψ1⟩= (|0⟩+ |1⟩)/
√
2 with equal probabilities
one half each (see Exercise 12.8). ρ has orthonormal decomposition p|¯0⟩⟨¯0| + (1 −
p)|¯1⟩⟨¯1|, where |¯0⟩= cos π
8 |0⟩+ sin π
8 |1⟩, |¯1⟩= −sin π
8 |0⟩+ cos π
8 |1⟩, and p =
[3 + tan(π/8)]/4. In this basis, a block of n qubits can be written as the state

X={¯0¯0...¯0, ¯0...¯0¯1, ...,¯1¯1...¯1}
CX|X⟩.
(12.60)
By Theorem 12.6, only |X⟩for which the Hamming weight is approximately equal
to np (that is, a basis for the typical subspace) need be transmitted in order to enable
reconstruction of the original state with high ﬁdelity. This is easy to appreciate,
because |⟨¯0|ψk⟩| = cos(π/8) (for k = {0, 1}) is much larger than |⟨¯1|ψk⟩| =
sin(π/8), and for X with large Hamming weight, the coefﬁcients CX are very
small.
How do we realize such a compression scheme? One approximate way is the fol-
lowing. Suppose we have quantum circuit Un, which permutes basis states |X⟩
such that states are re-ordered lexicographically according to Hamming weight.
For example, for n = 4 it does
0000 →0000
1000 →0100
1001 →1000
1011 →1100
0001 →0001
0011 →0101
1010 →1001
1101 →1101
0010 →0010
0101 →0110
1100 →1010
1110 →1110
0100 →0011
0110 →0111
0111 →1011
1111 →1111
Such a transform, which can be realized using just controlled-
and Toffoli
gates, reversibly packs the typical subspace into the ﬁrst ≈nH(p) qubits (from
left to right). To complete the scheme, we also need a quantum gate V , which
rotates single qubits into the |¯0⟩, |¯1⟩basis. The desired compression scheme is then
Cn = (V †)⊗nUnV ⊗n, and we need send only the ﬁrst nH(p) qubits output from
Cn, to enable a sequence of states from the source to be reconstructed with high
ﬁdelity, using a decoder which is the inverse of this circuit. A more efﬁcient coding
scheme would pack just the states with Hamming weight ≈np into the ﬁrst nH(p)
qubit space; this can be done using a quantum version of arithmetic coding, for
example.
We say such a code has rate 500/1000 = 1/2. A fundamental problem of information
theory is to determine the maximum rate for reliable communication through the channel
N , a number known as the capacity of the channel.
For noisy classical communications channels the capacity of the channel may be cal-
culated using a beautiful result known as Shannon’s noisy channel coding theorem. We
548
Quantum information theory
begin our investigation of the communication of classical information in the presence of
noise in Section 12.3.1 with a discussion of some of the main ideas behind Shannon’s noisy
channel coding theorem. We don’t get too detailed, however, because in Section 12.3.2
we move on to take a detailed look at a generalization of the problem whereby two parties
attempt to communicate classical information by the use of a noisy quantum channel!
12.3.1
Communication over noisy classical channels
Many of the main ideas about noisy channel coding, both quantum and classical, can
be understood by examining the binary symmetric channel. Recall from Section 10.1
that the binary symmetric channel is a noisy communications channel for a single bit of
information, whose effect is to ﬂip the bit being transmitted with probability p > 0, while
with probability 1 −p the bit is transmitted without error, as illustrated in Figure 12.4.

NNNNNNNNNNNNNN

pppppppppppppp
Figure 12.4. Binary symmetric channel.
How much information can we reliably transmit per use of a binary symmetric channel?
Using error-correcting codes it is possible to transmit information through the channel,
but at an overhead in the number of bits used to accomplish the communication. We
will argue that the maximum rate at which information can be reliably transmitted is
1 −H(p), where H(·) is the Shannon entropy.
What does it mean that the transmission be reliably accomplished? This is a good ques-
tion, since different answers give rise to different possible rates. We are going to use the
following deﬁnition for reliability: we assume that inputs to the channel may be encoded
in large blocks all at once, and require that the probability for an error in transmission
using the code goes to zero as the blocksize is made large. Another possible deﬁnition
of reliability is to suppose again that the encoding may be performed in blocks, but that
as the blocksize becomes large the probability of error becomes exactly zero. Unfortu-
nately, this deﬁnition turns out to be too optimistic about what can be achieved with
error-correction, and leads to zero capacity for the binary symmetric channel! Similarly,
if we don’t allow encoding to be performed in large blocks the capacity turns out to be
zero. Indeed, it is rather amazing (and not at all obvious) that even with our less ambitious
deﬁnition of reliability a non-zero rate of information transmission can be achieved. To
show that this is possible several clever ideas are needed.
Random coding for the binary symmetric channel
Suppose we want to transmit nR bits of information using n uses of our binary symmetric
channel; that is, we want to transmit information at a rate R through the channel. We
are going to outline a proof that an error-correcting code exists that accomplishes this
with low probability of error in the limit of large n, and provided R < 1 −H(p). The
Classical information over noisy quantum channels
549
ﬁrst idea we need is a random coding method for constructing an error-correcting code.
Suppose (q, 1 −q) is any ﬁxed probability distribution over the possible inputs to the
channel (0 and 1). (This distribution is often called the a priori distribution of the code –
the introduction of this distribution is just a technical device to enable the random coding
method to work, and the randomness present in the distribution should not be confused
with the randomness in the channel.) Then we pick out a codeword x = (x1, . . . , xn)
for our code simply by choosing xj = 0 with probability q and xj = 1 with probability
1−q, independently for each j = 1, . . . , n. We repeat this procedure 2nR times, creating
a codebook C of 2nR entries; we denote a generic entry in the codebook by xj.
Obviously it’s possible to construct some pretty lousy error-correcting codes using
this procedure! We might get really unlucky and construct a code all of whose codewords
consist of the string of n zeroes, which obviously is not much use for the transmission
of information. Nevertheless, it turns out that on average this random coding procedure
gives a pretty good error-correcting code. To understand why this is so, let’s look at
what the channel does to a single codeword in the code. Since all the codewords are
constructed in the same way, we may as well look at the ﬁrst, x1.
What is the effect of the binary symmetric channel on x1? On a codeword of length
n we expect roughly np of the bits to be ﬂipped, so with high probability the output
from the channel will be have a Hamming distance of about np from the codeword x1,
as illustrated in Figure 12.5; we say that such an output is on the Hamming sphere of
radius np around x1. How many elements are there in this Hamming sphere? The answer
to this question is roughly 2nH(p), since the Hamming sphere consists of all the typically
occurring outputs y = x1 ⊕e from the channel, where e is the error that occurs in the
channel, ⊕denotes bitwise addition modulo 2, and by the theorem of typical sequences
the number of such typical errors e is about 2nH(p).
We’ve focused on a single codeword, but of course this same type of corruption occurs
for all the codewords. We can imagine the space of all the codewords and their surround-
ing Hamming spheres, as depicted in Figure 12.6. If, as we’ve shown there, the Hamming
spheres don’t overlap, then there’s an easy way for Bob to decode the output from the
channel. He simply checks to see if the output is in one of the Hamming spheres, out-
puts the corresponding codeword if so, and outputs ‘error’ if not. Since we’ve assumed
the spheres are non-overlapping, given any codeword as input it’s highly likely that this
will result in a successful decoding. Indeed, even if the spheres overlap slightly, it is
still possible for Bob to perform the decoding with a good chance of success, provided
the overlap is small – with high probability the output from the channel will belong to
one (not zero or two or more) of the Hamming spheres, and will result in a successful
decoding.
When does this small overlap condition occur? To understand this we need to better
understand the structure of the possible outputs from the channel. We obtained the
codewords for our code by sampling 2nR times from a set (X1, . . . , Xn) of random
variables which are independent and identically distributed with Xj = 0 with probability
q and Xj = 1 with probability 1 −q. Suppose we let Yj be the result of sending Xj
through the binary symmetric channel. The theorem of typical sequences implies that
the set of typical values for (Y1, . . . , Yn) is of size roughly 2nH(Y ), where Y is distributed
as each of the Yj. What’s more, each of these typical output values has roughly equal
probability.
Now, if we sample one hundred times uniformly from a population of size one million,
550
Quantum information theory
(" :$>		
: 
#;$ $:
:	$ 
	@
:$	
	@ :	"
Figure 12.5. Suppose the codeword x1 is sent through n uses of the binary symmetric channel. Then a typical
output from the channel is an element of the Hamming sphere of radius np around the sequence which has been
sent. (This ﬁgure is a closeup of Figure 12.6.)
	@
$	
Figure 12.6. Randomly chosen codewords for the binary symmetric channel, surrounded by their Hamming
spheres of ‘typical’ outputs. A closeup of an individual codeword may be found in Figure 12.5.
it’s not too likely that we’re going to get any repeats. In fact, even if we sample one
hundred thousand times the number of repeats is going to be pretty small. It’s not
until we get out to about a million samples that the number of repeats is going to start
getting large relative to the size of the sample. In a similar fashion, the amount of overlap
between our 2nR Hamming spheres of radius np is not going to start getting large until
Classical information over noisy quantum channels
551
the combined number of elements in all the spheres approaches the size of the space
– 2nH(Y ) – that we are effectively sampling from. Since each sphere contains roughly
2nH(p) elements, this means that we are very likely to have a good error-correcting code
provided
2nR × 2nH(p) < 2nH(Y ) ,
(12.62)
which corresponds to the condition
R < H(Y ) −H(p) .
(12.63)
Now the entropy H(Y ) depends on the a priori distribution (q, 1 −q) chosen for the
Xj. To make the rate as high as possible we try to maximize H(Y ). A simple calculation
shows that this is achieved by using the uniform a priori distribution corresponding to
q = 1/2, for which H(Y ) = 1, and therefore it is possible to achieve the rate R for any
R less than 1 −H(p).
We’ve just outlined a proof that it is possible to reliably transmit information through
a binary symmetric channel at any rate up to 1−H(p). The proof is rather sketchy, but in
fact contains many of the key ideas needed for a rigorous treatment, even in the quantum
case. It turns out that the rates we have shown how to achieve are also the fastest it’s
possible to transmit the information through the binary symmetric channel; any faster
than a rate 1 −H(p) and the Hamming spheres start to overlap too much to determine
what codeword was sent, no matter how the codewords were chosen! Thus 1 −H(p) is
the capacity of the binary symmetric channel.
How practical is random coding as a method to achieve high rate codes for the binary
symmetric channel? It is true that if we use a random code then with high probability we
can operate at a rate near the capacity. Unfortunately, there is a major difﬁculty with this
procedure. In order to do the encoding and decoding, the sender and receiver (‘Alice’
and ‘Bob’) must ﬁrst agree on a strategy for doing these tasks. In the case of random
codes, this means that Alice must send Bob a list of all her random codewords. Doing
this takes as much or more communication than Alice and Bob will be able to extract
from the noisy channel. Clearly, this is undesirable for many applications! The random
coding method is merely a method of demonstrating the existence of high rate codes, it
is not a practical method for their construction. For wide practical application, what we
would like is a method for achieving rates near the channel capacity which does not incur
an unacceptable communication overhead for Alice and Bob. It is quite remarkable that
methods for constructing such codes have only recently been discovered even for noisy
classical channels, despite many decades of intense effort, and it remains an interesting
open problem to ﬁnd similar constructions for noisy quantum channels.
Shannon’s noisy channel coding theorem
Shannon’s noisy channel coding theorem generalizes the capacity result for the binary
symmetric channel to the case of a discrete memoryless channel. Such a channel has
a ﬁnite input alphabet I, and a ﬁnite output alphabet O. For the binary symmetric
channel, I = O = {0, 1}. The action of the channel is described by a set of conditional
probabilities, p(y|x), where x ∈I and y ∈O. These represent the probabilities of the
different outputs y from the channel, given that the input was x, and satisfy the rules
p(y|x) ≥0
(12.64)
552
Quantum information theory

y
p(y|x) = 1 for all x.
(12.65)
The channel is memoryless in the sense that the channel acts the same way each time it
is used, and different uses are independent of one another. We shall use the symbol N
to denote a classical noisy channel.
Of course, there are many interesting communications channels which aren’t discrete
memoryless channels, such as the telephone line example given earlier, which has a
continuous set of inputs and outputs. More general channels may be technically more
difﬁcult to understand than discrete memoryless channels, but many of the underlying
ideas are the same and we refer you to the end of chapter ‘History and further reading’
for books containing information on this subject.
Let’s look at the actual statement of Shannon’s noisy channel coding theorem. We
won’t give the details of the proof, since we prove a more general result for quantum
channels in the next section, but it is instructive to look at the statement of the classical
result. First, we need to make our notion of reliable information transmission a little more
precise. The basic idea is illustrated in Figure 12.7. In the ﬁrst stage one of 2nR possible
messages M is produced by Alice and is encoded using a map Cn : {1, . . ., 2nR} →In
which assigns to each of Alice’s possible messages an input string which is sent through
n uses of the channel to Bob, who decodes the channel output using a map Dn : On →
{1, . . ., 2nR} which assigns a message to each string for each possible output from the
channel. For a given encoding–decoding pair, the probability of error is deﬁned to be
the maximum probability over all messages M that the decoded output of the channel
D(Y ) is not equal to the message M:
p(Cn, Dn) ≡max
M p(Dn(Y ) ̸= M|X = Cn(M)) .
(12.66)
We say a rate R is achievable if there exists such a sequence of encoding–decoding pairs
(Cn, Dn), and require in addition that p(Cn, Dn) →0 as n →∞. The capacity C(N )
of a given noisy channel N is deﬁned to be the supremum over all achievable rates for
the channel.

*	::	

>""	
A"$

>""	
+$

%		B	
""
>""	
,	"
Figure 12.7. The noisy coding problem for classical messages. We require that every one of the 2nR possible
messages should be sent uncorrupted through the channel with high probability.
A priori it is not at all obvious how to calculate the capacity of a channel – a bare-
hands calculation would involve taking a supremum over a very large (inﬁnite!) class
of possible encoding and decoding methods, and does not appear to be a particularly
promising approach. Shannon’s noisy channel coding theorem enormously simpliﬁes the
calculation of capacity, reducing it to a simple and well-deﬁned optimization problem
that can be solved exactly in many cases, and which is computationally quite tractable
even when an exact solution is not feasible.
Classical information over noisy quantum channels
553
Theorem 12.7: (Shannon’s noisy channel coding theorem) For a noisy channel N
the capacity is given by
C(N ) = max
p(x) H(X :Y ) ,
(12.67)
where the maximum is taken over all input distributions p(x) for X, for one use
of the channel, and Y is the corresponding induced random variable at the
output of the channel.
As an example of the noisy channel coding theorem, consider the case of a binary
symmetric channel ﬂipping bits with probability p, and with input distribution p(0) =
q, p(1) = 1 −q. We have
H(X :Y ) = H(Y ) −H(Y |X)
(12.68)
= H(Y ) −

x
p(x)H(Y |X = x).
(12.69)
But for each x, H(Y |X = x) = H(p) so H(X :Y ) = H(Y ) −H(p), which is maximized
by choosing q = 1/2, so H(Y ) = 1 and therefore C(N ) = 1 −H(p) by Shannon’s noisy
channel coding theorem, just as our earlier intuitive calculation of the channel capacity
of the binary symmetric channel suggested.
Exercise 12.9:
The erasure channel has two inputs, 0 and 1, and three outputs, 0, 1
and e. With probability 1 −p the input is left alone. With probability p the input
is ‘erased’, and replaced by e.
(1) Show that the capacity of the erasure channel is 1 −p.
(2) Prove that the capacity of the erasure channel is greater than the capacity of
the binary symmetric channel. Why is this result intuitively plausible?
Exercise 12.10:
Suppose N1 and N2 are two discrete memoryless channels such that
the input alphabet of N2 is equal to the output alphabet of N1. Show that
C(N2 ◦N1) ≤min(C(N1), C(N2)) .
(12.70)
Find an example where the inequality is strict.
A slight peculiarity of the noisy channel coding theorem we have presented is that
nowhere does the notion of a classical information source appear! Recall that earlier
we deﬁned a classical information source as a sequence of independent and identically
distributed random variables. We can combine this notion of an information source in
an interesting way with the noisy channel coding theorem to obtain what is known
as a source–channel coding theorem. The basic idea is illustrated in Figure 12.8. An
information source with entropy rate H(X) is producing information. By Shannon’s
noiseless channel coding theorem it is possible to compress the information from the
source so that it only requires nH(X) bits to describe; this step is sometimes known as
source coding. The compressed output of the source is now used as the input message
for the noisy channel. Transmitting at a rate R less than capacity, it requires nH(X)/R
uses of the channel to reliably transmit the compressed data to the receiver, who can then
decompress it to recover the original output from the source.
You might wonder whether a better scheme for transmitting an information source
over the noisy channel is possible. Perhaps it is possible to do something more efﬁcient
554
Quantum information theory

A"
"
	

>""	
A"$

>""	
+$

%		B	
""
>""	
,	"


A"
"
	
*	::	
	
"
Figure 12.8. The noisy coding problem for a classical information source, sometimes known as the source-coding
model.
than this two stage compress–encode and decode–decompress method? In fact, this turns
out not to be the case, and the method of source–channel coding described is in fact
optimal, but a proof of this fact is beyond our scope; see the end of chapter ‘History and
further reading’ for more details.
12.3.2
Communication over noisy quantum channels
Suppose that instead of using a noisy classical communications channel to communicate
Alice and Bob make use of a noisy quantum communications channel. More precisely,
Alice has some message M that she wants to send to Bob. She encodes that message,
just as she did in the classical case, but now the message is encoded as a quantum state,
which is sent over the noisy quantum channel. By performing the encoding in just the
right way, we hope that Bob will be able to determine what Alice’s message was, with low
probability of failure. Moreover, we’d like the rate at which Alice can send information to
Bob to be as high as possible. What we want, in other words, is a procedure for computing
the capacity for classical information of a noisy quantum channel. This problem has
not yet been completely solved, but a great deal of progress has been made, and in this
section we examine this progress.
What is known is how to calculate the capacity for a channel E assuming that Alice
encodes her messages using product states of the form ρ1 ⊗ρ2 ⊗. . ., where each of the
ρ1, ρ2, . . . are potential inputs for one use of the channel E. We call the capacity with this
restriction the product state capacity, and denote it C(1)(E) to indicate that input states
cannot be entangled across two or more uses of the channel. Note that this restricted model
of communication between Alice and Bob does allow Bob to decode using measurements
entangled across multiple uses of the channel; in fact, it turns out that this is essential.
The only restriction (and an unfortunate restriction it is) is that Alice can only prepare
product state inputs. It is believed by many researchers, but has not been proved, that
allowing entangled signals doesn’t increase the capacity. The result which allows us to
calculate the product state capacity is known as the Holevo–Schumacher–Westmoreland
(HSW) theorem, after its discoverers. As does Shannon’s noisy channel coding theorem
for classical noisy channels, the HSW theorem provides an effective means for computing
the product state capacity for a speciﬁed noisy channel E, and in some instances may
even allow the derivation of an exact expression.
Classical information over noisy quantum channels
555
Theorem 12.8: (Holevo–Schumacher–Westmoreland (HSW) theorem) Let E be a
trace-preserving quantum operation. Deﬁne
χ(E) ≡max
{pj,ρj}
⎡
⎣S
⎛
⎝E
⎛
⎝
j
pjρj
⎞
⎠
⎞
⎠−

j
pjS(E(ρj))
⎤
⎦,
(12.71)
where the maximum is over all ensembles {pj, ρj} of possible input states ρj to
the channel. Then χ(E) is the product state capacity for the channel E, that is,
χ(E) = C(1)(E).
The maximum in (12.71) is potentially over an unbounded set. In practice, we use
the results of the following exercise to restrict the maximization to pure state ensembles
containing at most d2 elements, where d is the dimension of the input to the channel.
Exercise 12.11:
Show that the maximum in the expression (12.71) may be achieved
using an ensemble of pure states. Show further that it sufﬁces to consider only
ensembles of at most d2 pure states, where d is the dimension of the input to the
channel.
The proof of the HSW theorem involves several different ideas and it is easiest to
understand the proof by breaking the discussion up into smaller pieces, and then putting
the pieces together to obtain the HSW theorem.
Random coding
Suppose ρj is a set of possible inputs to the channel E and let σj ≡E(ρj) be the cor-
responding outputs. We are going to develop a random coding technique similar to that
described earlier for the binary symmetric channel, allowing Alice and Bob to commu-
nicate using codewords which are products of the states ρj. We let pj be a probability
distribution over the indices j, the a priori distribution. Alice wants to send a message
M chosen from the set {1, . . ., 2nR} to Bob. To each possible message M she associates
a codeword ρM1 ⊗ρM2 ⊗. . . ⊗ρMn, where M1, . . . , Mn are chosen from the index set
{j}. (The M1, . . . , Mn are not meant to be a decimal representation of M or anything
of that sort!) For each message M Alice chooses M1 by sampling from the distribution
{pj}. She chooses M2 similarly and so on through to Mn, which completes the speciﬁ-
cation of the codeword. Abusing notation slightly we write ρM ≡ρM1 ⊗· · · ⊗ρMn. The
corresponding output states are simply denoted with a σ instead of a ρ, so for example
we have σM1 = E(ρM1) and σM = E⊗n(ρM).
When Bob receives a particular state σM (corresponding to Alice trying to communi-
cate the message M) he performs a measurement in an attempt to determine what the
message was. Because we are only interested in the measurement statistics and not in
the post-measurement state of Bob’s system, it is sufﬁcient to describe this measurement
using the POVM formalism. We suppose that for each possible message M Bob has a cor-
responding POVM element EM. It is possible that Bob might have one (or more) POVM
elements that don’t correspond to any speciﬁc message sent by Alice; obviously these can
all be summed together into a single POVM element E0 satisfying E0 = I−
M ̸= 0 EM.
The probability of Bob successfully identifying M is tr(σMEM), and therefore the prob-
ability of an error being made for the message M is pe
M ≡1 −tr(σMEM).
What we want to do is prove the existence of high rate codes such that the probability
556
Quantum information theory
of error pe
M is small for all messages M. To do this we use a counter-intuitive and rather
clever trick introduced by Shannon for the classical problem. We imagine that Alice is
producing the messages M by choosing uniformly from the set {1, . . ., 2nR}, and analyze
the average probability of error
pav ≡

M pe
M
2nR
=

M (1 −tr(σMEM))
2nR
.
(12.72)
The ﬁrst step of the proof is to show that high rate codes exist with pav tending to zero
as n becomes large. After this has been done we will use Shannon’s trick to show that
this implies the existence of codes with essentially the same rate for which pe
M is close
to zero for all M. We begin by constructing a POVM {EM} which represents a pretty
good (though perhaps not optimal) method for Bob to decode the outputs σM from the
channel. The key idea in the construction, as for the classical binary symmetric channel,
is the idea of typicality.
Let ϵ > 0. Suppose we deﬁne ¯σ ≡
j pjσj, and let P be the projector onto the
ϵ-typical subspace of ¯σ⊗n. By the theorem of typical sequences it follows that for any
δ > 0 and for sufﬁciently large n,
tr

¯σ⊗n(I −P)
 ≤δ .
(12.73)
For a given message M we are also going to deﬁne a notion of an ϵ-typical subspace for
σM, based on the idea that typically σM is a tensor product of about np1 copies of ρ1, np2
copies of ρ2, and so on. Deﬁne ¯S ≡
j pjS(σj). Suppose σj has spectral decomposition

k λj
k|ej
k⟩⟨ej
k|, so
σM =

K
λM
K |EM
K ⟩⟨EM
K | ,
(12.74)
where K = (K1, . . . , Kn), and for convenience we deﬁne λM
K ≡λM1
K1 λM2
K2 . . . λMn
Kn and
|EM
K ⟩≡|eM1
K1 ⟩|eM2
K2 ⟩. . . |eMn
Kn ⟩. Deﬁne PM to be the projector onto the space spanned by
all |EM
K ⟩such that
++++
1
n log 1
λM
K
−¯S
++++ ≤ϵ .
(12.75)
(It will be useful to denote by TM the set of all K such that this condition is satisﬁed.) In a
similar manner to the proof of the theorem of typical sequences, the law of large numbers
implies that for any δ > 0 and for sufﬁciently large n we have E(tr(σMPM)) ≥1 −δ,
where the expectation is taken with respect to the distribution over codewords ρM (for
a ﬁxed message M) induced by random coding, and thus for each M,
E [tr (σM(I −PM))] ≤δ.
(12.76)
Also note that by the deﬁnition (12.75) the dimension of the subspace onto which PM
projects can be at most 2n( ¯S+ϵ), and thus
E(tr(PM)) ≤2n( ¯S+ϵ).
(12.77)
We now use the typicality notions to deﬁne Bob’s decoding POVM. We deﬁne
EM ≡

M′
PPM′P
−1/2
PPMP

M′
PPM′P
−1/2
,
(12.78)
Classical information over noisy quantum channels
557
where A−1/2 denotes the generalized inverse of A1/2, that is, the operator which is inverse
to A1/2 on the support of A and is otherwise zero. It follows that 
M EM ≤I, and
we can deﬁne one more positive operator E0 ≡I −
M EM to complete the POVM.
The intuition behind this construction is similar to the decoding method described for
the binary symmetric channel. In particular, up to small corrections EM is equal to the
projector PM, and Bob’s measurement of {EM} corresponds essentially to checking to
see if the output from the channel falls into the space on which PM projects; the space
onto which this projector projects can be thought of as analogous to the Hamming sphere
of radius np around the codewords used for the binary symmetric channel.
The main technical part of the proof that random coding works is to obtain an upper
bound on the average probability of error pav. The details of how this is done are given
in Box 12.5. The result is
pav ≤
1
2nR

M
⎡
⎢⎣3tr (σM(I −P)) +

M′ ̸= M
tr(PσMPPM′) + tr (σM(I −PM))
⎤
⎥⎦.
(12.79)
The quantity pav is deﬁned with respect to a speciﬁc choice of codewords. We are going
to calculate the expectation of this quantity over all random codes. By construction
E(σM) = ¯σ⊗n, and σM and PM′ are independent when M ′ ̸= M, so we obtain
E(pav) ≤3tr(¯σ⊗n(I −P)) + (2nR −1)tr(P ¯σ⊗nPE(P1)) + E (tr(σ1(I −P1))) .(12.80)
Substituting (12.73) and (12.76) we obtain
E(pav) ≤4δ + (2nR −1)tr(P ¯σ⊗nPE(P1)).
(12.81)
But P ¯σ⊗nP ≤2−n(S( ¯σ)−ϵ)I and by (12.77) we have E(tr(P1)) ≤2n( ¯S+ϵ) whence
E(pav) ≤4δ + (2nR −1)2−n(S( ¯σ)−¯S−2ϵ).
(12.82)
Provided R < S(¯σ) −¯S it follows that E(pav) →0 as n →∞. Indeed, by choosing
the ensemble {pj, ρj} to achieve the maximum in (12.71) we see that this must be true
whenever R < χ(E). Thus there must exist a sequence of codes of rate R such that
pav →0 as the block-size n of the code is increased. It follows that for any ﬁxed ϵ > 0
(note that this is a new meaning of ϵ to replace the old, which is no longer needed!) for
sufﬁciently large n
pav =

M pe
M
2nR
< ϵ.
(12.83)
Obviously in order for this to be true at least half the messages M must satisfy pe
M < 2ϵ.
So we construct a new code by deleting half the codewords (the codewords with high pe
M)
from the code with rate R and pav < ϵ, obtaining a new code with 2nR/2 = 2n(R−1/n)
codewords, and with pe
M < 2ϵ for all messages M. Obviously this code also has asymptotic
rate R, and the probability of an error can be made arbitrarily small for all codewords,
not just on average, as n becomes large.
Summing up, we have shown that for any rate R less than χ(E) as deﬁned in (12.71),
there exists a code using product state inputs enabling transmission through the channel
E at rate R. Our proof suffers the same ﬂaw as do random coding proofs of Shannon’s
558
Quantum information theory
classical noisy channel coding theorem, namely, it does not provide a constructive pro-
cedure for performing the coding, but it does at least demonstrate the existence of codes
at rates up to capacity.
Proof of the upper bound
Suppose R is greater than χ(E) as deﬁned in (12.71). We will show that it is impossible for
Alice to reliably send information to Bob at this rate through the channel E. Our general
strategy is to imagine that Alice is producing messages M uniformly at random from the
set {1, . . ., 2nR} and to show that her average error probability must be bounded away
from zero, and therefore the maximum error probability must also be bounded away from
zero.
Suppose Alice encodes message M as ρM = ρM
1
⊗· · · ⊗ρM
n with corresponding
outputs denoted using σ instead of ρ, and Bob decodes using a POVM {EM} which,
without loss of generality, we may suppose contains an element EM for each message,
and possibly an extra element E0 to ensure that the completeness relation 
M EM = I
is satisﬁed. This gives an average error probability:
pav =

M(1 −tr(σMEM))
2nR
.
(12.96)
From Exercise 12.3 we know that R ≤log(d), where d is the dimension of the input to
the channel, and thus the POVM {EM} contains at most dn + 1 elements. By Fano’s
inequality it follows that
H(pav) + pav log(dn) ≥H(M|Y ),
(12.97)
where Y is the measurement outcome from Bob’s decoding, and thus
npav log d ≥H(M) −H(M : Y ) −H(pav) = nR −H(M : Y ) −H(pav). (12.98)
Applying ﬁrst the Holevo bound and then the subadditivity of entropy gives
H(M : Y ) ≤S (¯σ) −

M
S(σM
1
⊗· · · ⊗σM
n )
2nR
(12.99)
≤
n

j=1

S(¯σj) −

M
S(σM
j )
2nR

,
(12.100)
where ¯σj ≡
M σM
j /2nR. Each of the n terms in the sum on the right hand side is no
greater than χ(E) as deﬁned in (12.71), so
H(M : Y ) ≤nχ(E) .
(12.101)
Substituting into (12.98) gives npav log d ≥n(R −χ(E)) −H(pav), and thus in the limit
as n becomes large we obtain
pav ≥(R −χ(E))
log(d)
,
(12.102)
which is bounded away from zero when R > χ(E), which completes the proof that χ(E)
is an upper bound on the product state capacity.
Classical information over noisy quantum channels
559
Box 12.5: HSW theorem: the error estimate
The most technically complicated part of the proof of the HSW theorem is obtaining
an estimate for pav. We outline the details of how this is done here; missing steps
should be regarded as exercises to be ﬁlled in. Suppose we deﬁne | ˜EM
K ⟩≡P|EM
K ⟩.
Then
EM =
⎛
⎝
M′

K∈TM′
| ˜EM′
K ⟩⟨˜EM′
K |
⎞
⎠
−1/2

K∈TM
| ˜EM
K ⟩⟨˜EM
K |
⎛
⎝
M′

K∈TM′
| ˜EM′
K ⟩⟨˜EM′
K |
⎞
⎠
−1/2
(12.84)
Deﬁning
α(M,K),(M′,K′) ≡⟨˜EM
K |
⎛
⎝
M′′

K′′∈TM′′
| ˜EM′′
K′′ ⟩⟨˜EM′′
K′′ |
⎞
⎠
−1/2
| ˜EM′
K′ ⟩, (12.85)
the average probability of error can be written
pav =
1
2nR

M
0
1 −

K

K′∈TM
λM
K |α(M,K),(M,K′)|2
1
.
(12.86)
Using 
K λM
K = 1 and omitting non-positive terms, we see that
pav ≤
1
2nR

M
⎡
⎣
K∈TM
λM
K (1 −α2
(M,K),(M,K)) +

K̸∈TM
λM
K
⎤
⎦.
(12.87)
Deﬁne a matrix Γ with entries γ(M,K),(M′,K′) ≡⟨˜EM
K | ˜EM′
K′ ⟩, where the indices
are such that K ∈TM and K′ ∈TM′. It is convenient to work in the matrix
space deﬁned by these index conventions, to let E denote the unit matrix with
respect to these indices, and to use sp (for spur) to denote the trace operations
with respect to these indices. A calculation shows that Γ1/2 = [α(M,K),(M′,K′)]
and it follows that α2
(M,K),(M,K) ≤γ(M,K),(M,K) ≤1. Using the observation that
1 −x2 = (1 + x)(1 −x) ≤2(1 −x) when 0 ≤x ≤1 together with (12.87) gives
pav ≤
1
2nR

M
⎡
⎣2

K∈TM
λM
K

1 −α(M,K),(M,K)
 +

K̸∈TM
λM
K
⎤
⎦.
(12.88)
Deﬁne the diagonal matrix Λ ≡diag(λM
K ) and observe that
2(E −Γ1/2) = (E −Γ1/2)2 + (E −Γ)
(12.89)
= (E −Γ)2(E + Γ1/2)−2 + (E −Γ)
(12.90)
≤(E −Γ)2 + (E −Γ) .
(12.91)
Thus,
2

M

K∈TM
λM
K (1 −α(M,K),(M,K)) = 2sp(Λ(E −Γ1/2)
(12.92)
≤sp(Λ(E −Γ)2) + sp(Λ(E −Γ)) . (12.93)
(continued)
560
Quantum information theory
Box 12.5 (continued):
Calculating the spurs on the right hand side, substituting into (12.88), and doing
some simple algebra gives
pav ≤
1
2nR

M
⎡
⎢⎣

K
λM
K
⎛
⎜
⎝2 −2γ(M,K),(M,K) +

K′ ̸= K
|γ(M,K),(M,K′)|2
+

M′ ̸= M,K′∈TM′
|γ(M,K),(M′,K′)|2
⎞
⎟
⎠+

K̸∈TM
λM
K
⎤
⎥⎦. (12.94)
Substituting deﬁnitions and doing some simple algebra gives
pav ≤
1
2nR

M
⎡
⎢⎣2tr (σM(I −P)) + tr(σM(I −P)PM(I −P))
+

M′ ̸= M
tr(PσMPPM′) + tr (σM(I −PM))
⎤
⎥⎦. (12.95)
The second term is less than tr(σM(I −P)), which gives the desired error esti-
mate, (12.79).
Examples
An interesting implication of the HSW theorem is that any quantum channel E what-
soever can be used to transmit classical information, provided the channel is not simply
a constant. For if the channel is not a constant then there exist pure states |ψ⟩and |ϕ⟩
such that E(|ψ⟩⟨ψ|) ̸= E(|ϕ⟩⟨ϕ|). Substituting the ensemble made up of these two states
with equal probabilities 1/2 into the expression (12.71) for the product state capacity we
see that
C(1)(E) ≥S
E(|ψ⟩⟨ψ|) + E(|ϕ⟩⟨ϕ|)
2

−1
2E(|ψ⟩⟨ψ|) −1
2E(|ϕ⟩⟨ϕ|) > 0 , (12.103)
where the second inequality follows from the strict concavity of the entropy established
in Section 11.3.5.
Let’s look at a simple example where the product state capacity can be calculated
exactly, the case of the depolarizing channel with parameter p. Let {pj, |ψj⟩} be an
ensemble of quantum states. Then we have
E(|ψj⟩⟨ψj|) = p|ψj⟩⟨ψj| + (1 −p)I
2,
(12.104)
a quantum state which has eigenvalues (1+p)/2 and (1−p)/2 from which it follows that
S(E(|ψj⟩⟨ψj|)) = H
1 + p
2

,
(12.105)
which does not depend on |ψj⟩at all. Thus the maximum in (12.71) is achieved by
Quantum information over noisy quantum channels
561
maximizing the entropy S(
j E(|ψj⟩⟨ψj|)), which may be done by simply choosing the
|ψj⟩to form an orthonormal basis (say |0⟩and |1⟩) for the state space of a single qubit,
giving a value for the entropy of one bit, and a product state capacity of
C(E) = 1 −H
1 + p
2

(12.106)
for the depolarizing channel with parameter p.
Exercise 12.12:
Adapt the proof of the HSW theorem to ﬁnd a proof of Shannon’s
noisy channel coding theorem, simplifying the proof wherever possible.
12.4
Quantum information over noisy quantum channels
How much quantum information can be reliably transmitted over a noisy quantum chan-
nel? This problem of determining the quantum channel capacity is less well understood
than is the problem of determining the capacity for sending classical information through
a noisy quantum channel. We now present some of the information-theoretic tools that
have been developed to understand the capacity of a quantum channel for quantum in-
formation, most notably quantum information-theoretic analogues to the Fano inequality
(Box 12.2 on page 536), data processing inequality (Section 11.2.4 on page 509) and
Singleton bound (Exercise 10.21 on page 449).
As for quantum data compression, our point of view in studying these problems is to
regard a quantum source as being a quantum system in a mixed state ρ which is entangled
with another quantum system, and the measure of reliability for transmission of quantum
information by the quantum operation E is the entanglement ﬁdelity F(ρ, E). It is useful
to introduce, as in Chapter 9, labels Q for the system ρ lives in and R, the reference
system, which initially puriﬁes Q. In this picture the entanglement ﬁdelity is a measure
of how well the entanglement between Q and R was preserved by the action of E on
system Q.
12.4.1
Entropy exchange and the quantum Fano inequality
How much noise does a quantum operation cause when applied to the state ρ of a quantum
system Q? One measure of this is the extent to which the state of RQ, initially pure,
becomes mixed as a result of the quantum operation. We deﬁne the entropy exchange of
the operation E upon input of ρ by
S(ρ, E) ≡S(R′, Q′) .
(12.107)
Suppose that the action of the quantum operation E is mocked up by introducing an
environment E, initially in a pure state, and then causing a unitary interaction between
Q and E, as described in Chapter 8. Then the state of RQE after the interaction is a pure
state, whence S(R′, Q′) = S(E′), so the entropy exchange may also be identiﬁed with
the amount of entropy introduced by the operation E into an initially pure environment
E.
Note that the entropy exchange does not depend upon the way in which the initial
state of Q, ρ, is puriﬁed into RQ. The reason is because any two puriﬁcations of Q
into RQ are related by a unitary operation on the system R, as shown in Exercise 2.81
on page 111. This unitary operation on R obviously commutes with the action of the
562
Quantum information theory
quantum operation on Q, and thus the ﬁnal states of R′Q′ induced by the two different
puriﬁcations are related by a unitary transformation on R, and thus give rise to the same
value for the entropy exchange. Furthermore, it follows from these results that S(E′)
does not depend upon the particular environmental model for E which is used, provided
the model starts with E in a pure state.
A useful explicit formula for the entropy exchange can be given based upon the
operator-sum representation for quantum operations. Suppose a trace-preserving quan-
tum operation E has operation elements {Ei}. Then, as shown in Section 8.2.3, a unitary
model for this quantum operation is given by deﬁning a unitary operator U on QE such
that
U|ψ⟩|0⟩=

i
Ei|ψ⟩|i⟩,
(12.108)
where |0⟩is the initial state of the environment, and |i⟩is an orthonormal basis for the
environment. Note that the state of E′ after application of E is:
ρE′ =

i,j
tr(EiρE†
j)|i⟩⟨j| .
(12.109)
That is, tr(EiρE†
j) are the matrix elements of E′ in the |i⟩basis. Given a quantum
operation with operation elements {Ei} it is therefore natural to deﬁne a matrix W (the
w-matrix) with matrix elements Wij ≡tr(EiρE†
j), that is, W is the matrix of E′ in
an appropriate basis. This representation for ρE′ gives rise to a formula for the entropy
exchange which is useful in making explicit calculations,
S(ρ, E) = S(W) ≡−tr(W log W).
(12.110)
Given a quantum operation E and a state ρ, it is always possible to choose operation
elements {Fj} for E such that W is diagonal; we say W is in canonical form. To see that
such a set of operation elements exists, recall from Chapter 8 that a quantum operation
may have many different sets of operation elements. In particular, two sets of operators
{Ei} and {Fj} are operation elements for the same quantum operation if and only if
Fj = 
j ujiEi, where u is a unitary matrix of complex numbers, and it may be necessary
to append 0 operators to the sets Ei or Fj so that the matrix u is a square matrix. Let W
be the w-matrix associated to a particular choice of operation elements {Ei} for E. W is a
matrix representation of the environmental density operator, and thus is a positive matrix
which may be diagonalized by a unitary matrix v, D = vWv†, where D is a diagonal
matrix with non-negative entries. Deﬁne operators Fj by the equation Fj ≡
i vjiEi,
so the Fj are also a set of operation elements for E, giving rise to a new w-matrix 6
W
with matrix elements
6
Wkl = tr(FkρF †
l ) =

mn
vkmv∗
lnWmn = Dkl .
(12.111)
Thus, the w-matrix is diagonal if calculated with respect to the operation elements {Fj}.
Any such set of operation elements {Fj} for E for which the corresponding w-matrix is
diagonal is said to be a canonical representation for E with respect to the input ρ. We see
later that canonical representations turn out to have a special signiﬁcance for quantum
error-correction.
Many properties of the entropy exchange follow easily from properties of the entropy
Quantum information over noisy quantum channels
563
discussed in Chapter 11. For example, working in a canonical representation for a trace-
preserving quantum operation E on a d-dimensional space, we see immediately that
S(I/d, E) = 0 if and only if E is a unitary quantum operation. Therefore, S(I/d, E) can
be thought of as quantifying the extent to which incoherent quantum noise occurs on
the system as a whole. A second example is that the matrix W is linear in ρ, and by the
concavity of the entropy it follows that S(ρ, E) is concave in ρ. Since the system RQ can
always be chosen to be at most d2-dimensional, where d is the dimension of Q, it follows
that the entropy exchange is bounded above by 2 log d.
Exercise 12.13:
Show that the entropy exchange is concave in the quantum operation
E.
Intuitively, if the quantum source Q is subject to noise which results in the entan-
glement RQ becoming mixed, then the ﬁdelity of the ﬁnal state R′Q′ with the initial
state RQ cannot be perfect. Moreover, the greater the noise the worse the ﬁdelity. In
Section 12.1.1 an analogous situation arose in the study of classical channels, where the
uncertainty H(X|Y ) about the input of a channel, X, given the output, Y , was related
to the probability of being able to recover the state of X from Y by the Fano inequality.
There is a very useful quantum analogue of this result, relating the entropy exchange
S(ρ, E) to the entanglement ﬁdelity F(ρ, E).
Theorem 12.9: (Quantum Fano inequality) Let ρ be a quantum state and E a
trace-preserving quantum operation. Then
S(ρ, E) ≤H(F(ρ, E)) + (1 −F(ρ, E)) log(d2 −1) ,
(12.112)
where H(·) is the binary Shannon entropy.
Inspection of the quantum Fano inequality reveals an attractive intuitive meaning: if
the entropy exchange for a process is large, then the entanglement ﬁdelity for the process
must necessarily be small, indicating that the entanglement between R and Q has not
been well preserved. Moreover, we note that in the quantum Fano inequality the entropy
exchange S(ρ, E) plays a role analogous to the role played by the conditional entropy
H(X|Y ) in classical information theory.
Proof
To prove the quantum Fano inequality, let |i⟩be an orthonormal basis for the system RQ
chosen so the ﬁrst state in the set |1⟩= |RQ⟩. If we form the quantities pi ≡⟨i|ρR′Q′|i⟩,
then from the results of Section 11.3.3 it follows that
S(R′, Q′) ≤H(p1, . . . , pd2) ,
(12.113)
where H(pi) is the Shannon information of the set {pi}. Elementary algebra shows that
H(p1, . . . , pd2) = H(p1) + (1 −p1)H

p2
1 −p1
, . . . ,
pd2
1 −p1

.
(12.114)
Combining this with the observation that H(
p2
1−p1 , . . . ,
pd2
1−p1 ) ≤log(d2 −1) and p1 =
F(ρ, E) by deﬁnition gives,
S(ρ, E) ≤H(F(ρ, E)) + (1 −F(ρ, E)) log(d2 −1) ,
(12.115)
564
Quantum information theory
which is the quantum Fano inequality.
12.4.2
The quantum data processing inequality
In Section 11.2.4 we discussed the classical data processing inequality. Recall that the
data processing inequality states that for a Markov process X →Y →Z,
H(X) ≥H(X :Y ) ≥H(X :Z),
(12.116)
with equality in the ﬁrst stage if and only if the random variable X can be recovered
from Y with probability one. Thus the data processing inequality provides information-
theoretic necessary and sufﬁcient conditions for error-correction to be possible.
There is a quantum analogue to the data processing inequality applicable to a two stage
quantum process described by quantum operations E1 and E2,
ρ
E1
−→ρ′
E2
−→ρ′′.
(12.117)
We deﬁne the quantum coherent information by
I(ρ, E) ≡S(E(ρ)) −S(ρ, E) .
(12.118)
This quantity, coherent information, is suspected (but not known) to play a role in
quantum information theory analogous to the role played by the mutual information
H(X : Y ) in classical information theory. One reason for this belief is that the coherent
information satisﬁes a quantum data processing inequality analogous to the classical data
processing inequality.
Theorem 12.10: (Quantum data processing inequality) Let ρ be a quantum state
and E1 and E2 trace-preserving quantum operations. Then
S(ρ) ≥I(ρ, E1) ≥I(ρ, E2 ◦E1) ,
(12.119)
with equality in the ﬁrst inequality if and only if it is possible to perfectly
reverse the operation E1, in the sense that there exists a trace-preserving reversal
operation R such that F(ρ, R ◦E) = 1.
Comparison with the classical data processing inequality shows that the coherent infor-
mation plays a role in the quantum data processing inequality identical to the role played
by the mutual information in the classical data processing inequality. Of course, such a
heuristic argument cannot be regarded as any sort of a rigorous justiﬁcation for the view
that the coherent information is the correct quantum analogue of the classical mutual
information. In order to obtain such a justiﬁcation, the coherent information ought to be
related to the quantum channel capacity in a similar way to the relation between classical
mutual information and classical channel capacity, and such a relationship has not yet
been established. (See the end of chapter ‘History and further reading’ for some partial
progress.)
How is the notion of perfect reversibility deﬁned in the quantum data processing
inequality connected with more familiar notions such as that arising in the context of
quantum error-correction? By deﬁnition, we say a trace-preserving quantum operation E
is perfectly reversible upon input of ρ if there exists a trace-preserving quantum operation
R such that
F(ρ, R ◦E) = 1.
(12.120)
Quantum information over noisy quantum channels
565
But from item (4) on page 423, it follows that a quantum operation is perfectly reversible
if and only if for every state |ψ⟩in the support of ρ,
(R ◦E)(|ψ⟩⟨ψ|) = |ψ⟩⟨ψ|.
(12.121)
This observation connects the notion of perfect reversibility to quantum error-correcting
codes. Recall that a quantum error-correcting code is a subspace of some larger Hilbert
space spanned by logical codewords. To be resilient against the noise induced by a
quantum operation E it is necessary that the quantum operation E be reversible by a
trace-preserving reversal operation R in the sense that for all states |ψ⟩in the code, (R ◦
E)(|ψ⟩⟨ψ|) = |ψ⟩⟨ψ|. This condition is equivalent to the criterion of perfect reversibility
in the statement of the data processing inequality, that F(ρ, R◦E) = 1, for some ρ whose
support is the code space.
Proof
The quantum data processing inequality is proved using a four system construction: R
and Q appear in their familiar roles, while E1 and E2 are systems initially in pure states,
chosen such that a unitary interaction between Q and E1 generates the dynamics E1, and
a unitary interaction between Q and E2 generates the dynamics E2. The proof of the ﬁrst
stage of the quantum data processing inequality is to apply the subadditivity inequality
S(R′, E′
1) ≤S(R′) + S(E′
1) to obtain
I(ρ, E1) = S(E1(ρ)) −S(ρ, E1)
(12.122)
= S(Q′) −S(E′
1)
(12.123)
= S(R′, E′
1) −S(E′
1)
(12.124)
≤S(R′) + S(E′
1) −S(E′
1) = S(R′)
(12.125)
= S(R) = S(Q) = S(ρ).
(12.126)
The proof of the second part of the data processing inequality is to apply the strong
subadditivity inequality,
S(R′′, E′′
1 , E′′
2 ) + S(E′′
1 ) ≤S(R′′, E′′
1 ) + S(E′′
1 , E′′
2 ).
(12.127)
From the purity of the total state of R′′Q′′E′′
1 E′′
2 it follows that
S(R′′, E′′
1 , E′′
2 ) = S(Q′′).
(12.128)
Neither of the systems R or E1 are involved in the second stage of the dynamics in
which Q and E2 interact unitarily. Thus, their state does not change during this stage:
ρR′′E′′
1 = ρR′E′
1. But from the purity of RQE1 after the ﬁrst stage of the dynamics,
S(R′′, E′′
1 ) = S(R′, E′
1) = S(Q′).
(12.129)
The remaining two terms in the strong subadditivity inequality (12.127) are now recog-
nized as entropy exchanges,
S(E′′
1 ) = S(E′
1) = S(ρ, E1);
S(E′′
1 , E′′
2 ) = S(ρ, E2 ◦E1).
(12.130)
Making these substitutions into (12.127) yields
S(Q′′) + S(ρ, E1) ≤S(Q′) + S(ρ, E2 ◦E1),
(12.131)
566
Quantum information theory
which can be rewritten as the second stage of the data processing inequality, I(ρ, E1) ≥
I(ρ, E2 ◦E1).
To complete the proof we need to show that E is perfectly reversible upon input of
ρ if and only if the ﬁrst inequality in the quantum data processing inequality is satisﬁed
with equality,
S(ρ) = I(ρ, E) = S(ρ′) −S(ρ, E).
(12.132)
To prove the necessity of this condition for reversal, suppose that E is perfectly reversible
upon input of ρ, with reversal operation R. From the second stage of the quantum data
processing inequality it follows that
S(ρ′) −S(ρ, E) ≥S(ρ′′) −S(ρ, R ◦E).
(12.133)
From the reversibility requirement it follows that ρ′′ = ρ. Furthermore, from the quan-
tum Fano inequality (12.112) and the perfect reversibility requirement F(ρ, R ◦E) = 1
it follows that S(ρ, R ◦E) = 0. Thus the second stage of the quantum data processing
inequality when applied to ρ →E(ρ) →(R ◦E)(ρ) may be rewritten
S(ρ′) −S(ρ, E) ≥S(ρ).
(12.134)
Combining this with the ﬁrst part of the quantum data processing inequality, S(ρ) ≥
S(ρ′) −S(ρ, E), we deduce that
S(ρ′) = S(ρ) −S(ρ, E),
(12.135)
for any E which is perfectly reversible upon input of ρ.
Next, we give a constructive proof that satisfaction of the condition
S(ρ) = S(ρ′) −S(ρ, E)
(12.136)
implies that the quantum operation E is reversible upon input of ρ. Noting that S(ρ) =
S(Q) = S(R) = S(R′), S(ρ′) = S(Q′) = S(R′, E′) and S(ρ, E) = S(E′), we see that
S(R′)+S(E′) = S(R′, E′), which we saw in Section 11.3.4 is equivalent to the condition
that ρR′E′ = ρR′ ⊗ρE′. Suppose the initial state of Q is ρ = 
i pi|i⟩⟨i|, and that
we purify this state into RQ as |RQ⟩= 
i
√pi|i⟩|i⟩, where the ﬁrst system is R and
the second system is Q. Note that ρR′ = ρR = 
i pi|i⟩⟨i|. Furthermore, suppose that
ρE′ = 
j qj|j⟩⟨j| for some orthonormal set |j⟩, so that
ρR′E′ =

ij
piqj|i⟩⟨i| ⊗|j⟩⟨j|.
(12.137)
This has eigenvectors |i⟩|j⟩so by the Schmidt decomposition we may write the total
state of R′Q′E′ after the quantum operation E has been applied as
|R′Q′E′⟩=

ij
√piqj|i⟩|i, j⟩|j⟩,
(12.138)
where |i, j⟩is some orthonormal set of states for system Q. Deﬁne projectors Pj by Pj ≡

i |i, j⟩⟨i, j|. The idea of the restoration operation is to ﬁrst perform a measurement
described by the projectors Pj, which reveals the state |j⟩of the environment, and then do
a unitary rotation Uj conditional on j which restores the state |i, j⟩to |i⟩: Uj|i, j⟩≡|i⟩.
Quantum information over noisy quantum channels
567
That is, j is the measurement syndrome, and Uj the corresponding recovery operation.
The complete recovery operation may be written
R(σ) ≡

j
UjPjσPjU †
j .
(12.139)
The projectors Pj are orthogonal, by the orthogonality of the states |i, j⟩, but may not
be complete. If this is the case, then to ensure that the quantum operation R is trace-
preserving, it is necessary to add an extra projector ˜P ≡I−
j Pj to the set of projectors
to make the operation trace-preserving.
The ﬁnal state of the system RQE after the reversal operation is given by

j
UjPj|R′Q′E′⟩⟨R′Q′E′|PjU †
j
=

j

i1i2
√pi1pi2qj|i1⟩⟨i2| ⊗(Uj|i1, j⟩⟨i2, j|U †
j ) ⊗|j⟩⟨j| (12.140)
=

i1,i2
√pi1pi2|i1⟩⟨i2| ⊗|i1⟩⟨i2| ⊗ρE′ ,
(12.141)
from which we see that ρR′′Q′′ = ρRQ, and thus F(ρ, R ◦E) = 1, that is, the operation
E is perfectly reversible upon input of the state ρ, as we desired to show.
This completes the proof of the information-theoretic reversibility conditions for trace-
preserving quantum operations. Intuition about the result may be obtained by imagining
that Q is a memory element in a quantum computer, R is the remainder of the quan-
tum computer, and E is an environment whose interaction with Q causes noise. The
information-theoretic reversibility condition is most elegantly understood as the state-
ment that the state of the environment E′ after the noise has occurred should not be
correlated with the state of the remainder of the quantum computer, R′. To state it in
more anthropomorphic terms, error-correction is possible precisely when the environ-
ment does not learn anything about the remainder of the quantum computer through
interacting with Q!
Even more concretely, suppose Q is an n qubit system and C is an [n, k] quantum
error-correcting code living in the system Q with orthonormal codewords |x⟩and pro-
jector P onto the codespace. Consider the density matrix P/2k, which may be puriﬁed
to a pure state of RQ:
1
√
2k

x
|x⟩|x⟩.
(12.142)
Imagine this code is able to correct arbitrary errors on some subset Q1 of the qubits.
Then, in particular, it must be able to correct that error which simply swaps those qubits
out into the environment and replaces them with some standard state. The information-
theoretic reversibility condition that ρR′E′ = ρR′ ⊗ρE′ can in this case be rephrased
as the condition ρRQ1 = ρR ⊗ρQ1. Thus the reference system R and subsystem Q1 on
which errors can be corrected must initially be uncorrelated if correction is to be possible!
Exercise 12.14:
Show that the condition ρRQ1 = ρR ⊗ρQ1 is also sufﬁcient to be able
to correct errors on the subsystem Q1.
568
Quantum information theory
The reasoning used in the proof of the quantum data processing inequality can be
adapted to prove a wide variety of other inequalities. For example, suppose we have a
quantum system Q in a state ρ that is subjected to the quantum operation E. The ﬁrst
stage of the data processing inequality follows by applying the subadditivity inequality
for entropy to the systems R′E′. What if instead we applied the subadditivity inequality
to the systems Q′E′, obtaining
S(ρ) = S(R) = S(R′) = S(Q′, E′) ≤S(Q′) + S(E′) = S(E(ρ)) + S(ρ, E) . (12.143)
That is,
ΔS + S(ρ, E) ≥0,
(12.144)
where ΔS ≡S(E(ρ)) −S(ρ) is the change in entropy caused by the process E. Loosely
speaking, this inequality says that the change in entropy of the system plus the change in
entropy of the environment must be non-negative, an eminently reasonable statement in
accord with the second law of thermodynamics, and one which will aid us in Section 12.4.4
in our thermodynamic analysis of quantum error-correction!
Exercise 12.15:
Apply all possible combinations of the subadditivity and strong
subadditivity inequalities to deduce other inequalities for the two stage quantum
process ρ →ρ′ = E1(ρ) →ρ′′ = (E2 ◦E1)(ρ), expressing the results whenever
possible in terms of entropy exchanges and the entropies S(ρ), S(ρ′), S(ρ′′).
When it is not possible to express a quantity appearing in such an inequality in
these terms, give a prescription for calculating the quantity using only a
knowledge of ρ and operation elements {Ej} for E1 and {Fk} for E2.
12.4.3
Quantum Singleton bound
The information-theoretic approach to quantum error-correction can be used to prove a
beautiful bound on the ability of quantum error-correcting codes to correct errors, the
quantum Singleton bound. Recall that an [n, k, d] code uses n qubits to encode k qubits,
and is able to correct located errors (Exercise 10.45) on up to d −1 of the qubits. The
quantum Singleton bound states that we must have n −k ≥2(d −1). Contrast this
with the classical Singleton bound, Exercise 10.21 on page 449, which states that for an
[n, k, d] classical code we must have n −k ≥d −1. Because a quantum code to correct
errors on up to t qubits must have distance at least 2t + 1 it follows that n −k ≥4t.
Thus, for example, a code to encode k = 1 qubits and capable of correcting errors on
t = 1 of the qubits must satisfy n −1 ≥4, that is, n must be at least 5, so the ﬁve qubit
code described in Chapter 10 is the smallest possible code for this task.
The proof of the quantum Singleton bound is an extension of the information-theoretic
techniques we have been using to analyze quantum error-correction. Suppose the code is
a 2k-dimensional subspace associated with the system Q, with orthonormal basis denoted
by |x⟩. Introduce a 2k-dimensional reference system R also with 2k orthonormal basis
vectors denoted |x⟩, and consider the entangled state of RQ,
|RQ⟩=
1
√
2k

x
|x⟩|x⟩.
(12.145)
We divide the n qubits of Q up into three disjoint blocks, the ﬁrst and second, Q1 and Q2,
consisting of d−1 qubits each, and the third Q3 consisting of the remaining n−2(d−1)
Quantum information over noisy quantum channels
569
qubits. Because the code has distance d any set of d −1 located errors may be corrected,
and thus it is possible to correct errors on either Q1 or Q2. It follows that R and Q1 must
be uncorrelated, as are R and Q2. By this observation, the overall purity of the state of
RQ1Q2Q3, and the subadditivity of entropy we have:
S(R) + S(Q1) = S(R, Q1) = S(Q2, Q3) ≤S(Q2) + S(Q3)
(12.146)
S(R) + S(Q2) = S(R, Q2) = S(Q1, Q3) ≤S(Q1) + S(Q3).
(12.147)
Adding these two inequalities gives
2S(R) + S(Q1) + S(Q2) ≤S(Q1) + S(Q2) + 2S(Q3).
(12.148)
Canceling terms and substituting S(R) = k gives k ≤S(Q3). But Q3 is n −2(d −1)
qubits in size, so S(Q3) ≤n−2(d−1), giving k ≤n−2(d−1), whence 2(d−1) ≤n−k,
the quantum Singleton bound.
As an example of the quantum Singleton bound in action, consider the depolarizing
channel E(ρ) = pρ + (1 −p)/3(XρX + Y ρY + ZρZ). Suppose the depolarizing channel
acts independently on a large number n of qubits. If p < 3/4 then more than one quarter
of those qubits will suffer errors, so any code capable of recovering from the errors must
have t > n/4. But the quantum Singleton bound implies that n −k ≥4t > n, and
thus k must be negative, that is, it is not possible to encode any qubits at all in this
case. Thus, when p < 3/4, the quantum Singleton bound implies that the capacity of
the depolarizing channel for quantum information is zero!
12.4.4
Quantum error-correction, refrigeration and Maxwell’s demon
Quantum error-correction may be thought of as a type of refrigeration process, capable of
keeping a quantum system at a constant entropy, despite the inﬂuence of noise processes
which tend to change the entropy of the system. Indeed, quantum error-correction may
even appear rather puzzling from this point of view, as it appears to allow a reduction
in entropy of the quantum system in apparent violation of the second law of thermody-
namics! To understand why there is no violation of the second law we do an analysis of
quantum error-correction similar to that used to analyze Maxwell’s demon in Box 3.5 on
page 162. Quantum error-correction is essentially a special type of Maxwell’s demon –
we can imagine a ‘demon’ performing syndrome measurements on the quantum system,
and then correcting errors according to the result of the syndrome measurement. Just
as in the analysis of the classical Maxwell’s demon, the storage of the syndrome in the
demon’s memory carries with it a thermodynamic cost, in accordance with Landauer’s
principle. In particular, because any memory is ﬁnite, the demon must eventually begin
erasing information from its memory, in order to have space for new measurement re-
sults. Landauer’s principle states that erasing one bit of information from the memory
increases the total entropy of the system – quantum system, demon, and environment –
by at least one bit.
Rather more precisely, we can consider a four-stage error-correction ‘cycle’ as
depicted in Figure 12.9:
(1) The system, starting in a state ρ, is subjected to a noisy quantum evolution that
takes it to a state ρ′. In typical scenarios for error-correction, we are interested in
cases where the entropy of the system increases, S(ρ′) > S(ρ), although this is not
necessary.
570
Quantum information theory
(2) A demon performs a (syndrome) measurement on the state ρ′ described by
measurement operators {Mm}, obtaining result m with probability
pm = tr(Mmρ′M †
m), resulting in the posterior state ρ′
m = Mmρ′M †
m/pm.
(3) The demon applies a unitary operation Vm (the recovery operation) that creates a
ﬁnal system state
ρ′′
m = Vmρ′
mV †
m = VmMmρ′M †
mV †
m
pm
.
(12.149)
(4) The cycle is restarted. In order that this actually be a cycle and that it be a
successful error-correction, we must have ρ′′
m = ρ for each measurement outcome
m.
Figure 12.9. The quantum error-correction cycle.
We now show that any reduction in entropy during the second and third stages – the
error-correction stages – comes at the expense of entropy production in the environment
which is at least as large as the entropy reduction in the quantum system being error-
corrected. After the third stage the only record of the measurement result m is the
record kept in the demon’s memory. To reset its memory for the next cycle, the demon
must erase its record of the measurement result, causing a net increase in the entropy
of the environment by Landauer’s principle. The number of bits that must be erased
is determined by the representation the demon uses to store the measurement result
m; by Shannon’s noiseless channel coding theorem, at least H(pm) bits are required,
on average, to store the measurement result, and thus a single error-correction cycle on
average involves the dissipation of H(pm) bits of entropy into the environment when the
measurement record is erased.
Before error-correction the state of the quantum system is ρ′. After error-correction
the state of the quantum system is ρ, so the net change in entropy of the system due
to the error-correction is ΔS ≡S(ρ) −S(ρ′). There is an additional entropic cost of
H(pm) (on average) associated with erasing the measurement record, for a total cost of
Δ(S)+H(pm). Our goal is to bound this thermodynamic cost, and in so doing demonstrate
that the second law of thermodynamics is never violated. To do so, it helps to introduce
two items of notation: let E represent the noise process occurring during stage 1 of the
error-correction cycle, ρ →ρ′ = E(ρ), and let R be the quantum operation representing
Entanglement as a physical resource
571
the error-correction operation,
R(σ) ≡

m
VmMmσM †
mV †
m .
(12.150)
With input ρ′ the w-matrix for this process has elements Wmn = tr(VmMmρ′M †
nV †
n),
and thus has diagonal elements Wmm = tr(VmMmρ′M †
mV †
m) = tr(Mmρ′M †
m), which is
just the probability pm the demon obtains measurement outcome m when measuring the
error syndrome. By Theorem 11.9 on page 515 the entropy of the diagonal elements of
W is at least as great as the entropy of W, so
H(pm) ≥S(W) = S(ρ′, R) ,
(12.151)
with equality if and only if the operators VmMm are a canonical decomposition of R
with respect to ρ′ so that the off-diagonal terms in W vanish. By Equation (12.144) on
page 568 it follows that
ΔS + S(ρ′, R) = S(ρ) −S(ρ′) + S(ρ′, R) ≥0.
(12.152)
Combining this result with (12.151) we deduce that ΔS + H(pm) ≥0. But ΔS + H(pm)
was the total entropy change caused by the error-correction procedure. We conclude that
error-correction can only ever result in a net increase in total entropy, with any decrease
in system entropy due to error-correction being paid for with entropy production when
the error syndrome produced during error-correction is erased.
Exercise 12.16:
Show that in the case where R perfectly corrects E for the input ρ,
the inequality
S(ρ) −S(ρ′) + S(ρ′, R) ≥0
(12.153)
must actually be satisﬁed with equality.
12.5
Entanglement as a physical resource
Thus far our study of quantum information has been focused on resources that are not
too far distant from the resources considered in classical information theory. For your
convenience Figure 12.10 summarizes many of these results in both their quantum and
classical guises. One of the delights of quantum computation and quantum information is
that quantum mechanics also contains essentially new types of resource that differ vastly
from the sort of resource traditionally regarded as information in classical information
theory. Perhaps the best understood of these is quantum entanglement, and it is to this
resource that we now turn.
We say ‘best understood’, but that is not saying a whole lot! We are a long way
from having a general theory of quantum entanglement. Nevertheless, some encouraging
progress towards such a general theory has been made, revealing an intriguing structure
to the entangled states, and some quite remarkable connections between the properties of
noisy quantum channels and various types of entanglement transformation. We are just
going to take a quick peek at what is known, focusing on the transformation properties of
entanglement distributed between two systems (‘bi-partite’ entanglement), Alice and Bob.
There is, of course, a great deal of interest in developing a general theory of entanglement
for multi-partite systems, but how to do this is not well understood.
572
Quantum information theory
Information Theory
Classical
Quantum
Shannon entropy
H(X) = −

x
p(x) log p(x)
von Neumann entropy
S(ρ) = −tr(ρ log ρ)
Distinguishability and accessible information
Letters always distinguishable
N = |X|
Holevo bound
H(X :Y ) ≤S(ρ) −

x
pxS(ρx)
ρ = 
x pxρx
Noiseless channel coding
Shannon’s theorem
nbits = H(X)
Schumacher’s theorem
nqubits = S

x
pxρx

Capacity of noisy channels for classical information
Shannon’s noisy coding
theorem
C(N) = max
p(x) H(X :Y )
Holevo–Schumacher–Westmoreland
theorem
C(1)(E) = max
{px,ρx}
0
S(ρ′)−

x
pxS(ρ′
x)
1
ρ′
x = E(ρx) ,
ρ′ =

x
pxρ′
x
Information-theoretic relations
Fano inequality
H(pe) + pe log(|X| −1)
≥H(X|Y )
Mutual information
H(X :Y ) = H(Y ) −H(Y |X)
Data processing inequality
X →Y →Z
H(X) ≥H(X :Y ) ≥H(X :Z)
Quantum Fano inequality
H(F(ρ, E)) + (1 −F(ρ, E)) log(d2 −1)
≥S(ρ, E)
Coherent information
I(ρ, E) = S(E(ρ)) −S(ρ, E)
Quantum data processing inequality
ρ →E1(ρ) →(E2 ◦E1)(ρ)
S(ρ) ≥I(ρ, E1) ≥I(ρ, E2 ◦E1)
Figure 12.10. Summary of some important classical information relations, and quantum analogues of those
relations.
Entanglement as a physical resource
573
12.5.1
Transforming bi-partite pure state entanglement
The starting point for our investigation is the following simple question: given that Alice
and Bob share an entangled pure state |ψ⟩, into what other types of entanglement |ϕ⟩
may they transform |ψ⟩, given that they can each perform arbitrary operations on their
local systems, including measurement, but can only communicate using classical commu-
nication? No quantum communication between Alice and Bob is allowed, constraining
the class of transformations they may achieve.
As an example, imagine that Alice and Bob share an entangled pair of qubits in the
Bell state (|00⟩+ |11⟩)/
√
2. Alice performs a two outcome measurement described by
measurement operators M1 and M2:
M1 =
 cos θ
0
0
sin θ

;
M2 =
 sin θ
0
0
cos θ

.
(12.154)
After the measurement the state is either cos θ|00⟩+ sin θ|11⟩or cos θ|11⟩+ sin θ|00⟩,
depending on the measurement outcome, 1 or 2. In the latter case, Alice applies a
gate after the measurement, resulting in the state cos θ|01⟩+ sin θ|10⟩. She then sends
the measurement result (1 or 2) to Bob, who does nothing to the state if the measurement
result is 1, and performs a
gate if the result is 2. The ﬁnal state of the joint system is
therefore cos θ|11⟩+sin θ|00⟩, regardless of the measurement outcome obtained by Alice.
That is, Alice and Bob have transformed their initial entangled resource (|00⟩+|11⟩)/
√
2
into the state cos θ|00⟩+sin θ|11⟩using only local operations on their individual systems,
and classical communication.
It is perhaps not immediately obvious what is the signiﬁcance of the problem of entan-
glement transformation. There is a certain intrinsic interest to the class of transformations
we are allowing – local operations and classical communication (LOCC) – however, it
is by no means clear a priori that this is truly an interesting problem. It turns out,
however, that generalizations of this entanglement transformation problem exhibit deep
and unexpected connections to quantum error-correction. Furthermore, the techniques
introduced in the solution to the problem are of quite considerable interest, and give
unexpected insight into the properties of entanglement. In particular, we will discover
a close connection between entanglement and the theory of majorization, an area of
mathematics that actually predates quantum mechanics!
Before jumping into the study of entanglement transformation, let’s ﬁrst acquaint
ourselves with a few relevant facts about majorization. Majorization is an ordering on
d-dimensional real vectors intended to capture the notion that one vector is more or less
disordered than another. More precisely, suppose x = (x1, . . . , xd) and y = (y1, . . . , yd)
are two d-dimensional vectors. We use the notation x↓to mean x re-ordered so the
components are in decreasing order so, for example, x↓
1 is the largest component of x.
We say x is majorized by y, written x ≺y, if k
j=1 x↓
j ≤k
j=1 y↓
j for k = 1, . . . , d, with
equality instead of inequality when k = d. What this deﬁnition has to do with notions of
disorder will become clear shortly!
The connection between majorization and entanglement transformation is easily stated
yet rather surprising. Suppose |ψ⟩and |ϕ⟩are states of the joint Alice–Bob system. Deﬁne
ρψ ≡trB(|ψ⟩⟨ψ|), ρϕ ≡trB(|ϕ⟩⟨ϕ|) to be the corresponding reduced density matrices
of Alice’s system, and let λψ and λϕ be the vectors whose entries are the eigenvalues of
ρψ and ρϕ. We will show that |ψ⟩may be transformed to |ϕ⟩by LOCC if and only if
λψ ≺λϕ! To demonstrate this we need ﬁrst a few simple facts about majorization.
574
Quantum information theory
Exercise 12.17:
Show that x ≺y if and only if for all real t,
d
j=1 max(xj −t, 0) ≤d
j=1 max(yj −t, 0), and d
j=1 xj = d
j=1 yj.
Exercise 12.18:
Use the previous exercise to show that the set of x such that x ≺y is
convex.
The following proposition gives a more intuitive meaning to the notion of majorization,
showing that x ≺y if and only if x can be written as a convex combination of permutations
of y. Intuitively, therefore, x ≺y if x is more disordered than y in the sense that x can
be obtained by permuting the elements of y and mixing the resulting vectors. This
representation theorem is one of the most useful results in the study of majorization.
Proposition 12.11: x ≺y if and only if x = 
j pjPjy for some probability
distribution pj and permutation matrices Pj.
Proof
Suppose x ≺y. Without loss of generality we may suppose x = x↓and y = y↓.
We will prove that x = 
j pjPjy by induction on the dimension d. For d = 1 the
result is clear. Suppose x and y are d + 1-dimensional vectors such that x ≺y. Then
x1 ≤y1. Choose j such that yj ≤x1 ≤yj−1, and deﬁne t in the range [0, 1] such that
x1 = ty1 + (1 −t)yj. Deﬁne a convex combination of permutations D ≡tI + (1 −t)T,
where T is the permutation matrix which transposes the 1st and jth matrix elements.
Then
Dy = (x1, y2, . . . , yj−1, (1 −t)y1 + tyj, yj+1, . . . , yd+1).
(12.155)
Deﬁne x′ ≡(x2, . . . , xd+1) and y′ ≡(y2, . . . , yj−1, (1 −t)y1 + tyj, yj+1, . . . , yd+1). In
Exercise 12.19 on this page you show x′ ≺y′, so by the inductive hypothesis x′ =

j p′
jP ′
jy′ for probabilities p′
j and permutation matrices P ′
j, whence x =
'
j p′
jP ′
j
(
Dy,
where the P ′
j are extended to d+1 dimensions by acting trivially on the ﬁrst entry. Since
D = (tI + (1 −t)T), and a product of permutation matrices is a permutation matrix, the
result follows.
Exercise 12.19:
Verify that x′ ≺y′.
Conversely, suppose x = 
j pjPjy. It is clear that Pjy ≺y and by Exercise 12.18 it
follows that x = 
j pjPjy ≺y.
Matrices which are convex combinations of permutation matrices have many interest-
ing properties. Notice, for example, that the entries of such a matrix must be non-negative,
and that each of the rows and columns must sum to one. A matrix with these properties is
known as a doubly stochastic matrix, and there is a result known as Birkhoff’s theorem
which implies that the doubly stochastic matrices correspond exactly to the set of ma-
trices which can be written as convex combinations of permutation matrices. We won’t
prove Birkhoff’s theorem here (see the end of chapter ‘History and further reading’), but
merely state it:
Theorem 12.12: (Birkhoff’s theorem) A d by d matrix D is doubly stochastic (that is,
has non-negative entries and each row and column sums to 1) if and only if D
can be written as a convex combination of permutation matrices, D = 
j pjPj.
Entanglement as a physical resource
575
From Birkhoff’s theorem and Proposition 12.11 it follows that x ≺y if and only
if x = Dy for some doubly stochastic D. This result allows us to prove a striking and
useful operator generalization of Proposition 12.11. Suppose H and K are two Hermitian
operators. Then we say H ≺K if λ(H) ≺λ(K), where we use λ(H) to denote the vector
of eigenvalues of a Hermitian operator H. Then we have:
Theorem 12.13: Let H and K be Hermitian operators. Then H ≺K if and only if
there is a probability distribution pj and unitary matrices Uj such that
H =

j
pjUjKU †
j .
(12.156)
Proof
Suppose H ≺K. Then λ(H) = 
j pjPjλ(K) by Proposition 12.11. Let Λ(H) denote
the diagonal matrix whose entries are the eigenvalues of H. Then the vector equation
λ(H) = 
j pjPjλ(K) may be re-expressed as
Λ(H) =

j
pjPjΛ(K)P †
j .
(12.157)
But H = V Λ(H)V † and Λ(K) = WKW † for some unitary matrices V and W, giving
H = 
j pjUjKU †
j where Uj ≡V PjW is a unitary matrix, completing the proof in the
forward direction.
Conversely, suppose H = 
j pjUjKU †
j . Similarly to before, this is equivalent to
Λ(H) = 
j pjVjΛ(K)V †
j for some unitary matrices Vj. Writing the matrix components
of Vj as Vj,kl, we have:
λ(H)k =

jl
pjVj,klλ(K)lV †
j,lk =

jl
pj|Vj,kl|2λ(K)l.
(12.158)
Deﬁne a matrix D with entries Dkl ≡
j pj|Vj,kl|2 so we have λ(H) = Dλ(K). The
entries of D are non-negative by deﬁnition, and the rows and columns of D all sum to
one because the rows and columns of the unitary matrices Vj are unit vectors, so D is
doubly stochastic and thus λ(H) ≺λ(K).
We now have in hand all the facts about majorization needed in the study of LOCC
transformations of bipartite pure state entanglement. The ﬁrst step of the argument is
to reduce the problem from the study of general protocols, which may involve two way
classical communication, to protocols involving only one-way classical communication.
Proposition 12.14: Suppose |ψ⟩can be transformed to |ϕ⟩by LOCC. Then this
transformation can be achieved by a protocol involving just the following steps:
Alice performs a single measurement described by measurement operators Mj,
sends the result j to Bob, who performs a unitary operation Uj on his system.
Proof
Without loss of generality we may suppose the protocol consists of Alice performing a
measurement, sending the result to Bob, who performs a measurement (whose nature may
depend on the information received from Alice), and sends the result back to Alice, who
performs a measurement... and so on. The idea of the proof is simply to show that the
576
Quantum information theory
effect of any measurement Bob can do may be simulated by Alice (with one small caveat)
so all Bob’s actions can actually be replaced by actions by Alice! To see that this is the
case, imagine Bob performs a measurement with measurement operators Mj on a pure
state |ψ⟩. Suppose this pure state has Schmidt decomposition |ψ⟩= 
l
√λl|lA⟩|lB⟩and
deﬁne operators Nj on Alice’s system to have a matrix representation with respect to
Alice’s Schmidt basis which is the same as the matrix representation of Bob’s operators
Mj with respect to his Schmidt basis. That is, if Mj = 
kl Mj,kl|kB⟩⟨lB| then we deﬁne
Nj ≡

kl
Mj,kl|kA⟩⟨lA| .
(12.159)
Suppose Bob performs the measurement deﬁned by the measurement operators Mj. Then
the post-measurement state is |ψj⟩∝Mj|ψ⟩= 
kl Mj,kl
√λl|lA⟩|kB⟩, with probability

kl λl|Mj,kl|2. On the other hand, if Alice had measured Nj then the post-measurement
state is |ϕj⟩∝Nj|ψ⟩= 
kl Mj,kl
√λl|kA⟩|lB⟩, also with probability 
kl λl|Mj,kl|2.
Note furthermore that |ψj⟩and |ϕj⟩are the same states up to interchange of Alice and
Bob’s systems via the map |kA⟩↔|kB⟩, and therefore must have the same Schmidt
components. It follows from Exercise 2.80 on page 111 that there exists a unitary Uj
on Alice’s system and Vj on Bob’s system such that |ψj⟩= (Uj ⊗Vj)|ϕj⟩. Therefore,
Bob performing a measurement described by measurement operators Mj is equivalent to
Alice performing the measurement described by measurement operators UjNj followed
by Bob performing the unitary transformation Vj. Summarizing, a measurement by Bob
on a known pure state can be simulated by a measurement by Alice, up to a unitary
transformation by Bob.
Imagine then that Alice and Bob engage in a multi-round protocol transforming |ψ⟩
to |ϕ⟩. Without loss of generality, we may suppose that the ﬁrst round of the protocol
consists of Alice performing a measurement and sending the result to Bob. The second
round consists of Bob performing a measurement (perhaps with the type of measurement
determined by the result of the ﬁrst) and sending the result to Alice. Instead, however, we
can suppose that this measurement is simulated by a measurement performed by Alice,
up to a unitary transformation by Bob. Indeed, we can replace all the measurements by
Bob and communication from Bob to Alice by measurements by Alice, with a unitary to
be done by Bob conditional on Alice’s measurement result. Finally, all the measurements
performed by Alice can be combined into one single measurement (Exercise 2.57 on
page 86), whose result determines a unitary transformation to be performed by Bob;
the net effect of this protocol is exactly the same as the original protocol with two-way
communication.
Theorem 12.15: A bipartite pure state |ψ⟩may be transformed to another pure state
|ϕ⟩by LOCC if and only if λψ ≺λϕ.
Proof
Suppose |ψ⟩may be transformed to |ϕ⟩by LOCC. By Proposition 12.14 we may assume
that the transformation is effected by Alice performing a measurement with measurement
operators Mj, then sending the result to Bob, who performs a unitary transformation
Uj. From Alice’s point of view she starts with the state ρψ and ends with the state ρϕ
regardless of the measurement outcome, so we must have
MjρψM †
j = pjρϕ,
(12.160)
Entanglement as a physical resource
577
where pj is the probability of outcome j. Polar decomposing Mj√ρψ implies that there
exists a unitary Vj such that
Mj√ρψ =

MjρψM †
j Vj = √pjρϕVj.
(12.161)
Premultiplying this equation by its adjoint gives
√ρψM †
j Mj√ρψ = pjV †
j ρϕVj.
(12.162)
Summing on j and using the completeness relation 
j M †
j Mj = I gives
ρψ =

j
pjV †
j ρϕVj,
(12.163)
whence λψ ≺λϕ by Theorem 12.13.
The proof of the converse is essentially to run the proof in the forwards direction
backwards. Suppose λψ ≺λϕ, so ρψ ≺ρϕ and by Theorem 12.13 there exist probabilities
pj and unitary operators Uj such that ρψ = 
j pjUjρϕU †
j . Assume for now that ρψ is
invertible (this assumption is easily removed; see Exercise 12.20) and deﬁne operators
Mj for Alice’s system by
Mj√ρψ ≡√pjρϕU †
j .
(12.164)
To see that these operators deﬁne a measurement we need to check the completeness
relation. We have Mj = √pjρϕU †
j ρ−1/2
ψ
and thus

j
M †
j Mj = ρ−1/2
ψ
⎛
⎝
j
pjUjρϕU †
j
⎞
⎠ρ−1/2
ψ
= ρ−1/2
ψ
ρψρ−1/2
ψ
= I,
(12.165)
which is the completeness relation. Suppose Alice performs the measurement described
by the operators Mj, obtaining the outcome j and the corresponding state |ψj⟩∝Mj|ψ⟩.
Let ρj denote Alice’s reduced density matrix corresponding to the state |ψj⟩, so
ρj ∝MjρψM †
j = pjρϕ,
(12.166)
by substitution of (12.164), and thus ρj = ρϕ. It follows by Exercise 2.81 that Bob may
convert |ψj⟩into |ϕ⟩by application of a suitable unitary transformation Vj.
Exercise 12.20:
Show that the assumption that ρψ is invertible may be removed from
the proof of the converse part of Theorem 12.15.
Exercise 12.21: (Entanglement catalysis)
Suppose Alice and Bob share a pair of
four level systems in the state |ψ⟩=
√
0.4|00⟩+
√
0.4|11⟩+
√
0.1|22⟩+
√
0.1|33⟩.
Show that it is not possible for them to convert this state by LOCC to the state
|ϕ⟩=
√
0.5|00⟩+
√
0.25|11⟩+
√
0.25|22⟩. Imagine, however, that a friendly bank
is willing to offer them the loan of a catalyst, an entangled pair of qubits in the
state |c⟩=
√
0.6|00⟩+
√
0.4|11⟩. Show that it is possible for Alice and Bob to
convert the state |ψ⟩|c⟩to |ϕ⟩|c⟩by local operations and classical communication,
returning the catalyst |c⟩to the bank after the transformation is complete.
Exercise 12.22: (Entanglement conversion without communication)
Suppose
Alice and Bob are trying to convert a pure state |ψ⟩into a pure state |ϕ⟩using
local operations only – no classical communication. Show that this is possible if
578
Quantum information theory
and only if λψ ∼= λϕ ⊗x, where x is some real vector with non-negative entries
summing to 1, and ‘∼=’ means that the vectors on the left and the right have
identical non-zero entries.
12.5.2
Entanglement distillation and dilution
Suppose that instead of being supplied with a single copy of a state |ψ⟩Alice and Bob
are supplied with a large number of copies. What types of entanglement transformation
can they accomplish with all these copies? We are going to focus on two particular types
of entanglement transformation, known as entanglement distillation and entanglement
dilution. The idea of entanglement distillation is for Alice and Bob to convert some
large number of copies of a known pure state |ψ⟩into as many copies of the Bell state
(|00⟩+|11⟩)/
√
2 as possible using local operations and classical communication, requiring
not that they succeed exactly, but only with high ﬁdelity. Entanglement dilution is the
reverse process of using LOCC to convert a large number of copies of the Bell state
(|00⟩+ |11⟩)/
√
2 into copies of |ψ⟩, again with high ﬁdelity in the limit where a large
number of copies of the Bell state are initially available.
What motivates the study of entanglement distillation and dilution? Suppose we take
seriously the idea that entanglement is a physical resource and that as such it should be
possible to quantify entanglement, much as we quantify other physical resources such
as energy or entropy. Suppose we decide to pick the Bell state (|00⟩+ |11⟩)/
√
2 as our
standard unit of entanglement – the basic measure, rather like the standard kilogram
or the standard meter. We can associate a measure of entanglement to a quantum state
|ψ⟩in a similar way to the way we associate a mass to an object. Suppose, for instance,
that it takes 15 chocolate biscuits of a particular brand to attain a mass equivalent to the
standard kilogram; we say that the chocolate biscuits have a mass of 1/15th of a kilogram.
Now, strictly speaking, if the chocolate biscuits had a mass of 1/14.8 kilograms we’d be
in a bit of trouble, because no integer number of chocolate biscuits is going to balance
the standard kilogram, and it’s not so obvious how to deﬁne a non-integer number of
chocolate biscuits. Fortunately, what we do is notice that 148 chocolate biscuits exactly
balances 10 standard kilograms, so the mass of the chocolate biscuits is 10/148 kilograms.
But what if the real mass is not 1/14.8 kilograms, but is something even more esoteric,
like 1/14.7982 . . . kilograms? Well, we simply go to the limit of balancing a large number
m of chocolate biscuits with another large number n of standard kilograms, and declare
the mass of a chocolate biscuit to be the limiting ratio n/m as both m and n become
very large.
In a similar way, a potential approach to deﬁning the amount of entanglement present
in a pure state |ψ⟩is to imagine that we are given a large number n of Bell states
(|00⟩+ |11⟩)/
√
2, and are asked to produce as many (high-ﬁdelity) copies of |ψ⟩as
possible using local operations and classical communication. If the number of copies
of |ψ⟩that can be produced is m then we deﬁne the limiting ratio of n/m to be the
entanglement of formation of the state |ψ⟩. Alternately, we might imagine performing
the process in reverse, going from m copies of |ψ⟩to n copies of (|00⟩+ |11⟩)/
√
2 using
LOCC, and deﬁning the limiting ratio n/m to be the distillable entanglement of the
state |ψ⟩. It is by no means obvious that these two deﬁnitions give the same number for
the quantity of entanglement; we will see that for pure states |ψ⟩the entanglement of
formation and the distillable entanglement are in fact exactly the same!
Let’s take a look at a simple protocol for entanglement dilution, and another for
Entanglement as a physical resource
579
entanglement distillation. Suppose an entangled state |ψ⟩has Schmidt decomposition
|ψ⟩=

x

p(x)|xA⟩|xB⟩.
(12.167)
We write the squared Schmidt co-efﬁcients p(x) in the form we usually reserve for prob-
ability distributions, both because the co-efﬁcients satisfy the usual rules of probability
distributions (non-negative and sum to one), and because ideas from probability theory
turn out to be useful in understanding entanglement distillation and dilution. The m-fold
tensor product |ψ⟩⊗m may be written
|ψ⟩⊗m =

x1,x2,...,xm

p(x1)p(x2) . . . p(xm)|x1Ax2A . . . xmA⟩|x1Bx2B . . . xmB⟩. (12.168)
Suppose that we deﬁned a new quantum state |ϕm⟩by omitting all those terms x1, . . . , xm
which are not ϵ-typical, in the sense deﬁned in Section 12.2.1:
|ϕm⟩≡

x ϵ−typical

p(x1)p(x2) . . . p(xm)|x1Ax2A . . . xmA⟩|x1Bx2B . . . xmB⟩. (12.169)
The state |ϕm⟩is not quite a properly normalized quantum state; to normalize it we deﬁne
|ϕ′
m⟩≡|ϕm⟩/

⟨ϕm|ϕm⟩. By part 1 of the theorem of typical sequences the ﬁdelity
F(|ψ⟩⊗m, |ϕ′
m⟩) →1 as m →∞. Furthermore, by part 2 of the theorem of typical
sequences the number of terms in the sum (12.169) is at most 2m(H(p(x))+ϵ) = 2m(S(ρψ)+ϵ),
where ρψ is the result of tracing out Bob’s part of |ψ⟩.
Suppose then that Alice and Bob are in joint possession of n = m(S(ρψ) + ϵ) Bell
states. Alice prepares ‘both parts’ of |ϕ′
m⟩locally, and then uses the Bell states shared
with Bob to teleport what should be Bob’s half of the state |ϕ′
m⟩over to Bob. In this way,
Alice and Bob can dilute their n Bell states to obtain |ϕ′
m⟩, a pretty good approximation
to |ψ⟩⊗m. This entanglement dilution procedure has n = m(S(ρψ) + ϵ), so the ratio
n/m tends to S(ρψ) + ϵ. We may choose ϵ as small as we like, so we conclude that the
entanglement of formation for the state |ψ⟩is no larger than S(ρψ), since we have just
shown that (asymptotically) S(ρψ) Bell states may be converted into a single copy of |ψ⟩.
An entanglement distillation protocol for converting copies of |ψ⟩into Bell states also
follows along similar lines. Suppose that Alice and Bob are in possession of m copies of
|ψ⟩. By performing a measurement onto the ϵ-typical subspace of ρψ Alice may, with high
ﬁdelity, convert the state |ψ⟩⊗m into the state |ϕ′
m⟩. The largest Schmidt co-efﬁcient
appearing in |ϕm⟩is at most 2−m(S(ρψ)−ϵ) by the deﬁnition of typical sequences. The re-
normalized state |ϕ′
m⟩has Schmidt co-efﬁcients at most a factor 1/√(1 −δ) larger, since
the theorem of typical sequences tells us that 1 −δ is a lower bound on the probability
that a sequence is ϵ-typical, and may be made arbitrarily close to 1 for sufﬁciently large
m. Thus, the largest eigenvalue of the state ρϕ′m is at most 2−m(S(ρψ)−ϵ)/(1−δ). Suppose
we choose any n such that
2−m(S(ρψ)−ϵ)
1 −δ
≤2−n.
(12.170)
Then the vector of eigenvalues for ρϕ′m is majorized by the vector (2−n, 2−n, . . ., 2−n),
and thus by Theorem 12.15 the state |ϕ′
m⟩may be converted to n Bell states by local
operations and classical communication. Examining (12.170) we see that this is possible
provided n ≈mS(ρψ), and thus the entanglement of distillation is at least S(ρψ).
We have exhibited strategies for distilling |ψ⟩into S(ρψ) Bell states and for diluting
580
Quantum information theory
S(ρψ) Bell states into a copy of |ψ⟩. In fact, it is not very difﬁcult to see that the procedures
we have described are really the optimal methods for doing entanglement dilution and
distillation! Suppose, for example, that a more efﬁcient protocol for entanglement dilution
existed, that could dilute |ψ⟩into S > S(ρψ) Bell states. Then starting with S(ρψ) Bell
states Alice and Bob could produce a copy of |ψ⟩using the protocol already described, and
then use the hypothetical protocol to produce S Bell states. Thus, by local operations
and classical communication, Alice and Bob have taken S(ρψ) Bell states and turned
them into S > S(ρψ) Bell states! It is not very difﬁcult to convince yourself (and see
Exercise 12.24) that increasing the number of Bell states present using local operations
and classical communication is not possible, so such a hypothetical dilution protocol can
not exist. In a similar way we can see that the procedure for entanglement distillation
is optimal. Thus, the entanglement of formation and entanglement of distillation for the
state |ψ⟩are the same, and both equal S(ρψ)!
Exercise 12.23:
Prove that the procedure for entanglement distillation we have
described is optimal.
Exercise 12.24:
Recall that the Schmidt number of a bi-partite pure state is the
number of non-zero Schmidt components. Prove that the Schmidt number of a
pure quantum state cannot be increased by local operations and classical
communication. Use this result to argue that the number of Bell states shared
between Alice and Bob cannot be increased by local operations and classical
communication.
We’ve learned how to transform Bell states of a bipartite quantum system into copies
of another entangled state, |ψ⟩, and back again, in an optimal fashion, motivating us to
deﬁne the amount of entanglement present in that quantum state to be the number of
Bell states into which copies of |ψ⟩may be inter-converted, that is, S(ρψ). What do we
learn from this deﬁnition? Below we will see that it is possible to obtain some interesting
insights into quantum error-correction by further generalizing the notion of distillable
entanglement. Nevertheless, at the time of writing it seems fair to say that the study
of entanglement is in its infancy, and it is not yet entirely clear what advances in our
understanding of quantum computation and quantum information can be expected as a
result of the study of quantitative measures of entanglement. We have a reasonable under-
standing of the properties of pure states of bi-partite quantum systems, but a very poor
understanding of systems containing three or more components, or even of mixed states
for bi-partite systems. Developing a better understanding of entanglement and connect-
ing that understanding to topics such as quantum algorithms, quantum error-correction
and quantum communication is a major outstanding task of quantum computation and
quantum information!
12.5.3
Entanglement distillation and quantum error-correction
We deﬁned entanglement distillation for pure states, but there is no reason that deﬁnition
cannot be extended to mixed states. More precisely, suppose ρ is a general state of a
bi-partite quantum system belonging to Alice and Bob. They are supplied with a large
number m of copies of these states, and using local operations and classical communication
attempt to convert these states to the largest possible number n of Bell states, with high
ﬁdelity. The distillable entanglement D(ρ) of ρ is the limiting value of the ratio n/m
Entanglement as a physical resource
581
for the best possible distillation protocol; for pure states |ψ⟩we have already shown that
D(|ψ⟩) = S(ρψ), but we do not yet know how to evaluate D(ρ) for a mixed state.
A considerable number of techniques for doing entanglement distillation have been
developed, giving lower bounds on the value for D(ρ) for speciﬁc classes of states ρ. We
won’t review these techniques here (see the end of chapter ‘History and further reading’).
What we will describe is a fascinating connection between distillable entanglement and
quantum error-correction.
Imagine that Alice is attempting to send Bob quantum information through a noisy
quantum channel E. We suppose the channel is a qubit channel such as the depolarizing
channel, although the same basic ideas adapt easily to non-qubit channels. One method
for sending quantum information through the channel is as follows. Alice prepares a large
number m of Bell states, and sends half of each Bell pair through the channel. Suppose
the result of applying E to half a Bell pair is to create the state ρ, so Alice and Bob end up
sharing m copies of ρ. Alice and Bob now perform entanglement distillation, producing
mD(ρ) Bell pairs. Alice can now prepare an mD(ρ) qubit state and teleport it to Bob
using the mD(ρ) Bell pairs which they share.
Thus, entanglement distillation protocols can be used as a type of error-correction
for quantum communications channels between two parties Alice and Bob, enabling
Alice to reliably send mD(ρ) qubits of information to Bob, where D(ρ) is the distillable
entanglement of ρ, the state that results when one half of a Bell pair is sent through the
noisy channel E connecting Alice and Bob.
What is truly remarkable is that this method of communication using entanglement
distillation may sometimes work even when conventional quantum error-correction tech-
niques fail. For example, for the depolarizing channel with p = 3/4 we saw in Sec-
tion 12.4.3 that no quantum information may be transmitted through the channel. How-
ever, entanglement distillation protocols are known which can produce a non-zero rate
of transmission D(ρ) even for this channel! The reason this is possible is because the en-
tanglement distillation protocols allow classical communication back and forth between
Alice and Bob, whereas conventional quantum error-correction does not allow any such
classical communication.
This example allows us to explain the claim we made way back in Chapter 1, illustrated
in Figure 12.11, that there are channels with zero capacity for quantum information
which, when one such channel is connecting Alice to Bob, and another connects Bob to
Alice, can be used to achieve a net ﬂow of quantum information! The way this is done is
very simple and is based on entanglement distillation. Now, in order that entanglement
distillation be possible, we need Alice and Bob to be able to communicate classically, so
we set aside half the forward uses of the channel, and all the backward uses of the channel
to be used for the transmission of classical information used by the distillation protocol;
these channels have non-zero rate for classical information transmission by the HSW
theorem. The remaining forward uses of the channel are used to transmit halves of Bell
pairs from Alice to Bob, with entanglement distillation being used to extract good Bell
pairs out of the resulting states, and then teleportation with the good Bell pairs to achieve
a net transmission of quantum information, providing yet another vivid demonstration
of the remarkable properties of quantum information!
582
Quantum information theory
Figure 12.11. Classically, if we have two very noisy channels of zero capacity running side by side, then the
combined channel has zero capacity to send information. Not surprisingly, if we reverse the direction of one of the
channels, we still have zero capacity to send information. Quantum mechanically, reversing one of the zero capacity
channels can actually allow us to send information!
12.6
Quantum cryptography
A ﬁtting conclusion to this chapter is provided by a most remarkable application of
quantum information. As we saw in Chapter 5, quantum computers can be used to break
some of the best public key cryptosystems. Fortunately, however, what quantum me-
chanics takes away with one hand, it gives back with the other: a procedure known as
quantum cryptography or quantum key distribution exploits the principles of quantum
mechanics to enable provably secure distribution of private information. In this section
we describe this procedure, and discuss its security. We begin by explaining the basic
ideas of a classical technique, private key cryptography, in Section 12.6.1. Private key
cryptography is a much older form of cryptography than the public key cryptosystems
(mentioned in Chapter 5), and the principles of private key cryptography are used in
quantum cryptosystems. Two other important classical techniques, privacy ampliﬁca-
tion and information reconciliation, which are also used in the quantum systems, are
described in Section 12.6.2. Three different protocols for quantum key distribution are
then presented in Section 12.6.3. How secure are these protocols? It turns out, as we see
in Section 12.6.4, that the coherent information, a measure of quantum information we
ﬁrst encountered in Section 12.4.1, gives an information-theoretic lower bound on the
in-principle ability to use a quantum communication channel to send private information!
This suggests that ideas of quantum information may be useful in proving the security
of speciﬁc quantum key distribution protocols, and indeed, they are: we conclude the
chapter in Section 12.6.5 with a sketch of how the theory of quantum error-correction
provides proof of the security of quantum cryptography.
12.6.1
Private key cryptography
Until the invention of public key cryptography in the 1970s, all cryptosystems operated
on a different principle, now known as private key cryptography. In a private key
cryptosystem, if Alice wishes to send messages to Bob then Alice must have an encoding
key, which allows her to encrypt her message, and Bob must have a matching decoding
key, which allows Bob to decrypt the encrypted message. A simple, yet highly effective
private key cryptosystem is the Vernam cipher, sometimes called a one time pad. Alice
Quantum cryptography
583
and Bob begin with n-bit secret key strings, which are identical. Alice encodes her n-bit
message by adding the message and key together, and Bob decodes by subtracting to
invert the encoding, as illustrated in Figure 12.12.
The great feature of this system is that as long as the key strings are truly secret, it is
provably secure. That is, when the protocol used by Alice and Bob succeeds, it does so
with arbitrarily high probability (an eavesdropper Eve can always jam the communication
channel, but Alice and Bob can detect this jamming and declare failure). And for any
eavesdropping strategy employed by Eve, Alice and Bob can guarantee that Eve’s mutual
information with their unencoded message can be made as small as desired. In contrast,
the security of public key cryptography (Appendix 5) relies on unproven mathematical
assumptions about the difﬁculty of solving certain problems like factoring (with classical
computers!), even though it is widely used and more convenient.
Q
U
A
N
T
U
M
G
Q
Y
R
W
A
D
W
L
Y
F
Q
U
P

W
L
Y
F
Q
U
P
−
−
−
−
−
−
−
G
Q
Y
R
W
A
D
Q
U
A
N
T
U
M
Figure 12.12. The Vernam cipher. Alice encrypts by adding the random key bits (or in this example, letters of the
alphabet) to the original message. Bob decrypts by subtracting the key bits to recover the message.
The major difﬁculty of private key cryptosystems is secure distribution of key bits.
In particular, the Vernam cipher is secure only as long as the number of key bits is at
least as large as the size of the message being encoded, and key bits cannot be reused!
Thus, the large amount of key bits needed makes such schemes impractical for general use.
Furthermore, key bits must be delivered in advance, guarded assiduously until used, then
destroyed afterwards; otherwise, in principle, such classical information can be copied
without disturbing the originals, thus compromising the security of the whole protocol.
Despite these drawbacks, private key cryptosystems such as the Vernam cipher continue
to be used because of their provable security, with key material delivered by clandestine
meetings, trusted couriers, or private secured communication links.
Original
message
Encryption
key
Encrypted
message
Received
message
Decryption
key
Decrypted
message
Public
Channel
+
+
+
+
+
+
+
584
Quantum information theory
Exercise 12.25:
Consider a system with n users, any pair of which would like to be
able to communicate privately. Using public key cryptography how many keys
are required? Using private key cryptography how many keys are required?
12.6.2
Privacy ampliﬁcation and information reconciliation
The ﬁrst step in private key cryptography is distribution of the key string. What if Alice
and Bob start out with imperfect keys? Speciﬁcally, suppose that Alice and Bob share
correlated random classical bit strings X and Y , and they also have an upper bound on
Eve’s mutual information with X and Y . From these imperfect keys, how can they obtain
a good enough key to conduct a secure cryptographic protocol? We now show that by
performing two steps, information reconciliation, followed by privacy ampliﬁcation,
they can systematically increase the correlation between their key strings, while reducing
eavesdropper Eve’s mutual information about the result, to any desired level of security.
These classical steps will be used in the next section, in the quantum key distribution
protocol.
Information reconciliation is nothing more than error-correction conducted over a
public channel, which reconciles errors between X and Y to obtain a shared bit string
W while divulging as little as possible to Eve. After this procedure, suppose Eve has
obtained a random variable Z which is partially correlated with W. Privacy ampliﬁcation
is then used by Alice and Bob to distill from W a smaller set of bits S whose correlation
with Z is below a desired threshold. Since this last step is conceptually new, let us
consider it ﬁrst.
A detailed proof of why privacy ampliﬁcation succeeds is beyond the scope of this
book, but we will describe the basic method and present the main theorem. One way
to accomplish privacy ampliﬁcation uses the class of universal hash functions G, which
map the set of n-bit strings A to the set of m-bit strings B, such that for any distinct
a1, a2 ∈A, when g is chosen uniformly at random from G, then the probability that
g(a1) = g(a2) is at most 1/|B|,
The collision entropy of the random variable X with probability distribution p(x) is
deﬁned as
Hc(X) = −log
0
x
p(x)2
1
.
(12.171)
(This is sometimes known as the R`enyi entropy of order 2.) Using the concavity of the log
function, it is not difﬁcult to show that the Shannon entropy provides an upper bound on
this quantity: H(X) ≥Hc(X). Hc is important in the following theorem about universal
hash functions:
Theorem 12.16: Let X be a random variable on alphabet X with probability
distribution p(x) and collision entropy Hc(X), and let G be the random variable
corresponding to the random choice (with uniform distribution) of a member of
the universal class of hash functions from X to {0, 1}m. Then
H(G(X)|G) ≥Hc(G(X)|G) ≥m −2m−Hc(X) .
(12.172)
Theorem 12.16 can be applied to privacy ampliﬁcation in the following manner. Alice
and Bob publicly select g ∈G and each apply it to W, giving a new bit string S, which
Quantum cryptography
585
they choose as their secret key. If Eve’s uncertainty about W given her knowledge Z = z
(about a speciﬁc instance of the protocol) is known in terms of the collision entropy
to be bounded below by some number, say Hc(W|Z = z) > d, then it follows from
Theorem 12.16 that
Hc(S|G, Z = z) ≥m −2m−d .
(12.173)
In other words, m may be chosen small enough such that Hc(S|G, Z = z) is nearly equal
to m. This maximizes Eve’s uncertainty about the key S, making it a secure secret.
Information reconciliation further reduces the number of bits that Alice and Bob can
obtain, but this can be bounded as follows. By computing a series of parity checks on
subsets of her bits X, Alice can compose a (classical) message u consisting of subset
speciﬁcations and parities, which, when transmitted to Bob, allows him to correct the
errors in his string Y , after which both have the same string W. Clearly, this will require
sending k > H(W|Y ) bits of information in u. However, this procedure gives Eve
additional knowledge U = u, thus increasing her collision entropy to Hc(W|Z = z, U =
u). On average (over possible reconciliation messages u), this increase is bounded below
by Hc(W|Z = z, U = u) ≥Hc(W|Z = z) −H(U), where H(U) is the usual Shannon
entropy of U, but this bound is too weak, because it implies that the probability that the
leaked information U = u decreases Hc by more than mH(U) is only at most 1/m. A
stronger bound is provided by this theorem:
Theorem 12.17: Let X and U be random variables with alphabets X and U,
respectively, where X has probability distribution p(x), and U is jointly
distributed with X according to p(x, u). Also let s > 0 be an arbitrary parameter.
Then, with probability at least 1 −2−s, U takes on a value u for which
Hc(X|U = u) ≥Hc(X) −2 log |U| −2s .
(12.174)
Here, s is known as a security parameter. Applying this to the reconciliation protocol
gives the conclusion that Alice and Bob can choose s such that Eve’s collision entropy is
bounded from below by Hc(W|Z = z, U = u) ≥d −2(k + s), with probability better
than 1 −2−s. Following this step with privacy ampliﬁcation allows them to distill m
secret key bits S, for which Eve’s total information is less than 2m−d+2(k+s) bits.
CSS code privacy ampliﬁcation & information reconciliation
As we noted above, information reconciliation is nothing more than error-correction; it
turns out that privacy ampliﬁcation is also intimately related to error-correction, and both
tasks can be implemented by using classical codes. This viewpoint provides a simple
conceptual picture which will be useful in the proof of the security of quantum key
distribution, in Section 12.6.5, since we have a well-developed theory of quantum error-
correction codes. With this in mind, it is useful to observe the following.
Decoding from a randomly chosen CSS code (see Section 10.4.2) can be thought of
as performing information reconciliation and privacy ampliﬁcation. Although CSS codes
are usually used to encode quantum information, for our present purpose we can just
consider their classical properties. Consider two classical linear codes C1 and C2 which
satisfy the conditions for a t error-correcting [n, m] CSS code: C2 ⊂C1 and C1 and C⊥
2
586
Quantum information theory
both correct t errors. Alice chooses a random n bit string X and transmits it to Bob, who
receives Y .
Let us assume that a priori it is known that along a communication channel between
Alice and Bob, the expected number of errors per code block caused by all noise sources
including eavesdropping is less than t; in practice, this can be established by random
testing of the channel. Furthermore, suppose that Eve knows nothing about the codes
C1 and C2; this can be ensured by Alice choosing the code randomly. Finally, suppose
that Alice and Bob have an upper bound on the mutual information between Eve’s data
Z, and their own data, X and Y .
Bob receives Y = X + ϵ, where ϵ is some error. Since it is known that less than t
errors occurred, if Alice and Bob both correct their states to the nearest codeword in
C1, their results X′, Y ′ ∈C1 are identical, W = X′ = Y ′. This step is nothing more
than information reconciliation. Of course, Eve’s mutual information with W may still
be unacceptably large. To reduce this, Alice and Bob identify which of the 2m cosets of
C2 in C1 their state W belongs to; that is, that they compute the coset of W + C2 in
C1. The result is their m bit key string, S. By virtue of Eve’s lack of knowledge about
C2, and the error-correcting properties of C2, this procedure can reduce Eve’s mutual
information with S to an acceptable level, performing privacy ampliﬁcation.
12.6.3
Quantum key distribution
Quantum key distribution (QKD) is a protocol which is provably secure, by which
private key bits can be created between two parties over a public channel. The key bits
can then be used to implement a classical private key cryptosystem, to enable the parties
to communicate securely. The only requirement for the QKD protocol is that qubits
can be communicated over the public channel with an error rate lower than a certain
threshold. The security of the resulting key is guaranteed by the properties of quantum
information, and thus is conditioned only on fundamental laws of physics being correct!
The basic idea behind QKD is the following fundamental observation: Eve cannot
gain any information from the qubits transmitted from Alice to Bob without disturbing
their state. First of all, by the no-cloning theorem (Box 12.1), Eve cannot clone Alice’s
qubit. Second, we have the following proposition:
Proposition 12.18: (Information gain implies disturbance) In any attempt to
distinguish between two non-orthogonal quantum states, information gain is only
possible at the expense of introducing disturbance to the signal.
Proof
Let |ψ⟩and |ϕ⟩be the non-orthogonal quantum states Eve is trying to obtain information
about. By the results of Section 8.2, we may assume without loss of generality that the
process she uses to obtain information is to unitarily interact the state (|ψ⟩or |ϕ⟩) with
an ancilla prepared in a standard state |u⟩. Assuming that this process does not disturb
the states, in the two cases one obtains
|ψ⟩|u⟩→|ψ⟩|v⟩
(12.175)
|ϕ⟩|u⟩→|ϕ⟩|v′⟩.
(12.176)
Eve would like |v⟩and |v′⟩to be different so that she can acquire information about
Quantum cryptography
587
the identity of the state. However, since inner products are preserved under unitary
transformations, it must be that
⟨v|v′⟩⟨ψ|ϕ⟩= ⟨u|u⟩⟨ψ|ϕ⟩
(12.177)
⟨v|v′⟩= ⟨u|u⟩= 1 ,
(12.178)
which implies that |v⟩and |v′⟩must be identical. Thus, distinguishing between |ψ⟩and
|ϕ⟩must inevitably disturb at least one of these states.
We make use of this idea by transmitting non-orthogonal qubit states between Alice
and Bob. By checking for disturbance in their transmitted states, they establish an upper
bound on any noise or eavesdropping occurring in their communication channel. These
‘check’ qubits are interspersed randomly among data qubits (from which key bits are later
extracted), so that the upper bound applies to the data qubits as well. Alice and Bob then
perform information reconciliation and privacy ampliﬁcation to distill a shared secret key
string. The threshold for the maximum tolerable error rate is thus determined by the
efﬁcacy of the best information reconciliation and privacy ampliﬁcation protocols. Three
different QKD protocols which work in this way are presented below.
The BB84 protocol
Alice begins with a and b, two strings each of (4 + δ)n random classical bits. She then
encodes these strings as a block of (4 + δ)n qubits,
|ψ⟩=
(4+δ)n
.
k=1
|ψakbk⟩,
(12.179)
where ak is the kth bit of a (and similarly for b), and each qubit is one of the four states
|ψ00⟩= |0⟩
(12.180)
|ψ10⟩= |1⟩
(12.181)
|ψ01⟩= |+⟩= (|0⟩+ |1⟩)/
√
2
(12.182)
|ψ11⟩= |−⟩= (|0⟩−|1⟩)/
√
2 .
(12.183)
The effect of this procedure is to encode a in the basis X or Z, as determined by b. Note
that the four states are not all mutually orthogonal, and therefore no measurement can
distinguish between (all of) them with certainty. Alice then sends |ψ⟩to Bob, over their
public quantum communication channel.
Bob receives E(|ψ⟩⟨ψ|), where E describes the quantum operation due to the combined
effect of the channel and Eve’s actions. He then publicly announces this fact. At this point,
Alice, Bob, and Eve each have their own states described by separate density matrices.
Note also that at this point, since Alice hasn’t revealed b, Eve has no knowledge of what
basis she should have measured in to eavesdrop on the communication; at best, she can
only guess, and if her guess was wrong, then she would have disturbed the state received
by Bob. Moreover, whereas in reality the noise E may be partially due to the environment
(a poor channel) in addition to Eve’s eavesdropping, it doesn’t help Eve to have complete
control over the channel, so that she is entirely responsible for E.
Of course, Bob also ﬁnds E(|ψ⟩⟨ψ|) uninformative at this point, because he does not
know anything about b. Nevertheless, he goes ahead and measures each qubit in basis X
or Z, as determined by a random (4 + δ)n bit string b′ which he creates on his own. Let
588
Quantum information theory
Bob’s measurement result be a′. After this, Alice publicly announces b, and by discussion
over a public channel, Bob and Alice discard all bits in {a′, a} except those for which
corresponding bits of b′ and b are equal. Their remaining bits satisfy a′ = a, since for
these bits Bob measured in the same basis Alice prepared in. Note that b reveals nothing
about either a, or the bits a′ resulting from Bob’s measurement, but it is important that
Alice not publish b until after Bob announces reception of Alice’s qubits. For simplicity
in the following explanation, let Alice and Bob keep just 2n bits of their result; δ can be
chosen sufﬁciently large so that this can be done with exponentially high probability.
Now Alice and Bob perform some tests to determine how much noise or eavesdropping
happened during their communication. Alice selects n bits (of their 2n bits) at random,
and publicly announces the selection. Bob and Alice then publish and compare the values
of these check bits. If more than t bits disagree, then they abort and re-try the protocol
from the start. t is selected such that if the test passes, then they can apply information
reconciliation and privacy ampliﬁcation algorithms to obtain m acceptably secret shared
key bits from the remaining n bits.
This protocol, known as BB84 after its inventors (see the end of chapter ‘History
and further reading’), is summarized in Figure 12.13, and an experimental realization is
described in Box 12.7. Related versions of this protocol, such as using fewer check bits,
are also known by the same name.
The BB84 QKD protocol
1: Alice chooses (4 + δ)n random data bits.
2: Alice chooses a random (4 + δ)n-bit string b. She encodes each data bit as
{|0⟩, |1⟩} if the corresponding bit of b is 0 or {|+⟩, |−⟩} if b is 1.
3: Alice sends the resulting state to Bob.
4: Bob receives the (4 + δ)n qubits, announces this fact, and measures each
qubit in the X or Z basis at random.
5: Alice announces b.
6: Alice and Bob discard any bits where Bob measured a different basis than
Alice prepared. With high probability, there are at least 2n bits left (if not,
abort the protocol). They keep 2n bits.
7: Alice selects a subset of n bits that will to serve as a check on Eve’s
interference, and tells Bob which bits she selected.
8: Alice and Bob announce and compare the values of the n check bits. If
more than an acceptable number disagree, they abort the protocol.
9: Alice and Bob perform information reconciliation and privacy ampliﬁca-
tion on the remaining n bits to obtain m shared key bits.
Figure 12.13. The four state quantum key distribution protocol known as BB84.
Exercise 12.26:
Let a′
k be Bob’s measurement result of qubit |ψakbk⟩, assuming a
noiseless channel with no eavesdropping. Show that when b′
k ̸= bk, a′
k is random
and completely uncorrelated with ak. But when b′
k = bk, a′
k = ak.
Quantum cryptography
589
Exercise 12.27: (Random sampling tests)
The random test of n of 2n check bits
allows Alice and Bob to place an upper bound on the number of errors in their
untested bits, with high probability. Speciﬁcally, for any δ > 0, the probability
of obtaining less than δn errors on the check bits, and more than (δ + ϵ)n errors
on the remaining n bits is asymptotically less than exp[−O(ϵ2n)], for large n.
We prove this claim here.
(1) Without loss of generality, you may assume that there are μn errors in the
2n bits, where 0 ≤μ ≤2. Now, if there are δn errors on the check bits, and
(δ + ϵ)n errors on the rest, then δ = (μ −ϵ)/2. The two conditional
statements in the claim thus imply the following:
< δn errors on check bits
⇒
< δn errors on check bits (12.184)
> (δ + ϵ)n errors on rest
⇒
> (μ −δ)n errors on rest , (12.185)
and in fact, the top claim on the right implies the bottom one on the right.
Using this, show that the probability p which we would like to bound satisﬁes
p <
 2n
n
−1  μn
δn
  (2 −μ)n
(1 −δ)n

δn .
(12.186)
(2) Show that for large n, you can bound
1
an + 12anH(b/a) ≤
 an
bn

≤2anH(b/a) ,
(12.187)
where H(·) is the binary entropy function, Equation (11.8). Apply this to the
above bound for p.
(3) Apply the bound H(x) < 1 −2(x −1/2)2 to obtain the ﬁnal result,
p < exp[−O(ϵ2n)]. You may replace μ by a constant which expresses the
worst possible case.
(4) Compare the result with the Chernoff bound, Box 3.4. Can you come up
with a different way to derive an upper bound on p?
The B92 protocol
The BB84 protocol can be generalized to use other states and bases, and similar con-
clusions hold. In fact, a particularly simple protocol exists in which only two states are
used. For simplicity, it is sufﬁcient to consider what happens to a single bit at a time;
the description easily generalizes to block tests just as is done in BB84.
Suppose Alice prepares one random classical bit a, and, depending on the result, sends
Bob
|ψ⟩=
⎧
⎪
⎨
⎪
⎩
|0⟩
if a = 0
|0⟩+ |1⟩
√
2
if a = 1.
(12.188)
Depending on a random classical bit a′ which he generates, Bob subsequently measures
the qubit he receives from Alice in either the Z basis |0⟩, |1⟩(if a′ = 0), or in the X basis
|±⟩= (|0⟩± |1⟩)/
√
2 (if a′ = 1). From his measurement, he obtains the result b, which
is 0 or 1, corresponding to the −1 and +1 eigenstates of X and Z. Bob then publicly
announces b (but keeps a′ secret), and Alice and Bob conduct a public discussion keeping
590
Quantum information theory
Box 12.7: Experimental quantum cryptography
Quantum key distribution is particularly interesting and astonishing because it is
easily experimentally realized. Here is a schematic diagram of one system employing
commercial ﬁber-optic components to deliver key bits over a ten kilometer distance,
which has been built at IBM:




     	

   
 
  

   	  
   
  	     
     	 
   
 
  
    	  
   
  	     
 
 
 
         
  
  
  
  

  
 
    
!    
 
"
    
   
#
    
   
  $ %
 
 
  *  

 
    
  + , 
-       
  -

.  
  
    	
  

 
   
μ
μ
 	 
    μ 
 	 
    μ 
Bob initially generates strong coherent states |α⟩using a diode laser emitting light
at a wavelength of 1.3 μm, and transmits them to Alice, who attenuates them to
(approximately) generate a single photon. She also polarizes the photon in one of the
four states of the BB84 protocol, using as |0⟩and |1⟩states horizontal and vertical
polarization. She then returns the photon to Bob, who measures it using a polar-
ization analyzer, in a random basis. By using this special conﬁguration in which the
photon traverses the same path twice, the apparatus can be made to autocompensate
for imperfections (such as slowly ﬂuctuating path lengths and polarization shifts)
along the ﬁber link. Alice and Bob then select the subset of results in which they
used the same basis, reconcile their information, and perform privacy ampliﬁcation,
communicating over a public channel with photons (over the same ﬁber) of 1.55
μm wavelength. Key bits can be exchanged at the rate of a few hundred per second.
Ultimately, improvements in the light source and detector should allow the rate to
be improved by a few orders of magnitude. Quantum key distribution over distances
exceeding 40 kilometers, and also in installed telecommunication ﬁber (under Lake
Geneva) has been demonstrated.
only those pairs {a, a′} for which b = 1. Note that when a = a′, then b = 0 always. Only
if a′ = 1 −a will Bob obtain b = 1, and that occurs with probability 1/2. The ﬁnal key
is a for Alice, and 1 −a′ for Bob.
This protocol, known as B92 (see the end of chapter ‘History and further reading’),
highlights how the impossibility of perfect distinction between non-orthogonal states lies
at the heart of quantum cryptography. As in BB84, because it is impossible for any eaves-
dropper to distinguish between Alice’s states without disrupting the correlation between
the bits Alice and Bob ﬁnally keep, this protocol allows Alice and Bob to created shared
Quantum cryptography
591
key bits while also placing an upper bound on the noise and eavesdropping during their
communication. They can then apply information reconciliation and privacy ampliﬁcation
to extract secret bits from their resulting correlated random bit strings.
Exercise 12.28:
Show that when b = 1, then a and a′ are perfectly correlated with
each other.
Exercise 12.29:
Give a protocol using six states, the eigenstates of X, Y , and Z, and
argue why it is also secure. Discuss the sensitivity of this protocol to noise and
eavesdropping, in comparison with that of BB84 and B92.
The EPR protocol
The key bits generated in the BB84 and B92 protocols may appear to have been originated
by Alice. However, it turns out that the key can be seen to arise from a fundamentally ran-
dom process involving the properties of entanglement. This is illustrated by the following
protocol.
Suppose Alice and Bob share a set of n entangled pairs of qubits in the state
|00⟩+ |11⟩
√
2
.
(12.189)
These states are known as EPR pairs. Obtaining these states could have come about in
many different ways; for example, Alice could prepare the pairs and then send half of
each to Bob, or vice versa. Alternatively, a third party could prepare the pairs and send
halves to Alice and Bob. Or they could have met a long time ago and shared them, storing
them until the present. Alice and Bob then select a random subset of the EPR pairs, and
test to see if they violate Bell’s inequality (Equation (2.225), on page 115 in Section 2.6),
or some other appropriate test of ﬁdelity. Passing the test certiﬁes that they continue to
hold sufﬁciently pure, entangled quantum states, placing a lower bound on the ﬁdelity of
the remaining EPR pairs (and thus any noise or eavesdropping). And when they measure
these in jointly determined random bases, Alice and Bob obtain correlated classical bit
strings from which they can obtain secret key bits as in the B92 and BB84 protocols.
Using an argument based on Holevo’s bound, the ﬁdelity of their EPR pairs can be used
to establish an upper bound on Eve’s accessible information about the key bits.
Where do the key bits come from in this EPR protocol? Since it is symmetric – Alice
and Bob perform identical tasks on their qubits, even possibly simultaneously – it cannot
be said that either Alice or Bob generates the key. Rather, the key is truly random. In
fact the same applies to the BB84 protocol, since it can be reduced to an instance of a
generalized version of the EPR protocol. Suppose that Alice prepares a random classical
bit b, and according to it, measures her half of the EPR pair in either the |0⟩, |1⟩basis, or
in the basis |±⟩= (|0⟩± |1⟩)/
√
2, obtaining a. Let Bob do identically, measuring in (his
randomly chosen) basis b′ and obtaining a′. Now they communicate b and b′ over a public
classical channel, and keep as their key only those {a, a′} for which b = b′. Note that
this key is undetermined until Alice or Bob performs a measurement on their EPR pair
half. Similar observations can be made about the B92 protocol. For this reason, quantum
cryptography is sometimes thought of not as secret key exchange or transfer, but rather
as secret key generation, since fundamentally neither Alice nor Bob can pre-determine
the key they will ultimately end up with upon completion of the protocol.
592
Quantum information theory
12.6.4
Privacy and coherent information
Thus far, we have described the basic protocol for QKD and argued that it is secure,
but we have not provided quantitative bounds. How secure is it? It turns out there is
an interesting and fundamental connection between the basic quantitative measures of
quantum information discussed in this chapter, and the in-principle obtainable security
of quantum cryptography, which we describe below.
The quantum coherent information I(ρ, E) gives a lower bound on the ability of a
quantum channel to send private information. In the most general circumstance, Alice
prepares states ρA
k , where k = 0, 1, . . . denotes the different possible states she may send,
each with some probability pk. Bob receives states ρB
k = E(ρA
k ) which may be different
from ρA
k because of channel noise or an eavesdropper, Eve. The mutual information
between the result of any measurement Bob may do, and Alice’s value k, Hbob:alice, is
bounded above by Holevo’s bound, (12.6),
Hbob:alice ≤χB = S(ρB) −

k
pkS(ρB
k ) ,
(12.190)
where ρB = 
k pkρB
k . Similarly, Eve’s mutual information is bounded above,
Heve:alice ≤χE = S(ρE) −

k
pkS(ρE
k ) .
(12.191)
Since any excess information Bob has relative to Eve (at least above a certain threshold) can
in principle be exploited by Bob and Alice to distill a shared secret key using techniques
such as privacy ampliﬁcation, it makes sense to deﬁne the quantity
P = sup

Hbob:alice −Heve:alice
	
,
(12.192)
as the guaranteed privacy of the channel, where the supremum is taken over all strategies
Alice and Bob may employ to use the channel. This is the maximum excess classical
information that Bob may obtain relative to Eve about Alice’s quantum signal. By the
HSW theorem, Alice and Bob can employ a strategy such that Hbob:alice = χB, while for
any strategy Eve may employ, Heve:alice ≤χE. Thus, P ≥χB −χE, for a suitable choice
of strategy.
From Exercise 12.11, it follows that a lower bound on the privacy P can be obtained by
assuming that all of Alice’s signal states ρA
k = |ψA
k ⟩⟨ψA
k | are pure states, which are initially
unentangled with Eve, who starts out in some state |0E⟩(that may also be assumed to be
pure, without loss of generality). In general, the channel from Alice to Bob will include
interactions with some environment other than Eve, but to give Eve the greatest possible
advantage, all such interactions may be attributed to her, such that the ﬁnal joint state
received by Eve and Bob after transmission is
|ψEB⟩= U|ψA
k ⟩|0E⟩.
(12.193)
Since this is a pure state, the reduced density matrices ρE
k and ρB
k will have the same
non-zero eigenvalues, and thus the same entropies, S(ρE
k ) = S(ρB
k ). Thus,
P ≥χB −χE
(12.194)
= S(ρB) −

k
pkS(ρB
k ) −S(ρE) +

k
pkS(ρE
k )
(12.195)
= S(ρB) −S(ρE)
(12.196)
Quantum cryptography
593
= I(ρ, E) .
(12.197)
That is, a lower bound on the guaranteed privacy of the channel E is given by the
quantum coherent information I(ρ, E), as deﬁned in Equation (12.118). Note that this
result is not speciﬁc to any protocol (which may have its own security ﬂaws). Also,
the protocol must perform tests, which are not considered in this calculation, to actually
determine properties of the channel E, after which this bound can be applied. So although
the information-theoretic bound we have arrived at here is quite elegant, we still have a
way to go before being able to quantify the security of QKD!
12.6.5
The security of quantum key distribution
How secure is quantum key distribution? Because of the inevitability of disturbance
of the communicated state, upon information gain by an eavesdropper, we have good
reason to believe in the security of QKD. What we need, however, to conclude that the
protocol truly is secure, is a quantiﬁable deﬁnition of security which explicitly bounds
Eve’s knowledge about the ﬁnal key, given some measure of Alice and Bob’s effort. The
following criterion is acceptable:
A QKD protocol is deﬁned as being secure if, for any security parameters s > 0
and ℓ> 0 chosen by Alice and Bob, and for any eavesdropping strategy, either the
scheme aborts, or it succeeds with probability at least 1 −O(2−s), and guarantees
that Eve’s mutual information with the ﬁnal key is less than 2−ℓ. The key string
must also be essentially random.
In this ﬁnal section, we give the main elements of a proof that BB84 is secure. This
proof serves as a ﬁtting conclusion to the chapter, because of its elegant use of many con-
cepts of quantum information in providing an argument that is sufﬁciently simple and
transparent so as to be incontestable. The origin of this proof, intriguingly, comes from
the observation that after information reconciliation and privacy ampliﬁcation are per-
formed, the ultimately obtainable key rate turns out to coincide with the achievable qubit
transmission rate for CSS codes (Section 10.4.2) over noisy communication channels!
Here is the main idea, in outline. It is relatively straightforward to exhaustively establish
that the BB84, B92, and EPR protocols are secure if Eve can only attack the transmission
one qubit at a time. The difﬁculty lies in dealing with the possibility of collective attacks,
in which Eve manipulates and possibly stores large blocks of the transmitted qubits. To
address this, we need a more general and powerful argument. Suppose we somehow know
that Eve never introduces more than t qubit errors per block. Then Alice could encode
her qubits in a t error-correcting quantum code, such that all of Eve’s meddling could be
removed by Bob decoding the code. Two things must be established to make this feasible:
ﬁrst, how can an upper bound be placed on t? This turns out to be possible by sampling
the channel in the appropriate manner, leaving us with a protocol which is secure, even
against collective attacks! Unfortunately, this protocol generally requires a fault-tolerant
quantum computer in order to encode and decode qubits robustly. The second challenge
is thus to choose a quantum code such that the full sequence of encoding, decoding,
and measurement can be performed using no quantum computation or storage – just
single qubit preparation and measurement. Using CSS codes does the trick (after some
simpliﬁcation), and in fact gives just the BB84 protocol. Below, we begin with a manifestly
secure EPR pair based QKD protocol, and then apply solutions to the two challenges to
systematically simplify the initial protocol into BB84.
594
Quantum information theory
Requirements for a secure QKD protocol
Suppose that Alice has n pairs of entangled qubits, each in the state
|β00⟩= |00⟩+ |11⟩
√
2
.
(12.198)
Denote this state as |β00⟩⊗n. Alice then transmits half of each pair to Bob; because of
noise and eavesdropping on the channel, the resulting state may be impure, and can be
described as the density matrix ρ. Alice and Bob then perform local measurements to
obtain a key, as described previously. The following Lemma can be used to show that the
ﬁdelity of ρ with respect to |β00⟩⊗n places an upper bound on the mutual information
Eve has with the key.
Lemma 12.19: (High ﬁdelity implies low entropy) If F(ρ, |β00⟩⊗n)2 > 1 −2−s,
then S(ρ) < (2n + s + 1/ ln 2)2−s + O(2−2s).
Proof
If F(ρ, |β00⟩⊗n)2 = ⊗n⟨β00|ρ|β00⟩⊗n > 1 −2−s, then the largest eigenvalue of ρ must
be larger than 1 −2−s. Therefore, the entropy of ρ is bounded above by the entropy of
a diagonal density matrix ρmax with diagonal entries 1 −2−s, 2−s/(22n −1), 2−s/(22n −
1), . . . , 2−s/(22n−1), that is, ρmax has a large entry 1−2−s, and the remaining probability
is distributed equally among the remaining 22n −1 entries. Since
S(ρmax) = −(1 −2−s) log(1 −2−s) −2−s log
2−s
22n −1 ,
(12.199)
the desired result follows.
By Holevo’s bound (12.6), S(ρ) is an upper bound on the information accessible to
Eve, resulting from Alice and Bob’s measurements of ρ. This implies that if a QKD
protocol can provide Alice and Bob with EPR pairs of ﬁdelity at least 1 −2−s (with high
probability), then it is secure.
Exercise 12.30:
Simplify (12.199) to obtain the expression for S(ρ) given in the
statement of the Lemma.
Exercise 12.31:
It may be unclear why S(ρ) bounds Eve’s mutual information with
Alice and Bob’s measurement results. Show that this follows from assuming the
worst about Eve, giving her all the control over the channel.
Random sampling can upper-bound eavesdropping
How can a protocol place a lower bound on the ﬁdelity of Alice and Bob’s EPR pairs?
The key idea is a classical argument, based on random sampling, which we encountered
in the description of the BB84 protocol (Exercise 12.27). Arguments based on classical
probability, however, need not apply when considering outcomes of quantum measure-
ments. This is vividly demonstrated by Bell’s inequality (Section 2.6). On the other
hand, quantum experiments do allow a classical interpretation whenever measurement
observables that refer to only one basis are considered. And fortunately, it happens that
measurements in only one basis are required for Alice and Bob to bound the ﬁdelity of
their EPR pairs.
Quantum cryptography
595
According to (10.14), a qubit transmitted through a noisy quantum channel can be
described as having had one of four things happen to it: nothing (I), a bit ﬂip (X), a
phase ﬂip (Z), or a combined bit and phase ﬂip (Y ). Recall that the Bell basis is deﬁned
by the four states
|β00⟩= |00⟩+ |11⟩
√
2
, |β10⟩= |00⟩−|11⟩
√
2
, |β01⟩= |01⟩+ |10⟩
√
2
, |β11⟩= |01⟩−|10⟩
√
2
.
(12.200)
Let the second qubit in each pair be the one that Alice sends to Bob. If a bit ﬂip error
occurs to this qubit, then |β00⟩is transformed into |β01⟩. Similarly, a phase ﬂip gives
|β10⟩, and a combination of the two errors gives |β11⟩(up to irrelevant overall phases).
A natural measurement which detects if a bit ﬂip has occurred is given by projectors
Πbf = |β01⟩⟨β01| + |β11⟩⟨β11| and I −Πbf, and likewise, the measurement described by
projectors Πpf = |β10⟩⟨β10| + |β11⟩⟨β11| and I −Πpf detects phase ﬂips. Since both these
measurements commute with the Bell basis, their outcomes obey classical probability
arguments. In fact, any measurement which commutes with the Bell basis will also satisfy
the same classical arguments.
More precisely, Alice and Bob and can bound the ﬁdelity of their EPR pairs by
randomly sampling a subset of them. Suppose Alice sends 2n EPR pair halves to Bob.
They subsequently randomly select n of them and check these qubits by jointly measuring
either Πbf or Πpf (again, chosen randomly). By the same classical arguments used in the
random sampling tests in BB84 (Exercise 12.27), if δn bit or phase ﬂip errors are detected,
then the remaining n EPR pairs would be exponentially certain to have the same number
of errors, were they also to be measured in the Bell basis.
Bell states are nonlocal, and generally measurements in the Bell basis require nonlocal
operations, which can be difﬁcult. Fortunately, however, they are not required in the
present scheme, because Πbf = (I ⊗I −Z ⊗Z)/2, and Πpf = (I ⊗I −X ⊗X)/2.
Thus, Alice and Bob can perform the desired checks with local measurements of Pauli
operators, either by both measuring Z, or both measuring X.
Exercise 12.32:
Note that the local measurements that Alice and Bob perform, such
as I ⊗X and X ⊗I, do not commute with the Bell basis. Show that despite
this, the statistics which Alice and Bob compile from their measurements are the
same as those which they would have obtained had they actually measured Πbf
and Πpf.
The modiﬁed Lo–Chau protocol
Random sampling in the Bell basis thus provides Alice and Bob with EPR pairs ρ with
known ﬁdelity to the ideal state |β00⟩⊗n, and as previously discussed, this bounds Eve’s
mutual information with any measurements that might be performed on ρ. For ρ to be
useful for key generation, however, Alice and Bob must reduce Eve’s mutual information
with their state until it is exponentially small. This task can be achieved by applying
classical privacy ampliﬁcation to their measurement results. Equivalently, Alice and Bob
can ﬁrst perform entanglement distillation, as introduced in Section 12.5.2, to obtain ρ′
which is very close to |β00⟩⊗m for some m < n, then measure the ﬁnal state. This sort
of ‘quantum privacy ampliﬁcation’ will be useful to us.
A rough argument is as follows. Entanglement distillation can be accomplished by per-
596
Quantum information theory
forming quantum error-correction. Since ρ is nearly certain to have δn errors, encoding
these qubits in a δn correcting quantum error-correction code allows up to δn errors to be
perfectly corrected. As we saw in Sections 10.5.5 and 10.5.8, if an [n, m] stabilizer code
is used, then encoding, syndrome measurement and error recovery can be performed by
measurements of Pauli operators determined by the rows of the check matrix for the code.
Alice and Bob simply perform identical measurements and recovery operations on their
respective n qubit halves of ρ, producing an error-corrected state which has a ﬁdelity
relative to |β00⟩⊗m that is on the order of one minus the the probability of more than
δn errors occurring. By construction, the syndrome measurements turn out to commute
with the Bell basis, since Alice and Bob perform identical tasks.
Putting the random sampling and entanglement distillation pieces together gives us
the modiﬁed Lo–Chau protocol, presented in Figure 12.14. A few notes about this
protocol are in order. The random Hadamard transforms performed in steps 3 and 7
create a symmetry in Eve’s strategy between detecting information encoded in the X
and Z bases (and thus causing X and Z errors). They also enable random selection of
a measurement of Πbf or Πpf on the check qubits. The speciﬁc procedure employed
in step 9 can be justiﬁed for the case of any stabilizer code, as in Exercise 12.34. The
Gilbert–Varshamov bound for CSS codes, Equation (10.74), shows that good quantum
codes exist for large block lengths, so that n can be chosen sufﬁciently large that for a
δn error-correcting [n, m] quantum code, the criteria for security can be satisﬁed.
QKD protocol: modiﬁed Lo–Chau
1: Alice creates 2n EPR pairs in the state |β00⟩⊗2n.
2: Alice randomly selects n of the 2n EPR pairs to serve as checks to check
for Eve’s interference. She does not do anything with them yet.
3: Alice selects a random 2n-bit string b, and performs a Hadamard transform
on the second qubit of each pair for which b is 1.
4: Alice sends the second qubit of each pair to Bob.
5: Bob receives the qubits and publicly announces this fact.
6: Alice announces b and which n qubits are to provide check bits.
7: Bob performs Hadamards on the qubits where b is 1.
8: Alice and Bob each measure their n check qubits in the |0⟩, |1⟩basis, and
publicly share the results. If more than t of these disagree, they abort the
protocol.
9: Alice and Bob measure their remaining n qubits according to the check
matrix for a pre-determined [n, m] quantum code correcting up to t errors.
They share the results, compute the syndromes for the errors, and then
correct their state, obtaining m nearly perfect EPR pairs.
10: Alice and Bob measure the m EPR pairs in the |0⟩, |1⟩basis to obtain a
shared secret key.
Figure 12.14. A QKD protocol which is secure, by virtue of use of perfect quantum computers, error-correction,
and random testing of EPR pairs.
Quantum cryptography
597
Exercise 12.33:
Let {M1, M2, . . . , Mn} be a set of measurement observables which
produce respective results Xi when an input state ρ is measured. Argue that the
random variables Xi obey classical probability arguments if [Mi, Mj] = 0, that
is, they commute with each other.
Exercise 12.34: (Entanglement distillation by error-correction)
In
Section 10.5.8, we saw that codewords of an [n, m] qubit stabilizer code can be
constructed by measuring its generators g1, . . . , gn−m on an arbitrary n qubit
quantum state, then applying Pauli operations to change the result to be a
simultaneous +1 eigenstate of the generators. Using that idea, show that if we
start out with n EPR pairs in the state |β00⟩⊗n, and perform identical generator
measurements on the two n qubit halves of the pairs, followed by Pauli
operations to correct for differences in the measurement results between the
pairs, then we obtain an encoded |β00⟩⊗m state. Also show that if the stabilizer
code corrects up to δn errors, then even if δn errors are suffered by an n qubit
half, we still obtain |β00⟩⊗m.
A quantum error-correction protocol
The modiﬁed Lo–Chau protocol makes use of quantum error-correction to perform en-
tanglement distillation, and is built essentially upon the EPR protocol. Entanglement is
a frail resource, and quantum error-correction generally requires robust quantum com-
puters, which are challenging to realize. Fortunately, however, this protocol can be sys-
tematically simpliﬁed in a series of steps, each of which provably does not compromise
the security of the scheme. Let us begin by removing the need to distribute EPR pairs.
Note that the measurements which Alice perform at the end of the modiﬁed Lo–Chau
protocol can be performed at the very start, with no change in any of the states held by
the rest of the world. Alice’s measurements of her halves of the check EPR pairs in step
8 collapse the pairs into n single qubits, so instead of sending entangled states, Alice can
simply send single qubits. This gives us the modiﬁed steps
1′: Alice creates n random check bits, and n EPR pairs in the state |β00⟩⊗n. She also
encodes n qubits as |0⟩or |1⟩according to the check bits.
2′: Alice randomly chooses n positions (out of 2n) and puts the check qubits in these
positions and half of each EPR pair in the remaining positions.
8′: Bob measures the n check qubits in the |0⟩, |1⟩basis, and publicly shares the
results with Alice. If more than t of these disagree, they abort the protocol.
Similarly, Alice’s measurements in steps 9 and 10 collapse EPR pairs into random
qubits encoded in a random quantum code. This can be seen in the following manner. A
particularly convenient choice of code, which we shall employ for the remainder of this
section, is an [n, m] CSS code of C1 over C2, CSS(C1, C2), which encodes m qubits in
n qubits and corrects up to t errors. Recall from Section 10.4.2 that for this code, H1
and H⊥
2 are the parity check matrices corresponding to the classical codes C1 and C⊥
2 ,
in which each of the codeword states is
1

|C2|

w∈C2
|vk + w⟩,
(12.201)
for vk being a representative of one of the 2m cosets of C2 in C1 (the notation vk is
598
Quantum information theory
chosen to suggest a vector v indexed by a key string k). Also recall that there exists a
family of codes equivalent to this one, CSSz,x(C1, C2), with codeword states
|ξvk,z,x⟩=
1

|C2|

w∈C2
(−1)z·w|vk + w + x⟩.
(12.202)
These states form an orthonormal basis for a 2n-dimensional Hilbert space (see Exer-
cise 12.35), and thus we may write Alice’s n EPR pair state as
|β00⟩⊗n =
2n

j=0
|j⟩|j⟩=

vk,z,x
|ξvk,z,x⟩|ξvk,z,x⟩.
(12.203)
Note that in this expression, we have separated the labels into two kets, where the ﬁrst
denotes the qubits Alice keeps, and the second, the qubits which are sent to Bob. When
Alice measures the stabilizer generators corresponding to H1 and H⊥
2 on her qubits in
step 9, she obtains random values for x and z, and similarly, her ﬁnal measurement in
step 10 gives her a random choice of vk. The remaining n qubits are thus left in the
state |ξvk,z,x⟩, which is the codeword for vk in CSSz,x(C1, C2). This is just the encoded
counterpart of a 2m qubit state |k⟩. Therefore, as claimed above, Alice’s measurements
produce random qubits encoded in a random code.
Thus, instead of sending halves of EPR pairs, Alice can equivalently randomly choose
x, z, and k, then encode |k⟩in the code CSSz,x(C1, C2), and send Bob the encoded n
qubits. This gives us the modiﬁed steps
1′′: Alice creates n random check bits, a random m bit key k, and two random n bit
strings x and z. She encodes |k⟩in the code CSSz,x(C1, C2). She also encodes n
qubits as |0⟩or |1⟩according to the check bits.
2′′: Alice randomly chooses n positions (out of 2n) and puts the check qubits in these
positions and encoded qubits in the remaining positions.
6′: Alice announces b, x, z, and which n qubits are to provide check bits.
9′: Bob decodes the remaining n qubits from CSSz,x(C1, C2).
10′: Bob measures his qubits to obtain the shared secret key k.
The resulting scheme, known as the CSS codes protocol, is shown in Figure 12.15.
Exercise 12.35:
Show that the states |ξvk,z,x⟩deﬁned in (12.202) form an orthonormal
basis for a 2n-dimensional Hilbert space, that is,

vk,z,x
|ξvk,z,x⟩⟨ξvk,z,x| = I .
(12.204)
Hint: for C1 an [n, k1] code, C2 an [n, k2] code, and m = k1 −k2, note that
there are 2m distinct values of vk, 2n−k1 distinct x, and 2k2 distinct z.
Exercise 12.36:
Verify Equation (12.203).
Exercise 12.37:
This is an alternative way to understand why Alice’s measurements in
steps 9 and 10 collapse EPR pairs into random qubits encoded in a random
quantum code. Suppose Alice has an EPR pair (|00⟩+ |11⟩)/
√
2. Show that if
she measures the ﬁrst qubit in the X basis, then the second qubit collapses into
an eigenstate of X determined by the measurement result. Similarly, show that if
Quantum cryptography
599
she measures in the Z basis, then the second qubit is left in a Z eigenstate
labeled by the measurement result. Using this observation and the results of
Section 10.5.8, conclude that Alice’s measurements of H1, H⊥
2 , and ¯Z on her
EPR pair halves result in a random codeword of CSSz,x(C1, C2) determined by
her measurement results.
QKD protocol: CSS codes
1′′: Alice creates n random check bits, a random m bit key k, and two random
n bit strings x and z. She encodes |k⟩in the code CSSz,x(C1, C2). She
also encodes n qubits as |0⟩or |1⟩according to the check bits.
2′′: Alice randomly chooses n positions (out of 2n) and puts the check qubits
in these positions and the encoded qubits in the remaining positions.
3: Alice selects a random 2n-bit string b, and performs a Hadamard transform
on each qubit for which b is 1.
4: Alice sends the resulting qubits to Bob.
5: Bob receives the qubits and publicly announces this fact.
6′: Alice announces b, x, z, and which n qubits are to provide check bits.
7: Bob performs Hadamards on the qubits where b is 1.
8′: Bob measures the n check qubits in the |0⟩, |1⟩basis, and publicly shares
the results with Alice. If more than t of these disagree, they abort the
protocol.
9′: Bob decodes the remaining n qubits from CSSz,x(C1, C2).
10′: Bob measures his qubits to obtain the shared secret key k.
Figure 12.15. A QKD protocol which is secure, by virtue of simpliﬁcation of the modiﬁed Lo–Chau protocol via
CSS codes.
Reduction to BB84
The CSS codes QKD protocol is secure by virtue of direct reduction from the modiﬁed
Lo–Chau protocol, and is much simpler because it does not make any evident use of EPR
pairs. Unfortunately, it is still unsatisfactory, because it requires perfect quantum compu-
tation to perform the encoding and decoding (instead of just single qubit preparation and
measurements), and Bob needs to temporarily store qubits in a quantum memory while
waiting for communication from Alice. The use of CSS codes, however, enables these
two requirements to be removed, essentially because they decouple phase ﬂip correction
from bit ﬂip correction.
First, note that Bob measures his qubits in the Z basis immediately after decoding;
thus, the phase correction information Alice sends as z is unnecessary. Thus, since C1 and
C2 are classical codes, instead of decoding then measuring, he can immediately measure
to obtain vk + w + x + ϵ (where ϵ represents some possible error, due to the channel and
to Eve), then decode classically: he subtracts Alice’s announced value of x, then corrects
the result to a codeword in C1, which is deﬁnitely vk + w, if the distance of the code is
600
Quantum information theory
not exceeded. The ﬁnal key k is the coset of vk + w + C2 in C1 (see Appendix 2 for an
explanation of cosets, and this notation). This gives us:
9′′: Bob measures the remaining qubits to get vk + w + x + ϵ, and subtracts x from
the result, correcting it with code C1 to obtain vk + w.
10′′: Bob computes the coset of vk + w + C2 in C1 to obtain the key k.
Second, since Alice need not reveal z, the state she effectively sends is a mixed state,
averaged over random values of z,
ρvk,x = 1
2n

z
|ξvk,z,x⟩⟨ξvk,z,x|
(12.205)
=
1
2n|C2|

z

w1,w2∈C2
(−1)z·(w1+w2)|vk + w1 + x⟩⟨vk + w2 + x|
(12.206)
=
1
|C2|

w∈C2
|vk + w + x⟩⟨vk + w + x| .
(12.207)
This state is simple to create: Alice need only classically choose w ∈C2 at random, and
construct |vk + w + x⟩, using her randomly determined x and k. We thus have:
1′′′: Alice creates n random check bits, a random n bit string x, a random vk ∈C1/C2,
and a random w ∈C2. She encodes n qubits in the state |0⟩or |1⟩according to
vk + w + x, and similarly, n qubits according to the check bits.
Steps 1′′′ and 9′′ can be simpliﬁed further, by changing step 6′ slightly. Currently,
Alice sends |vk + w + x⟩, Bob receives and measures to obtain vk + w + x + ϵ, then
Alice sends x, which Bob subtracts to obtain vk + w + ϵ. But if Alice chooses vk ∈C1
(as opposed to C1/C2), then w is unnecessary. Moreover, vk + x is then a completely
random n bit string, and instead of the above protocol, it is equivalent if Alice chooses x
at random, sends |x⟩, Bob receives and measures to obtain x+ϵ, then Alice sends x−vk,
which Bob subtracts to obtain vk + ϵ. Now, there is no difference between the random
check bits and the code bits! This gives us:
1′′′′: Alice chooses a random vk ∈C1, and creates 2n qubits in the state |0⟩or |1⟩
according to 2n random bits.
2′′′: Alice randomly chooses n positions (out of 2n) and designates these as the check
qubits, and the remainder as |x⟩.
6′′: Alice announces b, x −vk, and which n qubits are to provide check bits.
9′′′: Bob measures the remaining qubits to get x + ϵ, and subtracts x −vk from the
result, correcting it with code C1 to obtain vk.
10′′: Alice and Bob compute the coset of vk + C2 in C1 to obtain the key k.
Next, note that Alice need not perform Hadamard operations (although in practice, single
qubit operations are not so difﬁcult to accomplish with photons). She can instead encode
her qubits directly in either the |0⟩, |1⟩(Z) basis or in the |+⟩, |−⟩(X) basis, depending
on the bits of b:
1′′′′′: Alice creates (4 + δ)n random bits. For each bit, she creates a qubit in either the
|0⟩, |1⟩basis, or the |+⟩, |−⟩basis, according to a random bit string b.
Quantum cryptography
601
We are almost done: encoding and decoding are now performed classically. The re-
maining problem is to remove the need for a quantum memory. To solve this problem,
suppose that Bob goes ahead and measures immediately after receiving qubits from Alice,
choosing randomly to measure in either the X or Z bases. When Alice subsequently an-
nounces b, they can keep only those bits for which their bases happened to be the same.
This allows Bob to do away completely with his quantum storage device. Note that with
high probability they discard half their bits, so in order to obtain the same number of
key bits as before, they should start with a little (say, δ) more than twice the number of
original random bits. Of course, Alice now must delay her choice of which bits are to
be check bits, until after the discarding step. This gives us the ﬁnal protocol, shown in
Figure 12.16. This protocol is exactly the same as BB84, with only minor cosmetic differ-
ences. Note how the use of the classical code C1 performs information reconciliation, and
computing the coset of vk +C2 in C1 performs privacy ampliﬁcation (see Section 12.6.2).
QKD protocol: Secure BB84
1: Alice creates (4 + δ)n random bits.
2: For each bit, she creates a qubit in either the Z basis, or the X basis,
according to a random bit string b.
3: Alice sends the resulting qubits to Bob.
4: Alice chooses a random vk ∈C1.
5: Bob receives the qubits, publicly announces this fact, and measures each
in the Z or X bases at random.
6: Alice announces b.
7: Alice and Bob discard those bits Bob measured in a basis other than b.
With high probability, there are at least 2n bits left; if not, abort the
protocol. Alice decides randomly on a set of 2n bits to continue to use,
randomly selects n of these to be check bits, and announces the selection.
8: Alice and Bob publicly compare their check bits. If more than t of these
disagree, they abort the protocol. Alice is left with the n bit string x, and
Bob with x + ϵ.
9: Alice announces x −vk. Bob subtracts this from his result, correcting it
with code C1 to obtain vk.
10: Alice and Bob compute the coset of vk + C2 in C1 to obtain the key k.
Figure 12.16. The ﬁnal QKD protocol arrived at by systematic reduction of the CSS codes protocol, which is
exactly the same as BB84 (up to minor cosmetic differences). For clarity, we have dropped the ′ notation.
In summary, we have systematically proven the security of the BB84 quantum key dis-
tribution protocol, by starting with a manifestly secure scheme requiring perfect quantum
computation and quantum memories, and systematically reducing it to BB84. By virtue
of having only made modiﬁcations which manifestly leave Eve’s quantum state (condi-
tioned on all revealed classical information) unchanged, we conclude that BB84 is secure.
Naturally, there are some caveats. This proof only applies to an ideal situation, where
the states sent are those described. In practice, qubit sources are imperfect; for example,
602
Quantum information theory
such sources are often lasers attenuated to approximately generate single photons, repre-
senting qubits (as described in Section 7.4.1). Furthermore, the proof does not place any
bounds on the amount of effort Alice and Bob must go to in decoding; for practical key
distribution, C1 must be efﬁciently decodable. This proof also does not provide an upper
bound on the tolerable eavesdropping; it utilizes CSS codes, which are not optimal. It
is estimated that a rate of bit and phase errors up to 11% is acceptable using a protocol
similar to BB84, but with the aid of quantum computers to encode and decode, higher
error rates may be tolerable. The ultimate capability of quantum cryptography is an in-
teresting open issue, and we expect such fundamental questions about the physical limits
of computation and communication to continue to intrigue and challenge researchers in
the future.
Exercise 12.38:
Show that if you had the ability to distinguish non-orthogonal states,
then it would be possible to compromise the security of BB84, and indeed, all of
the QKD protocols we have described.
Problem 12.1:
In this problem we will work through an alternate proof of the Holevo
bound. Deﬁne the Holevo chi quantity,
χ ≡S(ρ) −

x
pxS(ρx).
(12.208)
(1) Suppose the quantum system consists of two parts, A and B. Show that
χA ≤χAB.
(12.209)
(Hint: Introduce an extra system which is correlated with AB, and apply
strong subadditivity.)
(2) Let E be a quantum operation. Use the previous result to show that
χ′ ≡S(E(ρ)) −

x
pxS(E(ρx)) ≤χ ≡S(ρ) −

x
pxS(ρx). (12.210)
That is, the Holevo chi quantity decreases under quantum operations. This
is an important and useful fact in its own right.
(3) Let Ey be a set of POVM elements. Augment the quantum system under
consideration with an ‘apparatus’ system, M, with an orthonormal basis |y⟩.
Deﬁne a quantum operation by
E(ρ ⊗|0⟩⟨0|) ≡

y

Eyρ

Ey ⊗|y⟩⟨y|,
(12.211)
where |0⟩is some standard pure state of M. Prove that after the action of E,
χM = H(X :Y ). Use this and the previous two results to show that
H(X :Y ) ≤S(ρ) −

x
pxS(ρx),
(12.212)
which is the Holevo bound.
Problem 12.2:
This result is an extension of the previous problem. Provide a proof of
the no-cloning theorem by showing that a cloning process for non-orthogonal
pure states would necessarily increase χ.
Chapter problems
603
Problem 12.3:
For a ﬁxed quantum source and rate R > S(ρ), design a quantum
circuit implementing a rate R compression scheme.
Problem 12.4: (Linearity forbids cloning)
Suppose we have a quantum machine
with two slots, A and B. Slot A, the data slot, starts out in an unknown
quantum state ρ. This is the state to be copied. Slot B, the target slot, starts out
in some standard quantum state, σ. We will assume that any candidate copying
procedure is linear in the initial state,
ρ ⊗σ →E(ρ ⊗σ) = ρ ⊗ρ,
(12.213)
where E is some linear function. Show that if ρ1 ̸= ρ2 are density operators such
that
E(ρ1 ⊗σ) = ρ1 ⊗ρ1
(12.214)
E(ρ2 ⊗σ) = ρ2 ⊗ρ2,
(12.215)
then any mixture of ρ1 and ρ2 is not copied correctly by this procedure.
Problem 12.5: (Classical capacity of a quantum channel – Research)
Is the
product state capacity (12.71) the true capacity of a noisy quantum channel for
classical information, that is, the capacity when entangled inputs to the channel
are allowed?
Problem 12.6: (Methods for achieving capacity – Research)
Find an efﬁcient
construction for codes achieving rates near the product state capacity (12.71) of a
noisy quantum channel for classical information.
Problem 12.7: (Quantum channel capacity – Research)
Find a method to
evaluate the capacity of a given quantum channel E for the transmission of
quantum information.
604
Quantum information theory
Summary of Chapter 12: Quantum information theory
• No-cloning: No quantum device can be constructed which outputs |ψ⟩|ψ⟩, given
|ψ⟩, for arbitrary |ψ⟩.
• Holevo’s bound: The maximum accessible classical information when trying to
distinguish between quantum states ρx sent with probability distribution px is
H(X :Y ) ≤χ ≡S

x
pxρx

−

x
pxS(ρx) .
• Schumacher’s quantum noiseless channel coding theorem: S(ρ) can be
interpreted as the number of qubits needed to faithfully represent a quantum
source described by ρ.
• The Holevo–Schumacher–Westmoreland theorem: The capacity of a noisy
quantum channel E for classical information is given by:
C(E) =
max
{px,|ψx⟩} S

x
pxE(|ψx⟩⟨ψx|)

−

x
pxS(E(|ψx⟩⟨ψx|)) . (12.216)
• Majorization condition for entanglement transformation: Alice and Bob
can transform |ψ⟩to |ϕ⟩by local operations and classical communication if and
only if λψ ≺λϕ, where λψ is a vector of the eigenvalues of the reduced density
matrix of |ψ⟩(similarly for λϕ).
• Pure state entanglement distillation and dilution: Alice and Bob can convert
between n copies of a joint state |ψ⟩and nS(ρ) Bell pairs, by local operations and
classical communication alone, as n →∞, where ρ is the reduced density matrix.
• Quantum cryptography: Provably secure key distribution is possible by com-
municating using non-orthogonal quantum states, with a protocol such as BB84.
Eavesdropping on the channel will cause a detectable increase in the error rate,
because information gain implies disturbance.
History and further reading
The book of Cover and Thomas[CT91] is a terriﬁc introduction to classical information
theory. The reader looking for a more advanced yet still readable treatment of information
theory should consult Csisz´ar and K¨orner[CK81]. Also well worth the read are Shannon’s
original papers, among the most inﬂuential in twentieth century science. These have
been reprinted together in a single volume by Shannon and Weaver[SW49]. Bennett and
Shor[BS98] and Bennett and DiVincenzo[BD00] have written excellent review articles on
quantum information theory.
The no-cloning theorem is due to Dieks[Die82] and to Wootters and Zurek[WZ82]. An
enormous amount of work has been done extending these results. By far the majority of
the papers consider various schemes for cloners which are interesting in some particular
way – they optimize some measure of cloning ﬁdelity, or some other property one might
wish a cloner to have. We will not attempt to give a full review of this work here, but
History and further reading
605
note that many of these papers may be found on the internet at http://arXiv.org/
in the quant-ph archive. Some papers of especial interest include the work of Barnum,
Caves, Fuchs, Jozsa and Schumacher[BCF+96] extending the range of application of the
no-cloning theorem to mixed states and non-unitary cloning devices; by Mor[Mor98] on
the cloning of states of composite systems; by Westmoreland and Schumacher[WS98] on a
possible equivalence between cloning and faster-than-light communication; and a rebuttal
by van Enk[van98b].
The Holevo bound was conjectured by Gordon in 1964[Gor64], and proved by Holevo
in 1973[Hol73]. The conceptually simple proof we have given is based upon the difﬁcult-to-
prove strong subadditivity inequality, however Holevo used a more direct approach which
has been simpliﬁed by Fuchs and Caves[FC94]. The approach via strong subadditivity is
due to Yuen and Ozawa[YO93]; see also Schumacher, Westmoreland and Wootters[SWW96].
The classical noiseless channel coding theorem is due to Shannon[Sha48, SW49]. The
quantum noiseless channel coding theorem is due to Schumacher[Sch95], and is described
in a pioneering paper that introduced in an integrated fashion many of the fundamental
notions of quantum information theory, including sources, ﬁdelity measures, and the
notion of quantum states as a resource that could be treated in information-theoretic
terms. This last observation, simple but profound, was driven home by Schumacher’s
introduction in the paper of the now-ubiquitous term qubit, attributed to a conversation
between Schumacher and Wootters. A paper by Jozsa and Schumacher[JS94] simpliﬁed
Schumacher’s original approach; this paper was published earlier than [Sch95], but was
was written at a later time. These papers were based on the ensemble-average ﬁdelity
measure discussed in Exercise 12.8, rather than the entanglement ﬁdelity based proof
we have given here, which is based on the approach of Nielsen[Nie98]. A slight hole in
the original papers by Schumacher, and Schumacher and Jozsa was ﬁlled by the work of
Barnum, Fuchs, Jozsa and Schumacher[BFJS96]. M. Horodecki[Hor97] subsequently pro-
vided a more powerful proof of the same result that also points the way towards a theory
of quantum data compression of ensembles of mixed states. The compression scheme
described in Box 12.4, which is the quantum analogue of Cover’s enumerative coding
method[CT91], is originally attributed to Schumacher[Sch95], and quantum circuits for it
are explicitly given by Cleve and DiVincenzo[CD96]. Braunstein, Fuchs, Gottesman, and
Lo have generalized this to provide a quantum analogue to Huffman encoding[BFGL98],
and Chuang and Modha to arithmetic coding[CM00].
The Holevo–Schumacher–Westmoreland (HSW) theorem has an interesting history.
The problem it addresses was ﬁrst discussed by Holevo[Hol79] in 1979, who made some
partial progress on the problem. Unaware of this work, Hausladen, Jozsa, Schumacher,
Westmoreland, and Wootters[HJS+96] solved a special case of the problem in 1996. Inde-
pendently, and shortly thereafter, Holevo[Hol98] and Schumacher and Westmoreland[SW97]
proved the HSW theorem giving the product state capacity of a noisy quantum channel
for classical information. Fuchs[Fuc97] has described some interesting examples of the
product state capacity, where the ensemble of states maximizing the expression (12.71)
for the capacity contains non-orthogonal members. King and Ruskai[KR99] have made
some promising headway on the problem of showing that the product state capacity is
the same as the capacity unrestricted to product states, but the general problem remains
open.
The entropy exchange was deﬁned by Lindblad[Lin91], and rediscovered by Schu-
macher[Sch96b], who proved the quantum Fano inequality. The coherent information was
606
Quantum information theory
introduced by Lloyd[Llo97] and by Schumacher and Nielsen[SN96] in the context of the
capacity of a noisy quantum channel; [SN96] proves the quantum data processing in-
equality. A table containing the inequalities mentioned in Exercise 12.15 may be found in
Nielsen’s Ph.D. dissertation[Nie98]. The as yet unsolved problem of determining the quan-
tum channel capacity (Problem 12.7) has an interesting history. Initial work on the subject
came from several different perspectives, as may be seen from the papers of Barnum,
Nielsen and Schumacher[BNS98], of Bennett, DiVincenzo, Smolin, and Wootters[BDSW96],
of Lloyd[Llo97], of Schumacher[Sch96b] and of Schumacher and Nielsen[SN96]. The equiv-
alence of several of these points of view has been understood through the work of Bar-
num, Knill and Nielsen[BKN98], and Barnum, Smolin and Terhal[BST98]. The capacity
has been established for some speciﬁc channels (most notably the quantum erasure chan-
nel) by Bennett, DiVincenzo and Smolin[BDS97], and a lower bound on the capacity of
the depolarizing channel making intriguing use of degenerate quantum codes has been
obtained by Shor and Smolin[SS96], and reﬁned by DiVincenzo, Shor and Smolin[DSS98].
Zurek[Zur89], Milburn[Mil96], and Lloyd[Llo96] analyzed examples of quantum Maxwell’s
demons, though not in the context of error-correction. The analysis here is based on the
work of Nielsen, Caves, Schumacher and Barnum[NCSB98]. This point of view has also
been pursued by Vedral[Ved99] to obtain limits on entanglement distillation procedures.
The quantum Singleton bound is due to Knill and Laﬂamme[KL97]. The proof we give
is due to Preskill[Pre98b].
The study of entanglement has blossomed into a major area of research, and there are
far too many papers on the subject to even begin to give an account here. Once again,
see the quant-ph archive at http://arXiv.org/. The conditions for entanglement
transformation based on majorization (Theorem 12.15) are due to Nielsen[Nie99a]. The-
orem 12.13 is due to Uhlmann[Uhl71, Uhl72, Uhl73]. Proposition 12.14 is due to Lo and
Popescu[LP97]. Entanglement catalysis was discovered by Jonathan and Plenio[JP99]. Mar-
shall and Olkin[MO79] is a comprehensive introduction to majorization, including the proof
of Birkhoff’s theorem. The limits for entanglement dilution and distillation are due to
Bennett, Bernstein , Popescu and Schumacher[BBPS96]. Entanglement distillation proto-
cols for mixed states were introduced by Bennett, Brassard, Popescu, Schumacher, Smolin
and Wootters[BBP+96], and the connection to quantum error-correction developed in a
ground-breaking paper by Bennett, DiVincenzo, Smolin and Wootters[BDSW96] that has
stimulated a lot of subsequent research. The example illustrated in Figure 12.11 was noted
by Gottesman and Nielsen (unpublished). We mention just a few more papers on entan-
glement of exceptional interest that may serve as an entry-point to the literature; unfortu-
nately many papers of note are omitted as a result. A series of papers by members of the
Horodecki family (Michal, Pawel and Ryszard) have investigated the properties of entan-
glement in depth; of especial note are [HHH96, HHH98, HHH99a, HHH99b, HHH99c].
Also of great interest are the papers by Vedral and Plenio[VP98] and by Vidal[Vid98].
For an excellent (early) overview of quantum cryptography at the lay level see the article
by Bennett, Brassard, and Ekert in Scientiﬁc American[BBE92]. Quantum cryptographic
ideas were ﬁrst put forward by Wiesner in the late 1960s. Unfortunately, Wiesner’s ideas
were not accepted for publication, and it wasn’t until the early 1980s that the ideas be-
came known. Wiesner proposed that (entangled) quantum states, if they could be stored
for long periods of time, could be used as a kind of counterfeit-proof money[Wie, Wie83].
Bennett developed several further protocols, one of which lead to the ﬁrst experimental
implementation, by Bennett, Bessette, Brassard, Salvail, and Smolin[BBB+92], which is
History and further reading
607
of historical interest (in principle) since it transmitted its information less than a meter
and, moreover, eavesdropping was facilitated by a loud buzzing sound which emanated
from the power supply whenever a ‘one’ was sent! The concept of privacy ampliﬁcation
was ﬁrst introduced by Bennett, Brassard, and Robert[BBR88]. For information recon-
ciliation protocols see [BBB+92] and [BS94]. Theorem 12.16 is stated and proved in
Bennett, Brassard, Cr`epeau, and Maurer[BBCM95], in a very readable general treatment
of privacy ampliﬁcation. Note that the information disclosed during reconciliation has
an important impact on the threshold for privacy ampliﬁcation, as bounded in Theo-
rem 12.17, proven by Cachin and Maurer[CM97]. Privacy ampliﬁcation has applications to
classical key generation using distant correlated random sources such as starlight sensed
by satellites[Mau93]. The four state protocol known as BB84 is named after the authors,
Bennett and Brassard[BB84], and similarly, the two state B92 protocol is named after
Bennett[Ben92]. The EPR protocol was devised by Ekert[Eke91]. The proof of the random
sampling bound in Exercise 12.27 is due to Ambainis. The limitations and security of
quantum cryptography have been discussed in depth in many publications. For a sam-
ple, see the works by Barnett and Phoenix[BP93]; Brassard[Bra93]; Ekert, Huttner, Palma,
and Peres[EHPP94]; also [Phy92]. The connection between coherent information and privacy
was recognized by Schumacher and Westmoreland[SW98]. Numerous papers have been
published on experimental implementations of quantum cryptographic systems. For a
good introduction, see Hughes, Alde, Dyer, Luther, Morgan, and Schauer[HAD+95]; the
demonstration of quantum cryptography under Lake Geneva was by Muller, Zbinden,
and Gisin[MZG96]. The experiment described in Box 12.7 was performed at IBM, by
Bethune and Risk[BR98, BR00], and we thank them for the schematic of their apparatus.
A large number of proofs of the security of various quantum key distribution protocols,
under different circumstances, have been presented. Of particular note is a complete (and
extensive, but somewhat complicated) proof of the security of QKD with BB84 given
by Mayers[May98]. Biham, Boyer, Brassard, van de Graaf, and Mor have also considered
attacks against BB84[BBB+98]. A simpler proof, which uses EPR states and requires per-
fect quantum computation has been given by Lo and Chau[LC99]; this is the protocol we
began with in Section 12.6.5. Lo has simpliﬁed it to begin with a test which ascertains the
error rate, before transmitting key material[Lo99]. The even simpler (and beautiful!) proof
given in Section 12.6.5 is due to Shor and Preskill[SP00], who also give the 11% estimate
mentioned in Section 12.6.5. Our presentation of this proof also beneﬁted greatly from
conversations with Gottesman.
Appendix 1: Notes on basic probability theory
This appendix collects some elementary deﬁnitions and results about probability theory.
It is assumed that the reader already has some familiarity with this material. The reader
who is not familiar with any of the results should take time out to prove them, as indicated
in the exercises.
The basic notion of probability theory is that of a random variable. A random variable
X may take one of a number of values, x, with probabilities p(X = x). We use upper
case to denote the random variable, and x to denote the values that random variable may
take. We often use the notation p(x) instead of p(X = x), leaving the ‘X =’ implicit.
In this book we shall only be concerned with random variables which take their value
from a ﬁnite set of values, and we always assume that this is the case. Occasionally it
is convenient to consider random variables whose values are vectors taking values, for
example, in the set (i, j), i = 1, . . . , m1, j = 1, . . . , m2.
The conditional probability that Y = y given that X = x is deﬁned by
p(Y = y|X = x) ≡p(X = x, Y = y)
p(X = x)
,
(A1.1)
where p(X = x, Y = y) is the probability that X = x and Y = y. When p(X = x) = 0 we
make the convention that p(Y = y|X = x) = 0. We often use the notation p(y|x) leaving
the ‘Y =’ and ‘X =’ implicit. Random variables X and Y are said to be independent if
p(X = x, Y = y) = p(X = x)p(Y = y) for all x and y. Note that when X and Y are
independent, it follows that p(y|x) = p(y) for all x and y.
Bayes’ rule relates the conditional probabilities for Y given X to those for X given
Y ,
p(x|y) = p(y|x)p(x)
p(y).
(A1.2)
The probability p(y) appearing in this expression is often re-expressed using the law of
total probability discussed below.
Exercise A1.1:
Prove Bayes’ rule.
One of the most important and frequently used results in probability theory is the law of
total probability. It states that if X and Y are two random variables, then the probabilities
for Y can be expressed in terms of the probabilities for X, and the conditional probabilities
for Y given X,
p(y) =

x
p(y|x)p(x),
(A1.3)
where the sum is over all values x that X can take.
History and further reading
609
Exercise A1.2:
Prove the law of total probability.
The expectation, average, or mean of a random variable X is deﬁned by
E(X) ≡

x
p(x)x,
(A1.4)
where the sum is over all values x which the random variable X can take.
Exercise A1.3:
Prove that there exists a value of x ≥E(X) such that p(x) > 0.
Exercise A1.4:
Prove that E(X) is linear in X.
Exercise A1.5:
Prove that for independent random variables X and Y ,
E(XY ) = E(X)E(Y ).
The variance of a random variable X is deﬁned by the expression
var(X) ≡E[(X −E(X))2] = E(X2) −E(X)2.
(A1.5)
The standard deviation, Δ(X) ≡√var(X), is a measure of the spread of a random
variable about the average. Chebyshev’s inequality quantiﬁes more precisely in what
sense the standard deviation is a measure of the spread of values a random variable may
take. It states that for any λ > 0 and random variable X with ﬁnite variance,
p(|X −E(X)| ≥λΔ(X)) ≤1
λ2 .
(A1.6)
Thus, the probability of being more than λ standard deviations away from the mean gets
small as λ goes to inﬁnity.
Exercise A1.6:
Prove Chebyshev’s inequality.
The main text of the book contains many other results in probability theory, including
the Chernoff bound on page 154, Fano’s inequality on page 536, and the law of large
numbers on page 541.
History and further reading
Probability theory enjoys a surfeit of superb texts. We especially recommend the text
of Grimmett and Stirzaker[GS92] as an excellent introduction to many basic ideas in the
theory of probability and stochastic processes. From a more purely mathematical point
of view, Williams[Wil91] is a superb introduction to the modern theory of probability,
with an emphasis on the beautiful theory of martingales. Finally, the classic two-volume
text of Feller[Fel68a, Fel68b] is an excellent indepth introduction to the ideas of probability
theory.
Appendix 2: Group theory
The mathematical theory of groups is useful at several points in the study of quantum
computation and quantum information. The generalization of the order-ﬁnding, factor-
ing, and period ﬁnding algorithms in Chapter 5 is based on the hidden subgroup problem;
the stabilizer formalism for quantum error-correction in Chapter 10 is based on some
elementary group theory. The number theory described in Appendix 4 uses the proper-
ties of the group Z∗
n. And, implicitly, the quantum circuits used throughout the book are
an example of the use of Lie groups. In this appendix we review some basic material on
group theory. We summarize many of the fundamental concepts and provide important
deﬁnitions, but do not attempt to explain very much, as group theory is a vast subject!
A2.1
Basic deﬁnitions
A group (G, ·) is a non-empty set G with a binary group multiplication operation ‘·’,
with the following properties: (closure) g1 · g2 ∈G for all g1, g2 ∈G; (associativity)
(g1 · g2) · g3 = g1 · (g2 · g3), for all g1, g2, g3 ∈G; (identity) there exists e ∈G such that
∀g ∈G, g · e = e · g = g; (inverses) for all g ∈G, there exists g−1 ∈G such that
g · g−1 = e and g−1 · g = e. We often leave out · and write g1 · g2 as simply g1g2. We also
often refer to the group G without referring explicitly to its multiplication operation, but
it must be deﬁned.
A group G is ﬁnite if the number of elements in G is ﬁnite. The order of a ﬁnite group
G is the number of elements it contains, denoted as |G|. A group G is said to be Abelian
if g1g2 = g2g1 for all g1, g2 ∈G. A simple example of a ﬁnite Abelian group is the additive
group Zn of integers modulo n, with ‘multiplication’ operation the operation of modular
addition. It is easily checked that this operation satisﬁes the closure and associativity
axioms; there is an identity element, 0, since x+0 = x(mod n) for all x, and every x ∈G
has an inverse, n −x, since x + (n −x) = 0(mod n).
The order of an element g ∈G is the smallest positive integer r such that gr (g
multiplied with itself r times) equals the identity element e.
A subgroup H of G is a subset of G which forms a group under the same group
multiplication operation as G.
Theorem A2.1: (Lagrange’s theorem) If H is a subgroup of a ﬁnite group G then
|H| divides |G|.
Exercise A2.1:
Prove that for any element g of a ﬁnite group, there always exists a
positive integer r such that gr = e. That is, every element of such a group has an
order.
Exercise A2.2:
Prove Lagrange’s theorem.
Basic deﬁnitions
611
Exercise A2.3:
Show that the order of an element g ∈G divides |G|.
If g1 and g2 are elements of G, then the conjugate of g2 with respect to g1 is the element
g−1
1 g2g1. If H is subgroup of G, then it is known as a normal subgroup if g−1Hg = H
for all g ∈G. The conjugacy class Gx of an element x in a group G is deﬁned by
Gx ≡{g−1xg|g ∈G}.
Exercise A2.4:
Show that if y ∈Gx then Gy = Gx.
Exercise A2.5:
Show that if x is an element of an Abelian group G then Gx = {x}.
An interesting example of a group which is not Abelian is the Pauli group on n qubits.
For a single qubit, the Pauli group is deﬁned to consist of all the Pauli matrices, with
multiplicative factors ±1, ±i allowed by the deﬁnition:
G1 ≡{±I, ±iI, ±X, ±iX, ±Y, ±iY, ±Z, ±iZ}.
(A2.1)
This set of matrices forms a group under the operation of matrix multiplication. You
might wonder why we don’t omit the multiplicative factors ±1 and ±i; the reason these
are included is to ensure that G1 is closed under multiplication, and thus forms a legitimate
group. The general Pauli group on n qubits is deﬁned to consist of all n-fold tensor
products of Pauli matrices, and again we allow multiplicative factors of ±1, ±i.
A2.1.1
Generators
The study of groups is often greatly simpliﬁed by the use of a set of group generators for
the group being studied. A set of elements g1, . . . , gl in a group G is said to generate the
group G if every element of G can be written as a product of (possibly repeated) elements
from the list g1, . . . , gl, and we write G = ⟨g1, . . . , gl⟩. For example, G1 = ⟨X, Z, iI⟩,
since every element in G can be written as a product of X, Z and iI. On the other hand,
⟨X⟩= {I, X}, a subgroup of G1, since not every element of G1 can be written as a
power of X. The notation ⟨· · ·⟩which we use for group generators may potentially be
confused with the notation for observable averages introduced in Section 2.2.5 (page 87);
however, in practice it is always clear from context how the notation is being used.
The great advantage of using generators to describe groups is that they provide a
compact means of describing the group. Suppose G has size |G|. Then it is pretty easy
to show that there is a set of log(|G|) generators generating G. To see this, suppose
g1, . . . , gl is a set of elements in a group G, and g is not an element of ⟨g1, . . . , gl⟩.
Let f ∈⟨g1, . . . , gl⟩. Then fg ̸∈⟨g1, . . . , gl⟩, since if it were then we would have
g = f −1fg ∈⟨g1, . . . , gl⟩, which we know is false by assumption. Thus for each element
f ∈⟨g1, . . . , gl⟩there is an element fg which is in ⟨g1, . . ., gl, g⟩but not in ⟨g1, . . . , gl⟩.
Thus adding the generator g to ⟨g1, . . . , gl⟩doubles (or more) the size of the group being
generated, from which we conclude that G must have a set of generators containing at
most log(|G|) elements.
A2.1.2
Cyclic groups
A cyclic group G possesses an element a such that any element g ∈G can be expressed
as an for some integer n. a is known as a generator of G, and we write G = ⟨a⟩. A cyclic
subgroup H generated by g ∈G is the group formed by {e, g, g2, . . . , gr−1}, where r is
the order of g. That is, H = ⟨g⟩.
612
Group theory
Exercise A2.6:
Show that any group of prime order is cyclic.
Exercise A2.7:
Show that every subgroup of a cyclic group is cyclic.
Exercise A2.8:
Show that if g ∈G has ﬁnite order r, then gm = gn if and only if
m = n(mod r).
A2.1.3
Cosets
For H a subgroup of G, the left coset of H in G determined by g ∈G is the set
gH ≡{gh|h ∈H}. The right coset is deﬁned similarly. Often whether a coset is a ‘left’
or ‘right’ coset is implied by context. In the case of a group like Zn where the group
operation is addition it is conventional to write cosets of a subgroup H in the form g+H,
for g ∈Zn. Elements of a particular coset gH are known as coset representatives of that
coset.
Exercise A2.9:
Cosets deﬁne an equivalence relation between elements. Show that
g1, g2 ∈G are in the same coset of H in G if and only if there exists some
h ∈H such that g2 = g1h.
Exercise A2.10:
How many cosets of H are there in G?
A2.2
Representations
Let Mn be the set of n×n complex matrices. A matrix group is a set of matrices in Mn
which satisfy the properties of a group under matrix multiplication. We shall denote the
identity element in such groups as I. A representation ρ of a group G may be deﬁned as a
function which maps G to a matrix group, preserving group multiplication. Speciﬁcally,
g ∈G is mapped to ρ(g) ∈Mn, such that g1g2 = g3 implies ρ(g1)ρ(g2) = ρ(g3). If the
map is many to one, it is known as a homomorphism; if it is one to one, then the map
is an isomorphism. A representation ρ which maps into Mn has dimension dρ = n. The
representations we have deﬁned are also referred to as being matrix representations;
there are more general ones, but these are sufﬁcient for our purposes. For the remainder
of this appendix all the groups G which we deal with should be taken to be ﬁnite groups.
A2.2.1
Equivalence and reducibility
Two important concepts about representations are equivalence and reducibility. The
character of a matrix group G ⊂Mn is a function on the group deﬁned by χ(g) = tr(g),
for g ∈G, where tr(·) is the usual trace function on matrices. It has the following
properties: (1) χ(I) = n, (2) |χ(g)| ≤n, (3) |χ(g)| = n implies g = eiθI, (4) χ is constant
on any given conjugacy class of G, (5) χ(g−1) = χ∗(g), and (6) χ(g) is an algebraic
number for all g. Two matrix groups are said to be equivalent if they are isomorphic,
and corresponding elements under the isomorphism have the same character.
Exercise A2.11: (Characters)
Prove the properties of characters given above.
Exercise A2.12: (Unitary matrix groups)
A unitary matrix group is comprised
solely of unitary matrices (those which satisfy U †U = I). Show that every matrix
group is equivalent to a unitary matrix group. If a representation of a group
Representations
613
consists entirely of unitary matrices, we may refer to it as being a unitary
representation.
A matrix group G in Mn is said to be completely reducible if it is equivalent to another
matrix group H which is of block diagonal form, that is all elements m ∈H are of the
form diag(m1, m2), for some m1 ∈Mn1 and m2 ∈Mn2. If no such equivalence exists,
then the matrix group is irreducible. The following is a useful property of irreducible
matrix groups:
Lemma A2.2: (Schur’s lemma) Let G ⊂Mn and H ⊂Mk be two matrix groups of
the same order, |G| = |H|. If there exists a k by n matrix S such that Sgi = hiS
for some ordering of all elements gi ∈G and hi ∈H, then either S is the zero
matrix, or n = k and S is a square nonsingular matrix.
Exercise A2.13:
Show that every irreducible Abelian matrix group is one dimensional.
Exercise A2.14:
Prove that if ρ is an irreducible representation of G, then |G|/dρ is
an integer.
The following theorem connects irreducibility with characters:
Theorem A2.3: A matrix group G is irreducible if and only if
1
|G|

g∈G
|χ(g)|2 = 1 .
(A2.2)
A2.2.2
Orthogonality
The key theorem of representation theory is the following:
Theorem A2.4: (Fundamental theorem) Every group G has exactly r inequivalent
irreducible representations, where r is the number of conjugacy classes of G.
And if ρp ∈Mdρ and ρq are any two of these, then the matrix elements satisfy
the orthogonality relations

g∈G

ρp(g)
	−1
ij

ρq(g)
	
kl = |G|
dρ
δilδjkδpq ,
(A2.3)
where δpq = 1 if ρp = ρq and is zero otherwise.
Exercise A2.15:
Using the Fundamental Theorem, prove that characters are
orthogonal, that is:
r

i=1
ri (χp
i )∗χq
i = |G|δpq
and
r

p=1
(χp
i )∗χp
j = |G|
ri
δij ,
(A2.4)
where p, q, and δpq have the same meaning as in the theorem, and χp
i is the
value the character of the pth irreducible representation takes on the ith
conjugacy class of G, and ri is the size of the ith conjugacy class.
Exercise A2.16:
S3 is the group of permutations of three elements. Suppose we order
these as mapping 123 to: 123; 231; 312; 213; 132, and 321, respectively. Show that
614
Group theory
there exist two one-dimensional irreducible representations of S3, one of which
is trivial, and the other of which is 1, 1, 1, −1, −1, −1, corresponding in order to
the six permutations given earlier. Also show that there exists a two-dimensional
irreducible representation, with the matrices
 1
0
0
1

,
1
2
0
−1
−
√
3
√
3
−1
1
,
1
2
0
−1
√
3
−
√
3
−1
1
,
 −1
0
0
1

,
1
2
0
1
√
3
√
3
−1
1
,
1
2
0
1
−
√
3
−
√
3
−1
1
.
(A2.5)
Verify that the representations are orthogonal.
A2.2.3
The regular representation
The number 1 is a valid one-dimensional matrix representation of any group. How-
ever, it is trivial. A representation is faithful if the matrix group of the representa-
tion is isomorphic to the original group. The regular representation is a faithful repre-
sentation which exists for any group, and is constructed in the following manner. Let
⃗v = [g1, g2, . . . , g|G|]T be a column vector of elements from G. Multiplying each element
of ⃗v by an element g ∈G permutes the entries of the vector; this permutation can be
represented by a |G|×|G| matrix acting by matrix multiplication on ⃗v. The |G| such ma-
trices corresponding to the possible different permutations form a faithful representation
of G, under matrix multiplication.
Exercise A2.17:
Prove that the regular representation is faithful.
Exercise A2.18:
Show that the character of the regular representation is zero except
on the representation of the identity element, for which χ(I) = |G|.
The decomposition of arbitrary representations into tensor sums of irreducible repre-
sentations obeys the following theorem:
Theorem A2.5: If ρ is an arbitrary representation of G with character χ, and ρp are the
inequivalent irreducible representations of G with characters χp, then
ρ = ⊕pcpρp, where ⊕denotes a direct sum, and cp are the numbers determined
by
cp =
1
|G|
r

i=1
ri (χp
i )∗χi .
(A2.6)
Exercise A2.19:
Use Theorem A2.5 to show that the regular representation contains
dρp instances of each irreducible representation ρp. Thus, if R denotes the
regular representation, and ˆG denotes the set of all inequivalent irreducible
representations, then
χR
i =

ρ∈ˆG
dρχρ
i .
(A2.7)
Exercise A2.20:
The character of the regular representation is zero except for the
Fourier transforms
615
conjugacy class i containing e, the identity element in G. Show, therefore, that

ρ∈ˆG
dρχρ(g) = Nδge .
(A2.8)
Exercise A2.21:
Show that 
ρ∈ˆG d2
ρ = |G|.
A2.3
Fourier transforms
Let G be a ﬁnite group of order N, and f be a function which maps group elements to
complex numbers. For an irreducible representation ρ of G, of dimension dρ, we deﬁne
the Fourier transform of f to be ˆf,
ˆf(ρ) ≡
)
dρ
N

g∈G
f(g) ρ(g) .
(A2.9)
Note that for ρ a matrix representation, ˆf(ρ) maps matrices to matrices. Let ˆG be a
complete set of inequivalent irreducible representations of G. We deﬁne the inverse
Fourier transform of ˆf to be
f(g) =
1
√
N

ρ∈ˆG

dρ tr( ˆf(ρ)ρ(g−1)) .
(A2.10)
Because 
ρ d2
ρ = N, f and ˆf can both be expressed as vectors of complex numbers of
length N. Note that the coefﬁcients in the above equations have been chosen such that
if ˆG consists of unitary representations, then the Fourier transformations are unitary.
The above deﬁnitions can be understood by substituting (A2.9) into (A2.10) to obtain
f(g) = 1
N

ρ∈ˆG

g′∈G
dρf(g′) tr

ρ(g′)ρ(g−1)

(A2.11)
= 1
N

ρ∈ˆG

g′∈G
dρf(g′) tr

ρ(g′ g−1)

(A2.12)
= 1
N

g′∈G
f(g′)

ρ∈ˆG
dρ χρ(g′ g−1) .
(A2.13)
Using (A2.8), we may simplify (A2.13) to
f(g) =

g′∈G
f(g′)δg′g ,
(A2.14)
as desired.
Exercise A2.22:
Substitute (A2.10) into (A2.9) and prove that ˆf(ρ) is obtained.
Exercise A2.23:
Let us represent an Abelian group G by g ∈[0, N −1], with
addition as the group operation, and deﬁne ρh(g) = exp[−2πigh/N] as the h
representation of g. This representation is one-dimensional, so dρ = 1. Show
that the Fourier transform relations for G are
ˆf(h) =
1
√
N
N−1

g=0
f(g) e−2πigh/N
and
f(h) =
1
√
N
N−1

g=0
ˆf(g) e2πihg/N . (A2.15)
616
Group theory
Exercise A2.24:
Using the results of Exercise A2.16, construct the Fourier transform
over S3 and express it as a 6×6 unitary matrix.
History and further reading
There are many outstanding texts on group theory, and virtually any book on algebra
has a section devoted to the theory of groups. The discussion here borrowed much
notation from the text on ﬁnite groups by Lomont[Lom87]. Hammermesh is a standard
reference for group theory in physics[Ham89]. Discussions of Fourier transforms over
groups are not so common. Diaconis and Rockmore have written a good article on the
efﬁcient computation of Fourier transforms over groups[DR90]; many of their results are
also reviewed in F¨assler and Stiefel[FS92]. Beth independently discovered the fast Fourier
transform over groups[Bet84], as did Clausen[Cla89].
Appendix 3: The Solovay–Kitaev theorem
In Chapter 4 we showed that an arbitrary unitary operation U may be implemented on
a quantum computer using a circuit consisting of single qubit and controlled-
gates.
Such universality results are important because they ensure the equivalence of apparently
different models of quantum computation. For example, the universality results ensure
that a quantum computer programmer may design quantum circuits containing gates
which have four input and output qubits, conﬁdent that such gates can be simulated by
a constant number of controlled-
and single qubit unitary gates.
An unsatisfactory aspect of the universality of controlled-
and single qubit unitary
gates is that the single qubit gates form a continuum, while the methods for fault-tolerant
quantum computation described in Chapter 10 work only for a discrete set of gates.
Fortunately, also in Chapter 4 we saw that any single qubit gate may be approximated to
arbitrary accuracy using a ﬁnite set of gates, such as the controlled-
gate, Hadamard
gate H, phase gate S, and π/8 gate. We also gave a heuristic argument that approximating
the chosen single qubit gate to an accuracy ϵ required only Θ(1/ϵ) gates chosen from the
ﬁnite set. Furthermore, in Chapter 10 we showed that the controlled-
, Hadamard,
phase and π/8 gates may be implemented in a fault-tolerant manner.
In this appendix we show that a much faster rate of convergence than Θ(1/ϵ) may be
achieved. The Solovay–Kitaev theorem shows that for any gate U on a single qubit,
and given any ϵ > 0, it is possible to approximate U to a precision ϵ using Θ(logc(1/ϵ))
gates from a ﬁxed ﬁnite set, where c is a small constant approximately equal to 2. The
best possible value for c isn’t known yet, so we are going to explain the proof of the
Solovay–Kitaev theorem for c approximately equal to 4, and then in the end of appendix
problems outline a method that may be used to reduce c down closer to 2. We will also
prove that c cannot be less than 1; determining the best possible value of c between 1
and 2 is an open problem!
To appreciate the importance of the Solovay–Kitaev theorem, imagine a quantum com-
puter programmer designs an algorithm using f(n) gates to solve a problem. Suppose
the algorithm he or she comes up with uses many gates outside the usual fault-tolerant
set of controlled-
, Hadamard, phase and π/8 gates. How many gates are required
to implement the algorithm fault-tolerantly? If the tolerance to error for the entire algo-
rithm is to be ϵ, then the individual gates must be accurate to a tolerance ϵ/f(n). By the
heuristic argument of Chapter 4 it would take Θ(f(n)/ϵ) gates in the fault-tolerant set
to approximate each one of the gates used in the algorithm, for a total cost Θ(f(n)2/ϵ), a
polynomial increase in the number of gates required by the algorithm. Using the Solovay–
Kitaev theorem each gate in the algorithm can be simulated by O(logc(f(n)/ϵ)) gates in
the fault-tolerant set, for a total cost O(f(n) logc(f(n)/ϵ)) for the fault-tolerant algo-
rithm, which is only polylogarithmically more than for the original algorithm. For many
618
The Solovay–Kitaev theorem
problems, such a polylogarithmic cost is quite acceptable, whereas the polynomial cost
provided by the heuristic argument of Chapter 4 may be much less desirable.
To state the Solovay–Kitaev theorem more precisely, we need to deﬁne some notation
and nomenclature. Recall that SU(2) is the set of all single qubit unitary matrices with
determinant equal to one. We restrict our attention to SU(2), since all single qubit unitary
gates may be written as the product of an element of SU(2) with an unimportant global
phase factor. Suppose G is a ﬁnite set of elements of SU(2); G plays the role of the ﬁnite
set of elementary gates our quantum computer programmer is using to simulate all the
other gates. For the sake of concreteness, think of G as containing the fault-tolerant set
H, S, and T, with appropriate global phases added to ensure the determinants are all
equal to one. We suppose for the sake of convenience that G contains its own inverses,
that is, if U ∈G, then U † ∈G. In the case of the fault-tolerant set, that means adding
S† = S3 and T † = T 7 to the set, which fortunately can be expressed in terms of gates
already in the set. A word of length l from G is a product g1g2 . . . gl ∈SU(2), where
gi ∈G for each i. We deﬁne Gl to be the set of all words of length at most l, and ⟨G⟩to
be the set of all words of ﬁnite length.
We need some notion of distance to quantify what we mean by an approximation to
a unitary matrix. The exact measure used is not all that important. It is convenient for
our purposes to use the trace distance studied in Chapter 9, D(U, V ) ≡tr|U −V |,
where |X| ≡
√
X†X is the positive square root of X†X. Actually, this deﬁnition differs
by a factor 2 from the deﬁnition used in Chapter 9; the reason for using a different
normalization is that it makes geometric visualization of the proof of the Solovay–Kitaev
theorem easier, as we shall see (it will also be helpful to think of elements in SU(2) as
being points in space). A subset S of SU(2) is said to be dense in SU(2) if for any element
U of SU(2) and ϵ > 0 there is an element s ∈S such that D(s, U) < ϵ. Suppose S and
W are subsets of SU(2). Then S is said to form an ϵ-net for W, where ϵ > 0, if every
point in W is within a distance ϵ of some point in S. Our interest is in how fast Gl ‘ﬁlls
up’ SU(2) as l is increased. That is, for how small an ϵ is Gl an ϵ-net for SU(2)? The
Solovay–Kitaev theorem says that ϵ gets small very quickly indeed as l is increased.
Exercise A3.1:
In Chapter 4 we made use of the distance measure
E(U, V ) = max|ψ⟩∥(U −V )|ψ⟩∥, where the maximum is over all pure states |ψ⟩.
Show that when U and V are single qubit rotations, U = R ˆm(θ), V = R ˆn(ϕ),
D(U, V ) = 2E(U, V ), and thus it does not matter whether we use the trace
distance or the measure E(·, ·) for the Solovay–Kitaev theorem.
Theorem A3.1: (Solovay–Kitaev theorem) Let G be a ﬁnite set of elements in SU(2)
containing its own inverses, such that ⟨G⟩is dense in SU(2). Let ϵ > 0 be given.
Then Gl is an ϵ-net in SU(2) for l = O(logc(1/ϵ)), where c ≈4.
As already noted, the best possible value of c is somewhat lower than 4, but it is
convenient to give the proof for this particular case. In Problem 3.1 we explain how
modiﬁcations of the proof can be used to lower c. The ﬁrst part of the proof is to show
that the points of Gl get very dense in a small neighbourhood of the identity matrix I as
l is increased, a conclusion encapsulated in the following lemma. To state the lemma, we
deﬁne Sϵ to be the set of all points U in SU(2) such that D(U, I) ≤ϵ.
The Solovay–Kitaev theorem
619
Lemma A3.2: Let G be a ﬁnite set of elements in SU(2) containing its own inverses,
and such that ⟨G⟩is dense in SU(2). There exists a universal constant ϵ0
independent of G, such that for any ϵ ≤ϵ0, if Gl is an ϵ2-net for Sϵ, then G5l is a
Cϵ3-net for S√
Cϵ3/2, for some constant C.
We prove Lemma A3.2 shortly, but ﬁrst let’s see how it implies the Solovay–Kitaev
theorem. There are two steps to the proof. The ﬁrst step is to apply Lemma A3.2
iteratively to show that the neighbourhood of the origin ﬁlls in very quickly as the word
length l is increased. Since G is dense in SU(2) we can ﬁnd an l0 such that Gl0 is an ϵ2
0-net
for SU(2), and thus also for Sϵ0. Applying Lemma A3.2 with ϵ = ϵ0 and l = l0 implies
that G5l0 is a Cϵ3
0-net for S√
Cϵ3/2
0 . Applying Lemma A3.2 again with ϵ =
√
Cϵ3/2
0
and
l = 5l0 implies that G52l0 is a C(
√
Cϵ3/2
0 )3-net for S√
C(
√
Cϵ3/2
0
)3/2. Iterating this procedure
k times, we ﬁnd that G5kl0 is an ϵ(k)2-net for Sϵ(k), where
ϵ(k) = (Cϵ0)(3/2)k
C
.
(A3.1)
Without loss of generality we may suppose ϵ0 has been chosen such that Cϵ0 < 1, and
therefore ϵ(k) gets small very fast as k increases. It will also be useful to note that,
provided ϵ0 is chosen small enough, ϵ(k)2 < ϵ(k + 1).
The second step is to let U be any element of SU(2) and use the translation idea
illustrated in Figure A3.1 to approximate U using products of elements of G. Let U0 ∈Gl0
be an ϵ(0)2-approximation to U. Now deﬁne V so that V U0 = U, that is, V ≡UU †
0 .
Thus D(V, I) = tr|V −I| = tr|(U −U0)U †
0 | = tr|U −U0| < ϵ(0)2 < ϵ(1). From the
iterated application of Lemma A3.2 discussed above, we can ﬁnd U1 ∈G5l0 which is
an ϵ(1)2-approximation to V . It follows that U1U0 is an ϵ(1)2-approximation to U. Now
deﬁne V ′ so that V ′U1U0 = U, that is, V ′ ≡UU †
0 U †
1 . Thus D(V ′, I) = tr|V −I| =
tr|(U −U1U0)U †
0 U †
1 | = tr|U −U1U0|/ < ϵ(1)2 < ϵ(2). It follows from the iterated
application of Lemma A3.2 that we can ﬁnd U2 ∈G52l0 which is an ϵ(2)2-approximation
to V ′, and thus U2U1U0 is an ϵ(2)2-approximation to U. Continuing in this way we
construct Uk ∈G5kl0 so that UkUk−1 . . . U0 is an ϵ(k)2-approximation to U.
Putting it all together, a sequence of l0 + 5l0 + · · · + 5kl0 < 5
45kl0 gates can be used to
approximate any unitary gate U to an accuracy ϵ(k)2. To approximate to some desired
accuracy ϵ, we therefore must choose k such that
ϵ(k)2 < ϵ.
(A3.2)
Substituting (A3.1) this can be restated as
3
2
k
< log(1/C2ϵ)
2 log(1/Cϵ0).
(A3.3)
It follows that the number of gates required to approximate to within ϵ satisﬁes (c =
log 5/ log(3/2) ≈4)
number of gates < 5
45kl0 = 5
4
3
2
kc
l0 < 5
4
 log(1/C2ϵ)
2 log(1/Cϵ0)
c
l0.
(A3.4)
That is, the number of gates required to approximate to within ϵ is O(logc(1/ϵ)), com-
pleting the proof of the Solovay–Kitaev theorem.
The proof of Lemma A3.2 uses a few elementary facts about multiplication of elements
620
The Solovay–Kitaev theorem
Figure A3.1. The translation step used in the proof of the Solovay–Kitaev theorem. To approximate an arbitrary
single qubit gate we ﬁrst approximate to within a distance ϵ(0)2 using l0 gates from G. Then we improve the
approximation by adding 5l0 more gates, for a total accuracy better than ϵ(1)2, and continue on in this way, quickly
converging to U.
of SU(2), which we now recall. The key idea of the lemma is to work in the neighborhood
of the identity, which greatly simpliﬁes the rather complicated operation of multiplication
in SU(2). More precisely, suppose U and V are elements of SU(2), and deﬁne the group
commutator of U and V by
[U, V ]gp ≡UV U †V †.
(A3.5)
Suppose U and V are both close to the identity, so that they may be written U = e−iA
and V = e−iB, where A and B are Hermitian matrices such that tr|A|, tr|B| ≤ϵ for
some small ϵ. Expanding e±iA and e±iB to terms quadratic in A and B gives
D([U, V ]gp, e−[A,B]) = O(ϵ3),
(A3.6)
where [A, B] = AB −BA is the usual commutator for matrices (in fact, the commutator
for the Lie algebra of SU(2)). Thus, in the neighborhood of the identity we can study
the group commutator by studying instead the much simpler matrix commutator.
The Solovay–Kitaev theorem
621
Indeed, for qubits the matrix commutator has an especially nice form. An arbitrary
element of SU(2) may be written U = u(⃗a) ≡exp(−i⃗a · ⃗σ/2) for some real vector ⃗a.
Similarly V = u(⃗b) = exp(−i⃗b·⃗σ/2) for some real vector⃗b. Recalling from Exercise 2.40
that
[⃗a · σ,⃗b · σ] = 2i
'
⃗a ×⃗b
(
· ⃗σ,
(A3.7)
we see from (A3.6) that
D
'
[U, V ]gp, u
'
⃗a ×⃗b
((
= O(ϵ3).
(A3.8)
The basic idea of the proof of Lemma A3.2 is now easy to understand. The details,
most of which relate to approximation issues, are ﬁlled in below for completeness; for now
we just give the main idea, as illustrated in Figure A3.2. Suppose we wish to approximate
some element U = u(⃗x) in Sϵ2. You will see in Exercise A3.4 that trace distances like
D(U, I) are equal (up to small corrections) to the Euclidean distance ∥⃗x∥, so to a good
approximation ∥⃗x∥≤ϵ2. We can always choose ⃗y and ⃗z of length at most ϵ, such that
⃗x = ⃗y×⃗z. Pick ⃗y0 and ⃗z0 such that u(⃗y0) and u(⃗z0) are elements of Gl that ϵ2-approximate
u(⃗y) and u(⃗z), respectively. Applying (A3.6) to the commutator [u(⃗y0), u(⃗z0)]gp we get an
O(ϵ3)-approximation to U. This gives a O(ϵ3)-net for Sϵ2; to ﬁnish the proof of the lemma
we apply a translation step like that in the main part of the proof of the Solovay–Kitaev
theorem, obtaining in 5l gates an O(ϵ3)-approximation to any element of SO(ϵ3/2).

ϵ2
ϵ
ϵ2
Cϵ3
Figure A3.2. The main idea in the proof of Lemma A3.2. Taking group commutators of elements U1 and U2 in Sϵ
ﬁlls in Sϵ2 much more densely. Note that the density of circles appearing on the right hand side ought to be much
higher than is shown, as there should be one for each pair of circles on the left; the lower density is merely for
clarity. The proof of the lemma is completed by applying a translation step (not shown) to get good approximations
to any element of S√
3ϵ3/2.
Exercise A3.2:
Suppose A and B are Hermitian matrices such that tr|A|, tr|B| ≤ϵ.
Prove that for all sufﬁciently small ϵ,
D
'e−iA, e−iB
gp , e−[A,B](
≤dϵ3,
(A3.9)
622
The Solovay–Kitaev theorem
for some constant d, establishing Equation (A3.6). (Comment: for practical
purposes it may be interesting to obtain good bounds on d.)
Exercise A3.3:
Let ⃗x and ⃗y be any two real vectors. Show that
D(u(⃗x), u(⃗y)) = 2
√
2

1 −cos(x/2) cos(y/2) −sin(x/2) sin(y/2) ˆx · ˆy ,
(A3.10)
where x ≡∥⃗x∥, y ≡∥⃗y∥, and ˆx and ˆy are unit vectors in the ⃗x and ⃗y directions,
respectively.
Exercise A3.4:
Show that in the case ⃗y = 0 the formula for D(u(⃗x), u(⃗y)) reduces to
D(u(⃗x), I) = 4 sin
++++
x
4
++++ .
(A3.11)
Exercise A3.5:
Show that when x, y ≤ϵ,
D(u(⃗x), u(⃗y)) = ∥⃗x −⃗y∥+ O(ϵ3).
(A3.12)
Proof
(Lemma A3.2)
Suppose Gl is an ϵ2-net in Sϵ. The ﬁrst step of the proof is to show that [Gl, Gl]gp is
a Cϵ3-net for Sϵ2 and some constant C.
Let U ∈Sϵ2 and pick ⃗x such that U = u(⃗x). By Exercise A3.4 it follows that x ≤
ϵ2 + O(ϵ6). Choose any pair of vectors ⃗y and ⃗z of length at most ϵ + O(ϵ5) such that
⃗x = ⃗y × ⃗z. Gl is an ϵ2-net for Sϵ, so choose U1 and U2 in Gl ∩Sϵ such that
D(U1, u(⃗y)) < ϵ2 + O(ϵ5)
(A3.13)
D(U2, u(⃗z)) < ϵ2 + O(ϵ5),
(A3.14)
and let ⃗y0 and ⃗z0 be chosen such that U1 = u(⃗y0) and U2 = u(⃗z0). By Exercise A3.4 it
follows that y0, z0 ≤ϵ + O(ϵ3). Our goal is to show that D(U, [U1, U2]gp) is smaller than
Cϵ3. To do this, we use the triangle inequality,
D(U, [U1, U2]gp) ≤D(U, u(⃗y0 × ⃗z0)) + D(u(⃗y0 × ⃗z0), [U1, U2]gp).
(A3.15)
The second term is at most d′ϵ3, by Exercise A3.2, where d′ is a constant slightly larger
than d because of the possible contribution due to the fact that y0, z0 < ϵ + O(ϵ3), rather
than y0, z0 < ϵ. Substituting U = u(⃗x), making use of Exercise A3.5, introducing an
appropriate constant d′′ and doing some elementary algebra gives
D(U, [U1, U2]gp) ≤D(u(⃗x), u(⃗y0 × ⃗z0) + d′ϵ3
(A3.16)
= ∥⃗x −⃗y0 × ⃗z0∥+ d′′ϵ3
(A3.17)
= ∥⃗y × ⃗z −⃗y0 × ⃗z0∥+ d′′ϵ3.
(A3.18)
= ∥[(⃗y −⃗y0) + ⃗y0] × [(⃗z −⃗z0) + ⃗z0] −⃗y0 × ⃗z0∥+ d′′ϵ3 (A3.19)
≤(d′′ + 2)ϵ3 + O(ϵ4)
(A3.20)
≤Cϵ3,
(A3.21)
where C is an appropriately chosen constant.
The second step in the proof of the lemma is to apply a translation step like that
used in the main part of the proof of the Solovay–Kitaev theorem. Speciﬁcally, given
Chapter problems
623
U ∈S√
Cϵ3, we can ﬁnd V in Gl such that D(U, V ) ≤ϵ2, and thus UV † ∈Sϵ2. Then
ﬁnd W1 and W2 in Gl such that D([W1, W2]gp, UV †) ≤Cϵ3, and therefore
D([W1, W2]gpV, U) ≤Cϵ3,
(A3.22)
which completes the proof.
Exercise A3.6:
Fixing the set G of elementary gates, describe an algorithm which,
given a description of a single qubit unitary gate U and a desired accuracy ϵ > 0,
efﬁciently computes a sequence of gates from G that ϵ-approximates U.
The analysis in this appendix is rather crude, and a much tighter analysis can be
made. One issue of especial interest is the best possible value of the exponent c in the
O(logc(1/ϵ)) bound. It is not difﬁcult to show that c can be no less than 1. To see this,
imagine we have a collection of N little balls, all of radius ϵ, in SU(2). The volume of
these balls scales like ϵd, for some unimportant constant d. Therefore, if the balls are
to cover SU(2), N must be of size Ω(1/ϵd). Suppose we consider all possible sequences
U1U2 . . . Ug consisting of g gates chosen from G. Clearly such sequences can generate at
most |G|g distinct unitary operations. Thus we must have |G|g = Ω(1/ϵd), which implies
the desired lower bound on the number of gates,
g = Ω

log
1
ϵ

.
(A3.23)
Problem 3.1:
The following problem outlines a more elaborate construction that
achieves an O(log2(1/ϵ) logc(log(1/ϵ))) bound on the number of gates required to
approximate to within ϵ of a desired target, for any c > 2.
(1) Suppose N is a δ-net in Sϵ, for 0 < δ < ϵ ≤ϵ0, ϵ0 sufﬁciently small. Show
that [N , N ]gp is a dδϵ-net in Sϵ2, for some constant d.
(2) Suppose Gl is a δ-net in Sϵ, for 0 < δ < ϵ ≤ϵ0. Show that G4kl is a
dkδϵ2k−1-net in Sϵ2k .
(3) Suppose we deﬁne k by
k ≡
4
log
 log(1/ϵ)
log(1/ϵ0)
5
,
(A3.24)
and suppose we can ﬁnd l such that Gl is a δ0-net for Sϵ0, where
dkδ0 = ϵ0.
(A3.25)
Show that G4kl is an ϵ-net for Sϵ2k
0 .
(4) Use the already-proved version of the Solovay–Kitaev theorem to show that
choosing l = O(kc) sufﬁces in the previous part of this problem, where
c = log(5)/ log(3/2) is the constant appearing in the exponent in the
already-proved version of the Solovay–Kitaev theorem.
(5) Combine the previous results to prove that O(log2(1/ϵ) logc(log(1/ϵ))) gates
can be used to ϵ-approximate an arbitrary gate in SU(2).
(6) Show that any c > 2 can appear in the conclusion of the previous result.
624
The Solovay–Kitaev theorem
Problem 3.2: (Research)
If it exists, ﬁnd an approximation procedure
asympoticially faster than the result found in the previous problem. Ideally, a
procedure would (a) saturate the Ω(log(1/ϵ)) lower bound on the number of
gates required to perform the approximation, and (b) provide an efﬁcient
algorithm for constructing such approximating sequences of gates.
Problem 3.3: (Research)
Fix a ﬁnite set of single qubit gates G which can be
performed fault-tolerantly and which generate a set dense in the single qubit
gates; say the π/8 gate and the Hadamard gate. Develop an elegant, efﬁcient and
reasonably tight method which, given an arbitrary single qubit gate U and some
ϵ > 0, produces a sequence of gates from the fault-tolerant set giving an
ϵ-approximation to U, up to global phase.
History and further reading
The results in this appendix were proved by Solovay in 1995 (unpublished manuscript),
and independently by Kitaev, who gave an outline of the proof in [Kit97b]. In the
same paper Kitaev observed that the result can be generalized to many Lie groups other
than SU(2); roughly speaking, the key fact about SU(2) used in the proof was that
[Sϵ, Sϵ]gp ⊇SΩ(ϵ2), and other Lie groups for which this fact holds also obey some version
of the Solovay–Kitaev theorem. The Solovay–Kitaev theorem is true, for example, for the
Lie group SU(d) of d by d unitary matrices with unit determinant. After hearing of this
result, Solovay subsequently generalized his proof in a similar fashion. Our presentation
has beneﬁted substantially from a 1999 lecture of Freedman, and by discussions with
Freedman, Kitaev and Solovay.
Appendix 4: Number theory
Understanding some elementary number theory is necessary if we are to understand
cryptosystems and how quantum computers can be used to break them. In this appendix
we review some basic facts about number theory.
A4.1
Fundamentals
Let’s start off by agreeing about a few conventions for nomenclature and notation. The
set of integers is the set {. . . , −2, −1, 0, 1, 2, . . .}, denoted Z. We may occasionally refer
to the natural numbers, meaning non-negative integers, but more often we’ll say non-
negative integer or positive integer, in order to make the distinction between the case
when zero is included, and when zero is not included.
Suppose n is an integer. An integer d divides n (written d|n) if there exists an integer
k such that n = dk. We say in this case that d is a factor or divisor of n. Notice that
1 and n are always factors of n. When d does not divide (is not a factor of) n we write
d̸ | n. For example, 3|6 and 3|18, but 3̸ | 5 and 3̸ | 7.
Exercise A4.1: (Transitivity)
Show that if a|b and b|c then a|c.
Exercise A4.2:
Show that if d|a and d|b then d also divides linear combinations of a
and b, ax + by, where x and y are integers.
Exercise A4.3:
Suppose a and b are positive integers. Show that if a|b then a ≤b.
Conclude that if a|b and b|a then a = b.
A prime number is an integer greater than 1 which has only itself and 1 as factors. The
ﬁrst few prime numbers are 2, 3, 5, 7, 11, 13, 17, . . .. Perhaps the most important single
fact about the positive integers is that they may be represented uniquely as a product of
factors which are prime numbers. This result is given an appropriately impressive name,
the fundamental theorem of arithmetic:
Theorem A4.1: (Fundamental theorem of arithmetic) Let a be any integer greater
than 1. Then a has a prime factorization of the form
a = pa1
1 pa2
2 . . . pan
n ,
(A4.1)
where p1, . . . , pn are distinct prime numbers, and a1, . . . , an are positive
integers. Moreover, this prime factorization is unique, up to the order of the
factors.
626
Number theory
Proof
The reader who has never seen a proof of the fundamental theorem of arithmetic is
strongly encouraged to attempt to supply it for themselves. Failing that, a proof may
be found in any elementary number theory text; see the end of appendix ‘History and
further reading’ for references.
For small numbers it is easy to ﬁnd the prime factorization by trial and error, for
example 20 = 22 ·51. For large numbers no efﬁcient algorithm is known to ﬁnd the prime
factorization on a classical computer, despite immense effort aimed at ﬁnding such an
algorithm.
Exercise A4.4:
Find the prime factorizations of 697 and 36 300.
A4.2
Modular arithmetic and Euclid’s algorithm
We’re all thoroughly familiar with the techniques of ordinary arithmetic. Another type
of arithmetic, modular arithmetic, is extremely useful in understanding the properties
of numbers. We assume that you are familiar with the elementary ideas of modular
arithmetic, and so will quickly breeze through the basic ideas and notation, before coming
to more advanced theory.
Modular arithmetic can be thought of as the arithmetic of remainders. If we divide
18 by 7 we get the answer 2, with a remainder of 4. More formally, given any positive
integers x and n, x can be written (uniquely) in the form
x = kn + r,
(A4.2)
where k is a non-negative integer, the result of dividing x by n, and the remainder r lies
in the range 0 to n −1, inclusive. Modular arithmetic is simply ordinary arithmetic in
which we only pay attention to remainders. We use the notation (mod n) to indicate that
we are working in modular arithmetic. For instance, we write 2 = 5 = 8 = 11(mod 3),
because 2, 5, 8 and 11 all have the same remainder (2) when divided by 3. The appellation
‘(mod n)’ reminds us that we are working in modular arithmetic, with respect to the
number n.
Addition, multiplication, and subtraction operations for modular arithmetic may all
be deﬁned in the obvious ways, but it is perhaps not so obvious how to deﬁne a division
operation. To understand how this may be done we introduce another key concept from
number theory, that of the greatest common divisor of two integers. The greatest common
divisor of integers a and b is the largest integer which is a divisor of both a and b. We
write this number as gcd(a, b). For example, the greatest common divisor of 18 and 12 is
6. An easy way of seeing this is to enumerate the positive divisors of 18 (1, 2, 3, 6, 9, 18)
and 12 (1, 2, 3, 4, 6, 12), and then pick out the largest common element in the two lists.
This method is quite inefﬁcient and impractical for large numbers. Fortunately, there is
a much more efﬁcient way of working out the greatest common divisor, a method known
as Euclid’s algorithm, whose explication occupies us for the next few pages.
Theorem A4.2: (Representation theorem for the gcd) The greatest common
divisor of two integers a and b is the least positive integer that can be written in
the form ax + by, where x and y are integers.
Modular arithmetic and Euclid’s algorithm
627
Proof
Let s = ax + by be the smallest positive integer that can be written in this form. Since
gcd(a, b) is a divisor of both a and b it is also a divisor of s. It follows that gcd(a, b) ≤s.
To complete the proof we demonstrate that s ≤gcd(a, b) by showing that s is a divisor
of both a and b. The proof is by contradiction. Suppose s is not a divisor of a. Then
a = ks + r, where the remainder r is in the range 1 to s −1. Rearranging this equation
and using s = ax + by we see that r = a(1 −kx) + b(−ky) is a positive integer that
can be written as a linear combination of a and b, and which is smaller than s. But this
contradicts the deﬁnition of s as the smallest positive integer that can be written as a
linear combination of a and b. We conclude that s must divide a. By symmetry s must
also be a divisor of b, which completes the proof.
Corollary A4.3: Suppose c divides both a and b. Then c divides gcd(a, b).
Proof
By Theorem A4.2, gcd(a, b) = ax + by for some integers x and y. Since c divides a and
b it must also divide ax + by.
When does a number, a, have a multiplicative inverse in modular arithmetic? That is,
given a and n, when does there exist a b such that ab = 1(mod n)? For example, note
that 2 · 3 = 1(mod 5), so the number 2 has multiplicative inverse 3 in arithmetic modulo
5. On the other hand, trial and error shows that 2 has no multiplicative inverse modulo 4.
Finding multiplicative inverses in modular arithmetic turns out to be related to the gcd
by the notion of co-primality: integers a and b are said to be co-prime if their greatest
common divisor is 1. For example, 14 and 9 are co-prime, since the positive divisors
of 14 are 1, 2, 7 and 14, while 1, 3 and 9 are the positive divisors of 9. The following
corollary characterizes the existence of multiplicative inverses in modular arithmetic using
co-primality.
Corollary A4.4: Let n be an integer greater than 1. An integer a has a multiplicative
inverse modulo n if and only if gcd(a, n) = 1, that is, a and n are co-prime.
Proof
Suppose a has a multiplicative inverse, which we denote a−1, modulo n. Then aa−1 =
1+kn for some integer k, and thus aa−1 +(−k)n = 1. From Theorem A4.2 we conclude
that gcd(a, n) = 1. Conversely, if gcd(a, n) = 1 then there must exist integers a−1 and b
such that aa−1 + bn = 1, and therefore aa−1 = 1(mod n).
Exercise A4.5:
For p a prime prove that all integers in the range 1 to p −1 have
multiplicative inverses modulo p. Which integers in the range 1 to p2 −1 do not
have multiplicative inverses modulo p2?
Exercise A4.6:
Find the multiplicative inverse of 17 modulo 24.
Exercise A4.7:
Find the multiplicative inverse of n + 1 modulo n2, where n is any
integer greater than 1.
Exercise A4.8: (Uniqueness of the inverse)
Suppose b and b′ are multiplicative
inverses of a, modulo n. Prove that b = b′(mod n).
628
Number theory
The next theorem is the key to Euclid’s efﬁcient algorithm for ﬁnding the greatest
common divisor of two positive integers.
Theorem A4.5: Let a and b be integers, and let r be the remainder when a is divided
by b. Then provided r ̸= 0,
gcd(a, b) = gcd(b, r).
(A4.3)
Proof
We prove the equality by showing that each side divides the other. To prove that the left
hand side divides the right note that r = a−kb for some integer k. Since gcd(a, b) divides
a, b and linear combinations of these it follows that gcd(a, b) divides r. By Corollary A4.3,
gcd(a, b) divides gcd(b, r). To prove that the right hand side divides the left note that
gcd(b, r) divides b, and since a = r + kb is a linear combination of b and r it follows that
gcd(b, r) also divides a. By Corollary A4.3, gcd(b, r) divides gcd(a, b).
Exercise A4.9:
Explain how to ﬁnd gcd(a, b) if the prime factorizations of a and b are
known. Find the prime factorizations of 6825 and 1430, and use them to
compute gcd(6825, 1430)
Euclid’s algorithm for ﬁnding the greatest common divisor of positive integers a and
b works as follows. First, order a and b so that a > b. Divide b into a, with result k1 and
remainder r1: a = k1b + r1. By Theorem A4.5 gcd(a, b) = gcd(b, r1). Next, we perform
a second division with b playing the role of a, and r1 playing the role of b: b = k2r1 + r2.
By Theorem A4.5 gcd(a, b) = gcd(b, r1) = gcd(r1, r2). Next, we perform a third division
with r1 playing the role of a and r2 the role of b: r1 = k3r2 + r3. By Theorem A4.5
gcd(a, b) = gcd(b, r1) = gcd(r1, r2) = gcd(r2, r3). We continue in this manner, each time
dividing the most recent remainder by the second most recent remainder, obtaining a
new result and remainder. The algorithm halts when we obtain a remainder that is zero,
that is, rm = km+1rm+1 for some m. We have gcd(a, b) = gcd(rm, rm+1) = rm+1, so the
algorithm returns rm+1.
As an example of the use of Euclid’s algorithm we ﬁnd gcd(6825, 1430):
6825 = 4 × 1430 + 1105
(A4.4)
1430 = 1 × 1105 + 325
(A4.5)
1105 = 3 × 325 + 130
(A4.6)
325 = 2 × 130 + 65
(A4.7)
130 = 2 × 65.
(A4.8)
From this we see that gcd(6825, 1430) = 65.
An adaptation of Euclid’s algorithm may be used to efﬁciently ﬁnd integers x and y
such that ax + by = gcd(a, b). The ﬁrst stage is to run through the steps of Euclid’s
algorithm, as before. The second stage begins at the second last line of the running of
Euclid’s algorithm, and involves successive substitution of the lines higher up in the
algorithm as illustrated by the following example:
65 = 325 −2 × 130
(A4.9)
Modular arithmetic and Euclid’s algorithm
629
= 325 −2 × (1105 −3 × 325) = −2 × 1105 + 7 × 325
(A4.10)
= −2 × 1105 + 7 × (1430 −1 × 1105) = 7 × 1430 −9 × 1105
(A4.11)
= 7 × 1430 −9 × (6825 −4 × 1430) = −9 × 6825 + 37 × 1430.
(A4.12)
That is, 65 = 6825 × (−9) + 1430 × 37, which is the desired representation.
What resources are consumed by Euclid’s algorithm? Suppose a and b may be rep-
resented as bit strings of at most L bits each. It is clear that none of the divisors ki or
remainders ri can be more than L bits long, so we may assume that all computations
are done in L bit arithmetic. The key observation to make in a resource analysis is that
ri+2 ≤ri/2. To prove this we consider two cases:
• ri+1 ≤ri/2. It is clear that ri+2 ≤ri+1 so we are done.
• ri+1 > ri/2. In this case ri = 1 × ri+1 + ri+2, so ri+2 = ri −ri+1 ≤ri/2.
Since ri+2 ≤ri/2, it follows that the divide-and-remainder operation at the heart of
Euclid’s algorithm need be performed at most 2⌈log a⌉= O(L) times. Each divide-and-
remainder operation requires O(L2) operations, so the total cost of Euclid’s algorithm is
O(L3). Finding x and y such that ax + by = gcd(a, b) incurs a minor additional cost:
O(L) substitutions are performed, at a cost of O(L2) per substitution to do the arithmetic
involved, for a total resource cost of O(L3).
Euclid’s algorithm may also be used to efﬁciently ﬁnd multiplicative inverses in mod-
ular arithmetic. This is implicit in the proof of Corollary A4.4; we now make it explicit.
Suppose a is co-prime to n, and we wish to ﬁnd a−1, modulo n. To do so, use Euclid’s
algorithm and the co-primality of a and n to ﬁnd integers x and y such that
ax + ny = 1.
(A4.13)
Note then that ax = (1 −ny) = 1(mod n), that is, x is the multiplicative inverse of a,
modulo n. Furthermore, this algorithm is computationally efﬁcient, taking only O(L3)
steps, where L is the length in bits of n.
Now that we know how to efﬁciently ﬁnd inverses in modular arithmetic, it is only a
short step to solve simple linear equations, such as
ax + b = c(mod n).
(A4.14)
Suppose a and n are co-prime. Then using Euclid’s algorithm we may efﬁciently ﬁnd the
multiplicative inverse a−1 of a, modulo n, and thus the solution to the previous equation,
x = a−1(c −b)(mod n).
(A4.15)
An important result known as the Chinese remainder theorem extends the range of
equations we may solve much further, allowing us to efﬁciently solve systems of equations
in modular arithmetic.
Theorem A4.6: (Chinese remainder theorem) Suppose m1, . . . , mn are positive
integers such that any pair mi and mj (i ̸= j) are co-prime. Then the system of
equations
x = a1(mod m1)
(A4.16)
x = a2(mod m2)
(A4.17)
. . . . . . . . .
630
Number theory
x = an(mod mn)
(A4.18)
has a solution. Moreover, any two solutions to this system of equations are equal
modulo M ≡m1m2 . . . mn.
Proof
The proof is to explicitly construct a solution to the system of equations. Deﬁne Mi ≡
M/mi and observe that mi and Mi are co-prime. It follows that Mi has an inverse
modulo mi, which we denote Ni. Deﬁne x ≡
i aiMiNi. To see that x is a solution to
the system of equations, note that MiNi = 1(mod mi) and MiNi = 0(mod mj) when
i ̸= j, so x = ai(mod mi), which demonstrates the existence of a solution.
Suppose x and x′ are both solutions to the system of equations. It follows that x−x′ =
0(mod mi) for each i, and thus mi divides x −x′ for each i. Since the mi are co-prime,
it follows that the product M = m1 . . . mn also divides x −x′, so x = x′(mod M), as
we set out to show.
Euclid’s algorithm and the Chinese remainder theorem are two of the signal triumphs of
algorithmic number theory. How ironic then that they should play a role in the sequence
of ideas leading up to the RSA cryptosystem, whose presumed security is based on
the difﬁculty of performing certain algorithmic tasks in number theory. Nevertheless,
this is indeed the case! We turn now to the number-theoretic background necessary to
understand the RSA cryptosystem. The key ideas are a famous result of classical number
theory, Fermat’s little theorem – not to be confused with Fermat’s last theorem – and
a generalization of Fermat’s little theorem due to Euler. The proof of Fermat’s little
theorem relies on the following elegant lemma.
Lemma A4.7: Suppose p is prime and k is an integer in the range 1 to p −1. Then p
divides

p
k
.
Proof
Consider the identity
p(p −1) · · · (p −k + 1) =

p
k

k(k −1) · · · 1.
(A4.19)
Since k ≥1 the left hand side (and thus the right) is divisible by p. Since k ≤p −1 the
term k(k −1) · · ·1 is not divisible by p. It follows that

p
k
 must be divisible by p.
Theorem A4.8: (Fermat’s little theorem) Suppose p is a prime, and a is any integer.
Then ap = a(mod p). If a is not divisible by p then ap−1 = 1(mod p).
Proof
The second part of the theorem follows from the ﬁrst, since if a is not divisible by p then
a has an inverse modulo p, so ap−1 = a−1ap = a−1a = 1(mod p). We prove the ﬁrst part
of the theorem for positive a (the case of non-positive a follows easily) by induction on
a. When a = 1 we have ap = 1 = a(mod p), as required. Suppose the result holds true
Modular arithmetic and Euclid’s algorithm
631
for a, that is, ap = a(mod p) and consider the case of a + 1. By the binomial expansion,
(1 + a)p =
p

k=0

p
k

ak.
(A4.20)
By Lemma A4.7, p divides

p
k
 whenever 1 ≤k ≤p −1, so all terms except the ﬁrst and
last vanish from the sum modulo p, (1 + a)p = (1 + ap)(mod p). Applying the inductive
hypothesis ap = a(mod p) we see that (1 + a)p = (1 + a)(mod p), as required.
There is a remarkable generalization of Fermat’s little theorem due to Euler, based on
the Euler ϕ function. ϕ(n) is deﬁned to be the number of positive integers less than n
which are co-prime to n. As an example, note that all positive integers less than a prime
p are co-prime to p, and thus ϕ(p) = p −1. The only integers less than pα which are not
co-prime to pα are the multiples of p: p, 2p, 3p, . . ., (pα−1 −1)p, from which we deduce
ϕ(pα) = (pα −1) −(pα−1 −1) = pα−1(p −1).
(A4.21)
Furthermore, if a and b are co-prime, then the Chinese remainder theorem can be used
to show that
ϕ(ab) = ϕ(a)ϕ(b).
(A4.22)
To see this, consider the system of equations x = xa(mod a), x = xb(mod b). Applying
the Chinese remainder theorem to this set of equations we see that there is a one-to-one
correspondence between pairs (xa, xb) such that 1 ≤xa < a, 1 ≤xb < b, gcd(xa, a) =
1, gcd(xb, b) = 1, and integers x such that 1 ≤x < ab, gcd(x, ab) = 1. There are
ϕ(a)ϕ(b) such pairs (xa, xb) and ϕ(ab) such x, from which we deduce (A4.22).
Equations (A4.21) and (A4.22) together imply a formula for ϕ(n) based on the prime
factorization of n, n = pα1
1 · · · pαk
k :
ϕ(n) =
k
/
j=1
p
αj−1
j
(pj −1).
(A4.23)
Exercise A4.10:
What is ϕ(187)?
Exercise A4.11:
Prove that
n =

d|n
ϕ(d),
(A4.24)
where the sum is over all positive divisors d of n, including 1 and n. (Hint:
Prove the result for n = pα ﬁrst, then use the multiplicative property (A4.22) of
ϕ to complete the proof.)
Fermat’s little theorem has the following beautiful generalization, due to Euler:
Theorem A4.9: Suppose a is co-prime to n. Then aϕ(n) = 1(mod n).
Proof
We ﬁrst show by induction on α that aϕ(pα) = 1(mod pα). For α = 1 the result is just
Fermat’s little theorem. Assume the result is true for α ≥1, so
aϕ(pα) = 1 + kpα,
(A4.25)
632
Number theory
for some integer k. Then by (A4.21),
aϕ(pα+1) = apα(p−1)
(A4.26)
= apϕ(pα)
(A4.27)
= (1 + kpα)p
(A4.28)
= 1 +
p

j=1

p
j

kjpjα.
(A4.29)
Using Lemma A4.7 it is easy to see that pα+1 divides every term in the sum, so
aϕ(pα+1) = 1(mod pα+1),
(A4.30)
which completes the induction. The proof of the theorem is completed by noting that for
arbitrary n = pα1
1 · · · pαm
m , aϕ(n) = 1(mod pαj
j ) for each j, as ϕ(n) is a multiple of ϕ(pαj
j ).
Applying the construction in the proof of the Chinese remainder theorem we ﬁnd that
any solution to the set of equations x = 1(mod pαj
j ) must satisfy x = 1(mod n), and thus
aϕ(n) = 1(mod n).
Deﬁne Z∗
n to be the set of all elements in Zn which have inverses modulo n, that is,
the set of all elements in Zn which are co-prime to n. Z∗
n is easily seen to form a group
of size ϕ(n) under multiplication, that is, it contains the multiplicative identity, products
of elements in Z∗
n are in Z∗
n, and Z∗
n is closed under the multiplicative inverse operation.
(For an overview of elementary group theory, see Appendix 2.) What is not so obvious is
the remarkable structure Z∗
n has when n is a power of an odd prime p, n = pα. It turns
out that Z∗
pα is a cyclic group, that is, there is an element g in Z∗
pα which generates Z∗
pα in
the sense that any other element x may be written x = gk(mod n) for some non-negative
integer k.
Theorem A4.10: Let p be an odd prime, α a positive integer. Then Z∗
pα is cyclic.
Proof
The proof of this fact is a little beyond the scope of this book. It can be found in many
texts containing any considerable amount of number theory. See for example Section 3.2
of Knuth[Knu98a], especially pages 16 through 23.
Exercise A4.12:
Verify that Z∗
n forms a group of size ϕ(n) under the operation of
multiplication modulo n.
Exercise A4.13:
Let a be an arbitrary element of Z∗
n. Show that S ≡{1, a, a2, . . .}
forms a subgroup of Z∗
n, and that the size of S is the least value of r such that
ar = 1(mod n).
Exercise A4.14:
Suppose g is a generator for Z∗
n. Show that g must have order ϕ(n).
Exercise A4.15:
Lagrange’s theorem (Theorem A2.1 on page 610) is an elementary
result of group theory stating that the size of a subgroup must divide the order
of the group. Use Lagrange’s theorem to provide an alternate proof of
Theorem A4.9, that is, show that aϕ(n) = 1(mod n) for any a ∈Z∗
n.
Reduction of factoring to order-ﬁnding
633
A4.3
Reduction of factoring to order-ﬁnding
The problem of factoring numbers on a classical computer turns out to be equivalent to
another problem, the order-ﬁnding problem. This equivalence is important as it turns
out that quantum computers are able to quickly solve the order-ﬁnding problem, and
thus can factor quickly. In this section we explain the equivalence between these two
problems, focusing on the reduction of factoring to order-ﬁnding.
Suppose N is a positive integer, and x is co-prime to N, 1 ≤x < N. The order of
x modulo N is deﬁned to be the least positive integer r such that xr = 1(mod N). The
order-ﬁnding problem is to determine r, given x and N.
Exercise A4.16:
Use Theorem A4.9 to show that the order of x modulo N must
divide ϕ(N).
The reduction of factoring to order-ﬁnding proceeds in two basic steps. The ﬁrst step is
to show that we can compute a factor of n if we can ﬁnd a non-trivial solution x ̸= ±
1(mod N) to the equation x2 = 1(mod N). The second step is to show that a randomly
chosen y co-prime to N is quite likely to have an order r which is even, and such that
yr/2 ̸= ± 1(mod N), and thus x ≡yr/2(mod N) is a solution to x2 = 1(mod N).
Theorem A4.11: Suppose N is a composite number L bits long, and x is a non-trivial
solution to the equation x2 = 1(mod N) in the range 1 ≤x ≤N, that is, neither
x = 1(mod N) nor x = N −1 = −1(mod N). Then at least one of
gcd(x −1, N) and gcd(x + 1, N) is a non-trivial factor of N that can be
computed using O(L3) operations.
Proof
Since x2 = 1(mod N), it must be that N divides x2−1 = (x+1)(x−1), and thus N must
have a common factor with one or the other of (x+1) and (x−1). But 1 < x < N −1 by
assumption, so x −1 < x + 1 < N, from which we see that the common factor can not
be N itself. Using Euclid’s algorithm we may compute gcd(x −1, N) and gcd(x + 1, N)
and thus obtain a non-trivial factor of N, using O(L3) operations.
Lemma A4.12: Let p be an odd prime. Let 2d be the largest power of 2 dividing
ϕ(pα). Then with probability exactly one-half 2d divides the order modulo pα of
a randomly chosen element of Z∗
pα.
Proof
Note that ϕ(pα) = pα−1(p−1) is even, since p is odd, and thus d ≥1. By Theorem A4.10
there exists a generator g for Z∗
pα, so an arbitrary element may be written in the form
gk(mod pα) for some k in the range 1 through ϕ(pα). Let r be the order of gk modulo
pα and consider two cases. The ﬁrst case is when k is odd. From gkr = 1(mod pα) we
deduce that ϕ(pα)|kr, and thus 2d|r, since k is odd. The second case is when k is even.
Then
gkϕ(pα)/2 =
'
gϕ(pα)(k/2
= 1k/2 = 1(mod pα).
(A4.31)
Thus r|ϕ(pα)/2 from which we deduce that 2d does not divide r.
634
Number theory
Summarizing, Z∗
pα may be partitioned into two sets of equal size: those which may
be written gk with k odd, for which 2d|r, where r is the order of gk, and those which
may be written gk with k even, for which 2d ̸ | r. Thus with probability 1/2 the integer
2d divides the order r of a randomly chosen element of Z∗
pα, and with probability 1/2 it
does not.
Theorem A4.13: Suppose N = pα1
1 · · · pαm
m
is the prime factorization of an odd
composite positive integer. Let x be chosen uniformly at random from Z∗
N, and
let r be the order of x, modulo N. Then
p(r is even and xr/2 ̸= −1(mod N)) ≥1 −1
2m .
(A4.32)
Proof
We show that
p(r is odd or xr/2 = −1(mod N)) ≤1
2m .
(A4.33)
By the Chinese remainder theorem, choosing x uniformly at random from Z∗
N is equiv-
alent to choosing xj independently and uniformly at random from Z∗
p
αj
j , and requiring
that x = xj(mod p
αj
j ) for each j. Let rj be the order of xj modulo p
αj
j . Let 2dj be the
largest power of 2 that divides rj and 2d be the largest power of 2 that divides r. We
will show that to have r odd or xr/2 = −1(mod N) it is necessary that dj takes the same
value for all values of j. The result then follows, as from Lemma A4.12 the probability
of this occurring is at most 1/2m.
The ﬁrst case we consider is when r is odd. It is easy to see that rj|r for each j, and
therefore rj is odd, so dj = 0 for all i = 1, . . . , k. The second and ﬁnal case is when r
is even and xr/2 = −1(mod N). Then xr/2 = −1(mod pαj
j ), so rj ̸ | (r/2). Since rj|r we
must have dj = d for all j.
Theorems A4.11 and A4.13 can be combined to give an algorithm which, with high
probability, returns a non-trivial factor of any composite N. All the steps in the algorithm
can be performed efﬁciently on a classical computer except (as far as is known today) an
order-ﬁnding ‘subroutine’ which is used by the algorithm. By repeating the algorithm we
may ﬁnd a complete prime factorization of N. The algorithm is summarized below.
(1) If N is even, return the factor 2.
(2) Use the algorithm of Exercise 5.17 to determine whether N = ab for integers a ≥1
and b ≥2, and if so return the factor a.
(3) Randomly choose x in the range 1 to N −1. If gcd(x, N) > 1 then return the
factor gcd(x, N).
(4) Use the order-ﬁnding subroutine to ﬁnd the order r of x, modulo N.
(5) If r is even and xr/2 ̸= −1(mod N) then compute gcd(xr/2 −1, N) and
gcd(xr/2 + 1, N), and test to see which is a non-trivial factor, returning that factor.
Otherwise, the algorithm fails.
Steps 1 and 2 of the algorithm either return a factor, or else ensure that N is an odd
integer with more than one prime factor. These steps may be performed using O(1) and
O(L3) operations, respectively. Step 3 either returns a factor, or produces a randomly
Continued fractions
635
chosen element x of Z∗
N. Step 4 calls the order-ﬁnding subroutine, computing the order
r of x, modulo N. Step 5 completes the algorithm, since Theorem A4.13 guarantees
that with probability at least one-half r will be even and xr/2 ̸= −1(mod N), and then
Theorem A4.11 guarantees that either gcd(xr/2−1, N) or gcd(xr/2+1, N) is a non-trivial
factor of N.
Exercise A4.17: (Reduction of order-ﬁnding to factoring)
We have seen that an
efﬁcient order-ﬁnding algorithm allows us to factor efﬁciently. Show that an
efﬁcient factoring algorithm would allow us to efﬁciently ﬁnd the order modulo
N of any x co-prime to N.
A4.4
Continued fractions
There are many remarkable connections between the continuum of real numbers and
the integers. One such connection is the beautiful theory of continued fractions. In this
section we develop a few elements of the theory of continued fractions, elements crucial
to the application of the fast quantum algorithms for order-ﬁnding and factoring detailed
in Chapter 5.
As an example of a continued fraction, consider the number s deﬁned by the expression
s ≡
1
2 +
1
2+
1
2+···
.
(A4.34)
Informally, note that s = 1/(2+s), from which it is easy to satisfy oneself that s =
√
2−1.
The idea of the continued fractions method is to describe real numbers in terms of integers
alone, using expressions such as (A4.34). A ﬁnite simple continued fraction is deﬁned
by a ﬁnite collection a0, . . . , aN of positive integers,
[a0, . . . , aN] ≡a0 +
1
a1 +
1
a2+
1
···+
1
aN
.
(A4.35)
We deﬁne the nth convergent (0 ≤n ≤N) to this continued fraction to be [a0, . . . , an].
Theorem A4.14: Suppose x is a rational number greater than or equal to one. Then x
has a representation as a continued fraction, x = [a0, . . . , aN], which may be
found by the continued fractions algorithm.
Proof
The continued fractions algorithm is best understood by example. Suppose we are trying
to decompose 31/13 as a continued fraction. The ﬁrst step of the continued fractions
algorithm is to split 31/13 into its integer and fractional part,
31
13 = 2 + 5
13.
(A4.36)
Next we invert the fractional part, obtaining
31
13 = 2 + 1
13
5
.
(A4.37)
636
Number theory
These steps – split then invert – are now applied to 13/5, giving
31
13 = 2 +
1
2 + 3
5
= 2 +
1
2 + 1
5
3
.
(A4.38)
Next we split and invert 5/3:
31
13 = 2 +
1
2 +
1
1+ 2
3
= 2 +
1
2 +
1
1+ 1
3
2
.
(A4.39)
The decomposition into a continued fraction now terminates, since 3/2 = 1 + 1/2 may
be written with a 1 in the numerator without any need to invert, giving a ﬁnal continued
fraction representation of 31/13 as
31
13 = 2 +
1
2 +
1
1+
1
1+ 1
2
.
(A4.40)
It’s clear that the continued fractions algorithm terminates after a ﬁnite number of ‘split
and invert’ steps for any rational number, since the numerators which appear (31, 3, 2, 1
in the example) are strictly decreasing. How quickly does this termination occur? We’ll
come back to that question shortly.
The theorem above has been stated for x ≥1; however, in practice it is convenient to
relax the requirement that a0 to be positive and allow it to be any integer, which results in
the restriction x ≥1 becoming superﬂuous. In particular, if x is in the range 0 through
1 as occurs in applications to quantum algorithms, then the continued fraction expansion
has a0 = 0.
The continued fractions algorithm provides an unambiguous method for obtaining a
continued fraction expansion of a given rational number. The only possible ambiguity
comes at the ﬁnal stage, because it is possible to split an integer in two ways, either
an = an, or as an = (an −1) + 1/1, giving two alternate continued fraction expansions.
This ambiguity is actually useful, since it allows us to assume without loss of generality
that the continued fraction expansion of a given rational number has either an odd or
even number of convergents, as desired.
Exercise A4.18:
Find the continued fraction expansion for x = 19/17 and x = 77/65.
Theorem A4.15: Let a0, . . . , aN be a sequence of positive numbers. Then
[a0, . . . , an] = pn
qn
,
(A4.41)
where pn and qn are real numbers deﬁned inductively by p0 ≡a0, q0 ≡1 and
p1 ≡1 + a0a1, q1 ≡a1, and for 2 ≤n ≤N,
pn ≡anpn−1 + pn−2
(A4.42)
qn ≡anqn−1 + qn−2.
(A4.43)
In the case where aj are positive integers, so too are the pj and qj.
Continued fractions
637
Proof
We induct on n. The result is easily checked directly for the cases n = 0, n = 1, 2. By
deﬁnition, for n ≥3,
[a0, . . . , an] = [a0, . . . , an−2, an−1 + 1/an].
(A4.44)
Applying the inductive hypothesis, let ˜pj/ ˜qj be the sequence of convergents associated
with the continued fraction on the right hand side:
[a0, . . . , an−2, an−1 + 1/an] = ˜pn−1
˜qn−1
.
(A4.45)
It is clear that ˜pn−3 = pn−3, ˜pn−2 = pn−2 and ˜qn−3 = qn−3, ˜qn−2 = qn−2, so
˜pn−1
˜qn−1
= (an−1 + 1/an)pn−2 + pn−3
(an−1 + 1/an)qn−2 + qn−3
(A4.46)
= pn−1 + pn−2/an
qn−1 + qn−2/an
.
(A4.47)
Multiplying top and bottom of the right hand side by an we see that
˜pn−1
˜qn−1
= pn
qn
.
(A4.48)
Combining Equations (A4.48), (A4.45) and (A4.44) gives
[a0, . . . , an] = pn
qn
,
(A4.49)
as required.
Exercise A4.19:
Show that qnpn−1 −pnqn−1 = (−1)n for n ≥1. Use this fact to
conclude that gcd(pn, qn) = 1. (Hint: Induct on n.)
How many values of an must be determined to obtain a continued fraction expansion
for a rational number x = p/q > 1, where p and q are co-prime? Suppose a0, . . . , aN are
positive integers. From the deﬁnition of pn and qn it follows that pn and qn are increasing
sequences. Therefore pn = anpn−1 + pn−2 ≥2pn−2 and similarly qn ≥2qn−2, from
which it follows that pn, qn ≥2⌊n/2⌋. Thus the 2⌊N/2⌋≤q ≤p, and so N = O(log(p)).
It follows that if x = p/q is a rational number, p and q are L bit integers, then the
continued fraction expansion for x can be computed using O(L3) operations – O(L)
‘split and invert’ steps, each using O(L2) gates for elementary arithmetic.
Theorem A4.16: Let x be a rational number and suppose p/q is a rational number
such that
++++
p
q −x
++++ ≤
1
2q2 .
(A4.50)
Then p/q is a convergent of the continued fraction for x.
Proof
638
Number theory
Let p/q = [a0, . . . , an] be the continued fraction expansion for p/q, and deﬁne pj, qj as
in Theorem A4.15, so that pn/qn = p/q. Deﬁne δ by the equation
x ≡pn
qn
+
δ
2q2
n
,
(A4.51)
so |δ| < 1. Deﬁne λ by the equation
λ ≡2
qnpn−1 −pnqn−1
δ

−qn−1
qn
.
(A4.52)
The reason we deﬁne λ this way is because with a little algebra we can see that it satisﬁes
the equation
x = λpn + pn−1
λqn + qn−1
,
(A4.53)
and therefore x = [a0, . . . , an, λ]. Choosing n even we see from Exercise A4.19 that
λ = 2
δ −qn−1
qn
.
(A4.54)
By the increasing property of qn it follows that
λ = 2
δ −qn−1
qn
> 2 −1 > 1 .
(A4.55)
Therefore λ is a rational number greater than 1, and so has a simple ﬁnite continued
fraction, λ = [b0, . . . , bm], and so x = [a0, . . . , an, b0, . . . , bm] is a simple ﬁnite continued
fraction for x with p/q as a convergent.
Problem 4.1: (Prime number estimate)
Let π(n) be the number of prime
numbers which are less than n. A difﬁcult-to-prove result known as the prime
number theorem asserts that limn→∞π(n) log(n)/n = 1 and thus
π(n) ≈n/ log(n). This problem gives a poor man’s version of the prime number
theorem which gives a pretty good lower bound on the distribution of prime
numbers.
(1) Prove that n ≤log

2n
n
.
(2) Show that
log

2n
n

≤

p≤2n
?log(2n)
log p
@
log p,
(A4.56)
where the sum is over all primes p less than or equal to 2n.
(3) Use the previous two results to show that
π(2n) ≥
n
log(2n).
(A4.57)
History and further reading
639
History and further reading
There are many excellent books on number theory. We have made considerable use of
the excellent book by Koblitz[Kob94], which combines much introductory material about
number theory, algorithms, and cryptography all in one location. A similar combina-
tion forming a small part of a much more comprehensive presentation oriented towards
algorithms may be found in Chapter 33 of Cormen, Leiserson and Rivest[CLR90]. Our
discussion of continued fractions is based upon Chapter 10 of the classic text on number
theory by Hardy and Wright[HW60]. Problem 4.1 is adapted from Papadimitriou[Pap94].
Appendix 5: Public key cryptography and the RSA
cryptosystem
Cryptography is the art of enabling two parties to communicate in private. For example,
a consumer wishing to make a purchase on the internet wants to transmit their credit card
number over the internet in such a way that only the company they are purchasing from
gains access to the number. Rather more ominously, in wartime each of the warring parties
wants the means to carry on private communication. To achieve privacy a cryptographic
protocol or cryptosystem is used. Effective cryptosystems make it easy for parties who
wish to communicate to do so, but make it very difﬁcult for third parties to ‘eavesdrop’
on the contents of the conversation.
A particularly important class of cryptosystems are the public key cryptosystems. The
basic idea of public key cryptography is illustrated by the analogy depicted in Figure A5.1.
Alice sets up a mailbox with the property that anybody can send her mail, by putting it
into the mailbox, but only she can retrieve mail out of the mailbox. To achieve this she
gives the mailbox two doors. On top of the mailbox is a locked trap door. Any person
able to open the trap door can drop mail into the box. However, the chute from the trap
door into the box is one way, so they can’t reach into the box and ﬁsh mail out. Alice
makes the key to the trapdoor freely available to the public – it is a public key – so that
she can receive mail from absolutely anybody. On the front of the mail box is a second
door, from which mail already inside the box can be retrieved. Alice is in possession of
the sole key for that door; it is her own secret key. This arrangement – involving two
keys, one secret and one public – allows anybody in the world to communicate with Alice
while maintaining privacy.
Public key cryptosystems operate according to similar principles. Suppose Alice wishes
to receive messages using a public key cryptosystem. She must ﬁrst generate two cryp-
tographic keys, one a public key, P, the other a secret key, S. The exact nature of these
keys depends on the details of the cryptosystem being used. Some cryptosystems use
simple objects like numbers as keys, while other cryptosystems use much more compli-
cated mathematical objects, like elliptic curves, as keys. Once Alice has generated her
keys, she publishes the public key so that anybody can obtain access to the key.
Now suppose Bob wishes to send Alice a private message. He ﬁrst obtains a copy
of Alice’s public key P, and then encrypts the message he wishes to send Alice, using
Alice’s public key to perform the encryption. Exactly how the encryption transformation
is performed depends on the details of the cryptosystem in use. The key point is that in
order to be secure against eavesdropping the encryption stage needs to be very difﬁcult
to reverse, even making use of the public key used to encrypt the message in the ﬁrst
place! It’s like the trap door for mail – what you can put in you can’t take back out, even
if you have the key to the trap door. Since the public key and the encoded message is the
only information available to an eavesdropper it won’t be possible for the eavesdropper to
recover the message. Alice, however, has an additional piece of information not available
Public key cryptography and the RSA cryptosystem
641
Figure A5.1. The key ideas of public key cryptography, illustrated in more familiar terms. Essentially the same
scheme is implemented by the Post Ofﬁce in many countries.
to an eavesdropper, the secret key, S. The secret key determines a second transformation,
this time on the encrypted message. This transformation is known as decryption, and is
inverse to encryption, allowing Alice to recover the original message.
In an ideal world that is how public key cryptography would work. Unfortunately, at
the time of writing it is not known whether there are any such secure schemes for doing
public key cryptography. There do exist several schemes which are widely believed to
be secure, and which are in common use for applications such as internet commerce,
but wide belief is not equivalent to a proof of security. The reason these schemes are
believed to be secure is because so much effort has been devoted to ﬁnding a means for
breaking these schemes (without success!), a sort of proof by attrition. The most widely
used of these public key cryptosystems is the RSA cryptosystem, named RSA for the
initials of its creators, Rivest, Shamir, and Adleman. The presumed security of the RSA
cryptosystem is based, as we shall now see, on the apparent difﬁculty of factoring on a
classical computer. Understanding RSA requires a little background in number theory,
which is covered in Appendix 4, notably Sections A4.1 and A4.2.
Suppose Alice wishes to create public and private keys for use with the RSA
cryptosystem. She uses the following procedure:
(1) Select two large prime numbers, p and q.
(2) Compute the product n ≡pq.
(3) Select at random a small odd integer, e, that is relatively prime to
ϕ(n) = (p −1)(q −1).
(4) Compute d, the multiplicative inverse of e, modulo ϕ(n).
(5) The RSA public key is the pair P = (e, n). The RSA secret key is the pair
S = (d, n).
642
Public key cryptography and the RSA cryptosystem
Suppose a second party, Bob, wishes to encrypt a message M to send to Alice, using the
public key (e, n). We assume the message M has only ⌊log n⌋bits, as longer messages may
be encrypted by breaking M up into blocks of at most ⌊log n⌋bits and then encrypting
the blocks separately. The encryption procedure for a single block is to compute:
E(M) = M e(mod n).
(A5.1)
E(M) is the encrypted version of the message M, which Bob transmits to Alice. Alice
can quickly decrypt the message using her secret key S = (d, n), simply by raising the
encrypted message to the dth power:
E(M) →D(E(M)) = E(M)d(mod n).
(A5.2)
For the decryption to be successful we need D(E(M)) = M(mod n). To see that this is
the case, note that by construction ed = 1(mod ϕ(n)) and thus ed = 1 + kϕ(n) for some
integer k. The proof now proceeds by considering two different cases. In the ﬁrst case,
M is co-prime to n. By Euler’s generalization of Fermat’s little theorem, Theorem A4.9,
it follows that M kϕ(n) = 1(mod n) and thus,
D(E(M)) = E(M)d(mod n)
(A5.3)
= M ed(mod n)
(A5.4)
= M 1+kϕ(n)(mod n)
(A5.5)
= M · M kϕ(n)(mod n)
(A5.6)
= M(mod n),
(A5.7)
which establishes that the decryption is successful when M is co-prime to n. Suppose
next that M is not co-prime to n, so that one or both of p and q divide M. To be speciﬁc,
we consider the case where p divides M and q does not divide M; the other possible
cases requre only minor modiﬁcations. Because p divides M we have M = 0(mod p) and
thus M ed = 0 = M(mod p). Because q does not divide M we have M q−1 = 1(mod q) by
Fermat’s little theorem, and thus M ϕ(n) = 1(mod q), since ϕ(n) = (p −1)(q −1). Using
ed = 1 + kϕ(n) we see that M ed = M(mod q). By the Chinese remainder theorem it
follows that we must have M ed = M(mod n), and thus the decryption is also successful
when M is not co-prime to n.
Exercise A5.1:
Written examples of the application of RSA tend to be rather opaque.
It’s better to work through an example yourself. Encode the word ‘QUANTUM’
(or at least the ﬁrst few letters!), one letter at a time, using p = 3 and q = 11.
Choose appropriate values for e and d, and use a representation of English text
involving 5 bits per letter.
How efﬁciently can RSA be implemented? There are two implementation issues to
be considered. First is the generation of public and private keys for the cryptosystem. If
this can’t be done quickly then RSA won’t be good for much. The main bottleneck is
the generation of the prime numbers p and q. The way this is attacked is to randomly
select a number of the desired length, and then to apply a primality test to determine
if the number is, in fact, prime. Fast primality tests such as the Miller–Rabin test can
be used to determine whether a number is prime using roughly O(L3) operations, where
L is the desired size of the cryptographic key. If the number is found to be composite
Public key cryptography and the RSA cryptosystem
643
then we simply repeat the procedure until a prime is found. The prime number theorem
(see Problem 4.1) implies that the probability of any given number being prime is about
1/ log(2L) = 1/L, so with high probability O(L) trials are required to obtain a prime
number, for a total cost of O(L4) operations to do key generation.
The second issue in the implementation of RSA is the efﬁciency of the encryption and
decryption transformations. These are accomplished by modular exponentiation, which
we know can be done efﬁciently using O(L3) operations – see Box 5.2 on page 228. Thus
all the operations required to use the RSA cryptosystem can be done quite quickly on a
classical computer, and in practice modest computing power can quite easily cope with
keys up to a few thousand bits in length.
How can RSA be broken? We describe two methods by which one might hope to break
RSA, one based on order-ﬁnding, the other based on factoring. Suppose Eve receives
an encrypted message M e(mod n), and knows the public key (e, n) used to encrypt the
message. Suppose she can ﬁnd the order of the encrypted message, that is, she can ﬁnd
the smallest positive integer r such that (M e)r = 1(mod n). (Without loss of generality,
we may suppose such an order exists, that is, M e is co-prime to n. If this is not the
case, then M e(mod n) and n have a common factor that may be extracted by Euclid’s
algorithm, which would allow us to break RSA, as in the second method described below.)
Then Exercise A4.16 implies that r divides ϕ(n). Since e is co-prime to ϕ(n) it must
also be co-prime to r, and thus has a multiplicative inverse modulo r. Let d′ be such
a multiplicative inverse, so ed′ = 1 + kr for some integer k. Then Eve can recover the
original message M by raising the encrypted message to the d′th power:
(M e)d′(mod n) = M 1+kr(mod n)
(A5.8)
= M · M kr(mod n)
(A5.9)
= M(mod n).
(A5.10)
It is interesting that Eve never actually learns the secret key (d, n); she only learns (d′, n).
Of course, d′ is closely related to d, since d′ is the inverse of e modulo r, d is the inverse of
e modulo ϕ(n), and r divides ϕ(n). Nevertheless, this example shows that it is possible to
break RSA without necessarily determining the exact value of the secret key. Of course,
this method only works if Eve has an efﬁcient method for order-ﬁnding, and no such
method is currently known for a classical computer. On a quantum computer, however,
order-ﬁnding can be accomplished efﬁciently, as described in Section 5.3.1, and thus
RSA can be broken.
Exercise A5.2:
Show that d is also an inverse of e modulo r, and thus d = d′(mod r).
A second method for breaking RSA allows one to determine the secret key completely.
Suppose Eve could factor n = pq, extracting p and q, and thus giving a means for
efﬁciently computing ϕ(n) = (p −1)(q −1). It is then an easy matter for Eve to compute
d, the inverse of e modulo ϕ(n), and thus completely determine the secret key (d, n). So,
if factoring large numbers were easy then it would be easy to break RSA.
The presumed security of RSA rests on the fact that these attacks rely on having
algorithms to solve problems which are believed (but not known) to be intractable on
a classical computer, the order-ﬁnding and factoring problems. Unfortunately, it’s not
even known to be the case that RSA is secure if these problems are hard. It could be that
these problems really are difﬁcult, yet there is some other way of breaking RSA. Despite
644
Public key cryptography and the RSA cryptosystem
these caveats, more than two decades of attempts to break RSA have resulted in failure,
and it is widely believed that RSA is secure against attacks by classical computers.
Problem 5.1:
Write a computer program for performing encryption and decryption
using the RSA algorithm. Find a pair of 20 bit prime numbers and use them to
encrypt a 40 bit message.
History and further reading
Public key cryptosystems were invented by Difﬁe and Hellman in 1976[DH76], and in-
dependently by Merkle at about the same time, although his work was not published
until 1978[Mer78]. The RSA cryptosystem was invented shortly after by Rivest, Shamir,
and Adleman[RSA78]. In 1997 it was disclosed that these ideas – public key cryptography,
the Difﬁe-Hellman and RSA cryptosystems – were actually invented in the late 1960s and
early 1970s by researchers working at the British intelligence agency GCHQ. An account
of this work may be found at ‘http://www.cesg.gov.uk/about/nsecret/’. Primal-
ity tests such as the Miller–Rabin and Solovay–Strassen tests are described in Koblitz’s
excellent book[Kob94] on number theory and cryptography, which contains a wealth of
additional material on public key cryptography. These primality tests were two of the
earliest indicators that randomized algorithms may be more efﬁcient for some purposes
than deterministic algorithms. The Solovay–Strassen algorithm is due to Solovay and
Strassen[SS76], and the Miller–Rabin test is due jointly to Miller[Mil76] and Rabin[Rab80].
Appendix 6: Proof of Lieb’s theorem
One of the most important and useful results in quantum information theory is the
strong subadditivity inequality for von Neumann entropies. This states that for a trio
of quantum systems, A, B, C,
S(A, B, C) + S(B) ≤S(A, B) + S(B, C) .
(A6.1)
Unfortunately, no transparent proof of strong subadditivity is known. Chapter 11 presents
a relatively simple proof, based upon a deep mathematical result known as Lieb’s theorem.
In this appendix we prove Lieb’s theorem. We begin with a few simple notations and
deﬁnitions.
Suppose f(A, B) is a real-valued function of two matrices, A and B. Then f is said
to be jointly concave in A and B if for all 0 ≤λ ≤1,
f(λA1 + (1 −λ)A2, λB1 + (1 −λ)B2) ≥λf(A1, B1) + (1 −λ)f(A2, B2). (A6.2)
For matrices A and B, we say A ≤B if B −A is a positive matrix. We say A ≥B if
B ≤A. Let A be an arbitrary matrix. We deﬁne the norm of A by
∥A∥≡max
⟨u|u⟩=1 |⟨u|A|u⟩|.
(A6.3)
In our proof of Lieb’s theorem we will have occasion to use the following easily veriﬁed
observations:
Exercise A6.1: (≤is preserved under conjugation)
If A ≤B, show that
XAX† ≤XBX† for all matrices X.
Exercise A6.2:
Prove that A ≥0 if and only if A is a positive operator.
Exercise A6.3: (≤is a partial order)
Show that the relation ≤is a partial order on
operators – that is, it is transitive (A ≤B and B ≤C implies A ≤C),
asymmetric (A ≤B and B ≤A implies A = B), and reﬂexive (A ≤A).
Exercise A6.4:
Suppose A has eigenvalues λi. Deﬁne λ to be the maximum of the set
|λi|. Prove that:
(1) ∥A∥≥λ.
(2) When A is Hermitian, ∥A∥= λ.
(3) When
A =
 1
0
1
1

,
(A6.4)
∥A∥= 3/2 > 1 = λ.
646
Proof of Lieb’s theorem
Exercise A6.5: (AB and BA have the same eigenvalues)
Prove that AB and
BA have the same eigenvalues. (Hint: For invertible A, show that
det(xI −AB) = det(xI −BA), and thus the eigenvalues of AB and BA are the
same. By continuity this holds even when A is not invertible.)
Exercise A6.6:
Suppose A and B are such that AB is Hermitian. Using the previous
two observations show that ∥AB∥≤∥BA∥.
Exercise A6.7:
Suppose A is positive. Show that ∥A∥≤1 if and only if A ≤I.
Exercise A6.8:
Let A be a positive matrix. Deﬁne a superoperator (linear operator on
matrices) by the equation A(X) ≡AX. Show that A is positive with respect to
the Hilbert–Schmidt inner product. That is, for all X, tr(X†A(X)) ≥0.
Similarly, show that the superoperator deﬁned by A(X) ≡XA is positive with
respect to the Hilbert–Schmidt inner product on matrices.
With these results in hand, we are now in a position to state and prove Lieb’s theorem.
Theorem A6.1: (Lieb’s theorem) Let X be a matrix, and 0 ≤t ≤1. Then the
function
f(A, B) ≡tr(X†AtXB1−t)
(A6.5)
is jointly concave in positive matrices A and B.
Lieb’s theorem is an easy corollary of the following lemma:
Lemma A6.2: Let R1, R2, S1, S2, T1, T2 be positive operators such that
0 = [R1, R2] = [S1, S2] = [T1, T2], and
R1 ≥S1 + T1
(A6.6)
R2 ≥S2 + T2.
(A6.7)
Then for all 0 ≤t ≤1,
Rt
1R1−t
2
≥St
1S1−t
2
+ T t
1 T 1−t
2
(A6.8)
is true as a matrix inequality.
Proof
We begin by proving the result for t = 1/2, and then use this to establish the result for
general t. It will be convenient to assume that R1 and R2 are invertible, and it is left as
an exercise to make the minor technical modiﬁcations to the proof necessary to establish
the result when this is not the case.
Let |x⟩and |y⟩be any two vectors. Applying the Cauchy–Schwarz inequality twice
and performing some straightforward manipulations, we have
|⟨x|(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)|y⟩|
≤|⟨x|S1/2
1
S1/2
2
|y⟩| + |⟨x|T 1/2
1
T 1/2
2
|y⟩|
(A6.9)
≤∥S1/2
1
|x⟩∥∥S1/2
2
|y⟩∥+ ∥T 1/2
1
|x⟩∥∥T 1/2
2
|y⟩∥
(A6.10)
≤
)'
∥S1/2
1
|x⟩∥2 + ∥T 1/2
1
|x⟩∥2
( '
∥S1/2
2
|y⟩∥2 + ∥T 1/2
2
|y⟩∥2
(
(A6.11)
Proof of Lieb’s theorem
647
=

⟨x|(S1 + T1)|x⟩⟨y|(S2 + T2)|y⟩.
(A6.12)
By hypothesis, S1 + T1 ≤R1 and S2 + T2 ≤R2, so
|⟨x|(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)|y⟩| ≤

⟨x|R1|x⟩⟨y|R2|y⟩.
(A6.13)
Let |u⟩be any unit vector. Then applying (A6.13) with |x⟩≡R−1/2
1
|u⟩and |y⟩≡
R−1/2
2
|u⟩gives
⟨u|R−1/2
1
(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)R−1/2
2
|u⟩
≤

⟨u|R−1/2
1
R1R−1/2
1
|u⟩⟨u|R−1/2
2
R2R−1/2
2
|u⟩
(A6.14)
=

⟨u|u⟩⟨u|u⟩= 1.
(A6.15)
Thus
∥R−1/2
1
(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)R−1/2
2
∥≤1.
(A6.16)
Deﬁne
A ≡R−1/4
1
R−1/4
2
(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)R−1/2
2
(A6.17)
B ≡R1/4
2 R−1/4
1
.
(A6.18)
Note that AB is Hermitian, so by Exercise A6.6 on page 646,
∥R−1/4
1
R−1/4
2
(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)R−1/4
2
R−1/4
1
∥
= ∥AB∥≤∥BA∥
(A6.19)
= ∥R−1/2
1
(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)R−1/2
2
∥
(A6.20)
≤1,
(A6.21)
where the last inequality is just (A6.16). AB is a positive operator, so by Exercise A6.7
on page 646 and the previous inequality,
R−1/4
1
R−1/4
2
(S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
)R−1/4
2
R−1/4
1
≤I.
(A6.22)
Finally, by Exercise A6.1 on page 645, and the commutativity of R1 and R2,
S1/2
1
S1/2
2
+ T 1/2
1
T 1/2
2
≤R1/2
1 R1/2
2 ,
(A6.23)
which establishes that (A6.8) holds for t = 1/2.
Let I be the set of all t such that (A6.8) holds. By inspection, we see that 0 and 1
are elements of I, and we have just shown that 1/2 is an element of I. We now use the
t = 1/2 case to prove the result for any t such that 0 ≤t ≤1. Suppose μ and η are any
two elements of I, so that
Rμ
1 R1−μ
2
≥Sμ
1 S1−μ
2
+ T μ
1 T 1−μ
2
(A6.24)
Rη
1 R1−η
2
≥Sη
1 S1−η
2
+ T η
1 T 1−η
2
.
(A6.25)
These inequalities are of the form (A6.6) and (A6.7) for which the t = 1/2 case has
already been proved. Using the t = 1/2 result we see that
'
Rμ
1 R1−μ
2
(1/2 '
Rη
1 R1−η
2
(1/2
≥
'
Sμ
1 S1−μ
2
(1/2 '
Sη
1 S1−η
2
(1/2
+
'
T μ
1 T 1−μ
2
(1/2 '
T η
1 T 1−η
2
(1/2
. (A6.26)
648
Proof of Lieb’s theorem
Using the commutativity assumptions 0 = [R1, R2] = [S1, S2] = [T1, T2], we see that for
ν ≡(μ + η)/2,
Rν
1 R1−ν
2
≥Sν
1 S1−ν
2
+ T ν
1 T 1−ν
2
.
(A6.27)
Thus whenever μ and η are in I, so is (μ + η)/2. Since 0 and 1 are in I, it is easy to see
that any number t between 0 and 1 with a ﬁnite binary expansion must be in I. Thus I is
dense in [0, 1]. The result now follows from the continuity in t of the conclusion, (A6.8).
The proof of Lieb’s theorem is a simple application of Lemma A6.2. The clever idea
that makes this possible is to choose the operators in Lemma A6.2 to be superoperators
– linear maps on operators. These will be chosen in such a way as to be positive with
respect to the Hilbert–Schmidt inner product (A, B) ≡tr(A†B).
Proof
(Lieb’s theorem)
Let 0 ≤λ ≤1 and deﬁne superoperators S1, S2, T1, T2, R1, R2 as follows:
S1(X) ≡λA1X
(A6.28)
S2(X) ≡λXB1
(A6.29)
T1(X) ≡(1 −λ)A2X
(A6.30)
T2(X) ≡(1 −λ)XB2
(A6.31)
R1 ≡S1 + T1
(A6.32)
R2 ≡S2 + T2.
(A6.33)
Observe that S1 and S2 commute, as do T1 and T2, and R1 and R2. Recall Exercise A6.8
on page 646, that all these operators are positive with respect to the Hilbert–Schmidt
inner product. By Lemma A6.2,
Rt
1R1−t
2
≥St
1S1−t
2
+ T t
1 T 1−t
2
.
(A6.34)
Using the Hilbert–Schmidt inner product to take the X·X matrix element of the previous
inequality gives
tr

X† (λA1 + (1 −λ)A2)t X (λB1 + (1 −λ)B2)1−t	
≥tr
X†(λA1)tX(λB1)1−t + tr
X†((1 −λ)A2)tX((1 −λ)B2)1−t (A6.35)
= λtr(X†At
1XB1−t
1
) + (1 −λ)tr(X†At
2XB1−t
2
)
,
(A6.36)
which is the desired statement of joint concavity.
History and further reading
The history of Lieb’s theorem is tied up with the proof of the strong subadditivity
inequality for quantum entropies, and may be found together with the history of the
proof of that inequality in the ‘History and further reading’ for Chapter 11.
Bibliography
Citations with ‘arXive e-print quant-ph/xxxxxxx’ designations are available on the in-
ternet at http://www.arXiv.org
[ABO97]
D. Aharonov and M. Ben-Or. Fault tol-
erant computation with constant error. In
Proceedings of the Twenty-Ninth An-
nual ACM Symposium on the Theory
of Computing, pages 176–188, 1997.
[ABO99]
D. Aharonov and M. Ben-Or. Fault-
tolerant quantum computation with con-
stant error rate. SIAM J. Comp., page
to appear, 1999. arXive e-print quant-
ph/9906129.
[ABOIN96] D. Aharonov, M. Ben-Or, R. Impagli-
azzo, and N. Nisan. Limitations of noisy
reversible computation. arXive e-print
quant-ph/9611028, 1996.
[ADH97]
L.
Adleman,
J.
Demarrais,
and
M. A. Huang. Quantum computability.
SIAM J. Comp., 26(5):1524–1540, 1997.
[Adl94]
L. M. Adleman. Molecular computation
of solutions to combinatorial problems.
Science, 266:1021, 1994.
[Adl98]
L. M. Adleman. Computing with DNA.
Sci. Am., 279:54–61, Aug. 1998.
[AE75]
L. Allen and J. H. Eberly. Optical Res-
onance and Two-level Atoms. Dover,
New York, 1975.
[Aha99a]
D. Aharonov. Noisy Quantum Compu-
tation. Ph.D. thesis, The Hebrew Uni-
vesity, Jerusalem, 1999.
[Aha99b]
D. Aharonov. Quantum computation. In
D. Stauffer, editor, Annual Reviews of
Computational Physics VI. World Sci-
entiﬁc, Singapore, 1999.
[AKN98]
D. Aharonov, A. Kitaev, and N. Nisan.
Quantum circuits
with
mixed
states.
STOC
1997, 1998.
arXive
e-print
quant-ph/9806029.
[AL70]
H. Araki and E. H. Lieb. Entropy inequal-
ities. Comm. Math. Phys., 18:160–170,
1970.
[AL97]
D. S. Abrams and S. Lloyd. Simula-
tion of many-body Fermi systems on a
quantum computer. Phys. Rev. Lett.,
79(13):2586–2589, 1997. arXive e-print
quant-ph/9703054.
[AL99]
A. Ashikhmin and S. Lytsin. Upper
bounds on the size of quantum codes.
IEEE Trans. Inf. Theory, 45(4):1206–
1215, 1999.
[Alb83]
P. M. Alberti. A note on the transition-
probability over c-* algebras. Lett. in
Math. Phys., 7(1):25–32, 1983.
[Amb00]
A. Ambainis. Quantum lower bounds by
quantum arguments.
arXive e-print
quant-ph/0002066, 2000.
[And79]
T. Ando. Concavity of certain maps on
positive deﬁnite matrices and applications
to Hadamard products. Linear Algebra
Appl., 26:203–241, 1979.
[Ash97]
A. Ashikhmin. Remarks on bounds for
quantum codes. arXive e-print quant-
ph/9705037, 1997.
[Bar78]
E. Barton. A reversible computer us-
ing conservative logic. Unpublished MIT
6.895 term paper, 1978.
[BB84]
C. H. Bennett and G. Brassard. Quan-
tum cryptography: Public key distribution
and coin tossing. In Proceedings of IEEE
International Conference on Computers,
Systems and Signal Processing, pages
175–179, IEEE, New York, 1984. Banga-
lore, India, December 1984.
[BBB+92]
C. H. Bennett, F. Bessette, G. Brassard,
L. Salvail, and J. Smolin. Experimen-
tal quantum cryptography. J. Cryptology,
5:3–28, 1992.
[BBB+98]
E.
Biham,
M.
Boyer,
G.
Brassard,
J. van de Graaf, and T. Mor. Security of
quantum key distribution against all col-
lective attacks.
arXive e-print quant-
ph/9801022, 1998.
650
Bibliography
[BBBV97]
C. H. Bennett, E. Bernstein, G. Brassard,
and U. Vazirani. Strengths and weak-
nesses of quantum computing. SIAM J.
Comput., 26(5):1510–1523, 1997. arXive
e-print quant-ph/9701001.
[BBC+93]
C. H. Bennett, G. Brassard, C. Cr´epeau,
R. Jozsa, A. Peres, and W. Wootters. Tele-
porting an unknown quantum state via
dual classical and EPR channels. Phys.
Rev. Lett., 70:1895–1899, 1993.
[BBC+95]
A. Barenco, C. H. Bennett, R. Cleve,
D. P. DiVincenzo, N. Margolus, P. Shor,
T. Sleator, J. Smolin, and H. Weinfurter.
Elementary gates for quantum computa-
tion. Phys. Rev. A, 52:3457–3467, 1995.
arXive e-print quant-ph/9503016.
[BBC+98]
R.
Beals,
H.
Buhrman,
R.
Cleve,
M. Mosca, and R. de Wolf. Quantum
lower bounds by polynomials. In Pro-
ceedings of the 39th Annual Sympo-
sium on Foundations of Computer Sci-
ence (FOCS’98), pages 352–361, IEEE,
Los Alamitos, California, November 1998.
arXive e-print quant-ph/9802049.
[BBCM95]
C. H. Bennett, G. Brassard, C. Cr´epeau,
and U. M. Maurer. Generalized privacy
ampliﬁcation. IEEE Trans. Inf. Theory,
41:1915–1923, 1995.
[BBE92]
C. H. Bennett, G. Brassard, and A. K. Ek-
ert. Quantum cryptography. Sci. Am.,
267(4):50, Oct. 1992.
[BBHT98]
M. Boyer, G. Brassard, P. Høyer, and
A. Tapp. Tight bounds on quantum
searching. Fortsch. Phys. – Prog. Phys.,
46(4–5):493–505, 1998.
[BBM+98]
D. Boschi, S. Branca, F. D. Martini,
L. Hardy, and S. Popescu. Experimen-
tal realization of teleporting an unknown
pure quantum state via dual classical and
Einstein-Podolski-Rosen channels. Phys.
Rev. Lett., 80:1121–1125, 1998. arXive
e-print quant-ph/9710013.
[BBP+96]
C. H. Bennett, G. Brassard, S. Popescu,
B.
Schumacher,
J.
A.
Smolin,
and
W. K. Wootters. Puriﬁcation of noisy
entanglement and faithful teleportation
via noisy channels. Phys. Rev. Lett.,
76:722, 1996. arXive e-print quant-
ph/9511027.
[BBPS96]
C.
H.
Bennett,
H.
J.
Bernstein,
S.
Popescu,
and
B.
Schumacher.
Concentrating
partial
entanglement
by
local
operations.
Phys.
Rev.
A,
53(4):2046–2052, 1996. arXive e-print
quant-ph/9511030.
[BBR88]
C.
H.
Bennett,
G.
Brassard,
and
J. M. Robert. Privacy ampliﬁcation by
public
discussion. SIAM
J. Comp.,
17:210–229, 1988.
[BCDP96]
D. Beckman, A. N. Chari, S. Devabhak-
tuni, and J. Preskill. Efﬁcient networks
for quantum factoring. Phys. Rev. A,
54(2):1034, 1996. arXive e-print quant-
ph/9602016.
[BCF+96]
H. Barnum, C. M. Caves, C. A. Fuchs,
R. Jozsa, and B. Schumacher. Noncom-
muting mixed states cannot be broad-
cast. Phys. Rev. Lett., 76(15):2828–
2821,
1996.
arXive
e-print
quant-
ph/9511010.
[BCJ+99]
S. L. Braunstein, C. M. Caves, R. Jozsa,
N. Linden, S. Popescu, and R. Schack.
Separability of very noisy mixed states and
implications for NMR quantum comput-
ing. Phys. Rev. Lett., 83(5):1054–1057,
1999.
[BCJD99]
G. K. Brennen, C. M. Caves, P. S. Jessen,
and I. H. Deutsch. Quantum logic gates in
optical lattices. Physical Review Letters,
82:1060–1063, 1999.
[BD00]
C. H. Bennett and D. P. DiVincenzo.
Quantum information and computation.
Nature, 404:247–55, 2000.
[BDG88a]
J. L. Balc´azar, J. Diaz, and J. Gabarr´o.
Structural
Complexity,
Volume
I.
Springer-Verlag, Berlin, 1988.
[BDG88b]
J. L. Balc´azar, J. Diaz, and J. Gabarr´o.
Structural
Complexity,
Volume
II.
Springer-Verlag, Berlin, 1988.
[BDK92]
R.
G.
Brewer,
R.
G.
DeVoe,
and
R. Kallenbach. Planar ion microtraps.
Phys. Rev. A, 46(11):R6781–4, 1992.
[BDS97]
C.
H.
Bennett,
D.
P.
DiVincenzo,
and J. A. Smolin. Capacities of quan-
tum erasure channels. Phys. Rev. Lett.,
78(16):3217–3220, 1997. arXive e-print
quant-ph/9701015.
[BDSW96]
C.
H.
Bennett,
D.
P.
DiVincenzo,
J. A. Smolin, and W. K. Wootters. Mixed
state entanglement and quantum error
correction. Phys. Rev. A, 54:3824, 1996.
arXive e-print quant-ph/9604024.
[Bel64]
J. S. Bell. On the Einstein-Podolsy-
Rosen paradox. Physics, 1:195–200, 1964.
Reprinted in J. S. Bell, Speakable and
Unspeakable in Quantum Mechanics,
Cambridge University Press, Cambridge,
1987.
[Ben73]
C.
H.
Bennett.
Logical
reversibility
Bibliography
651
of computation. IBM J. Res. Dev.,
17(6):525–32, 1973.
[Ben80]
P. Benioff. The computer as a physical
system: A microscopic quantum mechan-
ical Hamiltonian model of computers as
represented by Turing machines. J. Stat.
Phys., 22(5):563–591, 1980.
[Ben82]
C. H. Bennett. The thermodynamics of
computation - a review. Int. J. Theor.
Phys., 21:905–40, 1982.
[Ben87]
C. H. Bennett. Demons, engines and the
second law. Sci. Am., 295(5):108, 1987.
[Ben89]
C. H. Bennett. Time-space trade-offs for
reversible computation. SIAM J. Com-
put., 18:766–776, 1989.
[Ben92]
C. H. Bennett. Quantum cryptography
using any two nonorthogonal states. Phys.
Rev. Lett., 68(21):3121–3124, 1992.
[Bet84]
T.
Beth.
Methoden
der
Schnellen
Fouriertransformation.
Teubner,
Leipzig, 1984.
[BFGL98]
S. L. Braunstein, C. A. Fuchs, D. Gottes-
man, and H. Lo. A quantum analog of
Huffman coding. arXive e-print quant-
ph/9805080, 1998.
[BFJS96]
H. Barnum, C. A. Fuchs, R. Jozsa, and
B. Schumacher. General ﬁdelity limit
for quantum channels. Phys. Rev. A,
54:4707, 1996. arXive e-print quant-
ph/9603014.
[Bha97]
R. Bhatia. Matrix Analysis. Springer-
Verlag, New York, 1997.
[BHT98]
G. Brassard, P. Høyer, and A. Tapp.
Quantum counting.
arXive e-print
quant-ph/9805082, 1998.
[BK92]
V. B. Braginsky and F. Y. Khahili. Quan-
tum Measurement. Cambridge Univer-
sity Press, Cambridge, 1992.
[BK98a]
S. L. Braunstein and H. J. Kimble. Tele-
portation of continuous quantum vari-
ables. Phys. Rev. Lett., 80:869–72, 1998.
[BK98b]
S. B. Bravyi and A. Y. Kitaev. Quantum
codes on a lattice with boundary. arXive
e-print quant-ph/9811052, 1998.
[BK99]
S. L. Braunstein and H. J. Kimble. Dense
coding for continuous variables. arXive
e-print quant-ph/9910010, 1999.
[BKLW99]
D. Bacon, J. Kempe, D. A. Lidar,
and K.
B. Whaley. Universal fault-
tolerant computation on decoherence-
free subspaces.
arXive e-print quant-
ph/9909058, 1999.
[BKN98]
H. Barnum, E. Knill, and M. A. Nielsen.
On quantum ﬁdelities and channel capaci-
ties. arXive e-print quant-ph/9809010,
1998.
[BL95]
D. Boneh and R. J. Lipton. Quantum
cryptoanalysis of hidden linear functions
(extended abstract). In Don Coppersmith,
editor, Lecture notes in computer sci-
ence — Advances in Cryptology —
CRYPTO’95, pages 424–437, Springer-
Verlag, Berlin, 1995.
[BMP+99]
P. O. Boykin, T. Mor, M. Pulver, V. Roy-
chowdhury, and F. Vatan. On univer-
sal and fault-tolerant quantum comput-
ing. arXive e-print quant-ph/9906054,
1999.
[BNS98]
H.
Barnum,
M.
A.
Nielsen,
and
B. W. Schumacher. Information trans-
mission through a noisy quantum channel.
Phys. Rev. A, 57:4153, 1998.
[Boh51]
D. Bohm. Quantum Theory. Prentice-
Hall, Englewood Cliffs, New Jersey, 1951.
[BP93]
S. M. Barnett and S. J. D. Phoenix.
Information-theoretic limits to quantum
cryptography. Phys. Rev. A, 48(1):R5–
R8, 1993.
[BPM+97]
D. Bouwmeester, J. W. Pan, K. Mattle,
M. Eibl, H. Weinfurter, and A. Zeilinger.
Experimental quantum teleportation. Na-
ture, 390(6660):575–579, 1997.
[BR98]
D. S. Bethune and W. P. Risk. An au-
tocompensating quantum key distribution
system using polarization splitting of light.
In IQEC ’98 Digest of Postdeadline Pa-
pers, pages QPD12–2, Optical Society of
America, Washington, DC, 1998.
[BR00]
D. S. Bethune and W. P. Risk. An auto-
compensating ﬁber-optic quantum cryp-
tography system based on polarization
splitting of light. J. Quantum Electron-
ics, 36(3):100, 2000.
[Bra93]
G. Brassard. A bibliography of quantum
cryptography. Universit´e de Montr´eal
preprint, pages 1–10, 3 December 1993.
A preliminary version of this appeared in
Sigact News, vol. 24(3), 1993, pages 16-
20.
[Bra98]
S. L. Braunstein. Error correction for con-
tinuous quantum variables. Phys. Rev.
Lett., 80:4084–4087, 1998. arXive e-
print quant-ph/9711049.
[BS94]
G. Brassard and L. Salvail. Secret-key
reconciliation by public discussion. In
T. Helleseth, editor, Lecture Notes in
Computer Science: Advances in Cryp-
tology – EUROCRYPT’93, Volume
652
Bibliography
765,
pages
410–423,
Springer-Verlag,
New York, 1994.
[BS98]
C. H. Bennett and P. W. Shor. Quan-
tum information theory. IEEE Trans.
Inf. Theory, 44(6):2724–42, 1998.
[BST98]
H. Barnum, J. A. Smolin, and B. Ter-
hal. Quantum capacity is properly de-
ﬁned without encodings. Phys. Rev. A,
58(5):3496–3501, 1998.
[BT97]
B. M. Boghosian and W. Taylor. Sim-
ulating quantum mechanics on a quan-
tum computer.
arXive e-print quant-
ph/9701019, 1997.
[BV97]
E. Bernstein and U. Vazirani. Quantum
complexity theory. SIAM J. Comput.,
26(5):1411–1473, 1997. arXive e-print
quant-ph/9701001.
[BW92]
C. H. Bennett and S. J. Wiesner. Commu-
nication via one- and two-particle oper-
ators on Einstein-Podolsky-Rosen states.
Phys.
Rev.
Lett.,
69(20):2881–2884,
1992.
[CAK98]
N. J. Cerf, C. Adami, and P. Kwiat. Op-
tical simulation of quantum logic. Phys.
Rev. A, 57:R1477, 1998.
[Cav99]
C. M. Caves. Quantum error correction
and reversible operations. Journal of Su-
perconductivity, 12(6):707–718, 1999.
[CD96]
R. Cleve and D. P. DiVincenzo. Schu-
macher’s quantum data compression as
a quantum computation. Phys. Rev. A,
54:2636, 1996. arXive e-print quant-
ph/9603009.
[CEMM98] R. Cleve, A. Ekert, C. Macchiavello, and
M. Mosca. Quantum algorithms revisited.
Proc. R. Soc. London A, 454(1969):339–
354, 1998.
[CFH97]
D.
G.
Cory,
A.
F.
Fahmy,
and
T.
F.
Havel.
Ensemble
quantum
computing by NMR spectroscopy. Proc.
Nat. Acad. Sci. USA, 94:1634–1639,
1997.
[CGK98]
I.
L.
Chuang, N.
Gershenfeld, and
M. Kubinec. Experimental implementa-
tion of fast quantum searching. Phys.
Rev. Lett., 18(15):3408–3411, 1998.
[CGKL98]
I. L. Chuang, N. Gershenfeld, M. G. Ku-
binec, and D. W. Leung. Bulk quan-
tum computation with nuclear-magnetic-
resonance: theory and experiment. Proc.
R. Soc. London A, 454(1969):447–467,
1998.
[Che68]
P. R. Chernoff. Note on product formu-
las for operator semigroups. J. Functional
Analysis, 2:238–242, 1968.
[Cho75]
M.-D. Choi. Completely positive linear
maps on complex matrices. Linear Al-
gebra and Its Applications, 10:285–290,
1975.
[CHSH69]
J. F. Clauser, M. A. Horne, A. Shimony,
and R. A. Holt. Proposed experiment to
test local hidden-variable theories. Phys.
Rev. Lett., 49:1804–1807, 1969.
[Chu36]
A. Church. An unsolvable problem of el-
ementary number theory. Am. J. Math.
(reprinted in [Dav65]), 58:345, 1936.
[CK81]
I. Csisz´ar and J. K¨orner. Information
Theory: Coding Theorems for Discrete
Memoryless Systems. Academic Press,
New York, 1981.
[CL83]
A. O. Caldeira and A. J. Leggett. Quan-
tum tunnelling in a dissipative system.
Ann. Phys., 149(2):374–456, 1983.
[Cla89]
M. Clausen. Fast generalized Fourier
transforms. Theor. Comput. Sci., 67:55–
63, 1989.
[Cle99]
R.
Cleve.
The
query
complexity
of
order-ﬁnding.
arXive e-print quant-
ph/9911124, 1999.
[CLR90]
T. H. Cormen, C. E. Leiserson, and
R. L. Rivest. Introduction to Algorithms.
MIT Press, Cambridge, Mass., 1990.
[CM97]
C. Cachin and U. M. Maurer. Link-
ing information reconciliation and privacy
ampliﬁcation. J. Cryptology, 10:97–110,
1997.
[CM00]
I. L. Chuang and D. Modha. Reversible
arithmetic coding for quantum data com-
pression. IEEE Trans. Inf. Theory,
46(3):1104, May 2000.
[CMP+98]
D. G. Cory, W. Mass, M. Price, E. Knill,
R. Laﬂamme, W. H. Zurek, T. F. Havel,
and S. S. Somaroo. Experimental quan-
tum error correction.
arXive e-print
quant-ph/9802018, 1998.
[CN97]
I. L. Chuang and M. A. Nielsen. Prescrip-
tion for experimental determination of the
dynamics of a quantum black box. J. Mod.
Opt., 44(11-12):2455–2467, 1997. arXive
e-print quant-ph/9610001.
[Con72]
J. H. Conway. Unpredictable iterations. In
Proceedings of the Number Theory Con-
ference, pages 49–52, Boulder, Colorado,
1972.
[Con86]
J. H. Conway. Fractran: a simple universal
programming language. In T. M. Cover
and B. Gopinath, editors, Open Prob-
lems in Communication and Computa-
tion, pages 4–26, Springer-Verlag, New
York, 1986.
Bibliography
653
[Coo71]
S. A. Cook. The complexity of theorem-
proving procedures. In Proc. 3rd Ann.
ACM Symp. on Theory of Computing,
pages 151–158, Association for Comput-
ing Machinery, New York, 1971.
[Cop94]
D. Coppersmith. An approximate Fourier
transform useful in quantum factoring.
IBM Research Report RC 19642, 1994.
[CPZ96]
J. I. Cirac, T. Pellizzari, and P. Zoller. En-
forcing coherent evolution in dissipative
quantum dynamics. Science, 273:1207,
1996.
[CRSS97]
A.
R.
Calderbank,
E.
M.
Rains,
P. W. Shor, and N. J. A. Sloane.
Quantum error correction and orthogonal
geometry. Phys. Rev. Lett., 78:405–8,
1997.
[CRSS98]
A.
R.
Calderbank,
E.
M.
Rains,
P. W. Shor, and N. J. A. Sloane.
Quantum
error
correction
via
codes
over GF(4). IEEE Trans. Inf. Theory,
44(4):1369–1387, 1998.
[CS96]
A. R. Calderbank and P. W. Shor.
Good quantum error-correcting codes ex-
ist. Phys. Rev. A, 54:1098, 1996. arXive
e-print quant-ph/9512032.
[CST89]
R. A. Campos, B. E. A. Saleh, and
M. C. Tiech. Quantum-mechanical loss-
less beamsplitters: SU(2) symmetry and
photon statistics. Phys. Rev. A, 40:1371,
1989.
[CT91]
T. M. Cover and J. A. Thomas. Elements
of Information Theory. John Wiley and
Sons, New York, 1991.
[CTDL77a] C.
Cohen-Tannoudji,
B.
Diu,
and
F. Lalo¨e. Quantum Mechanics, Vol. I.
John Wiley and Sons, New York, 1977.
[CTDL77b] C.
Cohen-Tannoudji,
B.
Diu,
and
F. Lalo¨e. Quantum Mechanics, Vol. II.
John Wiley and Sons, New York, 1977.
[CVZ+98]
I. L. Chuang, L. M. K. Vandersypen,
X. L. Zhou, D. W. Leung, and S. Lloyd.
Experimental realization of a quantum
algorithm. Nature, 393(6681):143–146,
1998.
[CW95]
H. F. Chau and F. Wilczek. Simple re-
alization of the Fredkin gate using a se-
ries of two-body operators. Phys. Rev.
Lett., 75(4):748–50, 1995. arXive e-print
quant-ph/9503005.
[CY95]
I. L. Chuang and Y. Yamamoto. Sim-
ple
quantum
computer.
Phys.
Rev.
A, 52:3489–3496, 1995. arXive e-print
quant-ph/9505011.
[CZ95]
J. I. Cirac and P. Zoller. Quantum compu-
tations with cold trapped ions. Phys. Rev.
Lett., 74:4091, 1995.
[Dav65]
M. D. Davis. The Undecidable. Raven
Press, Hewlett, New York, 1965.
[Dav76]
E. B. Davies. Quantum Theory of Open
Systems. Academic Press, London, 1976.
[DBE95]
D. Deutsch, A. Barenco, and A. Ek-
ert. Universality in quantum computation.
Proc. R. Soc. London A, 449(1937):669–
677, 1995.
[Deu83]
D.
Deutsch.
Uncertainty
in
quan-
tum measurements. Phys. Rev. Lett.,
50(9):631–633, 1983.
[Deu85]
D.
Deutsch.
Quantum
theory,
the
Church-Turing Principle and the uni-
versal quantum computer. Proc. R. Soc.
Lond. A, 400:97, 1985.
[Deu89]
D. Deutsch. Quantum computational net-
works. Proc. R. Soc. London A, 425:73,
1989.
[DG98]
L.-M. Duan and G.-C. Guo. Probabilis-
tic cloning and identiﬁcation of linearly
independent quantum states. Phys. Rev.
Lett., 80:4999–5002, 1998. arXive e-
print quant-ph/9804064.
[DH76]
W. Difﬁe and M. Hellman. New direc-
tions in cryptography. IEEE Trans. Inf.
Theory, IT-22(6):644–54, 1976.
[DH96]
C. D¨urr and P. Høyer. A quantum algo-
rithm for ﬁnding the minimum. arXive
e-print quant-ph/9607014, 1996.
[Die82]
D. Dieks. Communication by EPR de-
vices. Phys. Lett. A, 92(6):271–272, 1982.
[DiV95a]
D. P. DiVincenzo. Quantum computa-
tion. Science, 270:255, 1995.
[DiV95b]
D. P. DiVincenzo. Two-bit gates are uni-
versal for quantum computation. Phys.
Rev. A, 51(2):1015–1022, 1995.
[DiV98]
D. P. DiVincenzo. Quantum gates and cir-
cuits. Proc. R. Soc. London A, 454:261–
276, 1998.
[DJ92]
D. Deutsch and R. Jozsa. Rapid solu-
tion of problems by quantum computa-
tion. Proc. R. Soc. London A, 439:553,
1992.
[DL98]
W. Difﬁe and S. Landau. Privacy on the
Line: the Politics of Wiretapping and
Encryption. MIT Press, Cambridge Mas-
sachusetts, 1998.
[DMB+93]
L. Davidovich, A. Maali, M. Brune,
J. M. Raimond, and S. Haroche. Phys.
Rev. Lett., 71:2360, 1993.
[DR90]
P. Diaconis and D. Rockmore. Efﬁcient
computation of the Fourier transform
654
Bibliography
on ﬁnite groups. J. Amer. Math. Soc.,
3(2):297–332, 1990.
[DRBH87]
L. Davidovich, J. M. Raimond, M. Brune,
and S. Haroche. Phys. Rev. A, 36:3771,
1987.
[DRBH95]
P. Domokos, J. M. Raimond, M. Brune,
and S. Haroche. Simple cavity-QED
two-bit universal quantum logic gate:
The principle and expected performances.
Phys. Rev. Lett., 52:3554, 1995.
[DS96]
D. P. DiVincenzo and P. W. Shor.
Fault-tolerant error correction with efﬁ-
cient quantum codes. Phys. Rev. Lett.,
77:3260, 1996.
[DSS98]
D. P. DiVincenzo, P. W. Shor, and
J. Smolin. Quantum-channel capacities
of very noisy channels. Phys. Rev. A,
57(2):830–839, 1998.
[Ear42]
S. Earnshaw. On the nature of the molec-
ular forces which regulate the constitution
of the luminiferous ether. Trans. Camb.
Phil. Soc., 7:97–112, 1842.
[EBW87]
R.
R.
Ernst,
G.
Bodenhausen,
and
A. Wokaun. Principles of Nuclear Mag-
netic Resonance in One and Two Dimen-
sions. Oxford University Press, Oxford,
1987.
[EH99]
M. Ettinger and P. Høyer. On quan-
tum algorithms for noncommutative hid-
den subgroups. In Symposium on The-
oretical Aspects in Computer Science.
University of Trier, 1999. arXive e-print
quant-ph/9807029.
[EHK99]
M. Ettinger, P. Høyer, and E. Knill. Hid-
den subgroup states are almost orthogo-
nal. arXive e-print quant-ph/9901034,
1999.
[EHPP94]
A. K. Ekert, B. Huttner, G. M. Palma,
and A. Peres. Eavesdropping on quantum-
cryptographical systems. Phys. Rev. A,
50(2):1047–1056, 1994.
[EJ96]
A. Ekert and R. Jozsa. Quantum computa-
tion and Shor’s factoring algorithm. Rev.
Mod. Phys., 68:733, 1996.
[EJ98]
A. Ekert and R. Jozsa. Quantum algo-
rithms: Entanglement enhanced informa-
tion processing. Proc. R. Soc. London A,
356(1743):1769–82, Aug. 1998. arXive e-
print quant-ph/9803072.
[Eke91]
A. K. Ekert. Quantum cryptography
based on Bell’s theorem. Phys. Rev. Lett.,
67(6):661–663, 1991.
[EM96]
A. Ekert and C. Macchiavello. Error cor-
rection in quantum communication. Phys.
Rev. Lett., 77:2585, 1996. arXive e-print
quant-ph/9602022.
[EPR35]
A. Einstein, B. Podolsky, and N. Rosen.
Can quantum-mechanical description of
physical reality be considered complete?
Phys. Rev., 47:777–780, 1935.
[Eps73]
H. Epstein. Commun. Math. Phys.,
31:317–325, 1973.
[Fan73]
M. Fannes. A continuity property of
the entropy density for spin lattice sys-
tems. Commun. Math. Phys., 31:291–
294, 1973.
[FC94]
C. A. Fuchs and C. M. Caves. Ensemble-
dependent bounds for accessible informa-
tion in quantum mechanics. Phys. Rev.
Lett., 73(23):3047–3050, 1994.
[Fel68a]
W. Feller. An Introduction to Probabil-
ity Theory and its Applications, Vol-
ume 1. Wiley, New York, 1968.
[Fel68b]
W. Feller. An Introduction to Probabil-
ity Theory and its Applications, Vol-
ume 2. Wiley, New York, 1968.
[Fey82]
R. P. Feynman. Simulating physics with
computers. Int. J. Theor. Phys., 21:467,
1982.
[FG98]
E. Farhi and S. Gutmann. An analog ana-
logue of a digital quantum computation.
Phys. Rev. A, 57(4):2403–2406, 1998.
arXive e-print quant-ph/9612026.
[FLS65a]
R. P. Feynman, R. B. Leighton, and
M. Sands. Volume III of The Feynman
Lectures on Physics. Addison-Wesley,
Reading, Mass., 1965.
[FLS65b]
R. P. Feynman, R. B. Leighton, and
M. Sands. Volume I of The Feynman
Lectures on Physics. Addison-Wesley,
Reading, Mass., 1965.
[FM98]
M. H. Freedman and D. A. Meyer. Pro-
jective plane and planar quantum codes.
arXive
e-print
quant-ph/9810055,
1998.
[FS92]
A. F¨assler and E. Stiefel. Group Theo-
retical Methods and Their Applications.
Birkha¨user, Boston, 1992.
[FSB+98]
A. Furusawa, J. L. Sørensen, S. L. Braun-
stein, C. A. Fuchs, H. J. Kimble, and
E. S. Polzik. Unconditional quantum tele-
portation. Science, 282:706–709, 1998.
[FT82]
E. Fredkin and T. Toffoli. Conservative
logic. Int. J. Theor. Phys., 21(3/4):219–
253, 1982.
[Fuc96]
C. A. Fuchs. Distinguishability and Ac-
cessible Information in Quantum The-
ory. Ph.D. thesis, The University of New
Bibliography
655
Mexico, Albuquerque, NM, 1996. arXive
e-print quant-ph/9601020.
[Fuc97]
C. A. Fuchs. Nonorthogonal quantum
states maximize classical information ca-
pacity. Phys. Rev. Lett., 79(6):1162–
1165, 1997.
[FvdG99]
C. A. Fuchs and J. van de Graaf. Cryp-
tographic distinguishability measures for
quantum-mechanical states. IEEE Trans.
Inf. Theory, 45(4):1216–1227, 1999.
[Gar91]
C.
W.
Gardiner.
Quantum
Noise.
Springer-Verlag, Berlin, 1991.
[GC97]
N. Gershenfeld and I. L. Chuang. Bulk
spin resonance quantum computation.
Science, 275:350, 1997.
[GC99]
D.
Gottesman
and
I.
L.
Chuang.
Quantum
teleportation
is
a
univer-
sal
computational
primitive.
Nature,
402:390–392,
1999.
arXive
e-print
quant-ph/9908010.
[GJ79]
M. R. Garey and D. S. Johnson. Com-
puters and Intractibility. W. H. Freeman
and Company, New York, 1979.
[GN96]
R. B. Grifﬁths and C.-S. Niu. Semi-
classical Fourier transform for quan-
tum computation. Phys. Rev. Lett.,
76(17):3228–3231, 1996. arXive e-print
quant-ph/9511007.
[Gor64]
J. P. Gordon. Noise at optical frequencies;
information theory. In P. A. Miles, edi-
tor, Quantum Electronics and Coherent
Light, Proceedings of the International
School of Physics ‘Enrico Fermi’ XXXI,
Academic Press, New York, 1964.
[Got96]
D. Gottesman. Class of quantum error-
correcting codes saturating the quantum
Hamming bound. Phys. Rev. A, 54:1862,
1996.
[Got97]
D. Gottesman. Stabilizer Codes and
Quantum Error Correction. Ph.D. the-
sis, California Institute of Technology,
Pasadena, CA, 1997.
[Got98a]
D. Gottesman. Fault-tolerant quantum
computation
with
higher-dimensional
systems.
arXive
e-print
quant-
ph/9802007, 1998.
[Got98b]
D. Gottesman. Theory of fault-tolerant
quantum computation. Phys. Rev. A,
57(1):127–137,
1998.
arXive
e-print
quant-ph/9702029.
[GP10]
D.
Gottesman
and
J.
Preskill.
The
Hitchiker’s guide to the threshold theo-
rem. Eternally in preparation, 1:1–9120,
2010.
[Gro96]
L. Grover. In Proc. 28th Annual ACM
Symposium on the Theory of Compu-
tation, pages 212–219, ACM Press, New
York, 1996.
[Gro97]
L. K. Grover. Quantum mechanics helps
in searching for a needle in a haystack.
Phys. Rev. Lett., 79(2):325, 1997. arXive
e-print quant-ph/9706033.
[Gru99]
J.
Gruska.
Quantum
Computing.
McGraw-Hill, London, 1999.
[GS92]
G. R. Grimmett and D. R. Stirza-
ker. Probability and Random Processes.
Clarendon Press, Oxford, 1992.
[HAD+95]
R. J. Hughes, D. M. Alde, P. Dyer,
G. G. Luther, G. L. Morgan, and
M.
Schauer.
Quantum
cryptography.
Contemp. Phys., 36(3):149–163, 1995.
arXive e-print quant-ph/9504002.
[Hal58]
P. R. Halmos. Finite-dimensional Vector
Spaces. Van Nostrand, Princeton, N.J.,
1958.
[Ham89]
M. Hammermesh. Group Theory and
its Application to Physical Problems.
Dover, New York, 1989.
[HGP96]
J.
L.
Hennessey, D. Goldberg,
and
D. A. Patterson. Computer Architec-
ture: A Quantitative Approach. Aca-
demic Press, New York, 1996.
[HHH96]
M.
Horodecki,
P.
Horodecki,
and
R.
Horodecki.
Separability
of
mixed
states: necessary and sufﬁcient conditions.
Phys. Lett. A, 223(1-2):1–8, 1996.
[HHH98]
M.
Horodecki,
P.
Horodecki,
and
R. Horodecki. Mixed-state entanglement
and distillation: is there a ‘bound’ en-
tanglement in nature? Phys. Rev. Lett.,
80(24):5239–5242, 1998.
[HHH99a]
M.
Horodecki,
P.
Horodecki,
and
R.
Horodecki.
General
teleportation
channel, singlet fraction, and quasidistil-
lation. Phys. Rev. A, 60(3):1888–1898,
1999.
[HHH99b]
M.
Horodecki,
P.
Horodecki,
and
R. Horodecki. Limits for entanglement
measures.
arXive
e-print
quant-
ph/9908065, 1999.
[HHH99c]
P.
Horodecki,
M.
Horodecki,
and
R.
Horodecki.
Bound
entanglement
can
be
activated. Phys. Rev. Lett.,
82(5):1056–1059, 1999.
[HJ85]
R. A. Horn and C. R. Johnson. Matrix
Analysis. Cambridge University Press,
Cambridge, 1985.
[HJ91]
R. A. Horn and C. R. Johnson. Topics in
Matrix Analysis. Cambridge University
Press, Cambridge, 1991.
656
Bibliography
[HJS+96]
P. Hausladen, R. Jozsa, B. Schumacher,
M. Westmoreland, and W. K. Wootters.
Classical information capacity of a quan-
tum channel. Phys. Rev. A, 54:1869,
1996.
[HJW93]
L.
P.
Hughston,
R.
Jozsa,
and
W. K. Wootters. A complete classi-
ﬁcation of quantum ensembles having
a given density matrix. Phys. Lett. A,
183:14–18, 1993.
[HK69]
K.-E. Hellwig and K. Kraus. Pure op-
erations and measurements. Commun.
Math. Phys., 11:214–220, 1969.
[HK70]
K.-E. Hellwig and K. Kraus. Operations
and measurements. II. Commun. Math.
Phys., 16:142–147, 1970.
[Hof79]
D. R. Hofstadter. G¨odel, Escher, Bach:
an Eternal Golden Braid. Basic Books,
New York, 1979.
[Hol73]
A. S. Holevo. Statistical problems in
quantum physics. In Gisiro Maruyama
and Jurii V. Prokhorov, editors, Proceed-
ings of the Second Japan–USSR Sym-
posium on Probability Theory, pages
104–119, Springer-Verlag, Berlin, 1973.
Lecture Notes in Mathematics, vol. 330.
[Hol79]
A. S. Holevo. Capacity of a quantum com-
munications channel. Problems of Inf.
Transm., 5(4):247–253, 1979.
[Hol98]
A. S. Holevo. The capacity of the quan-
tum channel with general signal states.
IEEE Trans. Inf. Theory, 44(1):269–
273, 1998.
[Hor97]
M. Horodecki. Limits for compression
of quantum information carried by en-
sembles of mixed states. Phys. Rev. A,
57:3364–3369, 1997.
[HSM+98]
A. G. Huibers, M. Switkes, C. M. Mar-
cus, K. Campman, and A. C. Gossard.
Dephasing in open quantum dots. Phys.
Rev. Lett., 82:200, 1998.
[HW60]
G. H. Hardy and E. M. Wright. An In-
troduction to the Theory of Numbers,
Fourth Edition. Oxford University Press,
London, 1960.
[IAB+99]
A.
Imamoglu,
D.
D.
Awschalom,
G. Burkard, D. P. DiVincenzo, D. Loss,
M. Sherwin, and A. Small. Quantum
information processing using quantum
dot spins and cavity qed. Phys. Rev.
Lett., 83(20):4204–7, 1999.
[IY94]
A. Imamoglu and Y. Yamamoto. Turn-
stile device for heralded single pho-
tons: Coulomb blockade of electron and
hole tunneling in quantum conﬁned p-
i-n heterojunctions. Phys. Rev. Lett.,
72(2):210–13, 1994.
[Jam98]
D. James. The theory of heating of
the quantum ground state of trapped
ions. arXive e-print quant-ph/9804048,
1998.
[Jay57]
E. T. Jaynes. Information theory and
statistical
mechanics.
ii.
Phys.
Rev.,
108(2):171–190, 1957.
[JM98]
J. A. Jones and M. Mosca. Implemen-
tation of a quantum algorithm to solve
Deutsch’s problem on a nuclear magnetic
resonance quantum computer. arXive e-
print quant-ph/9801027, 1998.
[JMH98]
J. A. Jones, M. Mosca, and R. H. Hansen.
Implementation of a quantum search
algorithm on a nuclear magnetic res-
onance
quantum
computer.
Nature,
393(6683):344,
1998.
arXive
e-print
quant-ph/9805069.
[Jon94]
K. R. W. Jones. Fundamental limits upon
the measurement of state vectors. Phys.
Rev. A, 50:3682–3699, 1994.
[Joz94]
R. Jozsa. Fidelity for mixed quantum
states. J. Mod. Opt., 41:2315–2323, 1994.
[Joz97]
R. Jozsa. Quantum algorithms and the
Fourier
transform.
arXive
e-print
quant-ph/9707033, 1997.
[JP99]
D.
Jonathan
and
M.
B.
Plenio.
Entanglement-assisted
local
manipu-
lation of pure states. Phys. Rev. Lett.,
83:3566–3569, 1999.
[JS94]
R. Jozsa and B. Schumacher. A new proof
of the quantum noiseless coding theorem.
J. Mod. Opt., 41:2343–2349, 1994.
[Kah96]
D. Kahn. Codebreakers: the Story of Se-
cret Writing. Scribner, New York, 1996.
[Kan98]
B. Kane. A silicon-based nuclear spin
quantum computer. Nature, 393:133–
137, 1998.
[Kar72]
R. M. Karp. Reducibility among com-
binatorial problems. In Complexity of
Computer Computations, pages 85–103,
Plenum Press, New York, 1972.
[KCL98]
E. Knill, I. Chuang, and R. Laﬂamme.
Effective pure states for bulk quantum
computation. Phys. Rev. A, 57(5):3348–
3363,
1998.
arXive
e-print
quant-
ph/9706053.
[Kit95]
A. Y. Kitaev. Quantum measurements and
the Abelian stabilizer problem. arXive e-
print quant-ph/9511026,, 1995.
[Kit97a]
A. Y. Kitaev. Fault-tolerant quantum
computation by anyons. arXive e-print
quant-ph/9707021, 1997.
Bibliography
657
[Kit97b]
A. Y. Kitaev. Quantum computations:
algorithms and error correction. Russ.
Math. Surv., 52(6):1191–1249, 1997.
[Kit97c]
A. Y. Kitaev. Quantum error correction
with imperfect gates. In A. S. Holevo
O.
Hirota
and
C.
M.
Caves,
edi-
tors, Quantum Communication, Com-
puting, and Measurement, pages 181–
188, Plenum Press, New York, 1997.
[KL51]
S. Kullback and R. A. Leibler. On in-
formation and sufﬁciency. Ann. Math.
Stat., 22:79–86, 1951.
[KL97]
E. Knill and R. Laﬂamme. A theory
of quantum error-correcting codes. Phys.
Rev. A, 55:900, 1997. arXive e-print
quant-ph/9604034.
[KL99]
E. Knill and R. Laﬂamme. Quantum com-
putation and quadratically signed weight
enumerators.
arXive e-print quant-
ph/9909094, 1999.
[Kle31]
O. Klein. Z. Phys., 72:767–775, 1931.
[KLV99]
E. Knill, R. Laﬂamme, and L. Viola.
Theory of quantum error correction for
general noise.
arXive e-print quant-
ph/9908066, 1999.
[KLZ98a]
E. Knill, R. Laﬂamme, and W. H. Zurek.
Resilient quantum computation. Science,
279(5349):342–345, 1998. arXive e-print
quant-ph/9702058.
[KLZ98b]
E. Knill, R. Laﬂamme, and W. H. Zurek.
Resilient quantum computation: error
models and thresholds. Proc. R. Soc.
London
A,
454(1969):365–384,
1998.
arXive e-print quant-ph/9702058.
[KMSW99] P.
G.
Kwiat,
J.
R.
Mitchell,
P. D. D. Schwindt, and A. G. White.
Grover’s search algorithm: An optical
approach.
arXive
e-print
quant-
ph/9905086, 1999.
[Kni95]
E.
Knill.
Approximating
quantum
circuits.
arXive
e-print
quant-
ph/9508006, 1995.
[Knu97]
D. E. Knuth. Fundamental Algorithms
3rd Edition, Volume 1 of The Art
of Computer Programming. Addison-
Wesley, Reading, Massachusetts, 1997.
[Knu98a]
D. E. Knuth. Seminumerical Algorithms
3rd Edition, Volume 2 of The Art
of Computer Programming. Addison-
Wesley, Reading, Massachusetts, 1998.
[Knu98b]
D. E. Knuth. Sorting and Searching
2nd Edition, Volume 3 of The Art
of Computer Programming. Addison-
Wesley, Reading, Massachusetts, 1998.
[Kob94]
N. Koblitz. A Course in Number The-
ory and Cryptography. Springer-Verlag,
New York, 1994.
[KR99]
C. King and M. B. Ruskai. Minimal en-
tropy of states emerging from noisy quan-
tum channels.
arXive e-print quant-
ph/9911079, 1999.
[Kra83]
K. Kraus. States, Effects, and Opera-
tions: Fundamental Notions of Quan-
tum Theory. Lecture Notes in Physics,
Vol. 190. Springer-Verlag, Berlin, 1983.
[Kra87]
K. Kraus. Complementary observables
and uncertainty relations. Phys. Rev. D,
35(10):3070–3075, 1987.
[KSC+94]
P.
G.
Kwiat,
A.
M.
Steinberg,
R.
Y.
Chiao,
P.
H.
Eberhard,
and
M.
D.
Petroff.
Absolute
efﬁciency
and
time-response
measurement
of
single-photon
detectors.
Appl.
Opt.,
33(10):1844–1853, 1994.
[KU91]
M. Kitagawa and M. Ueda. Nonlinear-
interferometric generation of number-
phase correlated Fermion states. Phys.
Rev. Lett., 67(14):1852, 1991.
[Lan27]
L. Landau. Das d¨ampfungsproblem in der
wellenmechanik. Z. Phys., 45:430–441,
1927.
[Lan61]
R. Landauer. Irreversibility and heat gen-
eration in the computing process. IBM J.
Res. Dev., 5:183, 1961.
[LB99]
S. Lloyd and S. Braunstein. Quantum
computation over continuous variables.
Phys. Rev. Lett., 82:1784–1787, 1999.
arXive e-print quant-ph/9810082.
[LBW99]
D. A. Lidar, D. A. Bacon, and K. B. Wha-
ley. Concatenating decoherence free sub-
spaces with quantum error correcting
codes. Phys. Rev. Lett., 82(22):4556–
4559, 1999.
[LC99]
H.
Lo
and
H.
F. Chau. Uncondi-
tional security of quantum key distribu-
tion over arbitrarily long distances. Sci-
ence, 283:2050–2056, 1999. arXive e-
print quant-ph/9803006.
[LCW98]
D.
A.
Lidar,
I.
L.
Chuang,
and
K. B. Whaley. Decoherence-free sub-
spaces for quantum computation. Phys.
Rev. Lett., 81(12):2594–2597, 1998.
[LD98]
D. Loss and D. P. DiVincenzo. Quantum
computation with quantum dots. Phys.
Rev. A, 57:120–126, 1998.
[Lec63]
Y.
Lecerf.
Machines
de
Turing
r´eversibles. Comptes Rendus, 257:2597–
2600, 1963.
[Leo97]
U. Leonhardt. Measuring the Quantum
658
Bibliography
State of Light. Cambridge University
Press, New York, 1997.
[Lev73]
L. Levin. Universal sorting problems.
Probl. Peredaci Inf., 9:115–116, 1973.
Original in Russian. English translation
in Probl. Inf. Transm. USSR 9:265–266
(1973).
[Lie73]
E. H. Lieb.
Convex trace
functions
and the Wigner-Yanase-Dyson conjec-
ture. Ad. Math., 11:267–288, 1973.
[Lie75]
E. H. Lieb. Bull. AMS, 81:1–13, 1975.
[Lin75]
G. Lindblad. Completely positive maps
and
entropy
inequalities.
Commun.
Math. Phys., 40:147–151, 1975.
[Lin76]
G. Lindblad. On the generators of quan-
tum dynamical semigroups. Commun.
Math. Phys., 48:199, 1976.
[Lin91]
G. Lindblad. Quantum entropy and quan-
tum measurements. In C. Bendjabal-
lah, O. Hirota, and S. Reynaud, edi-
tors, Quantum Aspects of Optical Com-
munications, Lecture Notes in Physics,
vol. 378, pages 71–80, Springer-Verlag,
Berlin, 1991.
[Lip95]
R. Lipton. DNA solution of hard compu-
tational problems. Science, 268:542–525,
1995.
[LKF99]
N. Linden, E. Kupce, and R. Freeman.
NMR quantum logic gates for homonu-
clear spin systems.
arXive e-print
quant-ph/9907003, 1999.
[LL93]
A. K. Lenstra and H. W. Lenstra Jr., ed-
itors. The Development of the Number
Field Sieve. Springer-Verlag, New York,
1993.
[Llo93]
S. Lloyd. A potentially realizable quantum
computer. Science, 261:1569, 1993.
[Llo94]
S. Lloyd. Necessary and sufﬁcient condi-
tions for quantum computation. J. Mod.
Opt., 41(12):2503, 1994.
[Llo95]
S. Lloyd. Almost any quantum logic gate
is universal. Phys. Rev. Lett., 75(2):346,
1995.
[Llo96]
S. Lloyd. Universal quantum simulators.
Science, 273:1073, 1996.
[Llo97]
S. Lloyd. The capacity of the noisy quan-
tum channel. Phys. Rev. A, 56:1613,
1997.
[LLS75]
R. E. Ladner, N. A. Lynch, and A. L. Sel-
man. A comparison of polynomial-time
reducibilities. Theor. Comp. Sci., 1:103–
124, 1975.
[LMPZ96]
R. Laﬂamme, C. Miquel, J.-P. Paz,
and W. H. Zurek. Perfect quantum er-
ror correction code. Phys. Rev. Lett.,
77:198, 1996. arXive e-print quant-
ph/9602019.
[LNCY97]
D.
W.
Leung,
M.
A.
Nielsen,
I.
L.
Chuang,
and
Y.
Yamamoto.
Approximate quantum error correction
can lead to better codes. Phys. Rev.
A, 56:2567–2573, 1997. arXive e-print
quant-ph/9704002.
[Lo99]
H. Lo. A simple proof of the uncondi-
tional security of quantum key distribu-
tion. arXive e-print quant-ph/9904091,
1999.
[Lom87]
J. S. Lomont. Applications of Finite
Groups. Dover, New York, 1987.
[Lou73]
W. H. Louisell. Quantum Statistical
Properties of Radiation. Wiley, New
York, 1973.
[LP97]
H.-K. Lo and S. Popescu. Concentrating
local entanglement by local actions – be-
yond mean values. arXive e-print quant-
ph/9707038, 1997.
[LP99]
N. Linden and S. Popescu. Good dynam-
ics versus bad kinematics. Is entangle-
ment needed for quantum computation?
arXive
e-print
quant-ph/9906008,
1999.
[LR68]
O. E. Lanford and D. Robinson. Mean en-
tropy of states in quantum-statistical me-
chanics. J. Math. Phys., 9(7):1120–1125,
1968.
[LR73a]
E. H. Lieb and M. B. Ruskai. A funda-
mental property of quantum-mechanical
entropy. Phys. Rev. Lett., 30(10):434–
436, 1973.
[LR73b]
E. H. Lieb and M. B. Ruskai. Proof of the
strong subadditivity of quantum mechan-
ical entropy. J. Math. Phys., 14:1938–
1941, 1973.
[LR90]
H. Leff and R. Rex. Maxwell’s De-
mon: Entropy, Information, Comput-
ing. Princeton University Press, Prince-
ton, NJ, 1990.
[LS93]
L. J. Landau and R. F. Streater. On
Birkhoff theorem for doubly stochastic
completely positive maps of matrix al-
gebras. Linear Algebra Appl., 193:107–
127, 1993.
[LS98]
S. Lloyd and J. E. Slotine. Analog quan-
tum error correction. Phys. Rev. Lett.,
80:4088–4091, 1998.
[LSP98]
H.-K. Lo, T. Spiller, and S. Popescu.
Quantum information and computation.
World Scientiﬁc, Singapore, 1998.
[LTV98]
M. Li, J. Tromp, and P. Vitanyi. Re-
versible simulation of irreversible com-
Bibliography
659
putation by pebble games. Physica D,
120:168–176, 1998.
[LV96]
M. Li and P. Vitanyi. Reversibility and
adiabatic computation: trading time and
space for energy. Proc. R. Soc. London
A, 452:769–789, 1996. arXive e-print
quant-ph/9703022.
[LVZ+99]
D. W. Leung, L. M. K. Vandersypen,
X. Zhou, M. Sherwood, C. Yannoni,
M. Kubinec, and I. L. Chuang. Exper-
imental realization of a two-bit phase
damping quantum code. Phys. Rev. A,
60:1924, 1999.
[Man80]
Y. Manin. Computable and Uncom-
putable (in Russian). Sovetskoye Radio,
Moscow, 1980.
[Man99]
Y. I. Manin. Classical computing, quan-
tum
computing,
and
Shor’s
factor-
ing algorithm.
arXive e-print quant-
ph/9903008, 1999.
[Mau93]
U. M. Maurer. Secret key agreement by
public discussion from common informa-
tion. IEEE Trans. Inf. Theory, 39:733–
742, 1993.
[Max71]
J. C. Maxwell. Theory of Heat. Long-
mans, Green, and Co., London, 1871.
[May98]
D. Mayers. Unconditional security in
quantum cryptography. arXive e-print
quant-ph/9802025, 1998.
[ME99]
M. Mosca and A. Ekert. The hidden sub-
group problem and eigenvalue estimation
on a quantum computer. arXive e-print
quant-ph/9903071, 1999.
[Mer78]
R. Merkle. Secure communications over
insecure channels. Comm. of the ACM,
21:294–299, 1978.
[Mil76]
G. L. Miller. Riemann’s hypothesis and
tests for primality. J. Comput. Syst. Sci.,
13(3):300–317, 1976.
[Mil89a]
G. J. Milburn. Quantum optical Fredkin
gate. Phys. Rev. Lett., 62(18):2124, 1989.
[Mil89b]
D. A. B. Miller. Optics for low energy
communications inside digital processors:
quantum detectors, sources, and modu-
lators as efﬁcient impedance converters.
Opt. Lett., 14:146, 1989.
[Mil96]
G. J. Milburn. A quantum mechanical
Maxwell’s demon. Unpublished, 1996.
[Mil97]
G. J. Milburn. Scr¨odinger’s Machines:
the Quantum Technology
Reshaping
Everyday Life. W. H. Freeman, New
York, 1997.
[Mil98]
G. J. Milburn. The Feynman Processor:
Quantum Entanglement and the Com-
puting Revolution. Perseus Books, Read-
ing, Mass., 1998.
[Min67]
M. L. Minsky. Computation: ﬁnite and
inﬁnite machines. Prentice-Hall, Engle-
wood Cliffs, N.J., 1967.
[MM92]
M. Marcus and H. Minc. A Survey of
Matrix Theory and Matrix Inequali-
ties. Dover, New York, 1992.
[MMK+95] C. Monroe, D. M. Meekhof, B. E. King,
W. M. Itano, and D. J. Wineland. Demon-
stration of a fundamental quantum logic
gate. Phys. Rev. Lett., 75:4714, 1995.
[MO79]
A. W. Marshall and I. Olkin. Inequalities:
Theory of Majorization and its Appli-
cations. Academic Press, New York, 1979.
[MOL+99]
J. E. Mooij, T. P. Orlando, L. Levi-
tov, L. Tian, C. H. van der Waal, and
S. Lloyd. Josephson persistent-current
qubit. Science, 285:1036–1039, 1999.
[Mor98]
T. Mor. No-cloning of orthogonal states
in composite systems. Phys. Rev. Lett.,
80:3137–3140, 1998.
[Mos98]
M. Mosca. Quantum searching, counting
and amplitude ampliﬁcation by eigenvec-
tor analysis. In R. Freivalds, editor, Pro-
ceedings of International Workshop on
Randomized Algorithms, pages 90–100,
1998.
[Mos99]
M. Mosca. Quantum Computer Algo-
rithms. Ph.D. thesis, University of Ox-
ford, 1999.
[MR95]
R. Motwani and P. Raghavan. Random-
ized Algorithms. Cambridge University
Press, Cambridge, 1995.
[MS77]
F. J. MacWilliams and N. J. A. Sloane.
The Theory of Error-correcting Codes.
North-Holland, Amsterdam, 1977.
[MU88]
H. Maassen and J. H. B. Ufﬁnk. General-
ized entropic uncertainty relations. Phys.
Rev. Lett., 60(12):1103–1106, 1988.
[MvOV96]
A. J. Menezes, P. C. van Oorschot, and
S. A. Vanstone. Handbook of Applied
Cryptography. CRC Press, 1996.
[MWKZ96] K. Mattle, H. Weinfurter, P. G. Kwiat,
and A. Zeilinger. Dense coding in exper-
imental quantum communication. Phys.
Rev. Lett., 76(25):4656–4659, 1996.
[MZG96]
A. Muller, H. Zbinden, and N. Gisin.
Quantum cryptography over 23 km in in-
stalled under-lake telecom ﬁbre. Euro-
phys. Lett., 33:334–339, 1996.
[NC97]
M. A. Nielsen and C. M. Caves. Re-
versible quantum operations and their ap-
plication to teleportation. Phys. Rev. A,
55(4):2547–2556, 1997.
660
Bibliography
[NCSB98]
M. A. Nielsen, C. M. Caves, B. Schu-
macher, and H. Barnum. Information-
theoretic approach to quantum error cor-
rection and reversible measurement. Proc.
R. Soc. London A, 454(1969):277–304,
1998.
[Nie98]
M. A. Nielsen. Quantum Information
Theory. Ph.D. thesis, University of New
Mexico, 1998.
[Nie99a]
M.
A.
Nielsen.
Conditions
for
a
class of entanglement transformations.
Phys. Rev. Lett., 83(2):436–439, 1999.
[Nie99b]
M. A. Nielsen. Probability distributions
consistent with a mixed state. arXive e-
print quant-ph/9909020, 1999.
[NKL98]
M. A. Nielsen, E. Knill, and R. Laﬂamme.
Complete quantum teleportation using
nuclear
magnetic
resonance.
Nature,
396(6706):52–55, 1998.
[NPT99]
Y.
Nakamura,
Y.
A.
Pashkin,
and
J. S. Tsai. Coherent control of macro-
scopic quantum states in a single-cooper-
pair box. Nature, 398:786–788, 1999.
[OP93]
M. Ohya and D. Petz. Quantum Entropy
and Its Use. Springer-Verlag, Berlin,
1993.
[Pai82]
A. Pais. Subtle is the Lord: The Science
and the Life of Albert Einstein. Oxford
University Press, Oxford, 1982.
[Pai86]
A. Pais. Inward Bound: Of Matter and
Forces in the Physical World. Oxford
University Press, Oxford, 1986.
[Pai91]
A. Pais. Niels Bohr’s Times: In Physics,
Philosophy, and Polity. Oxford Univer-
sity Press, Oxford, 1991.
[Pap94]
C. M. Papadimitriou. Computational
Complexity. Addison-Wesley, Reading,
Massachusetts, 1994.
[Pat92]
R. Paturi. On the degree of polynomi-
als that approximate symmetric Boolean
functions (preliminary version).
Proc.
24th Ann. ACM Symp. on Theory of
Computing (STOC ’92), pages 468–
474, 1992.
[PCZ97]
J. F. Poyatos, J. I. Cirac, and P. Zoller.
Complete characterization of a quantum
process: the two-bit quantum gate. Phys.
Rev. Lett., 78(2):390–393, 1997.
[PD99]
P. M. Platzman and M. I. Dykman. Quan-
tum computing with electrons ﬂoating on
liquid helium. Science, 284:1967, 1999.
[Pen89]
R. Penrose. The Emperor’s New Mind.
Oxford University Press, Oxford, 1989.
[Per52]
S. Perlis. Theory of Matrices. Addison-
Wesley, Reading, Mass., 1952.
[Per88]
A. Peres. How to differentiate between
non-orthogonal states. Phys. Lett. A,
128:19, 1988.
[Per93]
A. Peres. Quantum Theory: Concepts
and Methods. Kluwer Academic, Dor-
drecht, 1993.
[Per95]
A. Peres. Higher order schmidt decompo-
sitions. Phys. Lett. A, 202:16–17, 1995.
[Pet86]
D.
Petz.
Quasi-entropies
for
ﬁnite
quantum systems. Rep. Math. Phys.,
23(1):57–65, 1986.
[Phy92]
Physics Today Editor. Quantum cryptog-
raphy deﬁes eavesdropping. Physics To-
day, page 21, November 1992.
[PK96]
M. B. Plenio and P. L. Knight. Realistic
lower bounds for the factorization time of
large numbers on a quantum computer.
Phys. Rev. A, 53:2986–2990, 1996.
[PK98]
M. B. Plenio and P. L. Knight. The
quantum-jump approach to dissipative
dynamics in quantum optics. Rev. Mod.
Phys., 70(1):101–144, 1998.
[Pop75]
R. P. Poplavskii. Thermodynamical mod-
els of information processing (in Russian).
Usp. Fiz. Nauk, 115(3):465–501, 1975.
[PRB98]
M. Pueschel, M. Roetteler, and T. Beth.
Fast quantum Fourier transforms for a
class of non-abelian groups.
arXive e-
print quant-ph/9807064, 1998.
[Pre97]
J.
Preskill.
Fault-tolerant
quantum
computation.
arXive e-print quant-
ph/9712048, 1997.
[Pre98a]
J. Preskill. Fault-tolerant quantum com-
putation. In H.-K. Lo, T. Spiller, and
S. Popescu, editors, Quantum informa-
tion and computation. World Scientiﬁc,
Singapore, 1998.
[Pre98b]
J.
Preskill.
Physics
229:
Ad-
vanced
Mathematical
Methods
of
Physics
—
Quantum
Computa-
tion
and
Information.
California
Institute of Technology, 1998.
URL:
http://www.theory.caltech.edu/people/preskill/ph229/
[Pre98c]
J. Preskill. Reliable quantum computers.
Proc. R. Soc. London A, 454(1969):385–
410, 1998.
[Rab80]
M. O. Rabin. Probabilistic algorithm for
testing primality. J. Number Theory,
12:128–138, 1980.
[Rah99]
H. Z. Rahim. Richard Feynman and Bill
Gates: an imaginary encounter. 1999. URL:
http://www.trnsoft.com/features/1rfbg.htm
[Rai98]
E. M. Rains. Quantum weight enu-
merators. IEEE Trans. Inf. Theory,
44(4):1388–1394, 1998.
Bibliography
661
[Rai99a]
E. M. Rains. Monotonicity of the quan-
tum linear programming bound. IEEE
Trans. Inf. Theory, 45(7):2489–2492,
1999.
[Rai99b]
E. M. Rains. Nonbinary quantum codes.
IEEE Trans. Inf. Theory, 45(6):1827–
1832, 1999.
[Rai99c]
E. M. Rains. Quantum shadow enu-
merators. IEEE Trans. Inf. Theory,
45(7):2361–2366, 1999.
[RB98]
M. Roetteler and T. Beth. Polynomial-
time solution to the hidden subgroup
problem for a class of non-abelian groups.
arXive
e-print
quant-ph/9812070,
1998.
[Res81]
A. Ressler. The Design of a Conserva-
tive Logic Computer and A Graphical
Editor Simulator. Master’s thesis, Mas-
sachusetts Institute of Technology, 1981.
[RHSS97]
E. M. Rains, R. H. Hardin, P. W. Shor,
and N. J. A. Sloane. Nonadditive quan-
tum code. Phys. Rev. Lett., 79(5):953–
954, 1997.
[Roy96]
A. Royer. Reduced dynamics with initial
correlations, and time-dependent environ-
ment and Hamiltonians. Phys. Rev. Lett.,
77(16):3272–3275, 1996.
[RR67]
D. W. Robinson and D. Ruelle. Commun.
Math. Phys., 5:288, 1967.
[RSA78]
R. L. Rivest, A. Shamir, and L. M. Adle-
man. A method of obtaining digital sig-
natures and public-key cryptosystems.
Comm. ACM, 21(2):120–126, 1978.
[Rus94]
M. B. Ruskai. Beyond strong subadditiv-
ity: improved bounds on the contraction of
generalized relative entropy. Rev. Math.
Phys., 6(5A):1147–1161, 1994.
[RWvD84]
S.
Ramo,
J.
R.
Whinnery,
and
T. van Duzer. Fields and waves in
communication electronics. Wiley, New
York, 1984.
[RZBB94]
M. Reck, A. Zeilinger, H. J. Bern-
stein, and P. Bertani. Experimental real-
ization of any discrete unitary operator.
Phys. Rev. Lett., 73(1):58–61, 1994.
[Sak95]
J. J. Sakurai. Modern Quantum Mechan-
ics.
Addison-Wesley, Reading, Mass.,
1995.
[SC99]
R. Schack and C. M. Caves. Classical
model for bulk-ensemble NMR quantum
computation. Phys. Rev. A, 60(6):4354–
4362, 1999.
[Sch06]
E. Schmidt. Zur theorie der linearen und
nichtlinearen integralgleighungen. Math.
Annalen., 63:433–476, 1906.
[Sch36]
E. Schr¨odinger. Probability relations be-
tween separated systems. Proc. Cam-
bridge Philos. Soc., 32:446–452, 1936.
[Sch95]
B. Schumacher. Quantum coding. Phys.
Rev. A, 51:2738–2747, 1995.
[Sch96a]
B.
Schneier.
Applied
Cryptography.
John Wiley and Sons, New York, 1996.
[Sch96b]
B. W. Schumacher. Sending entangle-
ment through noisy quantum channels.
Phys. Rev. A, 54:2614, 1996.
[Sha48]
C. E. Shannon. A mathematical theory
of communication. Bell System Tech. J.,
27:379–423, 623–656, 1948.
[Sho94]
P. W. Shor. Algorithms for quantum com-
putation: discrete logarithms and factor-
ing. In Proceedings, 35th Annual Sym-
posium on Foundations of Computer
Science, IEEE Press, Los Alamitos, CA,
1994.
[Sho95]
P. Shor. Scheme for reducing decoherence
in quantum computer memory. Phys.
Rev. A, 52:2493, 1995.
[Sho96]
P. W. Shor. Fault-tolerant quantum com-
putation. In Proceedings, 37th Annual
Symposium on Fundamentals of Com-
puter Science, pages 56–65, IEEE Press,
Los Alamitos, CA, 1996.
[Sho97]
P. W. Shor. Polynomial-time algorithms
for prime factorization and discrete loga-
rithms on a quantum computer. SIAM
J. Comp., 26(5):1484–1509, 1997.
[Sim79]
B. Simon. Trace Ideals and Their Ap-
plications. Cambridge University Press,
Cambridge, 1979.
[Sim94]
D. Simon. On the power of quantum
computation. In Proceedings, 35th An-
nual Symposium on Foundations of
Computer Science, pages 116–123, IEEE
Press, Los Alamitos, CA, 1994.
[Sim97]
D. R. Simon. On the power of quan-
tum computation. SIAM J. Comput.,
26(5):1474–1483, 1997.
[SL97]
P. W. Shor and R. Laﬂamme. Quantum
analog of the MacWilliams identities for
classical coding theory. Phys. Rev. Lett.,
78(8):1600–1602, 1997.
[SL98]
D. Shasha and C. Lazere. Out of Their
Minds: The Lives and Discoveries of
15 Great Computer Scientists. Springer-
Verlag, New York, 1998.
[Sle74]
D. Slepian, editor. Keys Papers in the
Development of Information Theory.
IEEE Press, New York, 1974.
[Sli96]
C. P. Slichter. Principles of Magnetic
Resonance. Springer, Berlin, 1996.
662
Bibliography
[SN96]
B. W. Schumacher and M. A. Nielsen.
Quantum data processing and error cor-
rection. Phys. Rev. A, 54(4):2629, 1996.
arXive e-print quant-ph/9604022.
[SP00]
P. W. Shor and J. Preskill. Simple proof of
security of the BB84 quantum key distri-
bution protocol. arXive e-print quant-
ph/0003004, 2000.
[SS76]
R. Solovay and V. Strassen. A fast Monte-
Carlo test for primality. SIAM J. Com-
put., 6:84–85, 1976.
[SS96]
P. W. Shor and J. A. Smolin. Quan-
tum error-correcting codes need not com-
pletely reveal the error syndrome. arXive
e-print quant-ph/9604006, 1996.
[SS99]
A. T. Sornborger and E. D. Stewart.
Higher order methods for simulations
on quantum computers. Phys. Rev. A,
60(3):1956–1965, 1999. arXive e-print
quant-ph/9903055.
[ST91]
B. E. A. Saleh and M. C. Teich. Funda-
mentals of Photonics. Wiley, NY, 1991.
[Ste96a]
A. M. Steane. Error correcting codes
in quantum theory. Phys. Rev. Lett.,
77:793, 1996.
[Ste96b]
A. M. Steane. Multiple particle interfer-
ence and quantum error correction. Proc.
R. Soc. London A, 452:2551–76, 1996.
[Ste97]
A. Steane. The ion-trap quantum infor-
mation processor. Appl. Phys. B – Lasers
and Optics, 64(6):623–642, 1997.
[Ste99]
A. M. Steane. Efﬁcient fault-tolerant
quantum computing. Nature, 399:124–
126, May 1999.
[STH+99]
S. Somaroo, C. H. Tseng, T. F. Havel,
R. Laﬂamme, and D. G. Cory. Quan-
tum simulations on a quantum computer.
Phys. Rev. Lett., 82:5381–5384, 1999.
[Str76]
G. Strang. Linear Algebra and Its Ap-
plications. Academic Press, New York,
1976.
[SV99]
L. J. Schulman and U. Vazirani. Molec-
ular scale heat engines and scalable quan-
tum computation. Proc. 31st Ann. ACM
Symp. on Theory of Computing (STOC
’99), pages 322–329, 1999.
[SW49]
C. E. Shannon and W. Weaver. The
Mathematical Theory of Communica-
tion. University of Illinois Press, Urbana,
1949.
[SW93]
N. J. A. Sloane and A. D. Wyner, editors.
Claude Elwood Shannon: Collected Pa-
pers. IEEE Press, New York, 1993.
[SW97]
B. Schumacher and M. D. Westmore-
land. Sending classical information via
noisy quantum channels. Phys. Rev. A,
56(1):131–138, 1997.
[SW98]
B. Schumacher and M. D. Westmore-
land. Quantum privacy and quantum co-
herence. Phys. Rev. Lett., 80(25):5695–
5697, 1998.
[SWW96]
B. W. Schumacher, M. Westmoreland,
and W. K. Wootters. Limitation on
the amount of accessible information in
a quantum channel. Phys. Rev. Lett.,
76:3453, 1996.
[Szi29]
L. Szilard. Uber die entropievermin-
derung in einen thermodynamischen sys-
tem bei eingriffen intelligenter wesen. Z.
Phys., 53:840–856, 1929.
[TD98]
B. M. Terhal and D. P. DiVincenzo. The
problem of equilibration and the compu-
tation of correlation functions on a quan-
tum computer.
arXive e-print quant-
ph/9810063, 1998.
[THL+95]
Q. A. Turchette, C. J. Hood, W. Lange,
H. Mabuchi, and H. J. Kimble. Mea-
surement of conditional phase shifts for
quantum logic. Phys. Rev. Lett., 75:4710,
1995.
[Tro59]
H. F. Trotter. On the product of semi-
groups of operators. Proc. Am. Math.
Soc., 10:545–551, 1959.
[Tsi80]
B. S. Tsirelson. Quantum generalizations
of Bell’s inequality. Lett. Math. Phys.,
4:93, 1980.
[Tur36]
A. M. Turing. On computable num-
bers, with an application to the Entschei-
dungsproblem. Proc. Lond. Math. Soc.
2 (reprinted in [Dav65]), 42:230, 1936.
[Tur97]
Q. A. Turchette. Quantum optics with
single atoms and single photons. Ph.D.
thesis, California Institute of Technology,
Pasadena, California, 1997.
[Uhl70]
A. Uhlmann. On the Shannon entropy
and related functionals on convex sets.
Rep. Math. Phys., 1(2):147–159, 1970.
[Uhl71]
A. Uhlmann. S¨atze ¨uber dichtematrizen.
Wiss. Z. Karl-Marx-Univ. Leipzig,
20:633–637, 1971.
[Uhl72]
A.
Uhlmann.
Endlich-dimensionale
dichtematrizen I. Wiss. Z. Karl-Marx-
Univ. Leipzig, 21:421–452, 1972.
[Uhl73]
A.
Uhlmann.
Endlich-dimensionale
dichtematrizen II. Wiss. Z. Karl-Marx-
Univ. Leipzig, 22:139–177, 1973.
[Uhl76]
A. Uhlmann. The ‘transition probability’
in the state space of a
∗-algebra. Rep.
Math. Phys., 9:273–279, 1976.
[Uhl77]
A. Uhlmann. Relative entropy and the
Bibliography
663
Wigner-Yanase-Dyson-Lieb concavity in
an interpolation theory. Commun. Math.
Phys., 54:21–32, 1977.
[Ume62]
H. Umegaki. K¨odai Math. Sem. Rep.,
14:59–85, 1962.
[Vai94]
L. Vaidman. Teleportation of quantum
states. Phys. Rev. A, 49(2):1473–6, 1994.
[van98a]
W. van Dam. Quantum oracle interro-
gation: getting all information for half
the price. In Proceedings of the 39th
FOCS, 1998. arXive e-print quant-
ph/9805006.
[van98b]
S. J. van Enk. No-cloning and superlu-
minal signaling. arXive e-print quant-
ph/9803030, 1998.
[Ved99]
V. Vedral. Landauer’s erasure, error cor-
rection and entanglement.
arXive e-
print quant-ph/9903049, 1999.
[Vid98]
G.
Vidal.
Entanglement
monotones.
arXive
e-print
quant-ph/9807077,
1998.
[Vid99]
G. Vidal. Entanglement of pure states
for a single copy. Phys. Rev. Lett.,
83(5):1046–1049, 1999.
[von27]
J. von Neumann. G¨ottinger Nachrichten,
page 245, 1927.
[von56]
J. von Neumann. Probabilistic logics and
the synthesis of reliable organisms from
unreliable
components.
In
Automata
Studies, pages 329–378, Princeton Uni-
versity Press, Princeton, NJ, 1956.
[von66]
J. von Neumann. Fourth University of
Illinois lecture. In A. W. Burks, editor,
Theory of Self-Reproducing Automata,
page 66, University of Illinois Press, Ur-
bana, 1966.
[VP98]
V. Vedral and M. B. Plenio. Entangle-
ment measures and puriﬁcation proce-
dures. Phys. Rev. A, 57(3):1619–1633,
1998.
[VR89]
K. Vogel and H. Risken. Determination
of quasiprobability distributions in terms
of probability distributions for the ro-
tated quadrature phase. Phys. Rev. A,
40(5):2847–2849, 1989.
[VYSC99]
L. M. K. Vandersypen, C. S. Yannoni,
M. H. Sherwood, and I. L. Chuang. Re-
alization of effective pure states for bulk
quantum computation. Phys. Rev. Lett.,
83:3085–3088, 1999.
[VYW+99]
R. Vrijen, E. Yablonovitch, K. Wang,
H. W. Jiang, A. Balandin, V. Roychowd-
hury, T. Mor, and D. DiVincenzo. Elec-
tron spin resonance transistors for quan-
tum computing in silicon-germanium het-
erostructures.
arXive e-print quant-
ph/9905096, 1999.
[War97]
W.
Warren.
The
usefulness
of
NMR
quantum
computing.
Science,
277(5332):1688, 1997.
[Wat99]
J. Watrous. PSPACE has 2-round quan-
tum interactive proof systems. arXive e-
print cs/9901015, 1999.
[WC67]
S. Winograd and J. D. Cowan. Reliable
Computation in the Presence of Noise.
MIT Press, Cambridge, MA, 1967.
[Weh78]
A. Wehrl. General properties of entropy.
Rev. Mod. Phys., 50:221, 1978.
[Wel88]
D. J. A. Welsh. Codes and Cryptogra-
phy. Oxford University Press, New York,
1988.
[Wie]
S. Wiesner. Unpublished manuscript,
circa 1969, appeared as [Wie83].
[Wie83]
S. Wiesner. Conjugate coding. SIGACT
News, 15:77, 1983.
[Wie96]
S.
Wiesner.
Simulations
of
many-
body quantum systems by a quantum
computer.
arXive
e-print
quant-
ph/9603028, 1996.
[Wil91]
D. Williams. Probability with Martin-
gales. Cambridge University Press, Cam-
bridge, 1991.
[Win98]
E. Winfree. Algorithmic Self-Assembly
of DNA. Ph.D. thesis, California Insti-
tute of Technology, Pasadena, California,
1998.
[WMI+98]
D. J. Wineland, C. Monroe, W. M. Itano,
D.
Leibfried,
B.
E.
King,
and
D. M. Meekhof. Experimental issues in
coherent quantum-state manipulation of
trapped atomic ions. J. Res. Natl. Inst.
Stand. Tech., 103:259, 1998.
[WS98]
M. D. Westmoreland and B. Schumacher.
Quantum entanglement and the non-
existence of superluminal signals. arXive
e-print quant-ph/9801014, 1998.
[WY63]
E. P. Wigner and M. M. Yanase. Proc.
Natl. Acad. Sci. (US.A.), 49:910–918,
1963.
[WY90]
K. Watanabe and Y. Yamamoto. Limits
on tradeoffs between third-order optical
nonlinearity, absorption loss, and pulse
duration in self-induced transparency and
real excitation. Phys. Rev. A, 42(3):1699–
702, 1990.
[WZ82]
W. K. Wootters and W. H. Zurek. A sin-
gle quantum cannot be cloned. Nature,
299:802–803, 1982.
[Yao93]
A. C. Yao. Quantum circuit complexity.
Proc. of the 34th Ann. IEEE Symp. on
664
Bibliography
Foundations of Computer Science, pages
352–361, 1993.
[YK95]
S. Younis and T. Knight. Non dissipative
rail drivers for adiabatic circuits. In Pro-
ceedings, Sixteenth Conference on Ad-
vanced Research in VLSI 1995, pages
404–14, IEEE Computer Society Press,
Los Alamitos, CA, 1995.
[YKI88]
Y. Yamamoto, M. Kitagawa, and K. Igeta.
In Proc. 3rd Asia-Paciﬁc Phys. Conf.,
World Scientiﬁc, Singapore, 1988.
[YO93]
H. P. Yuen and M. Ozawa. Ultimate in-
formation carrying limit of quantum sys-
tems. Physical Review Letters, 70:363–
366, 1993.
[YY99]
F. Yamaguchi and Y. Yamamoto. Crystal
lattice quantum computer. Appl. Phys.
A, pages 1–8, 1999.
[Zal98]
C. Zalka. Simulating quantum systems on
a quantum computer. Proc. R. Soc. Lon-
don A, 454(1969):313–322, 1998.
[Zal99]
C. Zalka. Grover’s quantum searching
algorithm is optimal. Phys. Rev. A,
60(4):2746–2751, 1999.
[Zan99]
P. Zanardi. Stabilizing quantum informa-
tion. arXive e-print quant-ph/9910016,
1999.
[ZG97]
P. Zoller and C. W. Gardiner. Quan-
tum noise in quantum optics: the stochas-
tic Schr¨odinger equation. In S. Reynaud,
E. Giacobino, and J. Zinn-Justin, editors,
Quantum Fluctuations: Les Houches
Summer School LXIII, Elsevier, Ams-
terdam, 1997.
[ZHSL99]
K. Zyczkowski, P. Horodecki, A. Sanpera,
and M. Lewenstein. Volume of the set of
separable states. Phys. Rev. A, 58(2):883–
892, 1999.
[ZL96]
W. H. Zurek and R. Laﬂamme. Quan-
tum logical operations on encoded qubits.
Phys.
Rev.
Lett.,
77(22):4683–4686,
1996.
[ZLC00]
X. Zhou, D. W. Leung, and I. L. Chuang.
Quantum logic gate constructions with
one-bit ”teleportation”.
arXive e-print
quant-ph/0002039, 2000.
[ZR98]
P.
Zanardi
and
M.
Rasetti.
Noise-
less quantum codes. Phys. Rev. Lett.,
79(17):3306–3309, 1998.
[Zur89]
W. H. Zurek. Thermodynamic cost of
computation, algorithmic complexity and
the information metric. Nature, 341:119,
1989.
[Zur91]
W. H. Zurek. Decoherence and the transi-
tion from quantum to classical. Phys. To-
day, October 1991.
Index
Bold page numbers indicate the place where the concept is introduced, explained, or de-
ﬁned. Major theorems are listed together under ‘theorem’, and end-of-chapter problems
are listed together under ‘problem’.
Ω(·), 137
Θ(·), 137
ϵ-net, 618
≥relation for matrices, 645
ℏ, 82
≤relation for matrices, 645
π/8 gate, 174
fault-tolerant, 485
Toffoli construction, 182
* operation, 62
0-1 integer programming, 149
, 148
Abelian group, 240, 610
Abelian stabilizer problem, 241
Abrams, D. S., 214, 215, [AL97]
accessible information, 529
acyclic circuits, 23
Adami, C., 350, [CAK98]
additive quantum codes, 453
adjoint, 62, 69
Adleman, L. M., 11, 168, 214, 641, 644, [ADH97],
[Adl94], [Adl98], [RSA78]
Aharonov, D., xix, xxi, 276, 424, 498, 499, [ABO97],
[ABO99], [ABOIN96], [Aha99a], [Aha99b],
[AKN98]
Alberti, P. M., 424, [Alb83]
Alde, D. M., 607, [HAD+95]
algorithm
Deutsch–Jozsa, 36
discrete logarithm, 238
period-ﬁnding, 236
quantum order-ﬁnding, 232
quantum phase estimation, 225
quantum search, 254
quantum simulation, 208
reduction of factoring to order-ﬁnding, 233
algorithm design, 135
algorithms, 120, 122
Allen, L., 350, [AE75]
alphabet, 141
Ambainis, A., xxi, 276, 607, [Amb00]
Amer, N., xxi
amplitude, 81
amplitude damping, 380
analog computation, 5, 163, 287
ancilla, 94
ancilla bits, 131
, 20
gate, 130
Ando, T., 527, [And79]
angle between states, 413
angular momentum, 314
anti-commutator, 76
Araki, H., 526, 527, [AL70]
Araki-Lieb inequality, 516
architecture, quantum computer, 340
Ashikhmin, A., 498, [AL99], [Ash97]
asymptotic notation, 136
atom traps, 3
atypical sequences, 538
auxiliary system, 517
Awschalom, D. D., 351, [IAB+99]
B92 protocol for QKD, 589
Bacon, D. A., 498, [BKLW99], [LBW99]
Balandin, A., 351, [VYW+99]
Balc´azar, J. L., 168, [BDG88a], [BDG88b]
Bardeen, J., 4
Barenco, A., 214, [BBC+95], [DBE95]
Barnett, S. M., 607, [BP93]
Barnum, H., xxi, 424, 605, 606, [BCF+96], [BFJS96],
[BKN98], [BNS98], [BST98], [NCSB98]
Barton, E., 168, [Bar78]
basis for a vector space, 63
Bayes’ rule, 608
BB84 protocol for QKD, 587
Beals, R., 246, 276, [BBC+98]
beamsplitters, 288, 291
Beckman, D., xxi, 214, [BCDP96]
Bell basis, 98
Bell inequality, 17, 111, 115, 119
Bell states, 16, 25, 98
Bell, J. S., 17, 25, 112, 116, 119, [Bel64]
Ben-Or, M., 498, 499, [ABO97], [ABO99], [ABOIN96]
Benioff, P., 214, [Ben80]
Bennett, C. H., xix, 9, 11, 59, 119, 168, 169, 214, 276,
497, 604, 606, 607, [BB84], [BBB+92], [BBBV97],
[BBCM95], [BBC+93], [BBC+95], [BBE92],
[BBPS96], [BBP+96], [BBR88], [BD00], [BDS97],
[BDSW96], [Ben73], [Ben82], [Ben87], [Ben89],
[Ben92], [BS98], [BW92]
Bernstein, E., 200, 214, 276, [BBBV97], [BV97]
Bernstein, H. J., 214, 350, 606, [BBPS96], [RZBB94]
Bertani, P., 214, 350, [RZBB94]
beryllium, 310, 315
666
Index
Bessette, F., 606, 607, [BBB+92]
Beth, T., 246, 616, [Bet84], [PRB98], [RB98]
Bethune, D. S., 607, [BR00], [BR98]
Bhatia, R., 118, [Bha97]
big Ω notation, 137
big Θ notation, 137
big O notation, 136
Biham, E., 607, [BBB+98]
billiard ball computer, 155
binary entropy, 502
binary symmetric channel, 426
Birkhoff’s theorem, 574
bit, 13
bit ﬂip, 81
bit ﬂip channel, 376
bit ﬂip code, 427
bit ﬂip operator, 427
bit–phase ﬂip channel, 377
Bloch sphere, 15, 19, 105, 174
Bloch vector, 105, 174, 259
Bodenhausen, G., 351, [EBW87]
Boghosian, B. M., 214, [BT97]
Bohm, D., 119, [Boh51]
Bohr magneton, 309
Bohr, N., 111, 171
Boltzmann’s constant, 153
Boneh, D., 246, [BL95]
Boolean circuit, 133
Boolean function, 133
Boschi, D., 59, [BBM+98]
Bose condensate, 346
Bouwmeester, D., 59, [BPM+97]
Boyer, M., 276, 607, [BBB+98], [BBHT98]
Boykin, P. O., 214, 215, [BMP+99]
BPP, 152
BQP, 41, 200
bra, 62
Braginsky, V. B., 118, [BK92]
Branca, S., 59, [BBM+98]
Brassard, G., 11, 59, 276, 606, 607, [BB84], [BBB+92],
[BBB+98], [BBBV97], [BBCM95], [BBC+93],
[BBE92], [BBHT98], [BBP+96], [BBR88],
[BHT98], [Bra93], [BS94]
Brattain, W., 4
Braunstein, S. L., 59, 351, 352, 605, [BCJ+99],
[BFGL98], [BK98a], [BK99], [Bra98], [FSB+98],
[LB99]
Bravyi, S. B., 499, [BK98b]
Brennen, G. K., 351, [BCJD99]
Brewer, R. G., 350, [BDK92]
Brewster’s angle, 310
Brune, M., 350, [DMB+93], [DRBH87], [DRBH95]
Buhrman, H., xxi, 246, 276, [BBC+98]
Burkard, G., 351, [IAB+99]
c-numbers, 62
Cachin, C., 607, [CM97]
Caldeira, A. O., 398, [CL83]
Calderbank, A. R., 8, 9, 450, 497, 498, [CRSS97],
[CRSS98], [CS96]
Calderbank–Shor–Steane codes, 445, 450
Campman, K., 351, [HSM+98]
Campos, R. A., 349, [CST89]
canonical form for entropy exchange, 562
Capelin, S., xxi
Carr–Purcell–Meiboom–Gill technique, 331
cat state for fault-tolerant measurement, 490
catalyst, 577
Cauchy–Schwarz inequality, 68
Caves, C. M., xxi, 16, 351, 398, 424, 605, 606,
[BCF+96], [BCJD99], [BCJ+99], [Cav99], [FC94],
[NC97], [NCSB98], [SC99]
cavity quantum electrodynamics, 277, 297, 343
cellular automata, 340
centralizer, 465
Cerf, N. J., 350, [CAK98]
characteristic function, 68
Chari, A. N., 214, [BCDP96]
Chau, H. F., 215, 607, [CW95], [LC99]
Chebyshev’s inequality, 609
check matrix, 456
Chernoff bound, 154, 609
Chernoff, P. R., 214, [Che68]
chi-matrix representation, 391
Chiao, R. Y., 350, [KSC+94]
Childs, A. M., xxi
Chinese remainder theorem, 629
Choi, M.-D., 398, [Cho75]
Chong, F. T., xxi
CHSH inequality, 116, 119
Chuang, I. L., 215, 349–351, 398, 498, 499, 605,
[CGK98], [CGKL98], [CM00], [CN97],
[CVZ+98], [CY95], [GC97], [GC99], [KCL98],
[LCW98], [LNCY97], [LVZ+99], [VYSC99],
[ZLC00]
Church, A., 4, 122, 125, 167, [Chu36]
Church–Turing thesis, 4
strong form of, 5, 6, 140
Church-Turing thesis, 125, 226
Cirac, J. I., 350, 398, 498, [CPZ96], [CZ95], [PCZ97]
circuit family, 134
circuit model of computation, 129
classical information over noisy quantum channels, 546
classical noise, 354
classical physics, 2
Clausen, M., 616, [Cla89]
Clauser, J. F., 119, [CHSH69]
Cleve, R., xxi, 59, 214, 245, 246, 276, 605, [BBC+95],
[BBC+98], [CD96], [CEMM98], [Cle99]
clique, 149
closed quantum systems, 353
CNF, 148
co-prime, 627
Cohen-Tannoudji, C., 59, 118, [CTDL77a],
[CTDL77b]
coherent information, 564, 572, 592, 605
collusion entropy, 584
communication complexity, 164
commutator, 76
commuting operators, 76, 597
compare-and-swap based sorts, 137
complete positivity, 368
example of a positive map not completely positive,
368
completeness equation, 85, 102
completeness of a problem for a complexity class, 145
completeness relation, 67, 360
complex conjugate, 62, 70
complexity class, 142, 150
composite systems, 93
composition of linear operators, 64
computational complexity, 40, 135, 138
difﬁculty of obtaining results in, 140
concatenated codes, 480
Index
667
concavity, 504
conditional entropy
classical, 506
quantum, 514
conditional probability, 608
conjunctive normal form, 148
coNP, 142
conservative property of the Fredkin gate, 156
continued fraction expansion, 229, 230, 282, 335, 635
continued fractions algorithm, 635
controlled operation, 177
controlled-
gate, 20, 178
fault-tolerant, 484
convergent, 230, 635
Conway, J. H., xxi, 168, 169, [Con72], [Con86]
Cook, S. A., 138, 168, [Coo71]
Cooper pair, 344
Coppersmith, D., 245, [Cop94]
Cormen, T. H., 167, 639, [CLR90]
correctable errors, 440
correctable set of errors, 436
Cortese, J., xxi
Cory, D. G., 350, 351, [CFH97], [CMP+98],
[STH+99]
coset invariance, 237, 243
cosets, 586, 612
Coulomb blockade, 344
counting problem, 216
Cover, T. M., xxi, 59, 526, 539, 604, 605, [CT91]
Cowan, J. D., 168, 498, [WC67]
creation and annihilation operators, 284
Cr´epeau, C., 59, 607, [BBCM95], [BBC+93]
gate, 131
cryptography, 9, 582, 640
, 145
Csisz´ar, I., 604, [CK81]
CSS codes, 445, 450, 593
cycle, 143
cyclic group, 611
cyclic property of trace, 75
cyclic subgroup, 611
data compression, 536
data pipelining inequality, 510
data processing inequality, 572, 606
quantum, 564, 572
Davidovich, L., 350, [DMB+93], [DRBH87]
Davies, E. B., 398, [Dav76]
Davis, M. D., 59, 167, 652, [Dav65], 662
de Wolf, R., xxi, 246, 276, [BBC+98]
decision problems, 135, 141
decoherence
as a stochastic phase kick process, 384
estimates of, 278
decoherence free subspace, 498
degeneracy, 69
degenerate codes, 444
Demarrais, J., 214, [ADH97]
density matrix, 99
density operator, 99, 119
depolarized, 378
depolarizing channel, 378
DeShazo, M., xxi
deterministic query complexity, 272
Deutsch’s algorithm, 32
Deutsch’s problem, 34, 241
Deutsch, D., 6, 32, 34, 59, 171, 214, 245, 526,
[DBE95], [Deu83], [Deu85], [Deu89], [DJ92]
Deutsch, I. H., 351, [BCJD99]
Deutsch–Jozsa algorithm, 34, 59, 249
optical implementation, 294
Devabhaktuni, S., 214, [BCDP96]
deviation density matrix, 336
DeVoe, R. G., 321, 350, [BDK92]
Diaconis, P., 616, [DR90]
diagonal representation, 69
diagonalizable operator, 69
Diaz, J., 168, [BDG88a], [BDG88b]
Dieks, D., 604, [Die82]
Difﬁe, W., 11, 59, 644, [DH76], [DL98]
dimension of a vector space, 63
dipolar coupling, 328
Dirac notation, 13, 62
discrete logarithm problem, 216, 217, 241
quantum algorithm for, 238
discrete memoryless channel, 551
distance measures, 399
distance of a code, 448
distillable entanglement, 578
distributed computation, 164
distributed quantum computation, xvii
Diu, B., 59, 118, [CTDL77a], [CTDL77b]
DiVincenzo, D. P., xix, xxi, 214, 215, 246, 349–351,
497, 498, 604–606, [BBC+95], [BD00], [BDS97],
[BDSW96], [CD96], [DiV95a], [DiV95b],
[DiV98], [DS96], [DSS98], [IAB+99], [LD98],
[TD98], [VYW+99]
divisor, 625
DNA computing, 163
Domokos, P., 350, [DRBH95]
double stochasticity, 511
dual linear code, 449
dual vector, 65
dual-rail representation, 288
Duan, L.-M., 119, [DG98]
D¨urr, C., 276, [DH96]
Dyer, P., 607, [HAD+95]
Dykman, M. I., 351, [PD99]
dynamic measures of distance, 399, 401
Dyson, F. J., 527
Earnshaw’s theorem, 309
Earnshaw, S., 350, [Ear42]
Eberhard, P. H., 350, [KSC+94]
Eberly, J. H., 350, [AE75]
edges, 143
efﬁciency of quantum simulations, 206
Eibl, M., 59, [BPM+97]
eigenvalue, 68
eigenvector, 68
Einstein, A., 2, 17, 25, 60, 80, 119, [EPR35]
Einstein-Podolsky-Rosen thought experiment, 17
Ekert, A. K., 59, 214, 246, 497, 498, 606, 607, [BBE92],
[CEMM98], [DBE95], [EHPP94], [EJ96], [EJ98],
[Eke91], [EM96], [ME99]
electric dipole selection rules, 300
electron spin, 309
element of reality, 112
ENDOR, 350
energy, 83, 153
energy eigenstates, 83
ensemble of pure states, 99
entanglement, 11, 95
668
Index
as a physical resource, 571
catalysis, 577
dilution, 578
distillation, 578
mixed state, 580
of formation, 578
entanglement ﬁdelity, 420
entropy, 500
classical, 500
concavity of, 516
of an ensemble, 518
quantum, 510
rate, 538
strict concavity of, 504
entropy exchange, 561, 605
entropy Venn diagram, 508
entscheidungsproblem, 122
environmental models
quantum operations, 365
trace-preserving quantum operations, 363
EPR, 17, 111
EPR pairs, 16, 25, 98, 591
EPR protocol for QKD, 591
EPR states, 25
EPR thought experiment, 119
Epstein, H., 527, [Eps73]
equilibration of quantum systems, 211
Ernst, R. R., 351, [EBW87]
error propagation
in fault-tolerant circuits, 483
error syndrome, 428
classical, 448
error-correcting codes, 8, 425
errors, 436
Ettinger, M., 246, [EH99], [EHK99]
Euclid’s algorithm, 122, 626
Euler ϕ function, 631
Euler cycle, 143
Everitt, H., xxi
evolution matrix, 355
EXP, 151
expectation of a random variable, 609
exponential resources, 139
exponential time, 151
Fabry–Perot cavity, 298
factoring, 142
factoring decision problem, 142
factoring problem, 232
factors, 625
Fagin, R., xxi
Fahmy, A. F., 350, [CFH97]
, 23
Fannes’ inequality, 512
Fannes, M., 526, [Fan73]
Fano’s inequality, 534, 536, 563, 572, 609
quantum, see quantum Fano inequality
, 23
gate, 131
Farhi, E., 276, [FG98]
F¨assler, A., 616, [FS92]
fault-tolerant computation, 425
fault-tolerant quantum computation, 474
π/8 gate, 485
assumptions, 493
deﬁnition of operations, 476
error propagation, 478
measurement, 477, 489
quantum logic, 482
the threshold theorem, 480, 493
Toffoli gate, 488
feasible computational problems, 139
Feller, W., 609, [Fel68a], [Fel68b]
Feynman path integral, 398
Feynman, R. P., 7, 59, 118, 168, 204, 214, [Fey82],
[FLS65a], [FLS65b]
ﬁdelity, 281
classical, 400
joint concavity of, 415
quantum, 409
ﬁne structure constant, 301
ﬁnite simple continued fraction, 635
ﬁnite state control, 122
foosball, xxi
formal languages, 141
Fourier transform
discrete, 217
over a group, 615
over groups, 240
quantum, 191, 209, 217
shift-invariance property, 237
Fractran, 166
Fredkin gate, 156
optical, 295
Fredkin, E., 168, [FT82]
Freedman, M. H., xxi, 499, 624, [FM98]
Freeman, R., 351, [LKF99]
Fuchs, C. A., xix, xxi, 59, 352, 399, 424, 605,
[BCF+96], [BFGL98], [BFJS96], [FC94],
[FSB+98], [Fuc96], [Fuc97], [FvdG99]
full-adder, 132
fundamental theorem of arithmetic, 625
Furusawa, A., 59, 352, [FSB+98]
Gabarr´o, J., 168, [BDG88a], [BDG88b]
Gagen, M., xxi
Gardiner, C. W., 118, 349, 398, [Gar91], [ZG97]
Garey, M. R., 168, [GJ79]
Gauss, K. F., 232
generalized amplitude damping, 382
generalized measurements, 118
generators, 611
for the ﬁve qubit code, 469
for the Shor code, 468
of a group, 455
Gerlach, W., 43
Gershenfeld, N., xxi, 350, 351, [CGK98], [CGKL98],
[GC97]
Gibbs state, 211
Gilbert–Varshamov bound
for classical codes, 449
for CSS codes, 451, 495, 596
Gisin, N., 607, [MZG96]
global phase, 93
Goldberg, D., 168, [HGP96]
Gordon, J. P., 605, [Gor64]
Gossard, A. C., 351, [HSM+98]
Gottesman, D., xix, xxi, 9, 215, 453, 497–499, 605–607,
[BFGL98], [GC99], [Got96], [Got97], [Got98a],
[Got98b], [GP10]
Gram-Schmidt procedure, 66
graph, 143
graph isomorphism problem, 150, 242
graph theory, 143
Index
669
Gray code, 191
greatest common divisor, 626
Grifﬁths, R. B., 214, 246, [GN96]
Grimmett, G. R., 609, [GS92]
ground state, 83
group commutator, 620
group theory, 610
Grover iteration, 250
Grover operator, 250
Grover’s algorithm, 7, 38, 248
Grover, L. K., 7, 38, 276, [Gro96], [Gro97]
Gruska, J., xix, [Gru99]
Guo, G.-C., 119, [DG98]
Gutmann, S., 276, [FG98]
Hadamard gate, 19, 174
Hadamard transform, 31
half-adder, 132
Halmos, P. R., 118, [Hal58]
halting problem, 130
halting state of a Turing machine, 123
Hamiltonian, 82
Hamiltonian cycle problem, 143, 264
Hamiltonian cycle problem
inclusion in NP, 143
Hammermesh, M., 616, [Ham89]
Hamming code, 449
Hamming distance, 399, 448
Hamming sphere, 549
Hamming weight, 448, 547
Hammurabi, 4
Hansen, R. H., 351, [JMH98]
Hardin, R. H., 498, [RHSS97]
Hardy, G. H., 246, 639, [HW60]
Hardy, L., 59, [BBM+98]
Haroche, S., 350, [DMB+93], [DRBH87], [DRBH95]
Harris, J. S., xxi
Hausladen, P., 605, [HJS+96]
Havel, T. F., 350, 351, [CFH97], [CMP+98],
[STH+99]
, 143, 264
Heisenberg uncertainty principle, 88, 89
Heisenberg, W., 44
Hellman, M., 11, 644, [DH76]
Hellwig, K.-E., 398, [HK69], [HK70]
Hennessey, J. L., 168, [HGP96]
Hermitian conjugate, 62, 69, 70
Hermitian operator, 70
hidden linear function problem, 241
hidden subgroup problem, 38, 217, 234, 336
quantum algorithm for, 240
Hilbert space, 66
Hilbert’s problem, 122
Hilbert, D., 122
Hilbert-Schmidt inner product, 76
Hoffmann, B., 61
Hofstadter, D. R., 59, 167, [Hof79]
Holevo χ quantity, 531
Holevo bound, 531, 592
Holevo, A. S., xxi, 605, [Hol73], [Hol79], [Hol98]
Holt, R. A., 119, [CHSH69]
Hood, C. J., 306, 350, 398, [THL+95]
Horn, R. A., 118, [HJ85], [HJ91]
Horne, M. A., 119, [CHSH69]
Horodecki, M., 605, 606, [HHH96], [HHH98],
[HHH99a], [HHH99b], [HHH99c], [Hor97]
Horodecki, P., 351, 606, [HHH96], [HHH98],
[HHH99a], [HHH99b], [HHH99c], [ZHSL99]
Horodecki, R., 606, [HHH96], [HHH98], [HHH99a],
[HHH99b], [HHH99c]
Høyer, P., 246, 276, [BBHT98], [BHT98], [DH96],
[EH99], [EHK99]
HSW theorem, 581, 592
Huang, M. A., 214, [ADH97]
Hubbard model, 206
Hughes, R. J., 607, [HAD+95]
Hughston, L. P., 119, [HJW93]
Huibers, A. G., xxi, 351, [HSM+98]
Huttner, B., 607, [EHPP94]
hyperﬁne states, 315
i.i.d. source, 537
identity matrix, 65
identity operator, 63
Igeta, K., 350, [YKI88]
Imamoglu, A., 350, 351, [IAB+99], [IY94]
Impagliazzo, R., 499, [ABOIN96]
INADEQUATE, 338
independent generators of a group, 456
independent random variables, 608
infeasible computational problems, 139
information gain implies disturbance, 586
information reconciliation, 584
information source, classical, 399
information theory, 7
operational motivation for deﬁnitions in, 501
inner product, 62, 65
inner product space, 66
integers, 625
interference, 32
interferometers, 296
internal states of a Turing machine, 122
intractable computational problems, 139
ion trap, 277
cooling, 312
geometry, 309
quantum computer, 309, 343
toy model, 317
irreversible logic gate, 153
Ising model, 206
Itano, W. M., 350, [MMK+95], [WMI+98]
J-coupling, 328
James, D., 350, [Jam98]
Jaynes, E. T., 119, [Jay57]
Jaynes–Cummings Hamiltonian, 281, 300, 302, 308,
315, 318
Jessen, P. S., 351, [BCJD99]
Jiang, H. W., 351, [VYW+99]
Johnson noise, 312
Johnson, C. R., 118, [HJ85], [HJ91]
Johnson, D. S., 168, [GJ79]
joint concavity, 519, 645
joint convexity of trace distance, 408
joint entropy, 506
quantum, 514
joint entropy theorem, 513
Jonathan, D., 606, [JP99]
Jones, J. A., 351, [JM98], [JMH98]
Jones, K. R. W., 398, [Jon94]
Josephson junction, 344
Jozsa, R., 59, 119, 246, 351, 424, 605, [BBC+93],
[BCF+96], [BCJ+99], [BFJS96], [DJ92], [EJ96],
670
Index
[EJ98], [HJS+96], [HJW93], [Joz94], [Joz97],
[JS94]
Kahn, D., 59, [Kah96]
Kallenbach, R., 350, [BDK92]
Kane, B. E., 351, [Kan98]
Karp, R. M., 168, [Kar72]
Kay, A., 80, 120
Kempe, J., xxi, 498, [BKLW99]
Kerr effect, optical, 290
Kerr media, nonlinear, 293, 305
ket, 62, 62
Khahili, F. Y., 118, [BK92]
Kimble, H. J., 59, 306, 350, 352, 398, [BK98a],
[BK99], [FSB+98], [THL+95]
King, B. E., 350, [MMK+95], [WMI+98]
King, C., 605, [KR99]
Kitaev’s algorithm, 243
Kitaev, A. Y., xix, xxi, 38, 246, 424, 498, 499, 624,
[AKN98], [BK98b], [Kit95], [Kit97a], [Kit97b],
[Kit97c]
Kitagawa, M., 350, [KU91], [YKI88]
Klein’s inequality, 526
Klein, O., 526, [Kle31]
Knight, P. L., 350, 398, [PK96], [PK98]
Knight, T., 168, [YK95]
Knill, E., xxi, 59, 214, 246, 351, 424, 497–499, 606,
[BKN98], [CMP+98], [EHK99], [KCL98],
[KL97], [KL99], [KLV99], [KLZ98a], [KLZ98b],
[Kni95], [NKL98]
Knuth, D. E., 59, 122, 167, 171, 216, 232, 632,
[Knu97], [Knu98a], [Knu98b]
Koblitz, N., 639, 644, [Kob94]
Kolmogorov distance, 400
Kong, S., xxi
K¨orner, J., 604, [CK81]
Kraus, K., 118, 398, 526, [HK69], [HK70], [Kra83],
[Kra87]
Kronecker product, 74
Kubinec, M. G., 351, [CGK98], [CGKL98], [LVZ+99]
Kullback, S., 526, [KL51]
Kupce, E., 351, [LKF99]
Kurtsiefer, C., 321
Kwiat, P. G., 119, 350, [CAK98], [KMSW99],
[KSC+94], [MWKZ96]
L, 151
L1 distance, 400
Ladner, R. E., 168, [LLS75]
Laﬂamme, R., xxi, 59, 214, 351, 424, 497–499, 606,
[CMP+98], [KCL98], [KL97], [KL99], [KLV99],
[KLZ98a], [KLZ98b], [LMPZ96], [NKL98],
[SL97], [STH+99], [ZL96]
Lalo¨e, F., 59, 118, [CTDL77a], [CTDL77b]
Lamb–Dicke parameter, 312
Lamb-Dicke criterion, 312
Landahl, A., xxi
Landau, L., 119, [Lan27]
Landau, L. J., 398, [LS93]
Landau, S., 59, [DL98]
Landauer’s principle, 153, 569
Maxwell’s demon and, 162
Landauer, R., 1, 168, [Lan61]
Lanford, O. E., 527, [LR68]
Lange, W., 306, 350, 398, [THL+95]
Langevin equations, 353
language, 141
law of large numbers, 541, 609
law of total probability, 608
Lazere, C., 59, [SL98]
Lecerf, Y., 168, [Lec63]
Leff, H. S., 168, [LR90]
Legere, R., xxi
Leggett, A. J., 398, [CL83]
Leibfried, D., 350, [WMI+98]
Leibler, R. A., 526, [KL51]
Leighton, R. B., 59, 118, 168, [FLS65a], [FLS65b]
Leiserson, C. E., 167, 639, [CLR90]
Lenstra, A. K., 246, [LL93]
Lenstra Jr., H. W., 246, [LL93]
Leonhardt, U., 398, [Leo97]
Leung, D. W., xxi, 351, 498, 499, [CGKL98],
[CVZ+98], [LNCY97], [LVZ+99], [ZLC00]
Levin, L., 138, 168, [Lev73]
Levitov, L., 351, [MOL+99]
Lewenstein, M., 351, [ZHSL99]
Li, M., 169, [LTV98], [LV96]
Lidar, D. A., xxi, 498, [BKLW99], [LBW99],
[LCW98]
Lie formula
composition of Lie operations, 207
Lie, S., 215
Lieb’s theorem, 519, 526, 645, 646
Lieb, E. H., xxi, 527, [AL70], [Lie73], [Lie75],
[LR73a], [LR73b]
Lindblad form, 207, 386, 398
Lindblad operators, 388
Lindblad, G., 398, 527, 605, [Lin75], [Lin76], [Lin91]
Linden, N., 351, [BCJ+99], [LKF99], [LP99]
linear algebra, 61, 118
linear code, 445
linear dependence, 63
linear independence, 63
linear operators, 63
linearity of trace, 75
Lipton, R. J., 168, 246, [BL95], [Lip95]
literals, 148
Lloyd, S., 214, 215, 349, 351, 352, 606, [AL97],
[CVZ+98], [LB99], [Llo93], [Llo94], [Llo95],
[Llo96], [Llo97], [LS98], [MOL+99]
Lo, H.-K., xix, 605–607, [BFGL98], [LC99], [Lo99],
[LP97], [LSP98]
local realism, 116
LOCC, 573
logarithmic space, 151
logic gate, 129
logical labeling, 333
Lomont, J. S., 616, [Lom87]
Loss, D., 351, [IAB+99], [LD98]
Louisell, W. H., 349, [Lou73]
Luther, G. G., 607, [HAD+95]
Lynch, N. A., 168, [LLS75]
Lynn, T., xxi
Lytsin, S., 498, [AL99]
Maali, A., 350, [DMB+93]
Maassen, H., 526, [MU88]
Mabuchi, H., xxi, 306, 350, 398, [THL+95]
Macchiavello, C., 59, 246, 497, 498, [CEMM98],
[EM96]
MacWilliams, F. J., 59, 497, [MS77]
magnetic resonance, 326
majorization, 573
Manin, Y. I., xxi, 204, 214, [Man80], [Man99]
Index
671
Marcus, C. M., 351, [HSM+98]
Marcus, M., 118, [MM92]
Margolus, N., 214, [BBC+95]
Markov chain, 509
Markov processes, 354, 355
Marshall, A. W., 606, [MO79]
Martini, F. D., 59, [BBM+98]
Mass, W., 351, [CMP+98]
master equations, 353, 386
matrices, 64
matrix representation of an operator, 64
Mattle, K., 59, 119, [BPM+97], [MWKZ96]
Maurer, U. M., 607, [BBCM95], [CM97], [Mau93]
Maxwell’s demon, 161, 162, 569
Maxwell, J. C., 162, 168, [Max71]
Mayers, D., 607, [May98]
measurement, 84, 185, 356
fault-tolerant, 489
in the Bell basis, 187
in the stabilizer formalism, 463
of an operator, 188
measurement operators, 84, 102
Meekhof, D. M., 350, [MMK+95], [WMI+98]
Menezes, A. J., 59, 275, [MvOV96]
Merkle, R., 11, 644, [Mer78]
metric, 400
Meyer, D. A., 499, [FM98]
Milburn, G. J., xxi, 59, 350, 606, [Mil89a], [Mil96],
[Mil97], [Mil98]
Miller, D. A. B., 350, [Mil89b]
Miller, G. L., 644, [Mil76]
Minc, H., 118, [MM92]
Minsky machine, 165
program, 165
Minsky, M. L., 168, [Min67]
Miquel, C., 497, [LMPZ96]
mirror, 288
Mitchell, J. R., 350, [KMSW99]
mixed state, 100
Modha, D., 605, [CM00]
modular arithmetic, 626
modular exponential, 228
modular exponentiation, 227
monotonicity of the relative entropy, 524
Monroe, C., 322, 350, [MMK+95], [WMI+98]
Mooij, J. E., 351, [MOL+99]
Moore’s law, 4
Moore, G., 4
Mor, T., 214, 215, 351, 605, 607, [BBB+98],
[BMP+99], [Mor98], [VYW+99]
Morgan, G. L., 607, [HAD+95]
Mosca, M., xix, xxi, 59, 246, 276, 351, [BBC+98],
[CEMM98], [JM98], [JMH98], [ME99], [Mos98],
[Mos99]
Mossba¨uer effect, 312
Motwani, R., 168, [MR95]
Muller, A., 607, [MZG96]
multiplicative inverse, 627
Murphy, E. A., Jr., 546
mutual information, classical, 506
mutual information, quantum, 514
Nakamura, Y., 351, [NPT99]
, 20
gate, 130
natural numbers, 625
Nielsen, M. A., xix, 59, 119, 351, 398, 424, 498, 527,
605, 606, [BKN98], [BNS98], [CN97],
[LNCY97], [NC97], [NCSB98], [Nie98],
[Nie99a], [Nie99b], [NKL98], [SN96]
Nisan, N., 424, 499, [ABOIN96], [AKN98]
Niu, C.-S., 214, 246, [GN96]
NMR, 324, 343
no-cloning theorem, 3, 24, 530, 586
noise
classical, 354
quantum, 353
noiseless channel coding theorem, 8, 500
noiseless quantum codes, 498
noisy channel coding theorem, 8
noisy classical channels, 548
non-Abelian groups, 242
non-trace-preserving quantum operations, 367
non-uniform circuit family, 134
, 20
gate, 130
norm, 66
norm of a matrix, 645
normal operator, 70
normalization condition for state vectors, 81
normalized vectors, 66
normalizer of Gn, 461
normalizer operations, 461
fault-tolerant, 482
gate, 129
NP, 40, 142, 263
NPI, 149
nuclear spin, 309
number ﬁeld sieve, 216
O(·), 136
observable, 87
Ohya, M., 526, [OP93]
Olkin, I., 606, [MO79]
one time pad, 583
open quantum systems, 353
operation elements, 360
operator-sum representation, 360
freedom in, 370
optical computer, classical, 296
optical lattice, 346
optical pumping, 312
xenon, 341
, 20
gate, 130
oracle, 221
for quantum searching, 248
models of computation, 129
order
of x modulo N, 226, 633
of a group element, 610
of a permutation, 241
order-ﬁnding problem, 216, 226, 241, 633
Orlando, T. P., 351, [MOL+99]
orthogonal complement, 70
orthogonality, 66
orthonormal decomposition, 69
orthonormality, 66
outer product notation for operators, 67
output of a computation, 123
Ozawa, M., 605, [YO93]
P, 40, 141
Pais, A., 59, 112, [Pai82], [Pai86], [Pai91]
672
Index
Palma, G. M., 607, [EHPP94]
Pan, J. W., 59, [BPM+97]
Papadimitriou, C. M., 138, 168, 639, [Pap94]
parallel computers, 162
parity check matrix, 447
partial trace, 105
Pashkin, Y. A., 351, [NPT99]
Patterson, D. A., 168, [HGP96]
Patterson, M., xxi
Paturi, R., 276, [Pat92]
Pauli gates, see quantum logic gate
Pauli group, 454, 611
Pauli matrices, 65, 174, 427
Paz, J.-P., 497, [LMPZ96]
Pellizzari, T., 498, [CPZ96]
Penrose, R., 59, 167, [Pen89]
Peres, A., 59, 112, 118, 119, 607, [BBC+93],
[EHPP94], [Per88], [Per93], [Per95]
period-ﬁnding problem, 241
period-ﬁnding, quantum algorithm for, 236
Perlis, S., 118, [Per52]
Petroff, M. D., 350, [KSC+94]
Petz, D., 526, 527, [OP93], [Pet86]
phase, 81, 93
phase estimation, quantum algorithm for, 221
phase ﬂip, 81
phase ﬂip channel, 376
phase ﬂip operator, 430
phase gate, 174
phase kicks, as a model of quantum noise, 384
phase shifter, 288
Phoenix, S. J. D., 607, [BP93]
phonon, 311
photodetector, 288
physical quantum operations, 367
Pines, A., xxi
Planck’s constant, 82
Platzman, P. M., 351, [PD99]
Plenio, M. B., 350, 398, 606, [JP99], [PK96], [PK98],
[VP98]
Podolsky, B., 17, 25, 119, [EPR35]
Poisson equation, 205
polar decomposition, 78
polynomial resources, 139
polynomial time, 141
Polzik, E. S., 59, 352, [FSB+98]
Popescu, S., xix, 59, 351, 606, [BBM+98], [BBPS96],
[BBP+96], [BCJ+99], [LP97], [LP99], [LSP98]
Popeye the Sailor, 60
Poplavskii, R. P., 204, [Pop75]
positive deﬁnite operator, 71
positive operator, 71
Positive Operator-Valued Measures, 90
POVM, 90
POVM measurements, 90, 118
completeness relation for, 92
elements for, 90
Poyatos, J. F., 398, [PCZ97]
Preskill, J., xix, xxi, 214, 499, 606, 607, [BCDP96],
[GP10], [Pre97], [Pre98a], [Pre98b], [Pre98c],
[SP00]
Price, M., 351, [CMP+98]
primality decision problem, 141
prime number, 625
prime number theorem, 638
principle of deferred measurement, 185
principle of implicit measurement, 186
privacy ampliﬁcation, 584
private key cryptography, 582, 640
problem,
Addition by Fourier transforms, 244
Alternate characterization of the ﬁdelity, 423
Alternate universality construction, 212
Analogue of the triangle inequality for conditional
entropy, 525
Ancilla bits and efﬁciency of reversible computation,
167
Classical capacity of a quantum channel – Research,
603
Computable phase shifts, 212
Computing with linear optics, 346
Conditional forms of strong subadditivity, 526
Control via Jaynes–Cummings interactions, 347
Database retrieval, 275
Efﬁcient temporal labeling, 346
Encoding by teleportation, 496
Encoding stabilizer codes, 495
Feynman-Gates conversation, 58
Find a hard-to-compute class of functions
(Research), 167
Finding the minimum, 274
Fractran, 166
Functions of the Pauli matrices, 117
Generalized Klein’s inequality, 525
Generalized quantum searching, 274
Generalized relative entropy, 525
Gilbert–Varshamov bound, 495
Hardness of approximation of
, 166
Ion trap logic with two-level atoms, 348
Kitaev’s algorithm, 243
Lindblad form to quantum operation, 395
Linearity forbids cloning, 603
Measured quantum Fourier transform, 243
Methods for achieving capacity – Research, 603
Minimal Toffoli construction, 213
Minsky machines, 165
Non-Abelian hidden subgroups – Research, 244
Non-universality of two bit reversible logic, 166
Prime number estimate, 638
Properties of the Schmidt number, 117
Quantum channel capacity – Research, 603
Quantum searching and cryptography, 275
Random unitary channels, 396
Reversible PSPACE = PSPACE, 167
Reversible Turing machines, 166
Strong subadditivity – Research, 526
Teleportation as a quantum operation, 395
Tsirelson’s inequality, 118
Undecidability of dynamical systems, 166
Universality with prior entanglement, 213
Vector games, 166
process tomography, 308, 389
program for a Turing machine, 123
projective measurements, 87, 282
projectors, 70
promise problems, 243
pseudocode, 126
PSPACE, 150
public key cryptography, 582
public key cryptosystems, 11, 640
Pueschel, M., 246, [PRB98]
Pulver, M., 214, 215, [BMP+99]
pure state, 100
puriﬁcations, 109, 110, 119
Index
673
QKD, 582, 586
B92 protocol, 589
BB84 protocol, 587, 593
EPR protocol, 591
security of, 593
quantum chromodynamics, 206
quantum circuits, 22
quantum code, 435
quantum corollary to Moore’s law, 39
quantum cryptography, 10, 582
quantum dots, 344
quantum efﬁciency, 288
quantum electrodynamics, 2, 206
quantum error-correction, 425
quantum factoring, 216
quantum Fano inequality, 563, 572, 605
quantum ﬁeld theory, 6
quantum Fourier transform, 216
quantum circuit for, 219
quantum gravity, 6
quantum Hall effect, 346
quantum Hamming bound, 444
quantum information theory, 528
data processing inequality, 564
entanglement, 571
entropy exchange, 561
Holevo bound, 531
quantum cryptography, 582
quantum Fano inequality, 561
Schumacher’s noiseless coding theorem, 542
Singleton bound, 568
summary of important relations, 572
quantum key distribution, 586
quantum logic gate
π/8, 174, 485
controlled phase-ﬂip, 319
controlled-
, 20, 178, 321, 482
Hadamard, 174, 483
Pauli, 483
phase, 174, 483
single qubit, 174, 319
swap, 320
Toffoli, 29, 485
quantum money, 56
quantum noise, 353
estimates of, 278
quantum operations, 353, 353
chi-matrix representation, 391
environmental models for, 363, 365
limitations to the formalism, 394
non-trace-preserving, 367
partial trace map, 374
physical, 367
trace map, 374
trace-preserving, 367
quantum process tomography, 389, 398
quantum search algorithm, 248, 339
quantum searching, 263
quantum source, 542
quantum state tomography, 389, 398
quantum teleportation, see teleportation, quantum
quantum trajectories, 398
quantum Turing machine, 203
qubit, 13, 80, 605
charge representation, 344
harmonic oscillator representation, 283
implementation, 277
photon representation, 287
spin representation, 309, 324, 345
square well representation, 280
superconductor representation, 344
qutrit, 203, 343, 359
Rabi frequency, 318
Rabi oscillations, 303
Rabin, M. O., 120, 644, [Rab80]
Raghavan, P., 168, [MR95]
Rahim, H. Z., 59, [Rah99]
Raimond, J. M., 350, [DMB+93], [DRBH87],
[DRBH95]
Rains, E. M., 9, 497, 498, [CRSS97], [CRSS98],
[Rai98], [Rai99a], [Rai99b], [Rai99c], [RHSS97]
Rajagopalan, S., xxi
Ramo, S., 350, [RWvD84]
random coding, 549
randomized algorithms, 5
rank of a Hermitian operator, 117
Rasetti, M., 498, [ZR98]
read-write tape-head for a Turing machine, 123
Reck, M., 214, 350, [RZBB94]
reduced density operator, 105
reducibility of one language to another, 145
reduction, 144
reduction of factoring to order-ﬁnding, 232, 633
refocusing, 331
relative entropy, 526
classical, 504
quantum, 511
relative phase, 93
remainders, 626
R`enyi entropy, 584
Ressler, A., 168, [Res81]
reversible logic gates, 153
reversible Turing machine, 155
Rex, R., 168, [LR90]
Risk, W., xxi
Risk, W. P., 607, [BR00], [BR98]
Risken, H., 398, [VR89]
Rivest, R. L., 11, 167, 639, 644, [CLR90], [RSA78]
Robert, J. M., 607, [BBR88]
Robinson, D. W., 527, [LR68], [RR67]
Rockmore, D., 616, [DR90]
Roetteler, M., 246, [PRB98], [RB98]
Rosen, N., 17, 25, 119, [EPR35]
rotation operators, 174
Roychowdhury, V., 214, 215, 351, [BMP+99],
[VYW+99]
Royer, A., 398, [Roy96]
RSA cryptosystem, 11
Ruelle, D., 527, [RR67]
Ruskai, M. B., xxi, 424, 527, 605, [KR99], [LR73a],
[LR73b], [Rus94]
Sakurai, J. J., 59, 118, 349, [Sak95]
Saleh, B. E. A., 349, [CST89], [ST91]
Salvail, L., 606, 607, [BBB+92], [BS94]
Sands, M., 59, 118, 168, [FLS65a], [FLS65b]
Sanpera, A., 351, [ZHSL99]
scalar, 62
scanning tunneling microscope, 3
Schack, R., 351, [BCJ+99], [SC99]
Schauer, M., 607, [HAD+95]
Schmidt bases, 110
Schmidt co-efﬁcients, 109
674
Index
Schmidt decomposition, 109, 119
Schmidt number, 110
Schmidt, E., 119, [Sch06]
Schneider, S., xxi
Schneier, B., 59, 275, [Sch96a]
Schrader, R., xxi
Schr¨odinger equation, 82, 205, 280, 284, 301
quantum simulation of, 209
Schr¨odinger’s cat, 387
Schr¨odinger, E., 119, [Sch36]
Schulman, L. J., 351, [SV99]
Schumacher compression, 547
Schumacher’s quantum noiseless coding theorem, 542
Schumacher, B. W., xxi, 8, 398, 424, 605–607,
[BBPS96], [BBP+96], [BCF+96], [BFJS96],
[BNS98], [HJS+96], [JS94], [NCSB98], [Sch95],
[Sch96b], [SN96], [SW97], [SW98], [SWW96],
[WS98]
Schwindt, P. D. D., 350, [KMSW99]
security of quantum key distribution, 593
selection rules, electric dipole, 300
self-adjoint operator, 70
Selman, A. L., 168, [LLS75]
Shamir, A., 11, 641, 644, [RSA78]
Shannon entropy, 500, 526
Shannon’s noiseless coding theorem, 500, 537
Shannon, C. E., 8, 59, 526, 604, 605, [Sha48], [SW49]
Shasha, D., 59, [SL98]
Sherwin, M., 351, [IAB+99]
Sherwood, M. H., 351, [LVZ+99], [VYSC99]
shift-invariance property of the Fourier transform, 237
Shimony, A., 119, [CHSH69]
Shockley, W., 4
Shor code, 432
Shor’s algorithm, 7
Shor, J., 216
Shor, P. W., xix, xxi, 6, 8, 9, 214, 216, 245, 246, 265,
432, 450, 497–499, 604, 606, 607, [BBC+95],
[BS98], [CRSS97], [CRSS98], [CS96], [DS96],
[DSS98], [RHSS97], [Sho94], [Sho95], [Sho96],
[Sho97], [SL97], [SP00], [SS96]
similarity transformation, 75
Simon’s problem, 241
Simon, B., 527, [Sim79]
Simon, D. R., 246, [Sim94], [Sim97]
simple cycle, 143
simple harmonic oscillator, 277, 283
simulation of one computational model by another, 126
simulations, quantum and classical, 204
single photons, 287, 296, 343
Singleton bound, 445
quantum, 568
singular value decomposition, 78
singular values, 79
Sleator, T., 214, [BBC+95]
Slepian, D., 59, [Sle74]
Slichter, C. P., 351, [Sli96]
Sloane, N. J. A., 59, 497, 498, [CRSS97], [CRSS98],
[MS77], [RHSS97], [SW93]
Slotine, J. E., 352, [LS98]
Small, A., 351, [IAB+99]
Smolin, J. A., 214, 497, 606, 607, [BBB+92],
[BBC+95], [BBP+96], [BDS97], [BDSW96],
[BST98], [DSS98], [SS96]
Solovay, R., 5, 624, 644, [SS76]
Solovay–Kitaev theorem, 197, 617
Somaroo, S. S., 351, [CMP+98], [STH+99]
Sørensen, J. L., 59, 352, [FSB+98]
Sornborger, A. T., 214, 215, [SS99]
sorting, 137
source–channel coding, 553
space hierarchy theorem, 151
space-bounded computation, 150
span of a set of vectors, 63
spanning set, 62
spatial labeling, 333
spectral decomposition, 70, 72
Spiller, T., xix, [LSP98]
spin, 310
spin orbit interaction, 301
spin singlet, 113
spin valve, 345
spin–lattice relaxation, 280, 330
spin–spin relaxation, 280, 330
spontaneous emission, 315, 380
spur, 559
square well, 280
stabilizer codes, 453, 465
stabilizer formalism, 453
stabilizer of a vector space, 454
standard deviation of a random variable, 609
standard form
for a stabilizer code, 470
for parity check matrix, 447
starting state of a Turing machine, 123
state space, 80, 102
state tomography, 389
with NMR, 336
state vector, 80
static measures of distance, 399
stationary states, 83
Steane code, 453
Steane, A. M., 8, 350, 450, 497, 499, [Ste96a],
[Ste96b], [Ste97], [Ste99]
Steinberg, A. M., 350, [KSC+94]
Stern, O., 43
Stern–Gerlach experiment, 43
Stewart, E. D., 214, 215, [SS99]
Stiefel, E., 616, [FS92]
Stirzaker, D. R., 609, [GS92]
stochastic differential equations, 353
stochastic processes, 355
Stoll, S., xxi
Strang, G., 118, [Str76]
Strassen, V., xxi, 5, 216, 644, [SS76]
Streater, R. F., 398, [LS93]
strict concavity, 504
strictly self-dual linear codes, 449
string theory, 6
strong concavity of the ﬁdelity, 414
strong convexity of trace distance, 408
strong subadditivity
classical, 506
proof of, 519
quantum, 519
structural complexity, 168
subadditivity
classical, 506
quantum, 516
subgroup, 610
subset-sum, 149
superconductor, 344
superdense coding, 17, 97, 119, 275, 352
superposition principle, 94
Index
675
superpositions, 81
support of a Hermitian operator, 105
swap operation in Fourier transform, 219
Switkes, M., 351, [HSM+98]
Szilard, L., 168, [Szi29]
T1, 280, 330
T2, 280, 330
tape for a Turing machine, 123
Tapp, A., 59, 276, [BBHT98], [BHT98]
Tarjan, R., 217
Taylor, W., 214, [BT97]
Teich, M. C., 349, [ST91]
teleportation, quantum, 17, 26, 352
temporal labeling, 333
tensor product, 62, 71, 72
Terhal, B. M., 215, 606, [BST98], [TD98]
Thapliyal, A., 119
theorem
Z-Y decomposition for a single qubit, 175
Basic properties of Shannon entropy, 506
Basic properties of von Neumann entropy, 513
Birkhoff’s theorem, 574
Chaining rule for conditional entropies, 508
Characterization of density operators, 101
Chinese remainder theorem, 629
Concavity of the quantum conditional entropy, 520
Convexity of the relative entropy, 520
Cook–Levin, 146
Data processing inequality, 509
Error-correction conditions for stabilizer codes, 466
Euler’s theorem, 144
Fannes’ inequality, 512
Fermat’s little theorem, 630
Fundamental theorem, 613
Fundamental theorem of arithmetic, 625
Gottesman–Knill theorem, 464
High ﬁdelity implies low entropy, 594
Holevo–Schumacher–Westmoreland (HSW)
theorem, 555
Information gain implies disturbance, 586
Klein’s inequality, 511
Lagrange’s theorem, 610
Law of large numbers, 541
Lieb’s theorem, 520, 646
Monotonicity of the ﬁdelity, 414
Monotonicity of the relative entropy, 524
Non-negativity of the relative entropy, 505
Polar decomposition, 78
Projective measurements increase entropy, 515
Quantum data processing inequality, 564
Quantum error-correction conditions, 436
Quantum Fano inequality, 563
Representation theorem for the gcd, 626
Schmidt decomposition, 109
Schumacher’s noiseless channel coding theorem, 544
Schur’s lemma, 613
Shannon’s noiseless channel coding theorem, 540
Shannon’s noisy channel coding theorem, 553
Simultaneous diagonalization theorem, 77
Singular value decomposition, 79
Solovay–Kitaev theorem, 618
Spectral decomposition, 72
Strong concavity of the ﬁdelity, 414
Strong convexity of the trace distance, 407
Strong subadditivity, 521
Subadditivity of the conditional entropy, 523
The Chernoff bound, 154
The Holevo bound, 531
Theorem of typical sequences, 539
Trace-preserving quantum operations are
contractive, 406
Trotter formula, 207
Typical subspace theorem, 543
Uhlmann’s theorem, 410
Unitary freedom in the ensemble for density
matrices, 103
Unitary freedom in the operator-sum representation,
372
thermal equilibrium, 328
Thomas, J. A., 59, 526, 539, 604, 605, [CT91]
threshold condition, 480, 493
threshold for quantum computation, 493
threshold theorem, 425
Tian, L., 351, [MOL+99]
Tiech, M. C., 349, [CST89]
time hierarchy theorem, 151
TIMEf(n), 141
TOCSY, 339
Toffoli gate, 29, 155, 159
control bits, 159
fault-tolerant, 488
target bit, 159
Toffoli, T., 168, [FT82]
tomography, 389
trace distance, 618
classical, 400
operational meaning for, 400
quantum, 403
trace inner product, 76
trace of a matrix, 75
trace-preserving quantum operations, 360, 367
tractable computational problems, 139
transistor, 4
transpose, 62, 70
transpose operation, 368
transversal property of fault-tolerant operations, 483
triangle inequality, 516
Tromp, J., 169, [LTV98]
Trotter formula, 207
Trotter, H. F., 214, [Tro59]
Tsai, J. S., 351, [NPT99]
Tseng, C. H., 351, [STH+99]
Tsirelson’s inequality, 118, 119
Tsirelson, B. S., 119, [Tsi80]
Turchette, Q. A., 306, 307, 350, 398, [THL+95],
[Tur97]
Turing machine, 4, 120, 122
quantum, 203
Turing number, 125
Turing, A. M., 4, 59, 122, 125, 167, [Tur36]
two-level atom, 298, 327
resonance, 326
two-level unitary matrices, 189
typical sequences, 538
Ueda, M., 350, [KU91]
Ufﬁnk, J. H. B., 526, [MU88]
Uhlmann’s formula, 411
Uhlmann’s theorem, 410
Uhlmann, A., xxi, 119, 424, 527, 606, [Uhl70],
[Uhl71], [Uhl72], [Uhl73], [Uhl76], [Uhl77]
Umegaki, H., 526, [Ume62]
uncomputation, 158
676
Index
uniform circuit family, 134
unit vector, 66
unitary evolution, 356
unitary operators, 70
approximation, 194
efﬁcient decompositions, 191, 198
Universal Turing Machine, 4, 128
universality, 281
discrete set of universal operations, 194
family of quantum gates, 188, 281
of the Fredkin gate, 157
proof of, 132
single qubit gates and
, 191
two-level unitary operators, 189
Vaidman, L., 352, [Vai94]
van Dam, W., 276, [van98a]
van de Graaf, J, 424
van de Graaf, J., 424, 607, [BBB+98], [FvdG99]
van der Waal, C. H., 351, [MOL+99]
van Duzer, T., 350, [RWvD84]
van Enk, S. J., xxi, 605, [van98b]
van Oorschot, P. C., 59, 275, [MvOV96]
Vandersypen, L. M. K., xxi, 215, 351, [CVZ+98],
[LVZ+99], [VYSC99]
Vanstone, S. A., 59, 275, [MvOV96]
variance of a random variable, 609
Vatan, F., 214, 215, [BMP+99]
Vazirani, U., xxi, 200, 214, 276, 351, [BBBV97],
[BV97], [SV99]
vector game, 166
vector spaces, 61
vector subspace, 62
vectors, 61
Vedral, V., 606, [Ved99], [VP98]
Verhulst, A., xxi
Vernam cipher, 583
vertex cover, 149
vertices, 143
Vidal, G., 351, 606, [Vid98], [Vid99]
Viola, L., 498, [KLV99]
Vitanyi, P., 169, [LTV98], [LV96]
Vogel, K., 398, [VR89]
von Neumann entropy, 54, 510, 526
von Neumann, J., 4, 119, 164, 168, 498, [von27],
[von56], [von66]
Vrijen, R., 351, [VYW+99]
Wallach, D., xxi
Walsh–Hadamard transform, 31
Wang, K., 351, [VYW+99]
Warren, W., 351, [War97]
Watanabe, K., 350, [WY90]
Watrous, J., 214, [Wat99]
weakly self-dual linear codes, 449
Weaver, W., 59, 604, 605, [SW49]
Wehrl, A., 526, 527, [Weh78]
Weinfurter, H., 59, 119, 214, [BBC+95], [BPM+97],
[MWKZ96]
Welsh, D. J. A., 497, [Wel88]
Westmoreland, M. D., xxi, 605, 607, [HJS+96],
[SW97], [SW98], [SWW96], [WS98]
Whaley, K. B., 498, [BKLW99], [LBW99], [LCW98]
Whinnery, J. R., 350, [RWvD84]
White, A. G., 350, [KMSW99]
Wiesner, S. J., 9, 10, 119, 214, 215, 606, [BW92], 663,
[Wie83], [Wie96], [Wie]
Wigner, E. P., 527, [WY63]
Wilczek, F., 215, [CW95]
Williams, D., 609, [Wil91]
Wilson, E. O., 1
Wineland, D. J., xxi, 323, 350, [MMK+95], [WMI+98]
Winfree, E., 168, [Win98]
Winograd, S., 168, 498, [WC67]
Wiseman, H., xxi
witness, 142
Wokaun, A., 351, [EBW87]
Wootters, W. K., 59, 119, 497, 604–606, [BBC+93],
[BBP+96], [BDSW96], [HJS+96], [HJW93],
[SWW96], [WZ82]
word of length l from G, 618
work bits, 131
Wright, E. M., 246, 639, [HW60]
Wyner, A. D., 59, [SW93]
, 20
gate, 130
Yablonovitch, E., 351, [VYW+99]
Yamaguchi, F., 351, [YY99]
Yamamoto, Y., xxi, 349–351, 498, [CY95], [IY94],
[LNCY97], [WY90], [YKI88], [YY99]
Yanase, M. M., 527, [WY63]
Yannoni, C. S., 351, [LVZ+99], [VYSC99]
Yao, A. C., 214, [Yao93]
Yard, J., xxi
Younis, S., 168, [YK95]
Yuen, H. P., 605, [YO93]
Yurke, B., xxi, 349
Z gate, 19
Zalka, C., 214, 215, 276, [Zal98], [Zal99]
Zanardi, P., 498, [Zan99], [ZR98]
Zbinden, H., 607, [MZG96]
Zeilinger, A., 59, 119, 214, 350, [BPM+97],
[MWKZ96], [RZBB94]
zero operator, 63
zero vector, 62
Zhou, X. L., xxi, 246, 351, 499, [CVZ+98], [LVZ+99],
[ZLC00]
Zoller, P., 350, 398, 498, [CPZ96], [CZ95], [PCZ97],
[ZG97]
Zurek, W. H., xxi, 351, 398, 497–499, 604, 606,
[CMP+98], [KLZ98a], [KLZ98b], [LMPZ96],
[WZ82], [ZL96], [Zur89], [Zur91]
Zyczkowski, K., 351, [ZHSL99]

============================================================
Quantum Computing: An Applied Approach (Hidary)
============================================================

Jack D. Hidary
Quantum 
Computing: 
An Applied 
Approach
Second Edition
Quantum Computing: An Applied Approach 
Jack D. Hidary 
Quantum Computing:  
An Applied Approach 
 
Second Edition 
 
Jack D. Hidary 
ISBN 978-3-030-83273-5 
 
ISBN 978-3-030-83274-2 (eBook) 
https://doi.org/10.1007/978-3-030-83274-2 
 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of 
illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the 
editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations. 
 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Palo Alto, CA, USA
1st edition: © Jack D. Hidary under exclusive license to Springer Nature Switzerland AG 2019 
2nd edition: © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2021 
Contents
Preface to the Second Edition
xiii
Preface to the First Edition
xv
Acknowledgements
xix
Navigating this Book
xxi
I
Foundations
1 Superposition, Entanglement and Reversibility
3
1.1 Superposition and Entanglement
4
1.2 The Born Rule
5
1.3 Schrödinger’s Equation
8
1.4 The Physics of Computation
11
2 A Brief History of Quantum Computing
15
2.1 Early Developments and Algorithms
16
2.2 Shor and Grover
18
2.3 Deﬁning a Quantum Computer
19
3 Qubits, Operators and Measurement
23
3.1 Quantum Operators
28
Unary Operators
28
Binary Operators
32
Ternary Operators
34
v
vi
Contents
3.2 Comparison with Classical Gates
36
3.3 Universality of Quantum Operators
37
3.4 Gottesman-Knill and Solovay-Kitaev
37
3.5 The Bloch Sphere
38
3.6 The Measurement Postulate
39
3.7 Computation-in-Place
41
4 Complexity Theory
43
4.1 Problems vs. Algorithms
43
4.2 Time Complexity
44
4.3 Complexity Classes
46
4.4 Quantum Computing and the Church-Turing Thesis
49
II
Hardware and Applications
5 Building a Quantum Computer
53
5.1 Assessing a Quantum Computer
54
5.2 Neutral Atoms
55
5.3 NMR
56
5.4 NV Center-in-Diamond
57
5.5 Photonics
58
5.6 Spin Qubits
60
5.7 Superconducting Qubits
61
5.8 Topological Quantum Computation
63
5.9 Trapped Ion
64
5.10 Summary
65
Contents
vii
6 Development Libraries for Quantum Computer Programming
67
6.1 Quantum Computers and QC Simulators
68
6.2 Cirq
70
6.3 Qiskit
72
6.4 Forest
75
6.5 Quantum Development Kit
77
6.6 Dev Libraries Summary
80
Using the Libraries
81
Other Development Libraries
81
6.7 Additional Quantum Programs
82
Bell States
82
Gates with Parameters
84
7 Teleportation, Superdense Coding and Bell’s Inequality
87
7.1 Quantum Teleportation
87
7.2 Superdense Coding
90
7.3 Code for Quantum Teleportation and Superdense Com-
munication
91
7.4 Bell Inequality Test
94
8 The Canon: Code Walkthroughs
101
8.1 The Deutsch-Jozsa Algorithm
103
8.2 The Bernstein-Vazirani Algorithm
110
8.3 Simon’s Problem
113
8.4 Quantum Fourier Transform
114
8.5 Shor’s Algorithm
117
RSA Cryptography
117
The Period of a Function
119
Period of a Function as an Input to a Factorization Algo-
rithm
121
viii
Contents
8.6 Grover’s Search Algorithm
135
Grover’s Algorithm in Qiskit
140
3-Qubit Grover’s Algo
141
9 Quantum Computing Methods
143
9.1 Variational Quantum Eigensolver
143
VQE with Noise
148
More Sophisticated Ansatzes
150
9.2 Quantum Chemistry
151
9.3 Quantum Approximate Optimization Algorithm (QAOA)
156
Example Implementation of QAOA
159
9.4 Machine Learning on Quantum Processors
167
9.5 Quantum Phase Estimation
174
Implemention of QPE
177
9.6 Solving Linear Systems
180
Description of the HHL Algorithm
182
Example Implementation of the HHL Algorithm
184
9.7 Quantum Random Number Generator
192
9.8 Quantum Walks
194
Implementation of a Quantum Walk
196
9.9 Uniﬁcation Framework for Quantum Algorithms (QSVT)
202
9.10 Dequantization
203
9.11 Summary
205
10 Applications and Quantum Supremacy
207
10.1 Applications
207
Quantum Simulation and Chemistry
207
Sampling from Probability Distributions
208
Linear Algebra Speedup with Quantum Computers
208
Optimization
208
Tensor Networks
208
Contents
ix
10.2 Quantum Supremacy
208
Random Circuit Sampling
209
Other Problems for Demonstrating Quantum Supremacy
214
10.3 Quantum Error Correction
215
Context and Importance
215
Important Preliminaries
216
Motivating Example: The Repetition Code
217
The Stabilizer Formalism
219
10.4 Doing Physics with Quantum Computers
224
III
Toolkit
11 Mathematical Tools for Quantum Computing I
227
11.1 Introduction and Self-Test
227
11.2 Linear Algebra
229
Vectors
229
Introduction to Dirac Notation
230
Basic Vector Operations
231
The Norm of a Vector
234
The Dot Product
237
11.3 The Complex Numbers and the Inner Product
240
Complex Numbers
240
The Inner Product as a Reﬁnement of the Dot Product
242
The Polar Coordinate Representation of a Complex Num-
ber
246
11.4 A First Look at Matrices
254
Basic Matrix Operations
254
The Identity Matrix
261
Transpose, Conjugate and Trace
263
Matrix Exponentiation
270
11.5 The Outer Product and the Tensor Product
271
The Outer Product as a Way of Building Matrices
271
214
Quantum Advantage and Beyond Classical Computation
x
Contents
The Tensor Product
273
11.6 Set Theory
276
The Basics of Set Theory
276
The Cartesian Product
279
Relations and Functions
280
Important Properties of Functions
286
11.7 The Deﬁnition of a Linear Transformation
291
11.8 How to Build a Vector Space From Scratch
293
Groups
294
Rings
300
Fields
305
The Deﬁnition of a Vector Space
308
Subspaces
311
11.9 Span, Linear Independence, Bases and Dimension
313
Span
313
Linear Independence
315
Bases and Dimension
317
Orthonormal Bases
320
12 Mathematical Tools for Quantum Computing II
323
12.1 Linear Transformations as Matrices
323
12.2 Matrices as Operators
328
An Introduction to the Determinant
328
The Geometry of the Determinant
332
Matrix Inversion
334
12.3 Eigenvectors and Eigenvalues
341
Change of Basis
344
12.4 Further Investigation of Inner Products
346
The Kronecker Delta Function as an Inner Product
349
12.5 Hermitian Operators
350
Why We Can’t Measure with Complex Numbers
350
Hermitian Operators Have Real Eigenvalues
352
Contents
xi
12.6 Unitary operators
353
12.7 The Direct Sum and the Tensor Product
354
The Direct Sum
354
The Tensor Product
357
12.8 Hilbert Space
360
Metrics, Cauchy Sequences and Completeness
360
An Axiomatic Deﬁnition of the Inner Product
364
The Deﬁnition of Hilbert Space
365
12.9 The Qubit as a Hilbert Space
366
13 Mathematical Tools for Quantum Computing III
371
13.1 Boolean Functions
371
13.2 Logarithms and Exponentials
372
13.3 Euler’s Formula
374
14 Dirac Notation
377
14.1 Vectors
377
14.2 Vector operations
378
Inner and Outer Products
378
14.3 Tensor Products
379
14.4 Notation for PDF and Expectation Value
380
15 Table of Quantum Operators and Core Circuits
383
Works Cited
387
Index
421
Preface to the Second Edition
xiii
The ﬁeld of quantum computing has grown rapidly in the two years since the
printings in its ﬁrst year which mirrors the growing number of individuals
ramping up in the sector. I would like to thank all the faculty, students and
other readers from so many countries who provided helpful input for this new
edition.
Our world, of course, changed in many other ways as well since the
publication of the ﬁrst edition. The global pandemic impacted all areas
of society and will probably transform how we work together for years to
come. While in-person meetings were halted, quantum tech researchers and
engineers found ways to continue their collaborations online.
In the last year, many countries have announced new or expanded quantum
tech initiatives. These programs are injecting signiﬁcant resources into the
quantum tech ecosystem. We have also noted the rise in private investment in
the sector – both in quantum computing hardware companies as well as various
applications startups. All this activity has sharply increased the demand for
individuals who are trained in quantum computing as well as those with
additional hands-on experience in coding for these machines. This, in turn,
has led many universities to launch and expand their quantum information
science curricular programs and for corporations to start quantum tech training
programs for their teams.
With this new edition I continue to strive towards the goal of making
quantum computing more accessible to a wider range of individuals. This is
a technical book, but one that I hope opens the door to quantum computing
for individuals who would like to get a rigorous grasp of how to make these
novel computing devices work and which kinds of problems we should focus
on when using these platforms.
ﬁrst publication of this book. The ﬁrst page of the book went through ﬁve
xiv
Preface to the Second Edition
This second edition contains several updates and expanded areas of cover-
age, including:
 Expanded coverage of quantum machine learning and quantum error
correction
 Expanded chapter on principles of quantum mechanics
 Updated chapter on quantum hardware
 Coverage of the QSVT framework
 New chapter on Dirac notation
 This new edition also corrects known typos from the ﬁrst edition (thank-
fully, there were not many). If any new typos have entered into the
second edition please make a note of them in the online site.
 All the code throughout has been re-tested and updated for the current
versions of the various quantum computing frameworks and libraries.
All of these additions and updates are based on requests from readers. The
companion online site for the book is also updated and I recommend that
readers check the site for a range of resources including updated downloadable
code, problem sets and further instructional material.1
I look forward to all the creativity that will emerge in this ﬁeld and the
positive impact the discipline will have on so many important ﬁelds. Please
continue to send in your input via the online site.
Jack D. Hidary
1http://www.github.com/jackhidary/quantumcomputingbook
September 2021
Preface to the First Edition
We are entering a new era of computation that will catalyze discoveries in
science and technology. Novel computing platforms will probe the fundamen-
tal laws of our universe and aid in solving hard problems that affect all of us.
Machine learning programs powered by specialized chips are already yielding
breakthrough after breakthrough.
In this book we will explore quantum computing – an emerging platform
that is fundamentally different than the way we compute with current digital
platforms. To be sure, we are years away from scaled quantum computers. Yet,
we now know that such systems are possible; with advances in engineering
we are likely to see real impact.
Quantum computing is part of the larger ﬁeld of quantum information
sciences (QIS). All three branches of QIS – computation, communication and
sensing – are advancing at rapid rates and a discovery in one area can spur
progress in another. Quantum communication leverages the unusual properties
of quantum systems to transmit information in a manner that no eavesdropper
can read. This ﬁeld is becoming increasingly critical as quantum computing
drives us to a post-quantum cryptography regime. We will cover quantum tele-
portation and superdense coding, which are both quantum-speciﬁc protocols,
in chapter 7.
Quantum sensing is a robust ﬁeld of research which uses quantum devices
to move beyond classical limits in sensing magnetic and other ﬁelds. For ex-
ample, there is an emerging class of sensors for detecting position, navigation
and timing (PNT) at the atomic scale. These micro-PNT devices can provide
highly accurate positioning data when GPS is jammed or unavailable.
In this book we will focus on quantum computation. One of the critical
differences between quantum and classical computation is that in quantum
computation we are manipulating quantum states themselves; this gives us
a much larger computing space to work in than in classical computers. In
classical computers, if we wish to model a real-world quantum physical
xv
xvi
Preface to the First Edition
system, we can only do so with representations of such a system and we
cannot implement the physics itself.
This key difference leads to exciting possibilities for the future of comput-
ing and science. All this starts with fundamental truths about our world that
were developed during the quantum mechanics revolution in the ﬁrst half of
the 20th century. We will review a number of these core concepts in the ﬁrst
chapter.
I had the fortunate circumstance to have studied quantum mechanics before
learning classical physics and therefore relate to quantum physics as the norm
– it is my intellectual home. Until we change our educational system, most
students will learn the classical before the quantum and so the quantum will
seem doubly strange — both from their own human experience as well as from
the inculcation of classical ideas before quantum ideas can be introduced.
What is ironic about this state of affairs is that the primary mathematical
tool in quantum mechanics is linear algebra, a powerful but very accessible
branch of mathematics. Most students, however, only take linear algebra after
two or three semesters of calculus, if they take it at all. Yet, no calculus is
needed to introduce linear algebra! In any case, we will leave the remedying
of mathematics education to another day while we embark here on a journey
into a new form of computing.
In this book we will explore how to build a computer of a very different
kind than humans have ever built before. What is distinct about this book is
that we will go beyond the theoretical into the practical work of how we can
build such computers and how we can write applications for these systems.
There are now several development libraries which we can use to program
cloud-based quantum systems. We will walk through code examples and show
the reader how to build a quantum circuit comprised of a set of operators to
address a particular challenge. We will mainly use Python in this book.
We are currently in the regime of noisy intermediate-scale quantum (NISQ)
computers, a term coined by John Preskill of CalTech [224]. This refers to
systems that do not yet have full error-correction (thus noisy) and have dozens
to thousands of qubits – well short of the 106C necessary for scaled fault-
tolerant computing. Despite the limitations of these initial systems, the theory,
algorithms and coding techniques we cover in this book will serve readers as
they transition to larger systems that are to come in the future.
Preface to the First Edition
xvii
This work is three books in one: the ﬁrst part covers the necessary frame-
work that drives the design of quantum computers and circuits. We will also
explore what kinds of problems may be amenable to quantum computation in
our treatment of complexity classes.
The second part of the book is for those readers who wish to delve into
the programming that makes these new machines tick. If you already have
a background in quantum mechanics, quantum information theory and theo-
retical computer science (you know who you are!), you can jump right to the
second part and dig into the code. Please refer to the navigation guide in the
following pages to chart a course through this material.
In the third part we provide a set of critical tools to use in the journey to
master quantum computing (QC). We build up the core concepts of linear
algebra and tie them speciﬁcally to their use in QC. The table of operators
and circuit elements in chapter 15 is a handy reference as you design your
own quantum computing protocols.
The book is also a portal to the growing body of literature on the sub-
ject. We recommend that the reader use the bibliography to explore both
foundational and recent papers in the ﬁeld.
We will provide further online examples and code tutorials on a continual
basis. This is a living text that will develop as QC technology matures. We
are all travelers together on this new adventure; join us online at this book’s
GitHub site.2 We are excited to see what you will develop with these new
platforms and tools. Contact us via the site — we look forward to hearing
from you.
Jack D. Hidary
June 2019
35,000 ft up
2http://www.github.com/jackhidary/quantumcomputingbook
Acknowledgements
Let me start by thanking my publisher, Elizabeth Loew, who has been so
supportive throughout the process, and the entire SpringerNature team for
their excellence.
A book like this is a signiﬁcant undertaking and it took a team of people
to help in so many ways. The greatest of appreciation to Stefan Leichenauer
who did a great job as book editor and formatter-in-chief. Stefan reviewed
large parts of the book and I thank him for his commitment to the project. The
entire book is written in TeX and we often pushed the boundaries of what TeX
is capable of implementing.
Thank you to Sheldon Axler, author of Linear Algebra Done Right (also
by Springer) who generously shared his TeX template so that we could format
the book correctly for the Springer standards.
Thanks to the many experts who gave of their time to review key sections
of the book and provide technical advice. These include (in alphabetical
order): Scott Aaronson, Ryan Babbush, Sergio Boixo, Rufﬁn Evans, Eddie
Farhi, Patrick Hayden, Gerry Gilbert, Matt Reagor and Lenny Susskind. Each
improved the work materially with their input.
I would also like to recognize the signiﬁcant achievement of Michael
Nielsen and Isaac Chuang in developing their textbook [206]. Recognition
as well to John Preskill for his in-depth lecture notes [222]. Nielsen, Chuang
and Preskill as well as Mermin [194] and Rieffel [236] have helped many
individuals enter the ﬁeld.
Thanks to my many colleagues at Alphabet, Google, Sandbox and X who
have encouraged this effort including: Sergey Brin, Astro Teller and Hartmut
Neven and his excellent team.
Thank you to the many participants in the workshops I taught on quantum
computing as well as my courses on linear algebra and other mathematical
topics. Your feedback has been invaluable.
xix
xx
Acknowledgements
Hearty thanks to James Myer who worked with me intensively on the math
sections; James’ methodical approach assured us of success. We reviewed
these sections countless times, continually reworking them. James is not
only passionate about mathematics; he also cares deeply about pedagogy and
we had productive discussions on the best way to present the core concepts.
Thank you as well to Tai-Danae Bradley who also reviewed the math sections
and made very helpful suggestions.
Thank you to Ryan LaRose who worked with me on the code sections.
In this emerging ﬁeld where the code frameworks have only been developed
within the last few years, solid information and examples can be hard to come
by. Ryan combined great skill in research as well as compiling the information
in succinct forms. Ryan also did a great job on the book’s GitHub site.
Thank you to Ellen Cassidy who did such a professional job proofreading
the text for grammar and consistency of format. Ellen has an eagle eye and
I commend her work to any author. Joe Tricot also did a wonderful job
Naturally, even with all this help, there will remain items to ﬁx for the next
edition. All remaining errors are mine and I will be posting updates on the
GitHub site and then include ﬁxes in upcoming versions.
Thank you to my parents, David and Aimee Hidary, and my entire wonder-
ful family for their support through this process. It is a great feeling to share
this accomplishment with you.
coordinating the overall process. Thanks also to Nathan Schor and Sam
Ritchie for their eagle eyes on typo corrections for the second edition.
Navigating this Book
Here are our suggestions to make the best use of this book:
1. University instructors: You can build several different courses with
the material in this book. All code from the book is on the book’s
website. The math chapters have exercises embedded throughout; for
other chapters please consult the online site for coding exercises and
other problem sets.
(a) Course in Quantum Computing for STEM majors:
i. For this course we recommend assigning chapters 1 and 2
as pre-reading for the course and then proceeding chapter
by chapter with the exercises provided on the GitHub site.
Solutions are also available on the site.
ii. If the students do not have sufﬁcient depth in formal linear
algebra and related mathematical tools, Part III forms a strong
basis for a multi-week treatment with exercises.
(b) Course in Quantum Computing for physics graduate students:
i. For this course, we recommend using this book in conjunction
with Mike and Ike (which is the way many of us refer to
Nielsen and Chuang’s excellent textbook [206]) or another
suitable text which covers the theoretical concepts in depth.
We all owe a huge debt of gratitude to Michael Nielsen,
Isaac Chuang and authors of other textbooks over the last
twenty years. We also recommend referring to John Preskill’s
lecture notes as you build your course for advanced physics
students [222]. Our work is meant to be complementary to
Mike and Ike in several respects:
A. This work is more focused on coding. For obvious rea-
sons, books written prior to the past few years could not
have covered the dev tools and Python-based approaches
to quantum computing that now exist.
xxi
xxii
Navigating this Book
B. This book does not go into the depth that Mike and Ike
does on information theoretic concepts.
C. This book’s mathematical tools section has a more de-
tailed ramp-up for those students who may not have taken
a rigorous linear algebra course. The short summaries
of linear algebra and other requisite math tools in other
textbooks on quantum mechanics are often insufﬁcient
in our experience.
ii. We recommend ﬁrst assigning chapters 1 and 2 as pre-reading.
iii. Next, we suggest covering the chapters on unitary operators,
measurement and quantum circuits with exercises on the
Github site to check knowledge.
iv. We then recommend spending the bulk of the course in Part
II to provide the students with hands-on experience with the
code.
(c) Course in Quantum Computing for CS graduate students:
i. We suggest assigning the ﬁrst two chapters as pre-reading and
then a review of mathematical tools in Part III. Prior exposure
to only undergraduate linear algebra is typically insufﬁcient
as it was most likely taught without the full formalism.
ii. We then recommend chapters 3 and 4 to build up familiarity
with unitary operators, measurement and complexity classes
in the quantum regime. The instructor can make use of the
review questions and answers on the GitHub site.
iii. The course can then cover the approaches to building a quan-
tum computer followed by all the coding chapters.
Please check the book’s GitHub site to ﬁnd additional resources
including: code from the book, problem sets, solutions, links to
videos and other pedagogical resources.
2. Physicists: For physicists who specialize in ﬁelds outside of quantum
computing and wish to ramp up quickly in this area, we recommend
reading the brief history of QC as we provide more detail than typi-
cal treatments, then the survey of quantum hardware followed by the
applications in the second part of the book.
3. Software engineers: We recommend starting with the opening two chap-
ters, then reviewing the toolkits in Part III. We then suggest returning
to the treatment of qubits and unitary operators in Part I and proceeding
from there.
Navigating this Book
xxiii
4. Engineering and business leaders: For readers who will not be doing
hands-on coding, we recommend focusing on chapters 1- 4. The more
adventurous may want to work through some of the code examples to
get a tangible feel for the algorithms.
5. Independent study: This book can easily be used as a text for indepen-
dent study. We recommend combining it with online resources. Please
consult the GitHub site for an updated list of resources:
http://www.github.com/jackhidary/
quantumcomputingbook
We recommend ﬁrst assessing your current ﬂuency on the core tools in
Part III; there are numerous self-tests throughout the section that can be
used for this purpose. The reader can then proceed to Part I.
For those with a strong background in quantum mechanics and/or
information theory we recommend looking up the papers referenced
in chapters 2- 4 to gain a deeper understanding of the state of the ﬁeld
before proceeding to Part II: Hardware and Applications.
Please consult the book’s GitHub site to ﬁnd a range of resources includ-
ing: code from the book, problem sets, solutions, links to videos and other
pedagogical resources.
Part I
Foundations
CHAPTER1
Superposition, Entanglement and
Reversibility
What is a quantum computer? The answer to this question encompasses
quantum mechanics (QM), quantum information theory (QIT) and computer
science (CS).
For our purposes, we will focus on the core of what makes a quantum
computer distinct from classical computers.
1.1
Quantum Computer Deﬁnition
A quantum computer is a device that leverages speciﬁc properties de-
scribed by quantum mechanics to perform computation.
Every classical (that is, non-quantum) computer can be described by quantum
mechanics since quantum mechanics is the basis of the physical universe.
However, a classical computer does not take advantage of the speciﬁc proper-
ties and states that quantum mechanics affords us in doing its calculations.
To delve into the speciﬁc properties we use in quantum computers, let us
ﬁrst discuss a few key concepts of quantum mechanics:
 How do we represent the superposition of states in a quantum system?
 What is entanglement?
 What is the connection between reversibility, computation and physical
systems?
We will be using Dirac notation, linear algebra and other tools extensively
in this text; readers are encouraged to refer to the math chapters later in this
work as well as chapter 14 on Dirac notation to review as needed.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_1 
3
4
CHAPTER 1
Superposition, Entanglement and Reversibility
1.1
Superposition and Entanglement
According to the principles of quantum mechanics, systems are set to a deﬁnite
state only once they are measured. Before a measurement, systems are in an
indeterminate state; after we measure them, they are in a deﬁnite state. If we
have a system, for example, that can take on one of two discrete states when
measured, we can represent the two states in Dirac notation as j0i and j1i. We
can then represent a superposition of states as a linear combination of these
states, such as
1
p
2
j0i C 1
p
2
j1i
(1.2)
1.3
The Superposition Principle
The linear combination of two or more state vectors is another state vector
in the same Hilbert spacea and describes another state of the system.
aSee Part III for a treatment of Hilbert spaces
As an example, let us consider a property of light that illustrates a super-
position of states. Light has an intrinsic property called polarization which
we can use to illustrate a superposition of states. In almost all of the light
we see in everyday life — from the sun, for example — there is no preferred
direction for the polarization. Polarization states can be selected by means
of a polarizing ﬁlter, a thin ﬁlm with an axis that only allows light with
polarization parallel to that axis to pass through.
With a single polarizing ﬁlter, we can select one polarization of light,
for example vertical polarization, which we can denote as j"i. Horizontal
polarization, which we can denote as j!i, is an orthogonal state to vertical
polarization1. Together, these states form a basis for any polarization of light.
That is, any polarization state j i can be written as linear combination of
these states. We use the Greek letter  to denote the state of the system
j i D ˛ j"i C ˇ j!i
(1.4)
The coefﬁcients ˛ and ˇ are complex numbers known as amplitudes. In
this example, the coefﬁcient ˛ is associated with vertical polarization and the
1We could have equally used j0i and j1i to denote the two polarization states; the labels
used in kets are arbitrary.
SECTION 1.2
The Born Rule
5
coefﬁcient ˇ is associated with horizontal polarization. The amplitude has
important interpretation in quantum mechanics which we will see shortly.
After selecting vertical polarization with a polarizing ﬁlter, we can then
introduce a second polarizing ﬁlter after the ﬁrst. Imagine we oriented the
axis of the second ﬁlter perpendicular to the axis of the ﬁrst. Would we see
any light get through the second ﬁlter?
If you answered no to this question, you would be correct. The horizon-
tal state j!i is orthogonal to the ﬁrst, so there is no amount of horizontal
polarization after the ﬁrst vertical ﬁlter.
Suppose now we oriented the axis of the second polarizing ﬁlter at 45°
(i.e., along the diagonal % between vertical " and horizontal !) to the ﬁrst
instead of horizontally. Now we ask the same question — would we see any
light get through the second ﬁlter?
If you answered no to this question, you may be surprised to ﬁnd the answer
is yes. We would, in fact, see some amount of light get through the second
ﬁlter. How could this be if all light after the ﬁrst ﬁlter has vertical polarization?
The reason is that we can express vertical polarization as a superposition of
diagonal components. That is, letting j%i denote 45° polarization and j-i
denote  45° polarization, we may write
j"i D
1
p
2
j%i C 1
p
2
j-i
(1.5)
As you may expect from geometric intuition, the vertical state consists of
equal parts j%i and j-i.
It is for this reason that we see some amount of light get past the second
ﬁlter. Namely, the vertical polarization can be written as a superposition of
states, one of which is precisely the 45° diagonal state j%i we are allowing
through the second ﬁlter. Since the j%i state is only one term in the superposi-
tion, not all of the light gets through the ﬁlter, but some does. The amount that
gets transmitted is precisely 1=2 in this case. (More formally, the intensity of
the transmitted light is 1=2 that of the incident light.) This value is determined
from the amplitudes of the superposition state by a law known as Born’s rule,
which we now discuss.
1.2
The Born Rule
Max Born demonstrated in his 1926 paper that the modulus squared of
the amplitude of a state is the probability of that state resulting after
6
CHAPTER 1
Superposition, Entanglement and Reversibility
obtaining that state is
ˇˇˇ 1
p
2
ˇˇˇ
2
D 1
2; so the probability of measuring the light in
either the vertical or horizontal polarization state is 50%. Note that we chose
an amplitude of
1
p
2 in order to normalize the states so that the sum of the
modulus squared of the amplitudes will equal one; this enables us to connect
the amplitudes to probabilities of measurement with the Born rule.
1.6
The Born rule
In a superposition of states, the modulus squared of the amplitude of a state
is the probability of that state resulting after measurement. Furthermore,
the sum of the squares of the amplitudes of all possible states in the
superposition is equal to 1. So, for the state j i D ˛j0i C ˇj1i, we have
j˛j2 C jˇj2 D 1:
While in the polarization example above we have a 50/50 split in proba-
bility for each of two states, if we examined some other physical system it
may have a 75/25 split or some other probability distribution. One critical
difference between classical and quantum mechanics is that amplitudes (not
probabilities) can be complex numbers.
In other words, the coefﬁcients ˛ and ˇ which appear in the statement of
the Born rule can be complex numbers, such as i WD
p
 1 or .1 C i/=
p
2.
It is only after we take the square of the modulus of these amplitudes that
we get real numbers, hence actual probabilities. Refer to chapter 11 to
review complex numbers and how to calculate the square of the modulus of a
complex number. As if quantum superposition were not interesting enough,
QM describes a speciﬁc kind of superposition which stretches our imagination
even further: entanglement. In 1935, when Einstein worked with Podolsky
and Rosen to publish their paper on quantum entanglement, their aim was to
attack the ediﬁce of QM (this paper is now known as EPR [100]). Even though
Einstein earned the Nobel Prize for his 1905 work on the quantum nature of
the photoelectric effect, he nevertheless railed against the implications of QM
through his later years.
Einstein wrote in 1952 that quantum mechanics appears to him to be
“a system of delusion of an exceedingly intelligent paranoiac concocted of
incoherent elements of thought” [99]. He hoped that the EPR paper would
demonstrate what he perceived to be the deﬁciencies of QM.
measurement [47]. In this case, since the amplitude is
1
p
2, the probability of
SECTION 1.2
The Born Rule
7
EPR showed that if you take two particles that are entangled with each
other and then measure one of them, this automatically triggers a correlated
state of the second — even if the two are at a great distance from each other;
this was the seemingly illogical result that EPR hoped to use to show that
QM itself must have a ﬂaw. Ironically, we now consider entanglement to be a
cornerstone of QM. Entanglement occurs when we have a superposition of
states that is not separable. We will put this into a more formal context later
on in this text.
This "spooky action at a distance" seems at odds with our intuition and
with previous physics. Podolsky, the youngest of the co-authors, reportedly
leaked the paper to the New York Times to highlight this assault on the tower
of QM to the public. The Times ran the story on the front page of the May
4th, 1935 edition with the headline “Einstein Attacks Quantum Theory."
Not only is entanglement accepted as part of standard quantum mechanics,
we shall see later in this work that we can leverage entanglement to perform
novel types of computation and communication. From an information theo-
retic point of view, entanglement is a different way of encoding information.
If we have two particles that are entangled, the information about them is not
encoded locally in each particle, but rather in the correlation of the two.
John Preskill likes to give the analogy of two kinds of books: non-
entangled and entangled [224]. In the regular, non-entangled book we can
read the information on each page as we normally do. In the entangled book,
however, each page contains what appears to be gibberish. The information is
encoded in the correlation of the pages, not in each page alone. This captures
what Schrödinger expressed when he coined the term entanglement:
Another way of expressing the peculiar situation is: the
best possible knowledge of a whole does not necessarily
include the best possible knowledge of all its parts. [249]
Schrödinger further noted that in his opinion entanglement was not just
one of the phenomena described by quantum mechanics,“but rather the char-
acteristic trait of quantum mechanics, the one that enforces its entire departure
from classical lines of thought” [249].
p
64
CHAPTER 1
Superposition, Entanglement and Reversibility
1.7
Entanglement
Two systems are in a special case of quantum mechanical superposition
called entanglement if the measurement of one system is correlated with
the state of the other system in a way that is stronger than correlations
in the classical world. In other words, the states of the two systems are
not separable. We will explore the precise mathematical deﬁnitions of
separability and entanglement later in this book.
1.3
Schrödinger’s Equation
As we saw above, we can represent the state of a system with a state vector
using Dirac notation. For example, if we wish to represent the state of a system
of one photon that is in a superpositon of vertical and horizontal polarization
we can use the following notation
j%i D
1
p
2
j"i C 1
p
2
j!i
(1.8)
We can also represent the state vector with the Greek letter ‰ in the ket –
recall that the label in the ket is arbitrary. So we now have
j‰i D
1
p
2
j"i C 1
p
2
j!i
(1.9)
If we measure this photon for polarization we have a 50% probability
of ﬁnding the photon in a vertical polarization state and 50% probability of
ﬁnding it in a horizontal polarization state. In the Copenhagen school of QM
we say that the wave function has collapsed to one state or the other. There
are other interpretations of measurement of quantum systems, but these are
beyond the scope of this text.
Now that we have a method of representing the state of a system with a
wave function, how can we represent the evolution of this system through
time? For this exposition, let us consider the wave function of a particle that
is moving through free space which we can represent as
‰.x; t/
(1.10)
where x is its position at time t. The manner in which this wave function
evolves over time can be described by the time-dependent Schrödinger equa-
SECTION 1.3
Schrödinger’s Equation
9
tion (SE). One conventional way of writing down the time-dependent SE for a
particle along one dimension is as follows
iℏ@
@t ‰.x; t/ D OH‰.x; t/
(1.11)
There is no need to be intimidated by this mathematical notation if it is new
to you. We will explain the core concepts of the SE here and the reader can
ﬁnd links to helpful videos on our companion website for deeper exploration
of the SE.
On the left hand side (LHS) of this equation, we see the constants i and
ℏ. i of course is the symbol that represents the square root of  1. ℏis the
reduced form of the Planck constant h. We call it the reduced form since we
take h and divide it by 2. The next set of symbols is the partial derivative
derivative with respect to time refers to how the function, in this case the wave
function ‰.x; t/, will change as t changes while holding position, denoted as
x, constant.
The right hand side (RHS) of the equation denotes the Hamiltonian op-
erator applied to the wave function ‰. The Hamiltonian represents the total
energy of a system; the total energy includes all the kinetic energy, which we
can represent as OT and the potential energy, OV , of the particles in the system.
Note that some conventions use OU instead of OV to denote potential energy, but
we avoid that here to reserve U for the generic reference to unitary operators
which we will encounter later in this book.
An operator is a term we introduce in quantum mechanics for functions that
operate on wave functions. In addition to the Hamiltonian operator we often
encounter the position and momentum operators in QM which are represented
as Ox and Op, respectively. Note the carat on top of these operators to distinguish
them from the position and momentum variables, x and p.
The SE tells us that if we want to know how the wave function ‰ will
change over time we need to look at the total energy of the system. We can
break down OH to its component parts of OT and OV like so
OH‰ D OT ‰ C OV ‰
(1.12)
T D 1
2mv2
(1.13)
of the wave function  with respect to t which denotes time. A partial
We can then replace OT ‰ with the quantized version of the expression for
kinetic energy. We start with the classical formula for kinetic energy
10
CHAPTER 1
Superposition, Entanglement and Reversibility
then we can multiply both the numerator and denominator by m on the RHS
and then realize that since momentum, p, is mass times velocity, we yield the
following
T D .mv2/m
2m
D .mv/2
2m
D p2
2m
(1.14)
Now we can take this classical expression and upgrade it to its quantum
counterpart by replacing p with the momentum operator Op which is
Op D  iℏ@
@x
(1.15)
Since in equation 1.14 we have p2 we replace it like so
OT D
1
2m

 iℏ@
@x
 
 iℏ@
@x

(1.16)
To simplify the RHS, we can do the following: realize that the  i’s cancel out
to -1 so cancel those out and add a minus sign to the front of the RHS, then
collect the two ℏ’s to a single ℏ2 and ﬁnally multiply out the two ﬁrst order
partial derivatives to yield a second-order partial derivative as follows
OT D   ℏ2
2m
 @2
@x2

(1.17)
Now let’s turn to the LHS. Since we use the Schrödinger equation to
calculate the evolution over time of the wave function, we represent this by
the partial derivative of the wave function, ‰.x; t/, with respect to t like so
iℏ@‰.x; t/
@t
(1.18)
We now can put all the pieces together as follows
iℏ@‰.x; t/
@t
D   ℏ2
2m
@2‰.x; t/
@x2
C OV ‰.x; t/
(1.19)
Voila! Now, what about the term OV ‰.x; t/? This term has to be determined
for the system at hand. Let us consider the case of a quantum harmonic
oscillator. The potential energy of a harmonic oscillator such as a spring is
described by the following expression
V D 1
2kx2
(1.20)
where k is the spring constant of the oscillator. We then recall that
SECTION 1.4
The Physics of Computation
11
! D
r
k
m
(1.21)
where ! is the natural frequency of the system and m is the mass of the system.
We now have
V D 1
2kx2 D 1
2m!2x2
(1.22)
and therefore the complete time-dependent SE for a quantum harmonic oscil-
lator would be the following
iℏ@ ‰.x; t/
@t
D   ℏ2
2m
@2 ‰.x; t/
@x2
C 1
2m!2Ox2‰.x; t/
(1.23)
As an example of a quantum oscillator we can consider the molecules of
hydrogen or oxygen when found in their usual diatomic state. The vibrations
of these molecules follow the dynamics described here for quantum harmonic
oscillators.
Other quantum systems would have a different formula for their potential
energy. In those cases we would replace the oscillator potential energy formula
for that speciﬁc system’s potential energy formula to produce the SE for that
wave function’s evolution over time.
Note that the SE is a ﬁrst-order partial derivative on the LHS and a second-
order partial derivative on the RHS. This imbalance raises interesting questions
that are beyond the scope of this text, but we encourage the reader to investi-
gate further. The online companion site contains links to further videos and
resources.
1.4
The Physics of Computation
Now that we have covered two core ideas of quantum mechanics – superposi-
tion and entanglement – let us turn to another fundamental concept that is not
treated as often – the physicality of information. Rolf Landauer opened a new
line of inquiry when he asked the following question:
The search for faster and more compact computing
circuits leads directly to the question: What are the
ultimate physical limitations on the progress in this
direction? ...we can show, or at the very least strongly
suggest,
that information processing is inevitably
where Ox is evaluated as the x coordinate along the one dimensional line.
12
CHAPTER 1
Superposition, Entanglement and Reversibility
accompanied by a certain minimum amount of heat
generation. [163]
In other words, is there a lower bound to the energy dissipated in the
process of a basic unit of computation? Due to Landauer and others we
now believe that there is such a limit; this is called Landauer’s limit. More
speciﬁcally, the energy cost of erasure of n bits is nkT ln 2 where k is the
Boltzmann constant, T is the temperature in Kelvin of the heat sink sur-
rounding the computing device and ln 2 is, of course, the natural log of 2
( 0:69315). This limit is the minimum amount of energy dissipated for an
irreversible computation.
Landauer acknowledged that this minimum is not necessarily the constrain-
ing factor on the energy draw of the system:
It is, of course, apparent that both the thermal noise and
the requirements for energy dissipation are on a scale
which is entirely negligible in present-day computer
components. The dissipation as calculated, however, is
an absolute minimum. [163]
Landauer deﬁned logical irreversibility as a condition in which “the out-
put of a device does not uniquely deﬁne the inputs.” He then claimed that
“logical irreversibility...in turn implies physical irreversibility, and the latter
is accompanied by dissipative effects.” This follows from the second law
of thermodynamics which states that the total entropy of a system cannot
decrease and, more speciﬁcally, must increase with an irreversible process.
For further background on reversibility, thermodynamics and computation see
Feynman’s Lectures on Computation [112].
In classical computing we make use of irreversible computations. For
example, the Boolean inclusive OR (denoted _) gate has the following truth
table, where 0 denotes “false” and 1 denotes “true”:
X
Y
X _ Y
0
0
0
0
1
1
1
0
1
1
1
1
Note that an output of value 1 cannot be traced uniquely to a set of inputs.
We can arrive at that output through combinations of inputs; the state of the
inputs is lost once we move to the output. This does not violate the conser-
SECTION 1.4
The Physics of Computation
13
vation of information because the information was converted into dissipative
heat.
The exclusive OR is also irreversible as is the NAND gate, which is univer-
sal for classical computing. NAND stands for “NOT AND” and is the inverse
of the Boolean AND operator. Verify for yourself that NAND is irreversible
by examining its truth table:
X
Y
X " Y
0
0
1
0
1
1
1
0
1
1
1
0
In quantum computing, we limit ourselves to reversible logical opera-
tions [206, p. 29]. Later in this book we will consider which combinations of
quantum operators are universal, as well as examine algorithms in the query
model where uncomputing — the process of undoing a series of reversible
operations — is important. For now, let’s focus on the requirement in quantum
computation that we limit our set of operators to reversible gates.
This requirement derives from the nature of irreversible operations: if we
perform an irreversible operation, we have lost information and therefore
performed a measurement. Our computation cycle then will be done and we
can no longer continue with our program. Instead, by limiting all gates to
reversible operators, we may continue to apply operators to our set of qubits
as long as we can maintain coherence in the system. When we say reversible,
we are assuming a theoretical noiseless quantum computer. In a noisy QC
that decoheres, we cannot, of course, reverse the operation.
1.24
Reversibility of Quantum Computation
All operators used in quantum computation other than for measurement
must be reversible.
In this chapter, we have examined four essential principles of quantum
mechanical systems: superposition, the Born rule, entanglement and reversible
computation. All four are essential to understanding the difference between
classical and quantum computing as we shall see further in the book. We
provide references on this book’s website to a number of resources for those
who wish to deepen their understanding of quantum mechanics.
CHAPTER2
Our generous universe comes equipped
with the ability to compute.
—Dave Bacon [22]
A Brief History of Quantum
Computing
The possibility that we can leverage quantum mechanics to do computation
in new and interesting ways has been hiding in plain sight since the ﬁeld’s
early days; the principles of superposition and entanglement can form the
basis of a very powerful form of computation. The trick is to build such a
system that we can easily manipulate and measure.
While Richard Feynman is often credited with the conception of quantum
computers, there were several researchers who anticipated this idea. In 1979,
Paul Benioff, a young physicist at Argonne National Labs, submitted a pa-
per entitled “The computer as a physical system: A microscopic quantum
mechanical Hamiltonian model of computers as represented by Turing ma-
chines” [30].1 In this paper, Benioff demonstrated the theoretical basis for
quantum computing and then suggested that such a computer could be built:
That is, the whole computation process is described
by a pure state evolving under the action of a given
Hamiltonian.
Thus all the component parts of the
Turing machine are described by states which have a
deﬁnite phase relation to one another as the calculation
progresses...The existence of such models at least
suggests that the possibility of actually constructing such
coherent machines should be examined.
Yuri Manin also laid out the core idea of quantum computing in his 1980
book Computable and Non-Computable [181]. The book was written in
Russian, however, and only translated years later.
1Note: Benioff completed and submitted the paper in 1979. It was published in the
following year, 1980.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_2
15
16
CHAPTER 2
A Brief History of Quantum Computing
In 1981, Feynman gave a lecture entitled “Simulating Physics with Com-
puters” [111].2 In this talk, he argued that a classical system could not
adequately represent a quantum mechanical system:
...nature isn’t classical, dammit, and if you want to make
a simulation of nature, you’d better make it quantum
mechanical, and by golly it’s a wonderful problem,
because it doesn’t look so easy...
He then set out the features that a quantum computer should have to be
useful. At the time of this lecture, however, it was unclear to Feynman and
the physics community how one could build such a machine (for additional
background see [225]).
2.1
Early Developments and Algorithms
Once Benioff, Manin and Feynman opened the doors, researchers began to
investigate the nature of the algorithms that could be run on QCs. David
Deutsch, a physicist at Oxford, suggested a more comprehensive framework
for quantum computing in his 1985 paper [88]. In this work, he describes in
detail what a quantum algorithm would look like and anticipates that “one
day it will become technologically possible to build quantum computers.”
Deutsch then went on to develop an example of an algorithm that would
run faster on a quantum computer. He then further generalized this algorithm
in collaboration with Richard Jozsa [90]. We will cover these and the other
algorithms in more detail with code examples later on in this text.
In computer science and quantum computing, it is often important to
evaluate how efﬁcient an algorithm is — that is, how many steps would it take
to run such an algorithm. We use big-O notation to represent the upper bound
of the worst case of running an algorithm. The O in big-O notation comes
from the “order” of the algorithm. We use big- (Omega) notation to indicate
the lower bound of the worst-case scenario. So, while Deutsch’s problem
takes at worst O.n/ steps to solve on a classical computer, Deutsch-Jozsa’s
algorithm solves the problem in one step on a quantum computer. Big-O
notation will be helpful throughout this book in illuminating the difference
between the classical and quantum algorithms.
Umesh Vazirani and his student Ethan Bernstein picked up where Deutsch
and Jozsa left off. In 1993, Bernstein and Vazirani (BV) published a paper
which described an algorithm that showed clear quantum-classical separation
2Note: Feynman gave his lecture in 1981 and submitted the lecture for publication in May
of 1981. The lecture was published by IJTP in 1982.
SECTION 2.1
Early Developments and Algorithms
17
even when small errors are allowed [36]. Why is this signiﬁcant? While
Deutsch-Jozsa demonstrated a deterministic quantum advantage, if small
errors are allowed in the computation, both classical and quantum versions
can be run at worst in O.1/ steps, showing no separation. By contrast, the
Bernstein-Vazirani (BV) algorithm demonstrates separation even when small
errors are allowed, thus showing non-deterministic quantum advantage. The
problem posed in BV can be solved in O.n/ time on a classical computer and
in O.1/ using the BV circuit on a quantum computer.
BV made a further contribution in their 1993 paper. They described a
quantum version of the Fourier transform. This quantum Fourier transform
(QFT) would serve as a critical component for Peter Shor when he developed
his algorithm to factor large numbers.
The work of BV was quickly followed by Daniel Simon, then a postdoc at
the University of Montreal, in 1994. Simon outlined a problem that a quantum
computer would clearly solve exponentially faster than a classical one [257].
To be more speciﬁc, Simon’s algorithm has an upper bound of O.n/ on a
quantum computer, but a higher .2n=2/ on a classical computer. Since
the lower bound on the classical computer is of higher order than the upper
bound on the quantum computer, there is a clear demonstration of quantum
advantage.
Just prior to Daniel Simon’s work on algorithms, Seth Lloyd, working
at Los Alamos, published a paper in Science which described a method of
building a working quantum computer [173]. He proposed that a system
sending pulses into a unit can represent a quantum state:
Arrays of pulsed, weakly coupled quantum systems
provide a potentially realizable basis for quantum
computation. The basic unit in the array could be a
quantum dot, a nuclear spin, a localized electronic state
in a polymer, or any multistate quantum system that
interacts locally with its neighbors and can be compelled
to switch between states with resonant pulses of light.
Lloyd realized that:
The proposed device is capable of purely quantum-
mechanical information-processing capacities above
and beyond the conventional digital capacities already
presented. One of the most important of these capacities
is that bits can be placed in superpositions of 0 and 1 by
the simple application of pulses at the proper resonant
18
CHAPTER 2
A Brief History of Quantum Computing
frequencies but at a length different from that required to
fully switch the bit. Such bits have a number of uses,
including the generation of random numbers.
This was the ﬁrst practical approach to a working QC. It is interesting
that Lloyd noted the possible use case of generating random numbers from
a quantum system; this has been a topic of recent research in the quantum
computing community. See, for example. [139] and [1].
2.2
Shor and Grover
Enter Peter Shor. In 1994, Shor was a researcher in the mathematical division
of Bell Labs in New Jersey. Shor studied the work of Deutsch, BV and Simon
and realized he could construct an algorithm for factoring large numbers into
two prime factors; factoring large numbers is believed to be intractable on a
classical computer, but Shor’s factoring algorithm runs quickly on a QC. Fac-
toring large numbers is, of course, the intentionally hard problem at the core
of public key cryptography (PKC) as implemented in the RSA algorithm[239],
the kind of cryptography that is the basis of almost all communications today
over the internet. This includes securely sending credit card numbers, bank
payments and ensuring the security of online messaging systems.
RSA-based cryptography depends on the one-way hardness of the factoring
of large numbers into two prime factors. Producing the large number is easy;
we just multiply the two factors. Given an arbitrarily large number, however,
it is exponentially difﬁcult to ﬁnd its two prime factors.
Inspired by Simon, Shor realized that we can use a QC to solve another
problem that is equivalent to the factoring problem; the factoring problem is
in fact equivalent to the period-ﬁnding problem which Simon had tackled in
his paper [254]. He also realized that the QFT described by BV was exactly
what he needed to set up the amplitudes of each qubit prior to measurement
so that the measurement would yield the answer needed from the quantum
computation with high probability.
Shor’s breakthrough led more researchers to work on quantum algorithms
since it then became clear that QCs, if built, would be quite powerful. In fact,
Shor’s algorithm is one of the ﬁrst to have been demonstrated on early QC
physical systems. In 2001, Isaac Chuang et al. implemented Shor’s algorithm
on a nuclear magnetic resonance (NMR) system to factor the number 15 as a
demonstration [278].
SECTION 2.3
Deﬁning a Quantum Computer
19
After Shor, Lov Grover contributed to the quantum algorithm arsenal by
demonstrating that one can achieve some speedup in a search algorithm on a
QC [130]. Grover’s algorithm only achieves quadratic speedup, not exponen-
tial speedup (as Shor’s does), but this is still signiﬁcant. Quadratic speedup
means that if an algorithm would take O.N/ steps on a classical computer,
we can achieve the same goal in O.
p
N/ steps on a QC. A few months after
Grover’s paper in May of 1996, Farhi and Gutmann laid out the framework
for a continuous time Hamiltonian version of Grover’s algorithm [107]. This
introduced the concept of Hamiltonian oracles and the idea of implementing
continuous time models of quantum computation which are different than
gate-based approaches.
As one set of researchers were making progress in identifying algorithms
that would run on a quantum computer with speedup over classical computers,
others were making progress on the physical implementation of a QC. In 1999-
2001, Yasunobu Nakamura built and demonstrated a functioning, controllable
superconducting qubit [202, 203]. Nakamura used Josephson junctions to
create a two-level system that the user could manipulate between its two states.
We will discuss processors based on superconducting qubits in chapter 5.
Another approach to implementing a quantum computer is to trap and
manipulate ions. In 1995, Cirac and Zoller proposed an ion trap as the
physical system to perform quantum computation [79]. In this setup, lasers
are used to ionize atoms which are then trapped in electric potentials. We will
cover ion trap quantum computers in chapter 5 as well.
2.3
Deﬁning a Quantum Computer
As the activity in the quantum computing ﬁeld began to rise, researchers in the
ﬁeld formalized what constituted a quantum computer and computation. In
1996, David DiVincenzo outlined the key criteria of a quantum computer [93]
in this manner:3
1. A scalable physical system with qubits that are distinct from one another
and the ability to count exactly how many qubits there are in the system
(no fudging allowed). The system can be accurately represented by a
Hilbert space.
3Note that we are summarizing DiVincenzo’s criteria from his original 1996 paper. See [94]
for another version of his criteria.
20
CHAPTER 2
A Brief History of Quantum Computing
2. The ability to initialize the state of any qubit to a deﬁnite state in the
computational basis. For example, setting all qubits to state j0i if the
computational basis vectors are j0i and j1i.
3. The system’s qubits must be able to hold their state. This means that the
system must be isolated from the outside world, otherwise the qubits
will decohere. Some decay of state is allowed (, where  is a small
quantity). In practice, the system’s qubits must hold their state long
enough to apply the next operator with assurance that the qubits have
not changed state due to outside inﬂuences between operations.
4. The system must be able to apply a sequence of unitary operators to the
qubit states. The system must also be able to apply a unitary operator
to two qubits at once. This entails entanglement between those qubits.
As DiVincenzo states in his paper, “...entanglement between different
parts of the quantum computer is good; entanglement between the
quantum computer and its environment is bad, since it corresponds to
decoherence” [93, p. 4].
5. The system is capable of making “strong” measurements of each qubit.
By strong measurement, DiVincenzo means that the measurement says
“which orthogonal eigenstate of some particular Hermitian operator
the quantum state belongs to, while at the same time projecting the
wavefunction of the system irreversibly into the corresponding eigen-
function.” This means that the measuring technique in the system
actually does measure the state of the qubit for the property being mea-
sured and leaves the qubit in that state. DiVincenzo wants to prevent
systems that have weak measurement, in other words, measuring tech-
niques that do not couple with the qubit sufﬁciently to render it in that
newly measured state. At the time he wrote the paper, many systems
did not have sufﬁcient coupling to guarantee projection into the new
state.
In the last few years, we have witnessed a signiﬁcant increase in the
activity in the ﬁeld. On the hardware side, there are numerous companies,
noisy intermediate-scale quantum (NISQ) computers, a term coined by John
Preskill of CalTech [224]. This refers to systems that do not yet have full
error-correction (thus noisy) and have dozens to thousands of qubits – well
short of the 106C necessary for scaled fault-tolerant computing.
large and small, that are now pursuing the various approaches described in
chapter 5 to build quantum computers. This has landed us in the era of
SECTION 2.3
Deﬁning a Quantum Computer
21
With this increase in funding and attention in both the private and public
sectors, we anticipate signiﬁcant developments in the ﬁeld. Let us now turn
our attention to qubits and the operators we use in quantum computation.
CHAPTER3
Qubits, Operators and
Measurement
In this chapter we will cover qubits and the core set of operators we use to
manipulate the state of qubits.
A qubit is a quantum bit. A qubit is similar to a classical bit in that it can
take on 0 or 1 as states, but it differs from a bit in that it can also take on a
continuous range of values representing a superposition of states. In this text
we will use qubit to refer to quantum bits and the word bit to refer to classical
bits.
While in general we use two-level qubit systems to build quantum comput-
ers we can also choose other types of computing architectures. For example,
we could build a QC with qutrits which are three-level systems. We can think
of these as having states of 0, 1 or 2 or a superposition of these states.
The more general term for such a unit is qudit; qubits and qutrits are
speciﬁc instances of qudits which can be computing units of any number of
states. The Siddiqi Lab at UC Berkeley, for example, has designed a qutrit-
based QC [42, 43]. In a qutrit system we can represent more states than a
qubit system with the same number of computational units.
A qubit system of say 100 qubits can handle 2100 states (1:26765EC30),
while a qutrit system can handle 3100 states (5:15378EC47), a number which
is 17 orders of magnitude larger. Put another way, to represent the same
number space as a 100 qubit system, we only need  63 qutrits (log3.2100/).
Since it is more difﬁcult to build qutrit systems, the mainstream QCs are
currently based on qubits. Whether we choose qubits, qutrits or some other
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_3
23
24
CHAPTER 3
Qubits, Operators and Measurement
qudit number, each of these systems can run any algorithm that the others can,
i.e., they can simulate each other.1
In QM we represent states as vectors and operators as matrices; we use
Dirac notation instead of traditional linear algebra symbols to represent vectors
and other abstractions. Part III of this book contains a review of linear algebra,
Dirac notation and other mathematical tools that are crucial for our inquiry
into quantum computing. In this chapter we will assume knowledge of these
mathematical tools; we encourage the reader to use the math chapters to
review these concepts in the context of quantum computing.
Let us begin with the deﬁnition of a qubit:
3.1
What is a Qubit?
A physical qubit is a two-level quantum mechanical system. As we will
see in the chapter on building quantum computers, there are many ways to
construct a physical qubit. We can represent a qubit as a two-dimensional
complex Hilbert space, C2. The state of the qubit at any given time can
be represented by a vector in this complex Hilbert space.
The Hilbert space is equipped, by deﬁnition, with an inner product which
allows us to determine the relative position of two vectors representing qubit
states. We denote the inner product of vectors jui, jvi as hujvi ; this will equal
0 if jui and jvi are orthogonal and 1 if jui = jvi. To represent two or more
qubits we can tensor product Hilbert spaces together to represent the combined
states of the qubits. As we shall see, we have methods to represent separable
states, where the qubits are independent of one another, and entangled states
such as a Bell state, where we cannot separate the two qubit states.
We can represent the states j0i and j1i with vectors as shown below. We
call these two the computational basis of a two-level system. We can then
apply operators in the form of matrices to the vectors in the state space.
j0i D
1
0

;
j1i D
0
1

1Note that we could consider the same question in classical systems, i.e., we could have
used a 3-state “trit” instead of the bit, but we choose to use bits as there are distinct advantages
to the binary system.
CHAPTER 3
Qubits, Operators and Measurement
25
3.2
Quantum Operators
In gate-based quantum computers, the operators which we use to evolve
the state of the qubits are unitary and therefore reversible. Some of the
operators are unitary, reversible and involutive (i.e., they are their own
inverses); others are not involutive. A measurable quantity, or observable,
is a Hermitian operator; thus the measurement in a quantum computer
outputs real values from the system. We use the terms operators and gates
interchangeably.
In addition to an inner product of two vectors, linear algebra gives us
the outer product. In this operation, we take two vectors and form a matrix
(whereas an inner product gives us a scalar). If we take the outer product
j0ih0j, for example, we produce the following operator
j0ih0j D
1
0
  1
0

D
1
0
0
0

Similarly, we can take the outer product of the other three combinations to
produce these matrices
j0ih1j D
1
0
  0
1

D
0
1
0
0

j1ih0j D
0
1
  1
0

D
0
0
1
0

j1ih1j D
0
1
  0
1

D
0
0
0
1

We can take the sum of two of these matrices to form a unitary matrix, like
so
j0ih1j C j1ih0j D
 0
1
1
0

(3.3)
This, in fact, is the X or NOT operator which we will encounter shortly in
this chapter.
We have established that a qubit can be in one of the computational basis
states of 0 or 1 or in a superposition of these two states. How can we represent
the superposition of multiple states? We can do so as a linear combination of
the computational bases of the state space.
26
CHAPTER 3
Qubits, Operators and Measurement
3.4
Representing Superposition of States
We represent a superposition of states as the linear combination of com-
putational bases of the state space. Each term in the superposition has a
complex coefﬁcient or amplitude.
Using the two computational basis vectors in the case of a single qubit, two
examples of superpositions of states are
jCi WD
1
p
2
.j0i C j1i/
and
j i WD
1
p
2
.j0i   j1i/
These two states differ by a minus sign on the j1i state. More formally, we
call this difference a relative phase. The term phase has numerous meanings
in physics — in this context, it refers to an angle. The minus sign is related to
the angle  (180°) by Euler’s identity2
ei D  1
Relative phases are of fundamental importance for quantum algorithms in
that they allow for constructive interference and destructive interference. For
example, if we evaluate the sum of the above states, we obtain
1
p
2
.jCi C j i/ D 1
2.j0i C j1i/ C 1
2.j0i   j1i/ D j0i
Here, we say that the amplitudes of the j1i state interfere destructively — the
differing relative phases cause them to sum to zero. On the other hand, the
amplitudes of the j0i state interfere constructively — they have the same sign
(relative phase), so they do not sum to zero, and thus we are left with the state
j0i as the result.
We can also consider subtracting the two superposition states. We leave it
to the reader to verify that
1
p
2
.j i   jCi/ D  j1i
2For more on Euler’s identity, refer to chapter 13.
CHAPTER 3
Qubits, Operators and Measurement
27
Here, the amplitudes of the j0i state interfere destructively while the ampli-
tudes of the j1i state interfere constructively. In this example, we do not end
up with the j1i state exactly — it is multiplied by a minus sign. As we saw
above, we can interpret this minus sign as an angle (or phase) ei. Here, it is
applied to the entire state, not just one term in the superposition. We refer to
this type of phase as a global phase.
While it is true that the  j1i state is not exactly the j1i state, we will
see in future chapters that a global phase change has no impact on quantum
measurements. That is, the measurement statistics obtained by measuring the
 j1i state and the j1i state are exactly identical. In this case, we often say
that the two states are equal, up to global phase.
Quantum Circuit Diagrams
We use circuit diagrams to depict quantum circuits. We construct and read
these diagrams from left to right; we can think of circuit diagrams like a staff of
music which we read in the same direction. A quantum circuit speciﬁes which
operators we will apply to which qubit or qubits in which order. Barenco et al.
set forth a number of the foundational operators that we use today in QC [26].
Fredkin and Toffoli [275, 118] added to this set with two ternary operators.
We begin the construction of a quantum circuit diagram with the circuit
wire which we represent as a line
A line with no operator on it implies that the qubit remains in the state in
which it was previously prepared. This means that we are relying on the
quantum computer to maintain the state of the qubit.
We denote the initial prepared state with a ket and label on the left of the
wire
j0i
We denote n number of qubits prepared in that state with a slash n symbol
across the wire.
=n
28
CHAPTER 3
Qubits, Operators and Measurement
3.1
Quantum Operators
Let us now turn to the set of commonly used quantum operators. We denote a
single-qubit operator with a box containing the letter representing that operator
straddling the line. We denote a binary gate with an operator box spanning
two quantum wires and spanning three wires for a ternary operator, etc. Note
that we could have chosen a different set of operators to accomplish universal
quantum computation; the set of operators chosen is arbitrary and is sufﬁcient
as long as it meets the test of universality which we will cover later in this
chapter. Here are representations for unary and binary operators
U
U
Unary Operators
Let us now cover the set of one-qubit, or unary, quantum operators. The
ﬁrst three operators we will examine are the Pauli operators. These three
matrices along with the identity matrix and all of their ˙1 and ˙i multiples
constitute what is known as the Pauli group. First, we have X, which is the
NOT operator (also known as the bit ﬂip operator and can be referred to as x)
X WD
0
1
1
0

If we apply X to j0i then we have
0
1
1
0
 1
0

D
0 C 0
1 C 0

D
0
1

D j1i
We can represent the initial state of a qubit and the operators we apply to
them with a circuit diagram. We use the following symbol to represent the X
operator in circuit diagrams
This is different from the convention of the operator name in a box, which
one may also encounter in circuit diagrams
SECTION 3.1
Quantum Operators
29
X
As we have seen, we can represent the X operator in ket notation as
X WD j0ih1j C j1ih0j
and the application of the X operator like so:
X jji D jj ˚ 1i
where j 2 f0; 1g. Here the ˚ operation denotes addition modulo-2, and j ˚1
is equivalent to the NOT operation. So if we start with the qubit in state j0i
and apply NOT then we have
j0i
j1i
Next we have the Y operator, also denoted y, which rotates the state
vector about the y axis3.
Y D
0
 i
i
0

So that if we apply it to the j1i state we have
0
 i
i
0
 0
1

D
0   i
0 C 0

D
 i
0

D  i j0i
The circuit diagram for the Y operator is
Y
And the Z operator, also denoted z, which rotates the state vector about
the z axis (also called the phase ﬂip operator since it ﬂips it by  radians or
180 degrees)
Z WD
1
0
0
 1

If we apply Z to the computational basis state we have
Z jji D . 1/j jj i
or to show this in matrix form for the special case j D 0
3The x, y and z axes in this section refer to representation of the qubit’s state on a Bloch
sphere, which we will cover later in this chapter.
30
CHAPTER 3
Qubits, Operators and Measurement
1
0
0
 1
 1
0

D
1 C 0
0 C 0

D
1
0

D . 1/0 j0i D j0i
For the case where j D 1 we have
1
0
0
 1
 0
1

D
0 C 0
0   1

D
 0
 1

D . 1/1 j1i D   j1i
Note that we can multiply the bit-ﬂip operator X by the phase-ﬂip operator
Z to yield the Y operator with a global phase shift of i. That is, Y D iXZ.
The circuit diagram for the Z operator is
Z
Next we turn to the more general phase shift operator. When we apply this
operator we leave the state j0i as is and we take the state j1i and rotate it by
the angle (or phase) denoted by ', as speciﬁed in the matrix
R' WD
1
0
0
ei'

So the Pauli Z operator is just a special case of R' where ' D . Let’s
recall that ei D  1 by Euler’s identity (see chapter 13) so we can replace
ei with  1 in the Z matrix. The circuit diagram for the R operator is
R'
Let’s discuss two additional phase shift operators that are special cases of
the R' matrix. First, the S operator, where ' D =2
S WD
1
0
0
i

The S operator thus rotates the state about the z-axis by 90°. The circuit
diagram for the S operator is
S
Next let’s turn to the T operator which rotates the state about the z-axis by
45°. If we give ' the value of =4 then4
4Note that the T gate is also known as the =8 gate, since if we factor out ei=8, the
diagonal components each have j'j D =8, but this is of course the same operator.
SECTION 3.1
Quantum Operators
31
T WD
1
0
0
ei=4

Note that S D T 2. In other words, if we apply the T matrix to the vector
representing the state and then apply T again to the resulting vector from
the ﬁrst operation we have accomplished the same result as applying S once
(45° C 45° D 90°/. The circuit diagram for the T operator is
T
Now let’s turn to the Hadamard operator. This operator is crucial in quan-
tum computing since it enables us to take a qubit from a deﬁnite com-
putational basis state into a superposition of two states. The Hadamard
matrix is
H WD
1
p
2
1
1
1
 1

It was actually the mathematician John Sylvester who developed this matrix,
but we name it after Jacques Hadamard (see Stigler’s law of eponymy which,
of course, was probably conceived by Merton and others). The circuit diagram
for the H operator is
H
If we apply the Hadamard to state j0i we obtain
1
p
2
1
1
1
 1
 1
0

D
1
p
2
1 C 0
1 C 0

D
1
p
2
1
1

D j0i C j1i
p
2
And to state j1i we have
1
p
2
1
1
1
 1
 0
1

D
1
p
2
0 C 1
0   1

D
1
p
2
 1
 1

D j0i   j1i
p
2
So we can see that the H operator takes a computational basis state and
projects it into a superposition of states .j0i C j1i/=
p
2 or .j0i   j1i/=
p
2,
depending on the initial state.
What is the
p
2 doing in this state? Let us recall the Born rule that the
square of the modulus of the amplitudes of a quantum state is the probability
of that state. Furthermore, for all amplitudes ˛, ˇ, etc. of a state
j˛j2 C jˇj2 D 1
That is, the probabilities must sum to one since one of the states will emerge
from the measurement.
32
CHAPTER 3
Qubits, Operators and Measurement
Before moving on to the binary operators, let us deﬁne the identity operator
and then determine which operators can be expressed as sequences of other
operators. The identity operator is simply the matrix which maintains the
current state of the qubit. So for one qubit we can use
I WD
1
0
0
1

Having covered the set of unary operators, we can show the following identi-
ties:
HXH D Z
HZH D X
H YH D  Y
H  D H
H 2 D I
Please see chapter 15 for a list of additional identities.
Binary Operators
Let us now consider two qubit, or binary, operators. In a two-qubit system,
by convention, we use the following computational basis states:
j00i D
0
BB@
1
0
0
0
1
CCA ;
j01i D
0
BB@
0
1
0
0
1
CCA ;
j10i D
0
BB@
0
0
1
0
1
CCA ;
j11i D
0
BB@
0
0
0
1
1
CCA
Let us ﬁrst discuss the SWAP operator. The SWAP takes the state j01i
to j10i and, of course, j10i to j01i. We can represent this operator with the
following matrix
SWAP WD
0
BB@
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
1
CCA
And apply it to a 4-d vector representing the state j01i as follows
0
BB@
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
1
CCA
0
BB@
0
1
0
0
1
CCA D
0
BB@
0 C 0 C 0 C 0
0 C 0 C 0 C 0
0 C 1 C 0 C 0
0 C 0 C 0 C 0
1
CCA D
0
BB@
0
0
1
0
1
CCA D j10i
SECTION 3.1
Quantum Operators
33
Satisfy yourself that this operator applied to one of the two-qubit computa-
tional basis vectors will have the desired result. For the circuit diagram of the
SWAP operator we use


Now we come to a critical operator for quantum computing — controlled-
NOT (CNOT). In this binary operator, we identify the ﬁrst qubit as the control
qubit and the second as the target qubit. If the control qubit is in state j0i then
we do nothing to the target qubit. If, however, the control qubit is in state j1i
then we apply the NOT operator (X) to the target qubit. We use the CNOT
gate to entangle two qubits in the QC. We can represent CNOT with the
following matrix
CNOT WD
0
BB@
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
1
CCA
So, for example, we compute the action of CNOT on the state j10i as follows
0
BB@
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
1
CCA
0
BB@
0
0
1
0
1
CCA D
0
BB@
0 C 0 C 0 C 0
0 C 0 C 0 C 0
0 C 0 C 0 C 0
0 C 0 C 1 C 0
1
CCA D
0
BB@
0
0
0
1
1
CCA D j11i
And for the circuit diagram, we depict the CNOT in this way

Here is an identity connecting the SWAP and CNOT operators:
SWAPij D CNOTij CNOTjiCNOTij
Now let’s turn to another control operator: CZ. Here we have a control
qubit and a target qubit just as with CNOT; however, in this operation if the
control qubit is in state j1i then we will apply the Z operator to the target
qubit. We can represent the CZ operator in a circuit diagram as

Z
34
CHAPTER 3
Qubits, Operators and Measurement
and as a matrix
CZ WD
0
BB@
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
 1
1
CCA
We can represent the CZ operator in circuit diagrams as


Note that unlike the CNOT gate, the CZ gate is symmetric: we can choose
either qubit as the control or the target and we will end up with the same result.
This is why we can represent the CZ gate with a dot on both circuit wires.
Ternary Operators
We have discussed both unary and binary operators. Now let’s consider the
ternary or 3-qubit operators. First, we have the Toffoli operator, also known
as the CCNOT gate [275]. Just as in the CNOT operator, we have control and
target qubits. In this case, the ﬁrst two qubits are control and the third is the
target qubit. Both control qubits have to be in state j1i for us to modify the
target qubit. Another way of thinking about this is that the ﬁrst two qubits (x
and y) have to satisfy the Boolean AND function — if that equals TRUE then
we apply NOT to the target qubit, z. We can represent this action as
.x; y; z/ 7! .x; y; .z ˚ xy//
or, as a matrix,
0
BBBBBBBBBB@
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
1
CCCCCCCCCCA
As an example, we apply this gate to the state j110i
SECTION 3.1
Quantum Operators
35
0
BBBBBBBBBB@
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
1
CCCCCCCCCCA
0
BBBBBBBBBB@
0
0
0
0
0
0
1
0
1
CCCCCCCCCCA
D
0
BBBBBBBBBB@
0
0
0
0
0
0
0
1
1
CCCCCCCCCCA
D j111i
In circuit diagrams, we use the following to denote the Toffoli


Next, let’s consider the Fredkin gate, also known as the CSWAP gate [118].
When we apply this operator, the ﬁrst qubit is the control and the other two
are the target qubits. If the ﬁrst qubit is in state j0i we do nothing and if it is
in state j1i then we SWAP the other two qubits with each other. The matrix
representing this operations is
0
BBBBBBBBBB@
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
CCCCCCCCCCA
For example, the Fredkin gate applied to j110i gives
0
BBBBBBBBBB@
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
CCCCCCCCCCA
0
BBBBBBBBBB@
0
0
0
0
0
0
1
0
1
CCCCCCCCCCA
D
0
BBBBBBBBBB@
0
0
0
0
0
1
0
0
1
CCCCCCCCCCA
D j101i
In circuit diagrams we use this symbol for the Fredkin operator
36
CHAPTER 3
Qubits, Operators and Measurement



3.2
Comparison with Classical Gates
In classical computing we have a set of commonly used gates: AND, NOT,
OR, NAND, XOR, FANOUT, etc. We can use combinations of these gates
to perform any computation in classical computing. A classical computer
that can run these gates is Turing-complete or universal. In fact, we can
show that the NAND gate alone is sufﬁcient to construct all other classical
operators [252] (and so is the NOR gate!). We can construct classical circuits
with these basic building blocks such as a circuit for the half-adder
Figure 3.1: Half-adder in classical computing
Source: Wikimedia
We can then build a full-adder from those elements:
Figure 3.2: Full-adder in classical computing
Source: Wikimedia
SECTION 3.3
Universality of Quantum Operators
37
Neither AND, OR, XOR, NAND or FANOUT can be used in quantum
computing. The AND, OR, XOR and NAND gates are not reversible. The
FANOUT gate would not be allowed in quantum computing since it involves
the duplication, or cloning, of a state; this would violate the no-cloning
theorem. Of the primary classical gates, only the NOT operator can be used
in the quantum computing regime as it is reversible and does not involve
cloning. For more reading on classical gates and computer architectures,
see [27, 112, 167].
3.3
Universality of Quantum Operators
If NAND is universal for classical computing, is there such a gate or set of
gates that are universal for quantum computing? In fact, there are several
combinations of unary and binary operators that lead to universality. No set
of unary gates on their own can achieve universal QC. Two of the gate sets
that yield universality are:
1. The Toffoli gate is universal for QC when paired with a basis-changing
unary operator with real coefﬁcients (such as H) [253].
2. Another set of gates which is universal is fCNOT; T; Hg [53, 206].
Another way of stating this approach is to realize that S is T 2; we then recall
that the Clifford group of operators is generated by the set C D fCNOT; S; Hg
[128] [214]. Therefore, by deﬁnition we can achieve universality in quantum
computing by utilizing the Clifford group of operators along with the gate T .
3.4
Gottesman-Knill and Solovay-Kitaev
The Gottesman-Knill theorem states that circuits built with only Clifford gates
can be simulated efﬁciently on classical computers assuming the following
conditions:
 state preparation in the computational basis
 measurements in the standard basis
 any classical control conditioned on the measurement outcomes
A further theorem that is worth considering at this juncture is that of
Solovay-Kitaev. This theorem states that if a set of single-qubit quantum
gates generates a dense subset of SU.2/, which is the special unitary group
of unitary matrices which are 2 x 2, then that set is guaranteed to ﬁll SU.2/
quickly, i.e., it is possible to obtain good approximations to any desired
38
CHAPTER 3
Qubits, Operators and Measurement
gate using surprisingly short sequences of gates from the given generating
set [85]. The theorem generalizes to multi-qubit gates and for operators from
SU(d) [85].
A simpliﬁed version of this statement is that all ﬁnite universal gate sets
can simulate a given gate set to a degree ı of precision. More precisely, if L is
the size of the circuit (i.e., the number of gates) then the approximation L0 of
L has a bounded number of gates; this can be speciﬁed in big-O notation by
L0 D O

L log4
L
ı

If D denotes the depth of the circuit, i.e., the number of computational
steps, then the approximation D0 of D has a bounded depth speciﬁed in big-O
notation by
D0 D O

L log4
D
ı

So, these expressions demonstrate that the simulation is quite efﬁcient and
better than polynomial time.
3.5
The Bloch Sphere
There are several ways to represent the state of a qubit:
1. We can write out the state in Dirac notation. For example, if we have a
qubit that is prepared in state j0i and then apply the X operator, we will then
ﬁnd the qubit in state j1i (assuming no outside noise)
X j0i ! j1i
2. We can use the Bloch sphere to represent the state of a single qubit. Any
state in a quantum computation can be represented as a vector that begins at
the origin and terminates on the surface of the unit Bloch sphere. By applying
unitary operators to the state vectors, we can move the state around the sphere.
We take as convention that the two antipodes of the sphere are j0i on the top
of the sphere and j1i on the bottom.
As we can see in Figure 3.3, one of the advantages of visualization with
the Bloch sphere is that we can represent superposition states such as
j0i C j1i
p
2
SECTION 3.6
The Measurement Postulate
39
as we see at the X axis. We can also differentiate between states that contain
different phases as is shown in the states along the X and Y axes.
Let us return to computational universality which we treated above. Now
that we have introduced the Bloch sphere, another way to think about a set of
gates that satisﬁes universal computation is one which enables us to reach any
point on the Bloch sphere.
Figure 3.3: The Bloch sphere
Source: [122]
For interactive visualizations of qubits on the Bloch sphere, see the book’s
online website. Now that we have covered the main unitary operators which
we use in QC, let’s turn to the measurement of the QC’s state.
3.6
The Measurement Postulate
Measurement in classical physics is a seemingly straightforward process. The
act of measurement is assumed to have no effect on the item that we are
measuring. Furthermore, we have the ability to measure one property of a
system, get a reading, then measure another property and be conﬁdent that
the ﬁrst property measured still retains its observed value. Not so in quantum
mechanics; in this regime, the act of measurement has a profound effect on
the observation.
Building on the principles of quantum mechanics, we can state the mea-
surement postulate as:
3.5
Measurement Postulate
Every measurable physical quantity, o, is described by a corresponding
Hermitian operator, O, acting on the state ‰.
40
CHAPTER 3
Qubits, Operators and Measurement
According to this postulate, there exists a Hermitian operator, which
we call an observable, associated with each property. So, for example, the
observable Ox is associated with the position of a particle. We recall that a
Hermitian operator is equal to its adjoint (which is its complex conjugate
transpose). If O is Hermitian then we can state that O D O (see chapter 12
for more discussion on Hermitians).
Hermitian operators have the desirable property that their eigenvalues are
guaranteed to be real numbers. When measuring a physical system for proper-
ties such as momentum or position we need to specify a real number. Each
possible outcome of the measurement is an eigenvalue, ; of the observable
and is characterized by jP j i j2 where P is the projector onto the eigenspace
of the observable. Since we normalize the vectors, the state after measurement
is represented by a unit eigenvector of the observable with eigenvalue i.
We discussed earlier how a system can be in a superposition of two or
more states. We can represent this as a linear combination of the orthonormal
basis vectors. For example,
j‰i D
1
p
2
j0i C 1
p
2
j1i
This leads to the question of how we can set up the measurement in such
a way as to obtain the output we require. This in turn is dependent on the
amplitudes of each state since, as discussed previously, the square of the
modulus of the amplitude is the probability of that state appearing as the
output upon measurement (Born’s rule).
We can represent a measurement in a quantum circuit like this:
Now that we have our unitary and measurement operators, we will con-
struct some basic quantum circuits. Let’s ﬁgure out what this one does:
j0i
H

j0i
We start with two qubits, let’s call them q0 and q1, each prepared in
state j0i. We then apply the Hadamard operator to q0 which puts it into the
superposition of states
jq0i D
1
p
2
j0i C 1
p
2
j1i
SECTION 3.7
Computation-in-Place
41
We then apply a CNOT across q0 and q1. This entangles the two qubits so
we now have the combined, non-separable state of the two qubits of
1
p
2
j00i C 1
p
2
j11i
We have thus created a Bell state, or an EPR pair. We then measure q0 with a
50/50 chance of ﬁnding a 0 or a 1 value for the real-valued output.
3.7
Computation-in-Place
We can depict the circuit in a 3-dimensional manner as in Figure 3.4. Here
you can see the qubits beginning in a prepared state followed by a Hadamard
step to put them all into a superposition; then they undergo a series of one and
two-qubit operations such as X, Y , T and CZ. Measurement is then applied
in the ﬁnal step.
In this diagram, we see a crucial difference between classical and quantum
computing.
Figure 3.4: 3-D Quantum circuit diagram
Source: Google
42
CHAPTER 3
Qubits, Operators and Measurement
3.6
Computation-in-Place
In most forms of gate-based quantum computing, the information is rep-
resented in the states of the qubits as they evolve over time with the
successive application of unitary operators.
Computation-in-place is in stark contrast to classical computing, where we
shuttle data around the processor to various memory and calculation registers.
In most forms of quantum computers, all processing takes place on the qubits
themselves. After measurement in a QC we output real-valued bits which
we can share with the CPU that is controlling the quantum processor and, if
necessary, incorporate in further processing on classical machines.
We now come to one of the key questions in quantum computing: if
measurement is based on the modulus squared of the amplitude of each qubit
state, then how can we pre-determine which of the qubits will be the output?
Deutsch, Jozsa, Bernstein, Vazirani, Shor and others realized that we can
inﬂuence the output by setting up the amplitudes prior to measurement to
favor the output we need for that computational task.
One method for accomplishing this goal is the quantum Fourier transform
(QFT – not to be confused with quantum ﬁeld theory!). By applying a QFT
across all qubits prior to measurement, we can obtain phase information upon
measurement instead of amplitude information.
The QFT is an efﬁcient process on a quantum computer: the discrete
Fourier transform on 2n amplitudes only takes O.n2/ applications of
Hadamard and phase-shift operators (where n is the number of qubits). We
will cover the QFT in greater detail later in this text.
Here we display the circuit for QFT for n D 4:
jx1i
H
R=2
R=4
R=8
jx2i

H
R=2
R=4
jx3i


H
R=2
jx4i



H
Before we turn our attention to quantum hardware, let’s delve into com-
putational complexity in the next chapter. This will give us the foundation to
understand which sorts of problems are appropriate for a quantum computer.
CHAPTER4
If you take just one piece of information from
this blog: Quantum computers would not
solve hard search problems instantaneously
by simply trying all the possible solutions at
once.
—Scott Aaronson
Complexity Theory
Since quantum computing offers an alternative approach to computation, it is
logical to consider which classes of problems are now tractable in this new
regime that were not thought to be tractable in a classical framework. To do
so, let’s consider a range of problem classes.
4.1
Problems vs. Algorithms
Let us ﬁrst clarify the difference between computational problems (or tasks),
algorithms and programs. Here is an example of a computational problem:
Given a data set of n numbers, sort the numbers in
increasing numerical order.
We can then analyze several different algorithms to solve this problem:
quicksort, merge sort, insertion sort and others. An algorithm is a hardware-
independent method of solving a computational problem. We generally try to
ﬁnd algorithms that can solve a problem efﬁciently. A program is a particular
implementation of an algorithm in a given coding language.
Analysis of algorithms is the study of the resources that an algorithm needs
to run.1 We can bucket algorithms into different computational orders both
in time (number of steps) and space (amount of memory). Computational
complexity theory, by contrast, is the study of classes of problems; we will
deﬁne a number of important problem classes below.
1Note: It is common to refer to the analysis of the time and memory resource requirements
of an algorithm as the time and space complexity of the algorithm. Although the strict
deﬁnition of "complexity" would only apply to computational problems, the usage is sufﬁciently
widespread that we will occasionally use complexity in the context of algorithms as well.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_4
43
44
CHAPTER 4
Complexity Theory
When deﬁning complexity classes we focus on decision problems; these
are mathematical problems that can be answered with a binary yes/no response
given an input. Examples of decision problems include:
Given x, is x a prime number?
Given x and y, does y divide x evenly?
Complexity analysis for a decision problem determines the computational
resources needed to deliver an answer. We generally look at the characteristics
of the worst-case scenario for this determination.
4.2
Time Complexity
As we discussed in an earlier chapter, we can use big-O notation to denote
the upper bound of the worst case of a problem. For example, if we have a
series of items that we want to sort, the big-O time complexity will depend
on which algorithm we choose to sort the items. Here are some complexity
orders of common sorting algorithms:
 Insertion sort: O.n2/
 Mergesort: O.n log.n//
 Timsort: O.n log.n//
These differences can be signiﬁcant if n is large. When analyzing algo-
rithms for the computational resources needed to run them, we are performing
asymptotic analysis; in other words, what resources will be needed as the
input, n, gets very large. Refer to Figure 4.1 for a plot of common big-O time
computational orders.
In addition to big-O notation which designates the upper bound on the
worst case computational order for that algorithm, we can consider big-
(Omega) notation which designates the lower bound of the worst case compu-
tational order. Adding now to the list above we can compare lower and upper
bounds for the computational resources needed for these sorting algorithms:
 Insertion sort: .n/, O.n2/
 Mergesort: .n log.n//, O.n log.n//
 Timsort: .n/, O.n log.n//
SECTION 4.2
Time Complexity
45
Figure 4.1: Big-O time complexity chart
Source: [23]
We see that the lower bound for insertion sort and timsort are signiﬁcantly
better than their upper bound. However, since we have to plan for the upper
bound, we generally focus on the big-O complexity.
There is a third metric we use for cases in which the upper bound (big-O)
and the lower bound (big-) of the worst-case match. For these algorithms
we can describe their big-‚ computational order, which is both their big-O
and their big- since they match.
To use more formal notation, we say that a function f .n/ (which could
represent the time necessary to run an algorithm for an input of size n) is on
the order of a function g.n/ (which could represent the time necessary to run
a different algorithm given n inputs) if and only if the limit superior of the
absolute value of the quotient of f .n/ and g.n/ is ﬁnite as n tends to inﬁnity.
In symbols
f .n/ D O.g.n// if and only if lim sup
n!1
ˇˇˇˇ
f .n/
g.n/
ˇˇˇˇ < 1
In this expression, lim sup is the limit superior or supremum limit. Following
Hardy and Littlewood [134] and as described by Knuth [156], we state that
f .n/ D .g.n// iff g.n/ D O.f .n//
46
CHAPTER 4
Complexity Theory
and
f .n/ D ‚.g.n// iff f .n/ D O.g.n// and f .n/ D .g.n//
There are two more notations called little-o and little-!. Little-o provides
a strict upper bound (equality condition is removed from big-O) and little-!
provides a strict lower bound (equality condition removed from big-).
Let’s also introduce the concept of completeness. A problem, G, is said to
be H-complete if it is a part of class H and we can prove that all problems
in class H can be reduced to G. In other words, if we have a subroutine, S,
which when run can solve for problem G, and this subroutine can also solve
for any problem in H, then we can say that all problems in H can be reduced
to G and that G is H-complete.
4.3
Complexity Classes
Let us ﬁrst deﬁne a number of common complexity classes from classical
computing:
P – Polynomial time: problems that can be solved in polynomial time.
In other words, the problem can be solved in a reasonable amount of time
on a classical computer. Problems of O.1/, O.log.n//, O.n/, O.n log.n//,
and O.n2/, as examples, are in class P. Although O.log.n// doesn’t look
polynomial since log.n/ is not a polynomial in n, O.log.n// is in P since is it
upper-bounded by O.n2/, which is a polynomial.
PSPACE and related classes
Source: Wikimedia
NP – Non-deterministic polynomial
time: a problem is in NP if whenever the
answer is "yes," there’s a polynomial-
size witness or proof for the yes-answer,
which a polynomial-time algorithm can
verify. We can imagine a Turing ma-
chine which can switch to a state that is
not determined by its previous state. If
through such a move it happens upon a
correct solution to a problem, this solu-
tion can be veriﬁed in polynomial time.
A problem, A, is said to be NP-Complete iff2: (1) A is in NP, and (2) all
NP problems are polynomial-time reducible to A. If we only know (2), and
not necessarily (1), we say A is NP-hard.
2Note: we often abbreviate if and only ifas iff throughout the book.
SECTION 4.3
Complexity Classes
47
Figure 4.2: Complexity classes - note: it is not yet proven whether graph isomorphism is in P
or not
Source: [2]
One of the seven Clay Mathematics Institute Millennium Problems is the
question of whether the complexity class P is equivalent to the class NP. This
$1 million challenge is the most recently developed of all the Clay prizes and
is most closely tied to computer science; this question is still the subject of
active research.
PSPACE – Polynomial space: This class focuses on memory resources,
not time. PSPACE is the class of decision problems that are solvable by some
algorithm whose total space usage, in all instances, can be upper-bounded by
a polynomial in the instance size. See Figure 4.2 for the position of PSPACE
in the context of other classes.
BPP – Bounded-error probabilistic polynomial time: BPP is a class that
contains P; many believe that BPP = P, but we have not been able to prove
that yet [3, Lecture 4]. BPP is the class of decision problems for which there
exists a polynomial-time randomized algorithm that solves every instance
with a success probability of at least 2
3 (over the choice of random bits). The
core idea of BPP is that sometimes randomized algorithms give us faster
48
CHAPTER 4
Complexity Theory
time results than a deterministic algorithm trying to achieve the same goal.
Problems that are in BPP either have a deterministic algorithm that can run in
polynomial time or have a probabilistic algorithm which will give the wrong
answer to a decision problem no worse than 1
3 of the time.
The bound of 1
3 is not ﬁxed. We can choose any lower bound in the interval
Œ0; 1
2/ and BPP will not change. The reason is that when run many times the
overall probability of producing a wrong error is low. This is described by the
Chernoff bound.
Now let us consider a range of complexity classes that arise with quantum
computing:
BQP – Bounded-error quantum polynomial time: BQP is the quantum
analogue of the class BPP for classical computation. A decision problem is in
BQP if it can run in polynomial time and yields a correct result with a high
probability. BQP is the primary complexity class we focus on for QC.
As you can see from Figure 4.2, BQP contains problems which are thought
to be intractable in the classical regime but are thought to be tractable in
bounded-error polynomial time for a quantum computer. We say "thought to
be" as it is still not yet proven whether the computational problem of factoring
large numbers, for example, is in P or NP. Although we have no classical
algorithm at the moment that can factor large numbers, this does not mean
such an algorithm does not exist. Thus, complexity theorists are not sure of
the exact position of BQP with respect to P, NP and PSPACE [206].
EQP – Exact quantum polynomial time: this is the set of decision problems
solvable by a quantum circuit that yields the correct answer with a probability
of 1. In other words, this class is the same as BQP except that it must give
the correct answer with probability of 1 instead of having some bounded error
margin [6].
Note that a QC does not render all NP problems tractable; only problems
that have some structure we can exploit can be handled efﬁciently by a QC. For
example, Shor’s algorithm takes advantage of the periodicity of the function
which then enables us to solve the equivalent problem of factoring a large
number.
QMA – Quantum Merlin-Arthur: QMA is the quantum analogue to the
non-probabilistic class MA. In a Merlin-Arthur (MA) problem, a prover
(Merlin) sends a message to a veriﬁer (Arthur). In the classical complexity
class MA, Arthur can verify the message in polynomial time. Problems that
live in QMA are ones with the following characteristic [46]:
SECTION 4.4
Quantum Computing and the Church-Turing Thesis
49
 If the answer is Yes, then the veriﬁer can verify with probability greater
than 2
3 using a proof that can run in polynomial time on a quantum
computer.
 If the answer is No, then the veriﬁer can reject the proof with a wrong
outcome in no more than 1
3 of the cases.
Table of Classical and Quantum Complexity Classes3
Classical
Quantum
P
EQP
BPP
BQP
NP
QMA
4.4
Quantum Computing and the
Church-Turing Thesis
Let us now turn to the relationship between quantum computing and the
Church-Turing thesis. Alonzo Church and Alan Turing developed the initial
conjecture that:
4.1
Church-Turing Thesis (CTT)
If an algorithm can be performed on any piece of hardware (say, a modern
personal computer), then there is an equivalent algorithm for a Universal
Turing Machine (UTM) which performs exactly the same algorithm [206,
p. 5].
This conjecture was then updated to take into account the efﬁciency of the
algorithm. Algorithmic efﬁciency refers to the quantiﬁcation of a particular
resource that is used in running the algorithm. It is important in this analysis to
maintain consistency when comparing the efﬁciency of a set of algorithms; if
we are analyzing the number of steps of one algorithm, we should do so for all
others in the comparison and not switch to the analysis of memory resources,
for example, midway through the analysis. The additional requirement of
running an algorithm efﬁciently gives us the Strong Church-Turing Thesis
(SCTT):
3Readers interested in this subject should consult the complexity zoo [6].
50
CHAPTER 4
Complexity Theory
4.2
Strong Church-Turing Thesis (SCTT)
Any algorithmic process can be simulated efﬁciently using a Universal
Turing Machine (UTM) [206, p. 5].
Researchers then realized that there were probably counterexamples to
the SCTT, namely, algorithms that made use of randomness. For example,
Solovay and Strassen demonstrated that an algorithm using randomness could
test for primality of a number [262]. By repeating the algorithm a ﬁnite
number of times, one could obtain a correct answer with almost near certainty.
It was later shown that there is an efﬁcient polynomial-time algorithm for
testing primality [9], but historically, it was the initial insight that an algorithm
using randomness could get the task done that led to the reﬁnement of the
SCTT. The SCTT was thus updated to give us the Extended CTT (ECTT):
4.3
Extended Church-Turing Thesis (ECTT)
Any algorithmic process can be simulated efﬁciently using a Probabilistic
Turing Machine (PTM) [206, p. 6].
Enter quantum computation. QC challenges the ECTT by demonstrating
that it can solve certain problems exponentially faster than a classical computer.
This undermines the ECTT assertion that any UTM or even PTM can simulate
any other computing device in running any given algorithm [11]. We now
arrive at the quantum version of the ECTT, which still stands:
4.4
Quantum Extended Church-Turing Thesis (QECTT)
Any realistic physical computing device can be efﬁciently simulated by a
fault-tolerant quantum computer.
Now that we have a foundation of quantum operators, circuits and com-
plexity classes, let us explore how to build a physical quantum computer.
Part II
Hardware and Applications
CHAPTER5
Building a Quantum Computer
Now that we have covered the essential workings of a quantum computer,
let us discuss how we can physically realize these devices. There are many
different architectures and designs of gate-based quantum computers each
with its own pros and cons. In this chapter we will cover the leading paradigms
of quantum computational hardware. Check the book’s online site for updates
as the technology is changing rapidly.1
All of the architectures below require a set of classical computers for
control of the system. As you can see in Figure 5.6, a superconducting qubit
QC, for example, is controlled by a traditional computer. We develop our
quantum circuit protocols in high-level languages on the classical computer
which can then manipulate the quantum system to apply the operators in the
circuit. Upon measurement, the output of a QC is classical information which
is fed back to the classical computer for readout or further processing.
We can think of the QC portion of a computation as a subroutine in a much
larger computation, much of which may take place in the classical regime. We
can therefore refer to a quantum computer as a quantum processing unit (QPU)
and we envision these QPUs will be used in combination with CPUs, GPUs
and other processors to complete computational tasks. In the application of
Shor’s algorithm, for example, many tasks are performed by the classical
computer and then the hard part – the implementation of the period-ﬁnding
algorithm (which is a key step in prime number factoring) – is sent out as a
subroutine to the QC. This output is then integrated back into the classical
platform.
1The online site is at this URL: https://github.com/jackhidary/quantumcomputingbook
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_5
53
54
CHAPTER 5
Building a Quantum Computer
5.1
Assessing a Quantum Computer
There are many advances occurring in quantum hardware. Here is a useful
checklist to analyze the potential impact of engineering progress in this ﬁeld:
1. Universality: The ﬁrst question to ask about a hardware platform that
is presented as a quantum computer is whether it is Turing-complete,
or universal. The device may be a non-universal annealer, for instance.
We can turn to the DiVincenzo criteria we described in chapter 2 to
aid us in this test. For example, DiVincenzo calls for the qubits to
be individually addressable in a quantum computer. If we have, for
example, a two-level system comprised of an ensemble of atoms, but
where the qubits are not individually addressable, this system would
fail the QC test.
2. Fidelity: While it is tempting to focus on the horse race of the number
of qubits of each platform, we recommend ﬁrst examining the claims
made on the ﬁdelity of those qubits. Fidelity is a measure of the ability
for a qubit to remain in coherence through a computation; speciﬁcally
it is calculated as: 1 - the error rate. When presented with a system, it
is useful to examine the ﬁdelity of the qubits under one and two-qubit
operations. Fidelity is harder to maintain when spanning two qubits
with a CNOT, for example, than when applying single-qubit operators
such as X or Y .
3. Scalability: Is the architecture scalable to 106 qubits and beyond?
While it is of some beneﬁt to create NISQ-regime QCs across a mul-
titude of hardware frameworks at this stage, it is also important to
examine the ability to achieve a fault-tolerant platform in that architec-
ture.
4. Qubits: Once the above questions are considered, we can turn to num-
bers of qubits. Maintaining the simultaneous coherence of larger and
larger numbers of qubits is a technical challenge. Also, it is important
to look at architecture-speciﬁc limitations of the qubits, such as nearest-
neighbor connections. In some platforms, for example, if we operate
on two adjacent qubits, we cannot make use of other adjacent qubits at
the same time due to potential crosstalk.
5. Circuit depth: This refers to how many operations we can implement
before coherence breaks down. A 106 qubit computer would be won-
derful, but if it cannot implement more than a few operations before
losing coherence it is of limited value.
SECTION 5.2
Neutral Atoms
55
6. Logical connectivity: Can we implement two-qubit gates on any pair
of qubits, or only for certain pairs? Limited logical connectivity re-
quires that logical SWAP operations be inserted into our algorithms to
effectively simulate greater connectivity. More operators means more
potential for noise and errors.
7. Cloud access: Is the hardware easily made available over the cloud?
It is unlikely that most organizations will purchase or build their own
QCs. Instead, they will rely on a range of academic and commercial
providers who will provide cloud access. Criteria to consider in this
category include reset time between computations and labor required to
keep the platform available to meet its service level agreement (SLA).
With that introduction, let’s consider the leading QC architectures in
alphabetical order. Please consult this book’s online site for links to additional
papers and updates as they are posted. For additional background on the
approaches outlined in this chapter, please see [161].
5.2
Neutral Atoms
Neutral atoms present an intriguing approach to quantum computing. While
trapped-ion research has been going on for some time, several labs have more
recently ramped up their ability to control ensembles of neutral atoms.
To implement a neutral atom system, engineers can set up four laser
beams around the atom ensemble to form a magneto-optical trap (MOT).
Labs typically use either cesium (Cs) or rubidium (Rb) atoms for this work.
By conﬁning the atoms with this quadruple laser system, we can cool the
atoms down to mK temperatures. Now that we have hundreds of millions
of neutral atoms in a reservoir, we can transfer a small number of them into
an addressable array (see Figures 5.1 and 5.2). See [141] for an in-depth
discussion of neutral atom systems.
The Lukin Lab at Harvard has made good progress on neutral atom sys-
tems [220, 169]. In their recent paper [98], the Lukin lab reports a number
of breakthroughs in their ability to control larger arrays of neutral atoms.
This could lead to faster scaling of high-ﬁdelity qubits on this platform.
David Weiss and his group at Penn State have also focused on neutral atom
platforms and have demonstrated a Stern-Gerlach-inspired neutral atom ex-
periment [290, 301]. The group at Pascal in France have been developing
a neutral atom quantum computer and together with their academic partner
at IOGS have achieved a neutral atom QC of more than 100 qubits [216].
56
CHAPTER 5
Building a Quantum Computer
Figure 5.1: A. Diagram of addressing a 5  5  5 array of neutral atoms. Each addressing
beam can be parallel-translated within 5s to any line of atoms, so that any site can be put at
their intersection. The addressing beams are circularly polarized, and the 140mG magnetic
ﬁeld is in the same plane. B.) The relevant part of the ground state energy level structure for
addressing (not to scale) a target atom experiences twice the AC Stark shift of any other atom
(its shift is illustrated by the orange dashed lines), so that, starting in the storage basis, j3; 0i
and j4; 0i, it alone is resonant with !1. After it is transferred to the computation basis, j3; 1i
and j4; 1i, it alone is resonant with !2. Image and caption from [290]
IOGS has also demonstrated multiple atomic conﬁgurations for use in neutral
atom QCs [28]. Each of these conﬁgurations can lead to different topologies
optimized for different computational problems.
Neutral atom systems meet the DiVincenzo criteria for a quantum com-
puter: qubits are individually separable and addressable, they can hold their
state and we can perform a measurement on them. Saffman et al. offers a good
review of implementing gates on a neutral atom system [246]. The challenge
now is to scale such systems. See [294] [141] for helpful reviews.
For an example of programming for a neutral atom QC see [217] which
uses Pennylane and Cirq to code for the Pasqal neutral atom QC.
5.3
NMR
Some of the ﬁrst demonstrations of quantum computation made use of spin
qubits using nuclear magnetic resonance (NMR) devices. Chuang and col-
leagues demonstrated the factoring of the number 15 using Shor’s algorithm in
a liquid-state NMR setup (LSNMR) [278]. While the number 15 is trivial to
factor, it was one of the ﬁrst demonstrations that quantum principles could be
used in a physically realizable system for computation. Later, researchers ex-
SECTION 5.4
NV Center-in-Diamond
57
Figure 5.2: (a) Overview of the main hardware components constituting a quantum processor.
The trapping laser light (in red) is shaped by the spatial light modulator (SLM) to produce
multiple microtraps at the focal plane of the lens (see inset). The moving tweezers (in purple),
dedicated to rearranging the atoms in the register, are controlled by a 2D acousto-optic laser
beam deﬂector (AOD) and superimposed on the main trapping beam with a polarizing beam-
splitter (PBS). The ﬂuorescence light (in green) emitted by the atoms is split from the trapping
laser light by a dichroic mirror and collected onto a camera. (b) Photography of the heart of a
neutral-atom quantum co-processor. The register is prepared at the center of this setup. Image
and caption from [141]
perimented with solid state NMR (SSNMR) using nitrogen-vacancy centered
diamonds, which relates to another approach covered in this chapter. While a
number of researchers use NMR platforms to test various QC phenomena, this
platform is unlikely to scale for fault-tolerant quantum computation. We will
need millions of physical qubits (translating to thousands of logical qubits)
for such a device and NMR as currently implemented becomes impractical at
those scales.
5.4
NV Center-in-Diamond
In the nitrogen-vacancy (NV) center-in-diamond approach to QC, two carbon
atoms in the diamond lattice are missing, and one of them is replaced with a
nitrogen ion. The resulting system forms a paramagnetic defect that can act
as a qubit. The qubit state can be manipulated with microwave ﬁelds and read
out optically. A number of labs have demonstrated basic NV systems [248,
277, 70].
Instead of doping with nitrogen, several labs have doped with silicon [143,
101]. These and other materials each have their own unique advantages and
disadvantages [66, 157]. For helpful reviews of NV approaches see [73, 95,
305]. Researchers are currently investigating the successful application of a
two-qubit operator on this platform (see [172, 35, 219, 142]).
58
CHAPTER 5
Building a Quantum Computer
Figure 5.3: NV diamond schematic
Source: [305]
5.5
Photonics
Photonics can also be used to construct a gate-based quantum computer
(see [259] for a review). Linear Optics Quantum Computing (LOQC) uses
linear optical elements (such as mirrors, beam splitters and phase shifters) to
process quantum information [8, 155, 158]. These optical elements preserve
the coherence of input light and hence equivalently apply a unitary transfor-
mation on a ﬁnite number of qubits. However, photons do not interact with
each other in a vacuum. They can only interact indirectly via another medium.
In 2001, Knill, Laﬂamme and Milburn (KLM) [155] presented a method of
implementing scalable quantum computing using single photons, linear optics,
photodetection and post-processing. These elements enable the application of
nonlinear operations solely with linear optical elements [158]. More recently,
the PsiQuantum group has described the Fusion-Based Quantum Computing
(FBQC) approach to LOQC [29]. PsiQuantum is focused on building a fault-
tolerant photonic QC using this approach; they rely on silicon photonics
fabrication techniques as their method to scale to millions of qubits.
Another paradigm is the so-called one-way, measurement-based or cluster-
state quantum computer (MBQC) [229, 230, 231, 232] which ﬁrst prepares a
highly entangled multi-particle cluster state. Then, in order to implement a
quantum circuit, one has to perform a sequence of single-qubit projective mea-
surements. Every subsequent measurement choice is driven by the outcome
of the previous measurement. The Xanadu group of Canada has described
their approach to MBQC [276].
Arbitrary two-qubit processing requires the equivalent of three consecutive
entangling gates in the circuit model of quantum computing [133] which
is beyond the level of complexity that can be practically constructed and
maintained with free-space quantum optics [185, 65]. Photonic chips can take
advantage of the entire silicon-based infrastructure to miniaturize LOQC and
SECTION 5.5
Photonics
59
Figure 5.4: Quantum circuits and a schematic of a silicon photonic chip Source: [227]
bring down the cost [255]. Researchers from the University of Bristol, together
with researchers from China’s National University of Defense Technology,
have demonstrated a photonic quantum processor on a silicon photonics chip.
The processor generates two photonic qubits on which it performs arbitrary
two-qubit unitary operations, including arbitrary entangling operations [227].
The quantum processor was fabricated with mature Complementary Metal
Oxide Semiconductor (CMOS) compatible processing and comprises more
than 200 photonic components; the processor was programmed to implement
98 different two-qubit unitary operations with an average quantum process
ﬁdelity of 93:2 ˙ 4:5%.
Semiconductor quantum transistor
The lack of deterministic photon-photon interactions is a challenge for quan-
tum computation in this approach. To deterministically control an optical
signal with a single photon requires strong interactions with quantum memory.
Nanophotonic structures coupled to quantum emitters may offer an attrac-
tive approach to realize single-photon nonlinearities in a compact solid-state
device. Recently, there has been great progress in controlling photons with
solid-state qubits [16, 258, 38], as well as controlling a solid-state qubit with
a photon [267]. Arazolla et al. demonstrated a programmable optical chip for
QC using optical squeezed states [17].
Researchers at the University of Maryland and the Joint Quantum Insti-
tute have realized the ﬁrst single-photon switch and transistor enabled by
solid-state quantum memory using a semiconductor chip [266]. The device
60
CHAPTER 5
Building a Quantum Computer
allows one photon to switch other photons, and hence to produce strong and
controlled photon-photon interactions. It consists of a spin qubit strongly
coupled to a nanophotonic cavity (an idea ﬁrst proposed by Duan and Kim-
ble [96]). They combined a semiconductor membrane with quantum dots;
together they sit in the middle of the array. The array forms a photonic crystal,
which uses the Bragg reﬂection mechanism where light bounces around a
trap. That allows the quantum dot to store the information about the photon
with a single electron, which has spin properties. By using this transistor, they
should be able to apply quantum gates to photons. A scalable device suitable
for quantum information processing will require higher efﬁciencies, as photon
loss constitutes a dominant error source for photonic qubits.
Topological photonic chip
Topological insulators are exotic materials which insulate in their bulk but
conduct on their surface [137, 226]. These states exhibit remarkable properties
such as unidirectional propagation and robustness to noise. Since the discovery
of these phases of matter, several topological effects have been observed using
integrated photonics [179, 213, 304, 291, 131, 233, 132, 40, 197, 303, 69,
208, 171, 160, 280, 279, 310, 41, 198].
Topological photonics is a promising option for scalable quantum comput-
ers since they have the advantage of not requiring strong magnetic ﬁelds and
feature intrinsically high-coherence, room-temperature operation and easy
manipulation. Recently, scientists from RMIT University, Australia, in collab-
oration with researchers from the Politecnico di Milano and ETH Zurich, have
developed a topological photonic chip that encodes, processes and transfers
quantum information at a distance [270]. They have used photonic chips
to demonstrate that topological states can undergo quantum interference by
replicating the well known Hong-Ou-Mandel (HOM) experiment with 93.1
˙ 2.8% visibility [145].
5.6
Spin Qubits
Silicon-based spin-qubit technology represents another approach to quantum
computation. If we could construct qubits from common semiconductor mate-
rials, we could scale such a system by leveraging the decades of know-how in
the integrated chip industry. It is difﬁcult, however, to develop a stable, ad-
dressable qubit on the standard CMOS platform. Initial prototypes have been
demonstrated, but it has been challenging to stabilize these qubits for scale-up.
SECTION 5.7
Superconducting Qubits
61
Figure 5.5: Spin qubit experimental system
Source: [188]
In these early attempts, a pair of quantum dots is placed between the source
and sink in a traditional CMOS setup on a silicon semiconductor substrate.
The entire device is placed in a dilution fridge to bring it down to about 1K;
this is much warmer than the temperature needed for superconducting qubits.
Microwave pulses are then used to apply unitary operators to the qubits [292].
Intel, HRL Laboratories, as well as labs at the University of Cambridge,
Delft University of Technology, Harvard, and the University of New South
Wales (UNSW) are working on silicon-based spin qubit approaches.
5.7
Superconducting Qubits
Several groups are building quantum computers with superconducting qubits.
The core design is based on a qubit made from a Cooper pair with a Josephson
junction [51]. Microwave leads are attached to the qubit to control it. By send-
ing speciﬁc pulses of microwave frequencies for controlled amounts of time
into the physical qubit, the user can apply the range of unitary operators. The
entire apparatus must be cooled below 10mK to operate. The system is also
shielded from magnetic ﬁelds and other factors that could cause decoherence.
62
CHAPTER 5
Building a Quantum Computer
Figure 5.7: Superconducting processors (l to r): Google, IBM, Rigetti
Source (l to r): Google,
[164], [274]
The NAS report summarizes the various types of superconducting
qubits [204, page C-1]:
Fixed-frequency versus tunable qubits:
Frequency-
tunable qubits can be calibrated and corrected for
qubit frequency variations that arise from variations in
the fabrication process or as a result of device aging.
An advantage is that one microwave tone can control
multiple qubits, a savings in hardware. Gaining this
advantage requires an additional control signal to adjust
the frequency and adds an additional path for noise to
enter the qubit. The two most common qubits in use
today for digital superconducting quantum computing
are the “transmon qubit”, which comes in single-junction
nontunable and two-junction tunable forms, and the “ﬂux
Figure 5.6: Classical CPU controlling a superconducting quantum computer. The red circle
denotes the area within the dilution fridge for the quantum processor. In more recent implemen-
tations the temperature in this area is brought down below 10mK. See Figure 5.7 for images of
various superconducting quantum processors
Source: [215]
SECTION 5.8
Topological Quantum Computation
63
qubit.”...Both transmon designs are being used in leading
edge efforts.
Static versus tunable coupling:
Static
coupling
between qubits — for example, by using a capacitor or
an inductor to mediate interaction — is an “always-on”
coupling that is ﬁxed by design. The coupling is turned
“on” by bringing two qubits into resonance, and it is
turned off by detuning the qubits. Yet even in the off
state, there still is a small residual coupling. This tuning
can be further reduced by adding a third object — either
another coupler qubit or a resonator — between the two
qubits. The two qubits are then coupled by adjusting the
qubits and the resonator to the proper frequency.
A number of groups are working on superconducting qubit quantum com-
puters including: Google, IBM, Rigetti, QCI and others (see Figure 5.7)
and [211, 242]. Check this book’s GitHub site for updated information in this
area. See [154] and [159] for helpful reviews of this approach.
5.8
Topological Quantum Computation
Topological quantum computing (TQC) attempts to take advantage of the
unique properties of anyons. An anyon is a 2-dimensional quasiparticle which
is neither a boson (such as a photon) nor a fermion (such as an electron). By
braiding the pathways of an anyon in 4D spacetime, one can theoretically
create a system for quantum computation which is robust to decohering
effects (see Figure 5.8). This is due to the braided nature of the system —
even if there are a series of small perturbations to the system, the topology
of the system does not change, and therefore it stays coherent and quantum
computation can continue. See Roy and DiVincenzo for more background on
this approach [245].
Alexei Kitaev, then of the Landau Institute of Theoretical Physics, ﬁrst
proposed the idea of TQC [153]. Freedman, Kitaev, et al. then demonstrated
that such a topological computer would be Turing complete [119]. Freedman
and others then launched a program at Microsoft to investigate the physical
realization of a topological quantum computer. For useful surveys of this ap-
proach, see [162] and [207]. See [244] for an overview of the mathematics of
topological computing and see [269] for discussion of a gate implementation
in a TQC setting.
64
CHAPTER 5
Building a Quantum Computer
Figure 5.8: Braided anyons for topological computing
Source: [244]
5.9
Trapped Ion
Source: [300]
In the trapped-ion approach, ytterbium
atoms (or another element) are ionized
with lasers and trapped in electric po-
tentials to form a line of qubits. An
additional laser is then used to measure
the state of the qubits. Proponents of
trapped-ion systems point to the ability
to run their systems without having to
cool it to mK ranges which is a requirement of hardware realizations such
as superconducting qubits. Cirac and Zoller did early work in this ﬁeld [79].
Chris Monroe of the University of Maryland, College Park and Jungsang Kim
of Duke University have been active in this space and reported on several
advances [14, 25]. Other groups include those at NIST [44], Oxford [180],
Innsbruck [121], MIT [168] and ETH [115]. Pino et al. have demonstrated
an ion trap QC using their quantum charged-coupled device (QCCD) ap-
proach [221].
Figure 5.9 details two types of qubits that can be realized in a trapped
ion system: optical qubits and hyperﬁne qubits. Optical qubits leverage the
difference in energy levels between ground and metastable states; hyperﬁne
qubits distinguish between two different ground states. See [63] and [204] for
helpful reviews of the trapped-ion approach.
SECTION 5.10
Summary
65
Figure 5.9: Qubits in an atomic ion. (a) An optical qubit consists of one of the atomic ground
states and one of the metastable excited states, separated by approx. 1014 to 1015Hz. (b)
A hyperﬁne qubit consists of two of the ground states, separated by approx. 109 to 1010Hz.
Usually some excited states are used to support qubit manipulation operations. In both cases,
there are other (auxiliary) states in the ground, excited and metastable excited states than
those chosen to represent the qubit.
Source: [204, Appendix B]
5.10
Summary
Figure 5.10: Quantum computing roadmap
Source: Google
Researchers are pursuing a range of architectures in the quest for a fault-
tolerant, scaled universal quantum computer. This period in quantum com-
puting hardware is probably most akin to the 1940s and 1950s in classical
computing. Research groups today are still ﬁguring out which architecture
will scale and to understand the kinds of problems which can be addressed
with these platforms. Quantum hardware development is likely to move at
66
CHAPTER 5
Building a Quantum Computer
a faster pace as we have the beneﬁt of a global community of academic and
commercial researchers forging ahead.
Figure 5.10 indicates that we are in the NISQ era of 102 to 103 qubits. We
aim to reach >106 qubits in order to build a fully error-corrected quantum
computer. Current quantum error correction techniques require about 1,000
physical qubits for every logical qubit. Since at least a few thousand logical
qubits are needed for Shor’s and other important algorithms, we will have to
be in the 106 to 107 regime to realize this goal.
Now that we have surveyed the various approaches to quantum computing
hardware, let us turn to the QC development platforms and software.
CHAPTER6
Development Libraries for
Quantum Computer
Programming
With the growing interest in quantum computing, there are an increasing
number of development libraries and tools for the ﬁeld. There are development
environments and quantum circuit simulators in all the major languages
including Python, C/C++, Java and others. A comprehensive list can be found
on this book’s website.
Many of the leading QC research centers have focused on Python as
the language of choice for building quantum circuits. One of the reasons
for choosing Python is that it is a ﬂexible, high-level language that allows
programmers to focus on the problem being solved without worrying about
too many formal details. For example, Python is dynamically typed (meaning
variable types do not have to be declared by the programmer) and is an
interpreted language (meaning it does not have to be pre-compiled into a
binary executable). For these reasons and others, Python has a relatively easy
learning curve for new users and has already seen strong support from the QC
community.
In this chapter, we will provide an overview of how these libraries work
and provide code examples for each framework. In the upcoming chapters,
we will go into further detail with examples of algorithm implementations
using these libraries. These quantum development libraries have methods for
all the major unary and binary operators that we cover in this book. A few
offer built-in modules for ternary operators, but these can be constructed if
not offered in the library.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_6
67
68
CHAPTER 6 Development Libraries for Quantum Computer Programming
Figure 6.1: Quantum computing stack
Source: [123]
Figure 6.1 shows a schematic diagram of the quantum computing stack,
which ranges from quantum algorithms and applications at the highest level
to physical realizations of quantum computers at the lowest level. Numerous
components sit between these layers such as control, readout, and — in
the future — quantum error correction modules. Quantum programming
languages (QPL) are used to interface with the top of the stack [165]. A
quantum language consists of low-level instructions indicating which gates
to perform on which qubits. Since it would be very tedious to program
this by hand, when we program quantum computers, we interface with the
higher-level quantum programming language in a development library.
Many QPLs are now available including both functional and imperative
languages. The functional set includes: Qiskit, LIQUiji, Q# and Quipper.
Imperative languages for QC programming include: Cirq, Scaffold and Pro-
jectQ. See [114] for a helpful review of open source frameworks for quantum
computing. See [13] for an overview of the quantum computing stack.
6.1
Quantum Computers and QC Simulators
Cloud quantum computers make it possible to run algorithms on real quantum
hardware, and development libraries enable this capability. In order to test
code prior to running it on a quantum computer, most QC frameworks provide
a QC simulator which runs on a classical computer. This simulator can run
locally or in the cloud. Since it is running on a classical computer, it cannot, of
SECTION 6.1
Quantum Computers and QC Simulators
69
course, process actual quantum states, but it is helpful to test the code syntax
and ﬂow.
There are numerous techniques to classically simulate quantum circuits,
all of which suffer from the “exponential explosion” of classical memory: to
store the most general state of an n qubit system, all 2n complex numbers
of the system’s wavefunction must be stored. How much memory does this
require? For simplicity, suppose that each complex number is stored using
one byte. Then, for n D 30 qubits, 230 bytes, or a gigabyte, of memory is
required. For n D 40, a terabyte of memory is required, and for just n D 50,
a petabyte is required. These memory requirements are already reaching the
limits of today’s best supercomputers, even for a modest number of qubits.
Larger systems cannot hope to be simulated, since we wouldn’t even have
enough memory to write the wavefunction down!
The most basic method for simulating a quantum computer is to note
that a quantum circuit simply expresses a unitary transformation, U , on a
wavefunction, j i. The QC simulator algorithm then just performs matrix
multiplication to get the resulting state j 0i via
j 0i D U j i
Note that this method requires storing the entire unitary of the circuit,
which is a 2n 2n matrix, in memory (in addition to storing the wavefunction).
A method that improves on these memory requirements, only having to
store the wavefunction itself, works by applying one- and two-qubit gates to
the wavefunction individually. To apply a single-qubit gate
G D
G11
G12
G21
G22

to the ith qubit, one applies a matrix product to each amplitude ˛ whose index
differs in the ith bit [120, 260]
˛0i D G11˛0i C G12˛1i
˛1i D G21˛0i C G22˛1i
Here, the subscript i on either 0 or 1 denotes that this index is in the ith
position, and asterisks denote indices that are the same on either side of the
equation. A similar update equation exists for two-qubit gates. The algorithm
for this “state vector” or “wavefunction” simulator then consists of iterating
through all single-qubit and two-qubit gates in the circuit and applying the
appropriate update equation.
70
CHAPTER 6 Development Libraries for Quantum Computer Programming
Other types of QC simulators exist, such as Clifford circuit simulators,
which can efﬁciently simulate several hundreds or thousands of qubits. How-
ever, as discussed in chapter 3, these circuits are not universal. In this chapter,
we focus on programming universal QCs and QC simulators. When a program
is sent to a quantum backend, it is ﬁrst compiled into gates the computer can
actually implement. This compilation is expressed in a lower level language
— quantum assembly or instruction language — which is sent to the computer.
At the lowest level, gates are implemented by physical operations acting on
the qubits. These physical operations can include microwave pulses, laser
pulses or other interactions acting on a qubit, depending on which physical
realization of a qubit is used.
6.2
Cirq
Cirq Overview
Institution
Google
First Release
v0.1 on April 17, 2018
Open Source?
Yes
License
Apache-2.0
Github
https://github.com/quantumlib/Cirq
Documentation
https://cirq.readthedocs.io/en/stable/
OS
Mac, Windows, Linux
Classical Language
Python
Figure 6.2: Overview of the Cirq dev library. Modiﬁed with permission from [165].
Source:[80]
Cirq is Google’s quantum comput-
ing development library. Cirq enables
the developer to build and execute quan-
tum circuits comprised of all the usual
unary, binary and ternary operators we
have covered in this book. We will use
Cirq for the majority of code examples
throughout this book.
To get acquainted with the language,
a sample program in Cirq is provided below [80]. This program creates a
quantum circuit with one qubit and performs the NOT operator on it followed
SECTION 6.2
Cirq
71
by a measurement. This circuit is simulated several times and its measurement
outcomes are displayed to the console. The full program is shown below.1
"""Simple program in Cirq."""
# Import the Cirq package
import cirq
# Pick a qubit
qubit = cirq.GridQubit(0, 0)
# Create a circuit
circuit = cirq.Circuit(
cirq.X(qubit), # NOT.
cirq.measure(qubit, key=’m’) # Measurement
)
# Display the circuit
print("Circuit:")
print(circuit)
# Get a simulator to execute the circuit
simulator = cirq.Simulator()
# Simulate the circuit several times
result = simulator.run(circuit, repetitions=10)
# Print the results
print("Results:")
print(result)
In this circuit, we ﬁrst set up a qubit within a grid pattern. Cirq also
allows qubits to be set up in a linear fashion if needed, since both linear and
two-dimensional qubit arrays are the most popular for near-term quantum
computer architectures. We then apply the NOT operator to the qubit. If the
qubit was in a state of j0i before, it will now be in the state j1i and vice-versa.
We then measure the qubit to output the classical bit of the measurement result.
Note that the keyword argument key=’m’ in the measure operation makes it
easy to access measurement results using the histogram method of a Cirq
TrialResult. In this simple program, it is not necessary, but in more complex
programs it can be a useful tool.
The next lines of code get a QC simulator and execute the circuit ten times.
Then, the results of executing the circuit are printed to the screen. A sample
output from this program is shown below.
1To ﬁnd versions of each program shown in this chapter for the latest releases of each
development library, see the book’s GitHub page.
72
CHAPTER 6 Development Libraries for Quantum Computer Programming
Figure 6.3: The Bloch sphere
Source: [122]
Circuit:
(0, 0): ---X---M(’m’)---
Results:
m=1111111111
The circuit is printed out in ASCII text, as is standard for drawing circuits
in Cirq, and the results are indicated by a sequence of binary digits. As we
anticipate, on a noiseless QC simulator all measurement outcomes are equal
to 1. Finally, note that the string of measurement outcomes is labeled by the
measurement key m provided as a keyword argument to the measure operation.
We can represent the application of the X operator in this circuit on a
Bloch sphere (see Figure 6.3). We recall that the Bloch sphere represents the
computational basis states of j0i and j1i at the poles. By applying the X gate
to the prepared state of j0i we move the qubit to state j1i. If we then apply a
Hadamard gate, we can represent the new qubit state with a horizontal vector
on the Bloch sphere corresponding to:
j0i   j1i
p
2
For circuits of many operators we can use dynamic Bloch sphere simulation
software from Q-Ctrl (and other sites listed on the companion website) to
visualize the unitary transformations.
6.3
Qiskit
The Quantum Information Science Kit, or Qiskit for short, is a quantum
computing dev library created by IBM. The Qiskit library is a ﬂexible frame-
SECTION 6.3
Qiskit
73
Qiskit Overview
Institution
IBM
First Release
0.1 on March 7, 2017
Open Source?
Yes
License
Apache-2.0
Homepage
https://qiskit.org/
Github
https://github.com/Qiskit
Documentation
https://qiskit.org/documentation/
OS
Mac, Windows, Linux
Classical Host Language
Python
Quantum Language
OpenQASM
Figure 6.4: Overview of the Qiskit development library. Modiﬁed with permission from [165].
work for programming quantum computers. The Qiskit development library
consists of four core modules distributed across the quantum computing stack:
 Qiskit Terra: Terra provides core elements for composing quantum
programs at the level of circuits and pulses, and optimizing them for
the constraints of a particular physical quantum processor.
 Qiskit Aer: Aer provides a C++ simulator framework and tools for
constructing noise models for performing realistic noisy simulations of
the errors that occur during execution on real devices.
 Qiskit Ignis: Ignis is a framework for understanding and mitigating
noise in quantum circuits and devices.
 Qiskit Aqua: Aqua contains a library of cross-domain quantum algo-
rithms upon which applications for near-term quantum computing can
be built.
Except for Aqua, at the time of writing, each of these components is in-
stalled automatically with Qiskit. The Aqua module can be installed separately
and requires a working installation of the core Qiskit library. To demonstrate
the coding syntax in Qiskit, we include below the same example program that
was shown previously in Cirq.
"""Simple program in Qiskit."""
# Import the Qiskit package
import qiskit
# Create a quantum register with one qubit
qreg = qiskit.QuantumRegister(1, name=’qreg’)
# Create a classical register with one qubit
creg = qiskit.ClassicalRegister(1, name=’creg’)
74
CHAPTER 6 Development Libraries for Quantum Computer Programming
Figure 6.5: Circuit diagram drawn in Qiskit for the above program
# Create a quantum circuit with the above registers
circ = qiskit.QuantumCircuit(qreg, creg)
# Add a NOT operation on the qubit
circ.x(qreg[0])
# Add a measurement on the qubit
circ.measure(qreg, creg)
# Print the circuit
print(circ.draw())
# Get a backend to run on
backend = qiskit.BasicAer.get_backend("qasm_simulator")
# Execute the circuit on the backend and get the measurement results
job = qiskit.execute(circ, backend, shots=10)
result = job.result()
# Print the measurement results
print(result.get_counts())
This program in Qiskit closely mirrors that in Cirq, with minor differences
due to language design, syntax and notation. After importing the Qiskit
development library, the program declares a quantum and classical register
with one qubit, which it then uses to create a circuit. Note how this is different
from Cirq, in which (1) a classical register is never explicitly created and (2)
qubits are only referred to in a circuit when operations are added. Continuing
through the Qiskit program, the next lines add the appropriate operations (NOT
and measure) to the circuit, which is subsequently printed out by drawing the
circuit.
Qiskit has the ability to draw and save circuits as ﬁles in addition to printing
out text representations. Figure 6.5 shows the circuit for this program drawn
with the code shown below:
circ.draw(filename="qiskit-circuit", output="latex")
After printing the circuit, a backend is declared for executing the quantum
circuit. In the ﬁnal lines, the circuit is executed, the results are retrieved,
and ﬁnally the measurement statistics (counts) are printed to the screen. An
SECTION 6.4
Forest
75
example output of this program is shown below. Note that the circuit will
also be printed to the console in the above program — we omit the text
representation here.
{’1’: 10}
In Qiskit, measurement outcomes are stored as dictionaries (a Python data
type consisting of key-value pairs) where keys are bit strings and values are
the number of times each bit string was measured. Here, this output says that
the only bit string present in the measurement is 1. Similar to the Cirq output,
since we are running this program on a QC simulator without activating a
noise model, all measurement outcomes are 1 as expected.
6.4
Forest
Forest Overview
Institution
Rigetti
First Release
v0.0.2 on Jan 15, 2017
Open Source?
Yes
License
Apache-2.0
Homepage
https://www.rigetti.com/forest
GitHub
https://github.com/rigetti/pyquil
Documentation
pyquil.readthedocs.io/en/latest/
OS
Mac, Windows, Linux
Classical Language
Python
Quantum Programming Library
pyQuil
Quantum Language
Quil
Figure 6.6: Overview of the Forest development library. Modiﬁed with permission from [165].
Forest is a development library by Rigetti. Similar to the previous two
libraries, Forest is Python-based and features a collection of tools for effective
quantum programming. Users type quantum programs in pyQuil, and low-
level instructions are sent to the quantum computer as Quil, short for Quantum
Instruction Language [261]. The name Forest refers to the collection of
all programming tools in this toolchain including Quil, pyQuil and other
components such as Grove, a collection of quantum algorithms written in
pyQuil.
To get a sense for the language we implement the “NOT and measure”
program in pyQuil below.
76
CHAPTER 6 Development Libraries for Quantum Computer Programming
"""Simple program in pyQuil."""
# Import the pyQuil library
import pyquil
# Create a quantum program
prog = pyquil.Program()
# Declare a classical register
creg = prog.declare("ro", memory_type="BIT", memory_size=1)
# Add a NOT operation and measurement on a qubit
prog += [
pyquil.gates.X(0),
pyquil.gates.MEASURE(0, creg[0])
]
# Print the program
print("Program:")
print(prog)
# Get a quantum computer to run on
computer = pyquil.get_qc("1q-qvm")
# Simulate the program many times
prog.wrap_in_numshots_loop(10)
# Execute the program on the computer. NOTE: This requires the QVM
to be running
result = computer.run(prog)
# Print the results
print(result)
Here, we ﬁrst import the pyQuil library and then create a program, the
equivalent of a circuit in Cirq or Qiskit. After creating the program, we declare
a classical register of one bit, then add the operations for this circuit — NOT
and measure. Note that qubits can be indexed dynamically in pyQuil; there is
no explicit reference to a qubit register, but rather we provide an index (0) to
the gate operations. In contrast, classical memory does have to be explicitly
declared, and so we measure the qubit into a classical register.
After printing out the program to visualize its instructions, we send the
instructions to a QC for execution. Here, the string key designates that we
want a one qubit (1q) quantum virtual machine (qvm), which is Rigetti’s
terminology for their quantum computer simulator. To simulate the program
many times, we call the method wrap_in_numshots_loop on the program,
and provide as input a number of repetitions (shots). Finally, we run the
program on the speciﬁed computer and print the result. To execute this
SECTION 6.5
Quantum Development Kit
77
program on the quantum virtual machine, we can initiate the QVM with the
following command in a terminal:
qvm -S
Once executed in the QVM, the program will produce and the following
results:
Program:
DECLARE ro BIT[1]
X 0
MEASURE 0 ro[0]
Result:
[[1]
[1]
[1]
[1]
[1]
[1]
[1]
[1]
[1]
[1]]
Unlike Cirq and Qiskit, pyQuil does not produce a circuit diagram of the
program, but rather displays the Quil instructions line by line. In this particular
Quil program, a readout register called ro is declared, a NOT operation is
applied to qubit 0, and the measurement outcome of qubit 0 is stored into the
readout register. The results of simulating the circuit appears as a list of lists.
Each inner list is the measurement outcome for a particular execution of the
circuit. The number of inner lists is equal to the total number of times the
circuit was executed. All of these are wrapped in an outer list containing the
results of all simulations. As with the previous programs, all measurement
outcomes are 1 on a noiseless quantum computer simulator, as we expect.
6.5
Quantum Development Kit
The Quantum Development Kit (QDK) is a quantum computing development
library by Microsoft. Unlike the previous languages, which were based in
Python, the QDK contains its own language, called Q# (pronounced “Q
sharp”), for writing quantum programs.
The Q# language is different in several respects compared with Python-
based libraries such as Cirq, Qiskit and pyQuil. In Q# we need to explicitly
78
CHAPTER 6 Development Libraries for Quantum Computer Programming
QDK Overview
Institution
Microsoft
First Release
0.1.1712.901 on Jan 4, 2018
Open Source?
Yes
License
MIT
Homepage
microsoft.com/en-us/quantum/development-kit
Github
https://github.com/Microsoft/Quantum
Documentation
docs.microsoft.com/en-us/quantum/?view=qsharp-preview
OS
Mac, Windows, Linux
Quantum Prog. Lang.
Q#
Figure 6.7: Overview of the QDK development library. Modiﬁed with permission from [165].
declare types, as well as use curly braces instead of indents as in Python.
Additionally, three separate ﬁles are required to execute programs using the
QDK:
1. A ﬁle ending in .qs where quantum operations (analogues of functions
in Python) are stored
2. A driver ﬁle ending in .cs where quantum operations are executed in
the main program
3. A ﬁle ending in .csproj which deﬁnes the project and contains metadata
about computer architecture and package references
An example program in Q# that executes the same “NOT and measure”
circuit we have seen in other languages is included below. We ﬁrst show the
.qs ﬁle which deﬁnes the quantum operations we will use in the driver ﬁle.
namespace Quantum.Simple
{
// Importing the libraries
open Microsoft.Quantum.Primitive;
open Microsoft.Quantum.Canon;
// Sets a qubit in a desired state
operation Set(desired_state: Result, qubit: Qubit) : Unit {
let current = M(qubit);
if (current != desired_state) {
X(qubit);
}
}
// Executes the NOTandMeasure circuit for an input number
// of repetitions and returns the number of ones measured
operation NotAndMeasure(repetitions: Int) : Int {
// Variable to store the number of measured ones
mutable num_ones = 0;
SECTION 6.5
Quantum Development Kit
79
// Get a qubit to use
using (qubit = Qubit()) {
// Loop over the desired number of repetitions
for (test in 1..repetitions) {
// Get a qubit in the zero state
Set(Zero, qubit);
// Perform a NOT operation
X(qubit);
// Measure the qubit
let res = M (qubit);
// Keep track of the number of ones we measured
if (res == One) {
set num_ones = num_ones + 1;
}
}
// "Released qubits" must be in the zero state to avoid a
System.AggregateException
Set(Zero, qubit);
}
// Return the number of ones measured
return num_ones;
}
}
In the ﬁrst line we deﬁne a namespace for the operations. This namespace
is used in the driver ﬁle to access these operations. The next two lines are the
equivalent of import packages in Python — they make operations deﬁned in
the QDK available for us to use in the program (e.g., the X gate). Next, we
declare an operation called Set which inputs a desired computational basis
state and an arbitrary qubit state; it then changes the state of the qubit to
match the desired state. This is accomplished by measuring the qubit in the
computational basis and then performing a NOT operation if necessary.
We then deﬁne the NotAndMeasure operation which sets up the quantum
circuit and measures the qubit for a user-speciﬁed number of times. Explicitly,
the operation sets a qubit to be in the 0 state using the previous operation
we deﬁned, performs a NOT operation, then measures, keeping track of the
number of 1s measured. After the total number of repetitions, the number of
1s measured is returned.
In order to use this ﬁle, we must deﬁne a separate driver ﬁle ending in .cs
which executes these operations. The driver ﬁle used to execute this program
is shown below.
using System;
using Microsoft.Quantum.Simulation.Core;
using Microsoft.Quantum.Simulation.Simulators;
80
CHAPTER 6 Development Libraries for Quantum Computer Programming
namespace Quantum.Simple {
class Driver {
static void Main() {
// Get a quantum computer simulator
using (var qsim = new QuantumSimulator()) {
// Run the operation NotAndMeasure and get the result
var num_ones = NotAndMeasure.Run(qsim, 10).Result;
// Print the measurement outcome to the console
System.Console.WriteLine(
$"Number of ones measured: {num_ones, 0}.");
}
}
}
}
Here, we import System, which allows us to print to the console, and
the necessary tools to use the QC simulator in the QDK. Next, we invoke
the namespace declared in the .qs ﬁle above and deﬁne a Main function
inside the driver class. This Main function uses a QC simulator to run the
NotAndMeasure operation, then prints the results to the console. The output
of this program is shown below.
Number of ones measured: 10.
As in other languages, we obtain the correct output of measuring all ones.
The ﬁnal ﬁle .csproj needed to execute this code is included on this book’s
GitHub site. We omit the program here since it is nearly identical for each
project in the QDK.
6.6
Dev Libraries Summary
In the sample programs shown for these libraries, we have seen that most
frameworks are fairly similar in terms of constructing quantum circuits. The
general recipe for each of them is:
1. Build a quantum circuit consisting of quantum and/or classical registers
2. Add operations to the circuit
3. Simulate the circuit
However, there are some differences across the various libraries. For
example, qubits are deﬁned separately from a quantum circuit in Cirq, whereas
qubits are required as input to a quantum circuit in Qiskit. Similarly, all qubits
must be allocated in a quantum register before performing operations on them
SECTION 6.6
Dev Libraries Summary
81
in Qiskit, but qubits can be allocated dynamically when programming in
pyQuil.
While these differences may seem small, there are bigger differences in
the features each development library contains. For example, some libraries
include the ability to simulate noise, compile algorithms to arbitrary architec-
tures, access the wavefunction for debugging and so on. These differences
lead to certain algorithms being easier to implement in particular libraries.
While one development library is usually sufﬁcient for most tasks, having
experience in multiple libraries is helpful to choose the the appropriate library
for certain cases.
Using the Libraries
In the remainder of the book, we will use these development libraries to
explore the core algorithms that make up the canon of quantum computing
and more recent QC methods. While Cirq is the main library used, some
programs are shown in other libraries for comparison. Before continuing on
to this core part of the text, we brieﬂy mention other development libraries
that we have not yet covered.
Other Development Libraries
As mentioned, before the emergence of cloud quantum computing, many
quantum computer simulators were developed in languages ranging from C++
(e.g., Quantum++) to Java (e.g., jSQ) to Rust (e.g., QCGPU). A full list of
quantum computer simulators is available at https://quantiki.org/wiki/list-qc-
simulators. While many of these simulators are outdated or deprecated, a
number of them are still actively developed and being improved upon. Indeed,
while classical algorithms for simulating quantum circuits are widely believed
to be intractable, ﬁnding new ways to simulate more qubits with greater speed
is still an active area of research.
In addition to the libraries above, here are other libraries of interest: Pro-
jectQ, a Python library with a high-performance C++ quantum computer
simulator; Strawberry Fields, a Python library built around continuous vari-
able quantum computing; and Ocean, a Python library for quantum annealing
on D-Wave quantum computers. Additionally, a hardware independent quan-
tum programming framework called XACC is also actively being developed,
as is a library for simulating quantum circuits on Tensor Processing Units
(TPUs) called Floq [297]. Libraries for pulse-level control of superconducting
qubit computers have recently emerged, for example Qiskit Pulse and Quil-T,
82
CHAPTER 6 Development Libraries for Quantum Computer Programming
and a library for designing superconducting qubits called Qiskit Metal has
recently been introduced.
Several domain-speciﬁc libraries have also recently emerged. For example,
Google’s open-source quantum computing software includes OpenFermion, a
library for quantum chemistry; qsim, a high-performance classical simulator;
and TensorFlow-Quantum, a library for quantum machine learning. Other
researchers have developed additional quantum machine learning libraries,
for example Pennylane by the company Xanadu and Qiskit Aqua by IBM.
Finally, we mention a few notable cloud-level access providers (i.e., institu-
tions which allow users to execute quantum circuits on real hardware over the
cloud). IBM Quantum has several superconducting qubit computers available
for the public to use, with additional privileges given to researchers in the ﬁeld.
Amazon Web Services (AWS) allows users to execute circuits on trapped ion
and superconducting quantum computers as well as quantum annealers in
a paid service. Microsoft’s Azure Quantum provides access to trapped ion
quantum computers in a similar model. We expect current cloud-level ac-
cess providers to expand the types of quantum computing architectures they
support in the coming years. We also expect additional providers to emerge.
For a full list of open-source quantum software projects, see the companion
website to this book.
6.7
Additional Quantum Programs
In principle, knowing how to program circuits, simulate them and access
measurement results are the core skills needed for coding quantum algorithms
in future chapters. While the simple “NOT and measure” program contains
these three key features, it is nonetheless too rudimentary to prepare us for
further algo work. To help bridge this gap to the more complex programs to
come, we include two additional example quantum programs in this section.
Bell States
One common circuit pattern is the set of operators we use to prepare one of
the four Bell states
jˆCi D
1
p
2
Œj00i C j11i
from the ground state. It can be easily veriﬁed that such a circuit consists of a
Hadamard on the ﬁrst qubit and then a CNOT between the two qubits, where
the ﬁrst qubit is the control qubit. This structure appears, for example, in the
SECTION 6.7
Additional Quantum Programs
83
quantum teleportation circuit for creating entangled pairs of qubits between
two parties.
A circuit for preparing a Bell state, written in Cirq, is shown below. For
pedagogical purposes, we add measurements after the Bell state preparation
circuit to see the possible measurement outcomes.
"""Script for preparing the Bell state |\Phi^{+}> in Cirq."""
# Import the Cirq library
import cirq
# Get qubits and circuit
qreg = [cirq.LineQubit(x) for x in range(2)]
circ = cirq.Circuit()
# Add the Bell state preparation circuit
circ.append([cirq.H(qreg[0]),
cirq.CNOT(qreg[0], qreg[1])])
# Display the circuit
print("Circuit")
print(circ)
# Add measurements
circ.append(cirq.measure(*qreg, key="z"))
# Simulate the circuit
sim = cirq.Simulator()
res = sim.run(circ, repetitions=100)
# Display the outcomes
print("\nMeasurements:")
print(res.histogram(key="z"))
Note that in this Cirq program, we utilize the measurement key to access
measurement outcomes using the histogram method. An example output of
this program is shown below:
Circuit
0: ---H---@---
|
1: -------X---
Measurements:
Counter({3: 53, 0: 47})
Here, the circuit drawing shows the Bell state preparation circuit consisting
of a Hadamard and CNOT gate. After, the measurement outcomes enumerate
how many 0 and 3 states were measured. Note that this is a binary represen-
tation of bitstrings — i.e., 0 stands for 00 and 3 stands for 11. As expected,
these are the only two measurement outcomes in the program, meaning that
84
CHAPTER 6 Development Libraries for Quantum Computer Programming
the measurements are perfectly correlated: when one qubit is measured 0.1/,
the other qubit is always measured 0.1/.
Gates with Parameters
Several quantum gates are deﬁned in terms of angles, for example the stan-
dard rotation gates Rx./, Ry./ and Rz./. In many quantum algorithms,
known as variational quantum algorithms, these angles, or parameters, are
iteratively adjusted in order to minimize cost. For example, in the variational
quantum eigensolver, gate parameters are adjusted in order to minimize the
expectation of a Hamiltonian h ./jHj ./i. Regardless of the application,
such variational quantum algorithms depend critically on the ability to update
and change gate parameters.
Because of this, most quantum computing development libraries contain
built-in features and methods for working with parameterized gates. In the
following program, we demonstrate this functionality in Cirq. In particular,
we set up a simple quantum circuit with one parameterized gate, execute the
algorithm for a sweep of parameters and plot the measurement results.
"""Working with parameterized gates in Cirq."""
# Imports
import matplotlib.pyplot as plt
import sympy
import cirq
# Get a qubit and a circuit
qbit = cirq.LineQubit(0)
circ = cirq.Circuit()
# Get a symbol
symbol = sympy.Symbol("t")
# Add a parameterized gate
circ.append(cirq.XPowGate(exponent=symbol)(qbit))
# Measure
circ.append(cirq.measure(qbit, key="z"))
# Display the circuit
print("Circuit:")
print(circ)
# Get a sweep over parameter values
sweep = cirq.Linspace(key=symbol.name, start=0.0, stop=2.0,
length=100)
# Execute the circuit for all values in the sweep
sim = cirq.Simulator()
SECTION 6.7
Additional Quantum Programs
85
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Exponent of X gate
0.0
0.2
0.4
0.6
0.8
1.0
Frequency of 0 Measurements
Figure 6.8: Measurement outcomes at each value of the exponent t 2 Œ0; 2 in the circuit Xtj0i
res = sim.run_sweep(circ, sweep, repetitions=1000)
# Plot the measurement outcomes at each value in the sweep
angles = [x[0][1] for x in sweep.param_tuples()]
zeroes = [res[i].histogram(key="z")[0] / 1000 for i in
range(len(res))]
plt.plot(angles, zeroes, "--", linewidth=3)
# Plot options and formatting
plt.ylabel("Frequency of 0 Measurements")
plt.xlabel("Exponent of X gate")
plt.grid()
plt.savefig("param-sweep-cirq.pdf", format="pdf")
We highlight several key components in this program. First, a symbol
in Cirq is used to represent a numerical value of a parameter that will be
determined later, before executing the circuit. When the circuit is printed (see
below), the name of the symbol appears in the circuit.
Circuit:
0: ---X^t---M(’z’)---
Next, a sweep is a set of values that the symbol will take on. Simulators
in Cirq have a method, called run_sweep, for executing circuits at all values
in a sweep. At each value, the circuit is simulated a number of times set by
the repetitions keyword argument. The remaining code in the program is for
plotting the outcome of the run_sweep, which is shown in Figure 6.8.
Now that we have explored the various quantum computing development
libraries, let us look at three protocols — quantum teleportation, superdense
86
CHAPTER 6 Development Libraries for Quantum Computer Programming
coding, and Bell’s inequality test — and then turn to the canon of quantum
algorithms that helped establish the ﬁeld.
CHAPTER7
Teleportation, Superdense
Coding and Bell’s Inequality
Two of the most fascinating quantum circuits enable us to transmit information
in ways that are not possible in the classical regime. In this chapter, we will
learn how to build these two circuits. We will then examine a foundational
advance in quantum mechanics, the Bell Inequality.
7.1
Quantum Teleportation
Quantum teleportation, despite its name, does not teleport any physical object.
It does transmit the state of a qubit over any given distance in a way that is
completely secure. It is remarkable that it took more than seventy years from
the formulation of QM to realize that this framework gave us a new form of
secure communication. The protocol was developed in 1993 [33] by Bennett
and Brassard et al.; it was experimentally veriﬁed in 1997 [50]. Bennett and
Brassard had also developed quantum key distribution in 1984, known as
BB84 [32].
One of the key insights of quantum teleportation is that we can treat
entangled states as a resource. We can use entangled states (known as EPR
pairs or Bell states) to perform a range of tasks that cannot be accomplished
using classical means.
In quantum teleportation, Alice is the sender and she wishes to transmit
the state of a qubit, Q, to a receiver, Bob. The algorithm requires three qubits
in total: Alice’s qubit, Bob’s qubit, and an ancilla qubit to create the EPR pair.
An ancilla qubit is a qubit we add to the circuit in order to facilitate a quantum
computation; for example, we can entangle an ancilla qubit with a primary
qubit in order to keep track of a certain state.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_7
87
88
CHAPTER 7
Teleportation, Superdense Coding and Bell’s Inequality
Figure 7.1: Quantum teleportation diagram. Relative to the notation in the main text, jˆi is
the state of qubit Q, A D R and B D S.
Source: Wikimedia
Let’s walk through the protocol in detail:
1. We set up the system with three qubits:
(a) Alice has a qubit, Q, with state jˆi. Alice wishes to transmit the
state jˆi to Bob in a secure manner.
(b) To accomplish this goal, Alice also starts with two additional
qubits, which we will label R and S. One of these qubits, say S,
will be sent to Bob, and the other will stay with Alice. In practice,
S can be sent over a quantum channel such as an optical ﬁber if
the qubits are photons.
2. Alice prepares a Bell state with qubits R and S. This is done by applying
a Hadamard to qubit R and then a CNOT between R and S, controlling
on R. At this point, Alice sends qubit S to Bob.
3. Alice now performs a Bell measurement on her original qubit Q and her
half of the EPR pair, R. This is done by performing a CNOT between
the qubits, controlling on Q, then performing a Hadamard gate on Q,
and ﬁnally measuring both qubits in the computational basis.
4. After measuring, Alice now has two bits of classical information, one
from each measured qubit. Alice now transmits these bits to Bob over
SECTION 7.1
Quantum Teleportation
89
Figure 7.2: Circuit diagram for quantum teleportation
Source: Wikimedia
a classical communication channel. Note that there are four possible
outcomes from her measurements: 00, 01, 10, and 11.
5. Depending on which bit string Bob receives from Alice, he performs
a set of operations on his qubit, S. The dictionary of operations for
each measurement result is listed below. Performing the appropriate
operation guarantees Bob’s qubit, S, will be in the same state as Alice’s
original qubit Q — even though neither Alice nor Bob know what this
state is!
If Alice transmits
then Bob applies this operator
00
None — Bob’s qubit is in the right state
01
Z
10
X
11
XZ (apply Z ﬁrst then X)
Two ways to represent quantum teleportation in a circuit diagram can be
seen in ﬁgure 7.2 and below; they are equivalent and it is useful to be familiar
with different ways of presenting circuits:
Q W
jˆi

H

R W
j0i
H

S W
j0i
X
Z
jˆi
Let us highlight the following points on teleportation:
1. Note how Alice prepares a Bell state with an H on R and then a CNOT
across the pair of qubits, R and S.
90
CHAPTER 7
Teleportation, Superdense Coding and Bell’s Inequality
2. Note how Alice transmits two bits of classical information to Bob —
these are denoted with the sets of double wires in the quantum circuit.
3. In this circuit Alice successfully transmits the state of jˆi to Bob via
an EPR pair and the use of two classical bits. We can also represent this
transmission as
Œqq C Œcc  Œq
where Œqq represents an EPR pair, Œcc represents a pair of classical
bits and Œq is the state of a qubit we wish to transmit.
While quantum teleportation is a tool we can use in quantum communica-
tions, it also has applications in quantum computing [77]. We can potentially
use quantum teleportation to create a modular architecture for quantum com-
puting by sending a quantum state from one module to another in a scaled
quantum computer (see [77]).
7.2
Superdense Coding
Superdense coding is a method to transmit classical bits by sending only
one qubit from sender to receiver. If Alice wishes to transmit two classical
bits to Bob using a classical channel, she would have to use two bits. With
superdense coding, however, she can communicate the two bits with the
transmission of just one qubit.
This protocol was initially developed by Bennett and Wiesner [34] and
then further speciﬁed as a secure communications protocol [287]. Anton
Zeilinger experimentally demonstrated superdense coding transmission in
1995 [187].
To achieve superdense coding, Alice ﬁrst prepares an EPR pair. She then
performs one of four operations on her half of the pair. Let’s say that these are
a pair of photons. To create the Bell pair, Alice ﬁrst applies a Hadamard to
her photon and then a CNOT across the two photons as she did in the case of
preparing an EPR pair for quantum teleportation protocol. The pair of photons
is now entangled.
Now Alice chooses which of four classical states she wishes to transmit to
Bob as the intended message. Depending on the message she chooses to send,
Alice applies a speciﬁc quantum operator to her photon.
SECTION 7.3
Code for Quantum Teleportation and Superdense Communication
91
Figure 7.3: Circuit diagram for superdense coding
Source: Wikimedia
If Alice wants to send
Alice applies
00
I(identity operator)
01
X
10
Z
11
ZX (ﬁrst apply X, then apply Z)
Next she sends her photon to Bob via a quantum communications channel
that preserves entanglement. Upon receipt of the photon, Bob applies the
Hadamard to her photon and then a CNOT across the photon pair. He then
performs a measurement. The result will be two classical bits of information.
Let us recall that the output of a measurement is classical information.
We can represent superdense coding in shorthand as follows:
Œq C Œqq  Œcc
7.3
Code for Quantum Teleportation and
Superdense Communication
A program for quantum teleportation is provided in Cirq below. This program
encodes a random quantum state in Alice’s qubit and prints out its Bloch
sphere .x; y; z/ components. It then executes the quantum teleportation
circuit and prints out the Bloch sphere .x; y; z/ components of Bob’s qubit.
92
CHAPTER 7
Teleportation, Superdense Coding and Bell’s Inequality
"""Quantum teleportation in Cirq. Modified from
quantum_teleportation.py example at:
https://github.com/quantumlib/Cirq/tree/master/examples
"""
# Imports
import random
import cirq
def make_quantum_teleportation_circuit(ranX, ranY):
"""Returns a quantum teleportation circuit."""
circuit = cirq.Circuit()
msg, alice, bob = cirq.LineQubit.range(3)
# Creates Bell state to be shared between Alice and Bob
circuit.append([cirq.H(alice), cirq.CNOT(alice, bob)])
# Creates a random state for the Message
circuit.append([cirq.X(msg)**ranX, cirq.Y(msg)**ranY])
# Bell measurement of the Message and Alice’s entangled qubit
circuit.append([cirq.CNOT(msg, alice), cirq.H(msg)])
circuit.append(cirq.measure(msg, alice))
# Uses the two classical bits from the Bell measurement to
recover the
# original quantum Message on Bob’s entangled qubit
circuit.append([cirq.CNOT(alice, bob), cirq.CZ(msg, bob)])
return msg, circuit
def main():
# Encode a random state to teleport
ranX = random.random()
ranY = random.random()
msg, circuit = make_quantum_teleportation_circuit(ranX, ranY)
# Simulate the circuit
sim = cirq.Simulator()
message = sim.simulate(cirq.Circuit.from_ops(
[cirq.X(msg)**ranX, cirq.Y(msg)**ranY]))
# Print the Bloch Sphere of Alice’s qubit
print("Bloch Sphere of Alice’s qubit:")
b0X, b0Y, b0Z = cirq.bloch_vector_from_state_vector(
message.final_state_vector, 0)
print("x: ", round(b0X, 4),
"y: ", round(b0Y, 4),
"z: ", round(b0Z, 4))
# Display the teleportation circuit
print("\nCircuit:")
print(circuit)
SECTION 7.3
Code for Quantum Teleportation and Superdense Communication
93
# Record the final state of the simulation
final_results = sim.simulate(circuit)
# Print the Bloch sphere of Bob’s qubit
print("\nBloch Sphere of Bob’s qubit:")
b2X, b2Y, b2Z = cirq.bloch_vector_from_state_vector(
final_results.final_state_vector, 2)
print("x: ", round(b2X, 4),
"y: ", round(b2Y, 4),
"z: ", round(b2Z, 4))
if __name__ == ’__main__’:
main()
An example output of this program is shown below.
Bloch sphere of Alice’s qubit:
x: 0.654 y: -0.6177 z: -0.4367
Bloch sphere of Bob’s qubit:
x: 0.654 y: -0.6177 z: -0.4367
As can be seen, the Bloch sphere components of Alice and Bob’s qubit are
identical – in other words, the qubit has been “teleported” from Alice to Bob.
A program for superdense coding written in Cirq is provided below.
"""Superdense coding in Cirq."""
# Imports
import cirq
# Helper function for visualizing output
def bitstring(bits):
return ’’.join(’1’ if e else ’0’ for e in bits)
# Create two quantum and classical registers
qreg = [cirq.LineQubit(x) for x in range(2)]
circ = cirq.Circuit()
# Dictionary of operations for each message
message = {"00": [],
"01": [cirq.X(qreg[0])],
"10": [cirq.Z(qreg[0])],
"11": [cirq.X(qreg[0]), cirq.Z(qreg[0])]}
# Alice creates a Bell pair
circ.append(cirq.H(qreg[0]))
circ.append(cirq.CNOT(qreg[0], qreg[1]))
# Alice picks a message to send
m = "01"
print("Alice’s sent message =", m)
94
CHAPTER 7
Teleportation, Superdense Coding and Bell’s Inequality
# Alice encodes her message with the appropriate quantum operations
circ.append(message[m])
# Bob measures in the Bell basis
circ.append(cirq.CNOT(qreg[0], qreg[1]))
circ.append(cirq.H(qreg[0]))
circ.append([cirq.measure(qreg[0]), cirq.measure(qreg[1])])
# Print out the circuit
print("\nCircuit:")
print(circ)
# Run the quantum circuit on a simulator backend
sim = cirq.Simulator()
res = sim.run(circ, repetitions=1)
# Print out Bob’s received message: the outcome of the circuit
print("\nBob’s received message =",
bitstring(res.measurements.values()))
An example output of this program for the message 01 is shown below.
Alice’s sent message = 01
Circuit:
0: ---H---@---X---@---H---M---
|
|
1: -------X-------X-------M---
Bob’s received message = 01
As can be seen, Bob’s received message is exactly Alice’s sent message
via superdense coding.
7.4
Bell Inequality Test
Now let’s turn to another code walk-through: the Bell inequality test. After
brieﬂy describing the experiment, we will go through a complete program in
Cirq to simulate it.
The Bell inequality test is best understood through a cooperative game
involving two players, Alice and Bob, that make decisions based on input from
a referee. Alice and Bob are separated (sitting in different rooms, say) and
cannot communicate during the game. At each round of the game, the referee
sends one bit to Alice, call it x, and one bit to Bob, call it y. Depending on
the value of the bit, Alice sends a bit of her own, a.x/, back to the referee.
Similarly, Bob sends a bit of his own, b.y/, back to the referee. The referee
SECTION 7.4
Bell Inequality Test
95
looks at both bits and decides if Alice and Bob win or lose that round. The
condition for winning the round is
a.x/ ˚ b.y/ D xy
where ˚ denotes addition modulo-2 (or, equivalently, XOR).
Alice and Bob’s goal is to win as many rounds as possible. Although they
cannot communicate during the game, they are allowed to meet before the
game and set up a strategy. An example strategy might be “Alice always sends
back a.x/ D x, and Bob always sends back a.y/ D 0.” Since each of a.x/
and b.y/ can have two possible values, there are four possible deterministic
strategies that Alice and Bob can implement. Additionally, since there are
only four bits involved in the entire game, it’s not difﬁcult to enumerate all
possible outcomes and see which strategy allows Alice and Bob to win the
most rounds (or, equivalently, win each round with the highest probability).
x
y
a.x/
b.y/
a.x/ ˚ b.y/
xy
Win?
Strategy #
0
0
0
0
0
0
Yes
#1
0
0
0
1
1
0
No
#2
0
0
1
0
1
0
No
#3
0
0
1
1
0
0
Yes
#4
0
1
0
0
0
0
Yes
#1
0
1
0
1
1
0
No
#2
0
1
1
0
1
0
No
#3
0
1
1
1
0
0
Yes
#4
1
0
0
0
0
0
Yes
#1
1
0
0
1
1
0
No
#2
1
0
1
0
1
0
No
#3
1
0
1
1
0
0
Yes
#4
1
1
0
0
0
1
No
#1
1
1
0
1
1
1
Yes
#2
1
1
1
0
1
1
Yes
#3
1
1
1
1
0
1
No
#4
Table 7.1: All possible outcomes of the Bell inequality test game: the ﬁrst two columns show
bits the referee sends to Alice (x) and Bob (y). The next two columns show Alice’s response
(a.x/) and Bob’s response (b.y/. The next column computes a.x/˚b.y/, and the next column
shows the product xy. If these are equal, Alice and Bob win. The ﬁnal column shows which
strategy Alice and Bob are using in each row, numbered #1 - #4.
Table 7.1 enumerates all possible outcomes of the game. By analyzing
each strategy, one can see that Alice and Bob win at most 75% of the time. The
strategies that achieve this win percentage are #1, in which a.x/ D b.y/ D 0,
96
CHAPTER 7
Teleportation, Superdense Coding and Bell’s Inequality
and #4 in which a.x/ D b.y/ D 1. So the best that Alice and Bob can do is
to agree on either strategy #1 or strategy #4 before the game begins to achieve
the the best possible classical win rate of 75%.
An interesting phenomena happens when we allow a quantum strategy
between Alice and Bob. By a quantum strategy, we mean Alice and Bob are
allowed to use entanglement as a resource in their strategy. As we have seen
in this book, entanglement allows for stronger than classical correlations in
physical systems. If Alice and Bob are allowed to share entangled qubits, they
can remarkably win the Bell inequality test game with a higher probability!
The best quantum strategy achieves a winning probability of cos2.=8/, or
about 85%.
The quantum strategy for this game is shown in the circuit diagram below.
Here, the top (ﬁrst) qubit belongs to Alice, and the third qubit from the top
belongs to Bob. The ﬁrst part of the circuit creates entanglement between
Alice and Bob’s qubits. Then, the referee “sends” in a random bit each to
Alice and Bob. In the circuit, this is done by performing a Hadamard operation
on a “fresh qubit” (one in the j0i state) to produce equal superposition. Alice
and Bob then perform a control-
p
X operation on their qubits and measure to
record the results.
H

X  0:25
X 0:5
H

X 0:5
H

Below, we show the complete program in Cirq for setting up this circuit
and simulating the quantum strategy for the Bell inequality test.
"""Creates and simulates a circuit equivalent to a Bell inequality
test."""
# Imports
import numpy as np
import cirq
def main():
# Create circuit
circuit = make_bell_test_circuit()
print(’Circuit:’)
SECTION 7.4
Bell Inequality Test
97
print(circuit)
# Run simulations
print()
repetitions = 1000
print(’Simulating {} repetitions...’.format(repetitions))
result = cirq.Simulator().run(program=circuit,
repetitions=repetitions)
# Collect results
a = np.array(result.measurements[’a’][:, 0])
b = np.array(result.measurements[’b’][:, 0])
x = np.array(result.measurements[’x’][:, 0])
y = np.array(result.measurements[’y’][:, 0])
# Compute the winning percentage
outcomes = a ^ b == x & y
win_percent = len([e for e in outcomes if e]) * 100 / repetitions
# Print data
print()
print(’Results’)
print(’a:’, bitstring(a))
print(’b:’, bitstring(b))
print(’x:’, bitstring(x))
print(’y:’, bitstring(y))
print(’(a XOR b) == (x AND y):\n ’, bitstring(outcomes))
print(’Win rate: {}%’.format(win_percent))
def make_bell_test_circuit():
# Qubits for Alice, Bob, and referees
alice = cirq.GridQubit(0, 0)
bob = cirq.GridQubit(1, 0)
alice_referee = cirq.GridQubit(0, 1)
bob_referee = cirq.GridQubit(1, 1)
circuit = cirq.Circuit()
# Prepare shared entangled state between Alice and Bob
circuit.append([
cirq.H(alice),
cirq.CNOT(alice, bob),
cirq.X(alice)**-0.25,
])
# Referees flip coins
circuit.append([
cirq.H(alice_referee),
cirq.H(bob_referee),
])
# Players do a sqrt(X) based on their referee’s coin
circuit.append([
cirq.CNOT(alice_referee, alice)**0.5,
cirq.CNOT(bob_referee, bob)**0.5,
])
98
CHAPTER 7
Teleportation, Superdense Coding and Bell’s Inequality
# Then results are recorded
circuit.append([
cirq.measure(alice, key=’a’),
cirq.measure(bob, key=’b’),
cirq.measure(alice_referee, key=’x’),
cirq.measure(bob_referee, key=’y’),
])
return circuit
def bitstring(bits):
return ’’.join(’1’ if e else ’_’ for e in bits)
if __name__ == ’__main__’:
main()
An example output of this program is shown below, where we simulate
75 repetitions only to visualize the output more easily. The output of this
program is interpreted as follows. The bitstrings for Alice and Bob are shown
in the ﬁrst line. These are the bits each player sends back to the referee (the
underscore indicates the 0 bit). The next lines show the bitstrings sent to Alice
(x) and Bob (y). Finally, the winning condition a.x/ ˚ b.y/ D xy is shown
as a bitstring for each round. The win rate is computed from the number of 1s
in this bitstring, which indicate a win.
Simulating 75 repetitions...
Results
a: 1_1111_1_1111_11_1111_1_1______1_1__
11_1__111_1_1_111_1111__11_1111_1_1____
b: 1_1__11_1_1_1_11__11__1_1_1_____1___
_11__11_1_1___11__1__1111_1_1___1__11_1
x: 11_11_1_1111_____1_11____11_1__111_1
1__1_11_1___11_111111111__11__1_111__11
y: _1_11111111__11_11__1__1111___1111__
1__111_1__111___1__11__1_111_11___1___1
(a XOR b) == (x AND y):
1_11111_11__1111111111111_11111111111
1_1111_11111111111111_11_111_11111__11
Win rate: 84.0%
As can be seen, we achieve a win rate of 84% in this example, which is
greater than possible with purely classical strategies. Note that using a larger
SECTION 7.4
Bell Inequality Test
99
number of repetitions (i.e., simulating more rounds) we would see asymptotic
convergence to the optimal win rate of cos2.=8/  85%.
Summary
Quantum teleportation, superdense coding and Bell’s inequality test are some
of the most intriguing circuits for quantum processors. For additional quantum
networking protocols, see the Quantum Protocol Zoo at http://wiki.
veriqloud.fr. Now let us turn to the canon of quantum algorithms that
demonstrated the potential of quantum advantage over classical computing.
CHAPTER8
The Canon: Code Walkthroughs
In this chapter, we will walk through a number of fundamental quantum
algorithms. We call these algorithms the canon as they were all developed in
the early years of quantum computing and were the ﬁrst to establish provable
computational speedups with quantum computers. We discussed most of
these algorithms at a high level in chapter 2; we will now walk through
them in a more detailed manner. A number of these algorithms require a
quantum computer that is still in the future, but by analyzing them now we
can deepen our understanding of what will be possible. Additionally, variants
of these algorithms can be used to prove advantages with near-term quantum
computers in the noiseless [58] and even noisy [59] regimes.
Several of the algorithms considered in this chapter are known as “black
box” or “query model” quantum algorithms. In these cases, there is an
underlying function which is unknown to us. However, we are able to construct
another function, called an oracle, which we can query to determine the
relationship of speciﬁc inputs with speciﬁc outputs. More speciﬁcally, we
can query the oracle function with speciﬁc inputs in the quantum register and
reversibly write the output of the oracle function into that register. That is, we
have access to an oracle Of such that
Of .jxijyi/ D jx ˚ f .y/i
(8.1)
where ˚ denotes addition modulo-2. It’s easy to see that Of is unitary
(reversible) because it is self-inverse. This can seem like “cheating” at ﬁrst —
how could we construct a circuit to perform Of ? And how could we know
if it is an efﬁcient circuit? One reason to think about quantum algorithms
in the query model is that it provides a lower bound on the number of steps
(gates). Each query is at least one step in the algorithm, so if it cannot be done
efﬁciently with queries, it can certainly not be done efﬁciently with gates.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_8
101
102
CHAPTER 8
The Canon: Code Walkthroughs
Figure 8.1: Overview of studied quantum algorithms; paradigms include: Grover Opera-
tor (GO), Quantum Fourier Transform (QFT), Harrow/Hassidim/Lloyd (HHL), Variational
Quantum Eigensolver (VQE), and direct Hamiltonian simulation (SIM). The simulation match
column indicates how well the hardware quantum results matched the simulator results.
Table and Caption Source: [84]
The query model can also be used to prove fast quantum algorithms relative
to the oracle. We can give both a quantum computer and a classical computer
access to the same oracle and see which performs better. It’s possible to prove
lower bounds or exact expressions for the number of queries in the classi-
cal and quantum cases, thereby making it possible to prove computational
advantages relative to oracles. Examples of quantum algorithms with prov-
able relative speedups include Deutsch’s algorithm and the Berstein-Vazirani
algorithm.
Finally, if one can ﬁnd a way to instantiate the oracle in a number of gates
that scales polynomially in the size of the input register, one can ﬁnd “true”
(i.e., not relative) quantum speedups. This is the case with Shor’s algorithm for
quantum factoring.1 Shor’s algorithm built off previous work in query model
algorithms for artiﬁcial problems. Shor was able to modify this work and
instantiate the oracle to construct an explicit (i.e., not involving oracle access)
algorithm for factoring. Factoring is markedly not an artiﬁcial problem — it
is exceedingly important as we base most of our public-key cryptography on
the belief that factoring is hard to do! Section 8.5 discusses this further.
Before this, we discuss the historical quantum algorithms leading up to
Shor’s algorithm. We note that these black box/oracle algorithms are one
particular class of quantum algorithms. Other algorithm classes such as
1The classical complexity of factoring has not been proven to be intractable, but it is widely
believed to be so since no one has yet discovered an efﬁcient algorithm.
SECTION 8.1
The Deutsch-Jozsa Algorithm
103
quantum simulation are shown in Figure 8.1. In upcoming chapters, we will
cover additional applications and code for algorithms that are focused on the
NISQ-regime processors. For now, though, we cover the canon.
8.1
The Deutsch-Jozsa Algorithm
Deutsch’s algorithm was the ﬁrst to demonstrate a clear advantage of quantum
over classical computing. In Deutsch’s problem we are given a black box
which computes a one-bit Boolean function. That is, a function which takes
in one bit and outputs one bit. We can represent the function f as
f W f0; 1g ! f0; 1g
(8.2)
We can imagine, for example, as David Deutsch has pointed out, that the
black box function is computing some complicated function such as a routing
algorithm and the output (0 or 1) indicates which route is chosen [89].
There are exactly four one-input, one-output Boolean functions:2
x
f0
f1
fx
f Nx
0
0
1
0
1
1
0
1
1
0
The ﬁrst two of these are constant functions, f0 and f1. That is, they
always output a constant value. We call the other two, fx and f Nx, balanced.
Over their inputs 0 and 1, they have an equal number of 0s and 1s in their
truth table.
We can now state Deutsch’s problem:
Given access to a one bit input and one bit output
Boolean function, determine, by querying the function as
few times as possible, whether the function is balanced
or constant.
If you were to approach this with classical tools, you would have to query
the function at least twice: ﬁrst to see the output when the input is 0 and
then the output when the input is 1. The remarkable discovery by David
Deutsch is that you only need one query on a quantum computer! The original
Deutsch algorithm handles this case of a one-bit Boolean oracle [88], and the
2Code and exposition of the DJ algorithm adapted from “Textbook algorithms in Cirq”
linked on the book’s website.
104
CHAPTER 8
The Canon: Code Walkthroughs
Deutsch-Jozsa (DJ) algorithm generalizes the approach to handle Boolean
functions of n inputs [90]. It’s not difﬁcult to see that, with classical tools,
one must query an n bit Boolean function at least n times. The DJ algorithm
solves this problem in only one query.
8.3
Quantum Advantage Demonstrated by Deutsch-Jozsa
Classically, one must query the one-bit Boolean function twice to dis-
tinguish the constant function from the balanced function. For an n-bit
Boolean function, one must query n times. On a quantum computer using
DJ one only has to query once.
We now turn to the quantum approach to this problem. Above, we have
described a classical function on bits that is not reversible, e.g., the constant
functions f0 and f1. That is, knowing the value of the output does not allow
us to determine uniquely the value of the input. In order to run this on a
quantum computer, however we need to make this computation reversible. A
trick for taking a classical non-reversible function and making it reversible
is to compute the value in an extra register (or, equivalently, store inputs to
the function in an extra register). Suppose we have an n bit input x and we
are computing a (potentially non-reversible) Boolean function f .x/. Then we
can implement this via a unitary Uf that acts on n C 1 qubits
Uf .jxijyi/ WD jxijy ˚ f .x/i
Here the symbol ˚ denotes addition modulo-2 (a.k.a. XOR); in the ex-
pression above, we have identiﬁed how Uf acts on all computational basis
states jxi and on the single output qubit jyi. Since we have deﬁned Uf on
the computational basis, we extend its deﬁnition to all vectors in the state
space by linearity. To see that this is reversible, one can note that applying the
transformation twice returns the state to its original form. Even further, Uf
is unitary since it maps the orthonormal computational basis to itself and so
preserves the length of the vectors that it acts on.
One core idea in this algorithm is that we will measure in a different basis
than the computational basis. If we measure in the computational basis (e.g.,
the z-basis) then we will gain no quantum advantage as we have two basis
states, j0i and j1i, and those correspond to the classical bits, 0 and 1. One
of the tricks that makes this algorithm work is that we will measure in the
Hadamard basis state which is a superposition of j0i and j1i [176]. Figure 8.2
shows a circuit diagram of DJ.
Let’s see how to implement these functions in Cirq. f0 enacts the transform
SECTION 8.1
The Deutsch-Jozsa Algorithm
105
Figure 8.2: The DJ circuit
Source: Wikimedia
j00i ! j00i
j01i ! j01i
j10i ! j10i
j11i ! j11i
This is just the identity transform, i.e., an empty circuit. f1 enacts the trans-
form
j00i ! j01i
j01i ! j00i
j10i ! j11i
j11i ! j10i
This is a bit ﬂip gate on the second qubit.
To gain an understanding of how this newly deﬁned reversible operator
works, we will compute Ufx .j0ij0i/ as an example to demonstrate. Recall
that j00i is shorthand for j0ij0i. Then, by deﬁnition,
Ufx .j00i/ D Ufx .j0ij0i/ WD j0ij0 ˚ fx.0/i D j0ij0 ˚ 0i D j0ij0i
It is worthwhile to compute Ufx for each of j0ij1i, j1ij0i and j1ij1i; then let
us check that fx enacts the transform
j00i ! j00i
j01i ! j01i
j10i ! j11i
j11i ! j10i
106
CHAPTER 8
The Canon: Code Walkthroughs
This is nothing more than a CNOT from the ﬁrst qubit to the second qubit.
Finally f Nx enacts the transform
j00i ! j01i
j01i ! j00i
j10i ! j10i
j11i ! j11i
which is a CNOT from the ﬁrst qubit to the second qubit followed by a bit
ﬂip on the second qubit. We can encapsulate these functions into a dictionary
which maps oracle names to the operations in the circuit needed to enact this
function:
# Import the Cirq Library
import cirq
# Get two qubits, a data qubit and target qubit, respectively
q0, q1 = cirq.LineQubit.range(2)
# Dictionary of oracles
oracles = {’0’: [], ’1’: [cirq.X(q1)], ’x’: [cirq.CNOT(q0, q1)],
’notx’: [cirq.CNOT(q0, q1), cirq.X(q1)]}
Let us turn now to Deutsch’s algorithm. Suppose we are given access to the
reversible oracle functions we have deﬁned before. By a similar argument for
our irreversible classical functions, you can show that you cannot distinguish
the balanced from the constant functions by using this oracle only once. But
now we can ask the question: what if we are allowed to query this function in
superposition, i.e., what if we can use the power of quantum computing?
Deutsch was able to show that you could solve this problem with quantum
computers using only a single query of the function. To see how this works,
we need two simple insights. Suppose we prepare the second qubit in the
superposition state
j i D
1
p
2
.j0i   j1i/
and apply the oracle. Using the linearity of the operator Uf to acquire the
second equation and an observation for the third, we can check that
Uf jxij i D Uf jxi 1
p
2
.j0i   j1i/
D jxi 1
p
2
.jf .x/i   jf .x/ ˚ 1i/ D . 1/f .x/jxij i
SECTION 8.1
The Deutsch-Jozsa Algorithm
107
This is the phase kickback trick. By applying Uf onto a target which is in
superposition, the value of the function ends up in the global phase, thus
kicking back the information we need on whether the function is constant or
balanced; the information is encoded in the phase.
How can we leverage this to distinguish between constant and balanced
functions? Note that for constant functions the phase that is applied is the
same for all inputs jxi, whereas for balanced functions the phase is different
for each value of x. To use the phase kickback trick for each of the oracles,
we apply the following transform on the ﬁrst qubit
f0 ! I
f1 !  I
fx ! Z
f Nx !  Z
Uf0
D
Uf1
D
X
Ufx

D
Uf Nx

D
X
Now we only need to distinguish between the identity gate and the Z gate
on the ﬁrst qubit; we can do this by recalling that:
HZH D X
H D
1
p
2
1
1
1
 1

This means that we can turn a phase ﬂip into a bit ﬂip by applying Hadamard
gates before and after the phase ﬂip. If we look at the constant and balanced
functions we see that the constant functions will be proportional to I and the
balanced will be proportional to X. If we feed in j0i to this register, then in
the ﬁrst case we will only see j0i and in the second case we will see j1i. In
other words we will be able to distinguish constant from balanced using a
single query of the oracle.
108
CHAPTER 8
The Canon: Code Walkthroughs
H
Uf
H
X
H
# Import the Cirq Library
import cirq
# Get two qubits, a data qubit and target qubit, respectively
q0, q1 = cirq.LineQubit.range(2)
# Dictionary of oracles
oracles = {’0’: [], ’1’: [cirq.X(q1)], ’x’: [cirq.CNOT(q0, q1)],
’notx’: [cirq.CNOT(q0, q1), cirq.X(q1)]}
def deutsch_algorithm(oracle):
"""Yields a circuit for Deutsch’s algorithm given operations
implementing
the oracle."""
yield cirq.X(q1)
yield cirq.H(q0), cirq.H(q1)
yield oracle
yield cirq.H(q0)
yield cirq.measure(q0)
# Display each circuit for all oracles
for key, oracle in oracles.items():
print(’Circuit for {}...’.format(key))
print(cirq.Circuit.from_ops(deutsch_algorithm(oracle)),
end="\n\n")
# Get a simulator
simulator = cirq.Simulator()
# Execute the circuit for each oracle to distinguish constant from
balanced
for key, oracle in oracles.items():
result = simulator.run(
cirq.Circuit.from_ops(deutsch_algorithm(oracle)),
repetitions=10
)
print(’oracle: {:<4} results: {}’.format(key, result))
Now let’s extend the Deutsch problem to Boolean functions of n Boolean
inputs, not just single-input functions as above. We saw that with the Deutsch
algorithm we can double our speed, going from two queries classically to one
query quantum mechanically.
=n
H ˝n
Uf
H ˝n
X
H
SECTION 8.1
The Deutsch-Jozsa Algorithm
109
If we are able to query an n-bit oracle just once and determine whether
the function is constant or balanced we have a time complexity of O.1/, a
signiﬁcant speedup over O.n/ queries. All Boolean functions for one-bit input
are either constant or balanced. For Boolean functions with two input bits
there are two constant functions, f .x0; x1/ D 0 and f .x0; x1/ D 1, while
there are
 4
2

D 6 balanced functions. We call this more generalized form the
Deutsch-Jozsa algorithm. The following code gives you the operations for
querying these functions.
"""Deutsch-Jozsa algorithm on three qubits in Cirq."""
# Import the Cirq library
import cirq
# Get three qubits -- two data and one target qubit
q0, q1, q2 = cirq.LineQubit.range(3)
# Oracles for constant functions
constant = ([], [cirq.X(q2)])
# Oracles for balanced functions
balanced = ([cirq.CNOT(q0, q2)],
[cirq.CNOT(q1, q2)],
[cirq.CNOT(q0, q2), cirq.CNOT(q1, q2)],
[cirq.CNOT(q0, q2), cirq.X(q2)],
[cirq.CNOT(q1, q2), cirq.X(q2)],
[cirq.CNOT(q0, q2), cirq.CNOT(q1, q2), cirq.X(q2)])
def your_circuit(oracle):
"""Yields a circuit for the Deutsch-Jozsa algorithm on three
qubits."""
# phase kickback trick
yield cirq.X(q2), cirq.H(q2)
# equal superposition over input bits
yield cirq.H(q0), cirq.H(q1)
# query the function
yield oracle
# interference to get result, put last qubit into |1>
yield cirq.H(q0), cirq.H(q1), cirq.H(q2)
# a final OR gate to put result in final qubit
yield cirq.X(q0), cirq.X(q1), cirq.CCX(q0, q1, q2)
yield cirq.measure(q2)
# Get a simulator
simulator = cirq.Simulator()
# Execute circuit for oracles of constant value functions
print(’Your result on constant functions’)
for oracle in constant:
110
CHAPTER 8
The Canon: Code Walkthroughs
result =
simulator.run(cirq.Circuit.from_ops(your_circuit(oracle)),
repetitions=10)
print(result)
# Execute circuit for oracles of balanced functions
print(’Your result on balanced functions’)
for oracle in balanced:
result =
simulator.run(cirq.Circuit.from_ops(your_circuit(oracle)),
repetitions=10)
print(result)
We can now see how to query an oracle of n-bit boolean inputs to check
whether it is constant or balanced.
8.2
The Bernstein-Vazirani Algorithm
Now let’s turn to the Bernstein-Vazirani (BV) algorithm that we considered
earlier in this book [36]. As with DJ, the goal of BV is also to ascertain the na-
ture of a black-box Boolean function. While it is true that DJ demonstrates an
advantage of quantum over classical computing, if we allow for a small error
rate, then the advantage disappears: both classical and quantum approaches
are in the order of O.1/ time complexity [176].
BV was the ﬁrst algorithm developed that shows a clear separation between
quantum and classical computing even allowing for error, i.e., a true non-
deterministic speedup. Here is the BV problem statement:
Given an unknown function of n inputs:
f .xn 1; xn 2; :::; x1; x0/;
let a be an unknown non-negative integer less than 2n.
Let f .x/ take any other such integer x and modulo-2
sum x multiplied by a. So the output of the function is:
a  x D a0x0 ˚ a1x1 ˚ a2x2::::
Find a in one query of the oracle [194].
Just as in DJ, we prepare the states of two sets of qubits: the data register
qubits and the target qubit. The data register qubits are set to j0i and the target
set to j1i. We then apply H to both sets of qubits to put them in superposition.
H applied to the data register qubits prepares us to measure in the X basis.
SECTION 8.2
The Bernstein-Vazirani Algorithm
111
We then apply the unitary Uf , an H to the data register qubits and then
measure those qubits. Since we only applied Uf once, the time complexity of
BV is O.1/.
=n
H ˝n
Uf
H ˝n
X
H
The Bernstein-Vazirani Algorithm
"""Bernstein-Vazirani algorithm in Cirq."""
# Imports
import random
import cirq
def main():
"""Executes the BV algorithm."""
# Number of qubits
qubit_count = 8
# Number of times to sample from the circuit
circuit_sample_count = 3
# Choose qubits to use
input_qubits = [cirq.GridQubit(i, 0) for i in range(qubit_count)]
output_qubit = cirq.GridQubit(qubit_count, 0)
# Pick coefficients for the oracle and create a circuit to query
it
secret_bias_bit = random.randint(0, 1)
secret_factor_bits = [random.randint(0, 1) for _ in
range(qubit_count)]
oracle = make_oracle(input_qubits,
output_qubit,
secret_factor_bits,
secret_bias_bit)
print(’Secret function:\nf(x) = x*<{}> + {} (mod 2)’.format(
’, ’.join(str(e) for e in secret_factor_bits),
secret_bias_bit))
# Embed the oracle into a special quantum circuit querying it
exactly once
circuit = make_bernstein_vazirani_circuit(
input_qubits, output_qubit, oracle)
print(’\nCircuit:’)
print(circuit)
# Sample from the circuit a couple times
simulator = cirq.Simulator()
112
CHAPTER 8
The Canon: Code Walkthroughs
result = simulator.run(circuit, repetitions=circuit_sample_count)
frequencies = result.histogram(key=’result’, fold_func=bitstring)
print(’\nSampled results:\n{}’.format(frequencies))
# Check if we actually found the secret value.
most_common_bitstring = frequencies.most_common(1)[0][0]
print(’\nMost common matches secret factors:\n{}’.format(
most_common_bitstring == bitstring(secret_factor_bits)))
def make_oracle(input_qubits,
output_qubit,
secret_factor_bits,
secret_bias_bit):
"""Gates implementing the function f(a) = a*factors + bias (mod
2)."""
if secret_bias_bit:
yield cirq.X(output_qubit)
for qubit, bit in zip(input_qubits, secret_factor_bits):
if bit:
yield cirq.CNOT(qubit, output_qubit)
def make_bernstein_vazirani_circuit(input_qubits, output_qubit,
oracle):
"""Solves for factors in f(a) = a*factors + bias (mod 2) with one
query."""
c = cirq.Circuit()
# Initialize qubits
c.append([
cirq.X(output_qubit),
cirq.H(output_qubit),
cirq.H.on_each(*input_qubits),
])
# Query oracle
c.append(oracle)
# Measure in X basis
c.append([
cirq.H.on_each(*input_qubits),
cirq.measure(*input_qubits, key=’result’)
])
return c
def bitstring(bits):
"""Creates a bit string out of an iterable of bits."""
return ’’.join(str(int(b)) for b in bits)
if __name__ == ’__main__’:
main()
SECTION 8.3
Simon’s Problem
113
Here we initialize the target (or output) register to j1i with an X operator
and the data register qubits from state j0i to the jCi / j i basis by applying H.
We then query the oracle, apply H to each of the input qubits and measure the
input qubits. This gives us the answer that we were seeking, a, in one query.
Thus, no matter how many inputs we have, we can perform this algorithm in
O.1/ time.
"""
=== EXAMPLE OUTPUT for BV ===
Secret function:
f(x) = x*<0, 1, 1, 1, 0, 0, 1, 0> + 1 (mod 2)
Sampled results:
Counter({’01110010’: 3})
Most common matches secret factors:
True
"""
8.3
Simon’s Problem
Soon after BV’s result, Daniel Simon demonstrated the ability to determine
the periodicity of a function exponentially faster on a quantum computer
compared with a classical one.
Let us recall that a function can map two different inputs to the same
output but must not map the same input to two different outputs. In other
words, 2W1 is acceptable, but 1W2 is not. For example, the function f .x/ D x2
which squares each input is in fact a function; two different inputs, namely 1
and  1 both map to 1, i.e., f .x/ is 2:1. See Part III for a review of functions
and injectivity, surjectivity and bijectivity.
If we determine that the function is of the 2W1 type, then our next challenge
is to investigate the period of the function (see section 8.5 for an explanation
of periodicity); this is the core objective of Simon’s problem. Here is an
outline of the problem in more formal language:
The problem considers an oracle that implements a
function mapping an n-bit string to an m-bit string
f Wf0; 1gn ! f0; 1gm, with m  n, where it is promised
that f is a 1W1 type function (each input gives a different
output) or 2W1 type function (two inputs give the same
114
CHAPTER 8
The Canon: Code Walkthroughs
output) with non-zero period s 2 f0; 1gn such that for all
x; x0 we have f .x/ D f .x0/ if and only if x0 D x ˚ s,
where ˚ corresponds to addition modulo-2.3
The
problem is to determine the type of the function f and, if
it is 2W1, to determine the period s. [271]
Simon’s problem is in the same vein as Deutsch’s problem — you are
presented with an oracle — a black box function where you can observe the
output for speciﬁc inputs, but not the underlying function. The challenge is
to determine if this black-box process ever sends two different inputs to the
same output, and if so, determine how often that occurs. Note that in these
algorithms we do not ﬁnd out what the underlying function in the black box
is, only the relationship between the input and the output as seen from outside
the box. The actual function may be quite complicated and may require even
more steps to analyze than the input/output relationship. We explore the
concept of oracles further in this chapter.
Simon was successful in proving that we can solve this problem to deter-
mine the periodicity of a function exponentially faster on a quantum computer
compared with a classical one. Shor built on this key result in developing his
algorithm for factoring large numbers when he realized that the two problems
— ﬁnding the periodicity of a function and factoring a large composite number
— are in fact isomorphic.
Twenty years after Simon presented his problem, researchers did in fact
successfully use a quantum system to determine the periodicity of a func-
tion [271]. Check the book’s website for sample code for Simon’s problem.
8.4
Quantum Fourier Transform
In chapter 3, we discussed the Quantum Fourier Transform (QFT) as a method
to set up the amplitudes for measurement that will favor the qubit which
has the information we require. We can implement a QFT circuit on NISQ
hardware in a straightforward manner. Here is the standard circuit diagram
for QFT:
3When we do addition modulo-2, we equate 2 with 0. So, for example, 1 ˚ 1 D 0 and we
would say, “The sum of 1 and 1 modulo-2 is 0.”
SECTION 8.4
Quantum Fourier Transform
115
jx1i
H
R=2
R=4
R=8
jx2i

H
R=2
R=4
jx3i


H
R=2
jx4i



H
Now let’s walk through the code for QFT as we will use this technique in the
upcoming section on Shor’s algorithm. We’ll go through the program step by
step, explaining ﬁrst high-level details and then discussing implementation
details.
First, we import the necessary packages for this program including the
Cirq library.
"""Creates and simulates a circuit for Quantum Fourier Transform
(QFT)
on a 4 qubit system.
"""
# Imports
import numpy as np
import cirq
Next, we deﬁne the main function, which simply calls the circuit generation
function and then runs it — in this case on the simulator. The program then
prints the ﬁnal state of the wavefunction after the QFT has been applied.
def main():
"""Demonstrates the Quantum Fourier transform."""
# Create circuit and display it
qft_circuit = generate_2x2_grid_qft_circuit()
print(’Circuit:’)
print(qft_circuit)
# Simulate and collect the final state
simulator = cirq.Simulator()
result = simulator.simulate(qft_circuit)
# Display the final state
print(’\nFinalState’)
print(np.around(result.final_state, 3))
116
CHAPTER 8
The Canon: Code Walkthroughs
Figure 8.3: Modiﬁed QFT circuit including SWAP operations ﬁt for running on a 2x2 grid of
qubits with nearest-neighbor interactions.
Next, we deﬁne a helper function for building the QFT circuit. This
function yields a controlled-Rz rotation as well as a SWAP gate on the input
qubits.
def cz_and_swap(q0, q1, rot):
"""Yields a controlled-RZ gate and SWAP gate on the input
qubits."""
yield cirq.CZ(q0, q1)**rot
yield cirq.SWAP(q0,q1)
Finally, we use this helper function to write the entire circuit, which is
done in the function below. First, we deﬁne a 2 x 2 grid of qubits and label
them a through d. In many quantum computing systems there are constraints
one which qubits can interact with each other. For example perhaps only
nearest-neighbor qubits can interact. Then we cannot apply the standard QFT
circuit described above. Instead, a modiﬁed QFT circuit including SWAP
operations needs to be applied, as illustrated in Figure 8.3. This is the circuit
we will implement in our example.
We apply a series of Hadamard and control rotation operators as speciﬁed
by the diagram. In this example we perform the quantum Fourier transform on
the .1; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0/ vector, which means acting with
the QFT circuit on the ground state j0000i.
def generate_2x2_grid_qft_circuit():
"""Returns a QFT circuit on a 2 x 2 planar qubit architecture.
Circuit adopted from https://arxiv.org/pdf/quant-ph/0402196.pdf.
"""
# Define a 2*2 square grid of qubits
a, b, c, d = [cirq.GridQubit(0, 0), cirq.GridQubit(0, 1),
cirq.GridQubit(1, 1), cirq.GridQubit(1, 0)]
# Create the Circuit
circuit = cirq.Circuit.from_ops(
cirq.H(a),
cz_and_swap(a, b, 0.5),
cz_and_swap(b, c, 0.25),
SECTION 8.5
Shor’s Algorithm
117
cz_and_swap(c, d, 0.125),
cirq.H(a),
cz_and_swap(a, b, 0.5),
cz_and_swap(b, c, 0.25),
cirq.H(a),
cz_and_swap(a, b, 0.5),
cirq.H(a),
strategy=cirq.InsertStrategy.EARLIEST
)
return circuit
Finally, we can run this circuit by calling the main function:
if __name__ == ’__main__’:
main()
The output of this program is shown below:
FinalState
[0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j
0.25+0.j
0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j 0.25+0.j
0.25+0.j]
Figure 8.4 delineates the process of applying QFT.
8.5
Shor’s Algorithm
RSA Cryptography
Suppose Alice would like to send a private message to Bob via the internet.
Alice’s message could very well be intercepted by a malicious eavesdropper,
Eve, along its journey. This is embarrassing, yet harmless, if Alice is sending
Bob a note, but it is an issue if Alice is sending Bob her credit card number.
How can we send messages securely via the internet?
Cryptography is the study of the making and breaking of secret codes.
Cryptography refers to the writing of secret codes, while cryptanalysis refers
to the breaking of those codes. RSA is a cryptographic protocol used widely
across many platforms for the secure transfer of information via the internet.
The abbreviation RSA refers to Rivest, Shamir and Adelman, three pioneers
in its development [239].
The core conjecture of the RSA protocol is that multiplying two large
prime numbers is a trapdoor function; multiplying two large prime numbers
is easy, yet ﬁnding the two factors after the multiplication has occurred is hard.
118
CHAPTER 8
The Canon: Code Walkthroughs
Figure 8.4: QFT and measurement
Source: [204]
Later in this chapter, we’ll see that a fault-tolerant quantum computer will
have the capacity to surmount the difﬁculty posed by the factorization process
which puts the entire RSA scheme at risk [254]! This leads us to post-quantum
cryptography, a fascinating ﬁeld at the intersection of mathematics, physics
and computer science.
In 1994, Peter Shor published his landmark paper establishing a quantum
algorithm for the prime factorization of numbers [254]. The problem of
factoring a number into primes reduces to ﬁnding a factor, since if you can
ﬁnd one factor, you can use it to divide the original number and consider the
smaller factors. Eventually, using this divide (literally) and conquer strategy,
we can completely factor the number into primes.
SECTION 8.5
Shor’s Algorithm
119
His technique to ﬁnd a factor of any number n is to ﬁnd the period, r, of a
certain function f and then use the knowledge of the period of said function
to ﬁnd a factor of the number.
The Period of a Function
Periodic functions are functions whose outputs enjoy a “periodic” nature. In
other words, after having seen a particular output, we should expect to see
the same output after some period (often to be thought of as a period of time).
Most of the fun of periodic functions lies in determining what that period
is, i.e. how “long” we have to wait until we see the same output! Periodic
functions are not so unfamiliar and are encountered already in the early stages
of trigonometry and the study of the “circular” functions cosine, sine, etc.
To see this, play around with the trigonometric functions in the following
exercise:
8.4
Exercise
Perhaps the ﬁrst examples of periodic functions we en-
counter are the trigonometric functions cosine and sine. Verify that the
function x./ WD cos
  2
3  
 has the property that x.0/ D 1. Discover
the “next” value of  for which x.t/ equals 1. Verify also that the function
y./ WD sin
  2
3 

has the property that y.0/ D 0. Discover the “next” value
of  such that y./ D 0. How often should we expect the value of x./ to
be equal to 1? How often should we expect the value of y./ to be equal to
0? What is the period of x./? Of y./? Can you come up with a geometric
reason for why this is?
Shor realized that to ﬁnd a factor of any number n he could ﬁrst ﬁnd the
period, r, of a certain function f and then use the knowledge of the period
of said function to ﬁnd a factor of the number. To get an idea of what the
period of a function might be, consider raising some number, like 2, to higher
and higher powers, and then taking the result modulo a product of two prime
numbers such as 91 D 13  7. For example, we have:
120
CHAPTER 8
The Canon: Code Walkthroughs
20 (mod 91/
1
21 (mod 91/
2
22 (mod 91/
4
23 (mod 91/
8
24 (mod 91/
16
25 (mod 91/
32
26 (mod 91/
64
27 (mod 91/
37
28 (mod 91/
74
:::
:::
We see that the numbers cannot grow forever due to the modulo operation.
For example, 26 (mod 91/ D 64 and the next highest power 27 (mod 91/ D
37.
8.5
Exercise
Figure out if the powers of 2 ever cycle back to the number
1 in the table above. More precisely: ﬁnd the smallest number r beyond 0
such that
2r (mod 91/ D 1
Persistence pays off here. If you tried the above exercise, you found that,
yes, 212 (mod 91/ D 1, and that 12 is the smallest number beyond 0 that
makes this happen. Was it even obvious that it would return to 1? We refer to
the number 12 as the period of the function deﬁned by
f .x/ WD 2x (mod 91/
In general, for a function deﬁned by
f .x/ WD ax (mod n/
for some a 2 f1; 2; :::; n   1g relatively prime to n, the period of the function
f .x/ is the smallest value r of x greater than 0 such that f .r/ D 1 once again.
In other words, since at x D 0
a0 (mod n/ D 1
we must ﬁnd the next value r of x that again satisﬁes
ax (mod n/ D 1
SECTION 8.5
Shor’s Algorithm
121
A number a is relatively prime to n iff4 the greatest common divisor of
a and n is equal to 1, written gcd.a; n/ D 1. We refer to these functions
as modular functions. The smallest value r of x greater than 0 satisfying
f .x/ WD ax (modn/ is also referred to as the order of the element a in the
group .Z=nZ/, which denotes the multiplicative group whose underlying set
is the subset of numbers in f1; 2; :::; n   1g relatively prime to n, and whose
binary operation is multiplication modulo n, as above.
8.6
Exercise
Check that .Z=nZ/, whose underlying set of elements
is the subset of numbers in f1; 2; :::; n   1g relatively prime to n and whose
binary operation is multiplication modulo n, is actually a group! For any
number n, how many elements does the group .Z=nZ/ have? Can you ﬁnd
a pattern relating the number of elements in .Z=nZ/ to the number n?
The fascinating point we make now is that the difﬁculty in ﬁnding the
period of the function f .x/ D 2x (mod 91/ is not unique to the human
experience. Even a classical computer would have a difﬁcult time ﬁnding
the period of this function. Peter Shor realized that we can exploit quantum
computing to quickly ﬁnd the period of such a function [254]. We will now
explain how the period of a function can be used as an input to the factorization
algorithm that would crack RSA cryptography.
Period of a Function as an Input to a Factorization Algorithm
Suppose we are asked to factor some number n, and that we know how to ﬁnd
the period of any modular function, as described above. Remember that the
problem of factoring n reduces to the simpler problem of ﬁnding any factor
of n. So, let’s see how we could leverage our ability to ﬁnd the period of a
modular function to ﬁnd a factor of n:
1. Choose a random number a such that a < n.
2. Compute gcd.a; n/ using the extended Euclidean algorithm.
3. If gcd.a; n/ ¤ 1, i.e., a and n are not relatively prime, a is already a
nontrivial factor of n, and so we are done.
4. Otherwise, ﬁnd the period r of the modular function
f .x/ WD ax (modn/
5. If r is an odd number, or if a
r
2 D  1 (modn/, choose a new random
number and start over.
4Note: we abbreviate if and only if as iff throughout the book.
122
CHAPTER 8
The Canon: Code Walkthroughs
6. Otherwise, classical number theory guarantees that gcd.a
r
2 C 1; n/ and
gcd.a
r
2   1; n/ are both nontrivial factors of n.
8.7
Exercise
Run the above algorithm to factor the number n D 21.
A successful approach to the exercise above is the following:
1. begin with a D 2, since
2. gcd.a; n/ D gcd.2; 21/ D 1,
3. (Step 3 is omitted, since gcd.a; n/ D gcd.2; 21/ D 1),
4. the period of the modular function f .x/ WD 2x (mod 21/ is found to be
r D 6 and
5. r D 6 is neither an odd number, nor does it satisfy the equation
a
r
2 D  1 (mod)n;
6.
gcd.a
r
2 C1; n/ D gcd.2
6
2 C1; 21/ D gcd.8C1; 21/ D gcd.9; 21/ D 3
and
gcd.a
r
2  1; n/ D gcd.2
6
2  1; 21/ D gcd.23 1; 21/ D gcd.7; 21/ D 7
are the two nontrivial factors of n D 21.
We now see that being able to ﬁnd the period of any modular function
is the key to factoring. When we use a quantum computer, we can build a
circuit that ﬁnds the period quite easily. This cirquit uses the quantum Fourier
transform (QFT), described earlier in this book. Shor was inspired by Simon’s
algorithm and BV in his development of this algorithm. He built on the use
of the QFT by BV and the period ﬁnding of Simon’s approach to then arrive
at his number-factoring algorithm.5 Here is the circuit diagram for Shor’s
algorithm:
j0i
H
  

QFT 1
2n
:::
:::
:::
j0i
H

  
j0i
H

  
j1i
=n
U a20
U a21
  
U a22n 1
5Please see this book’s GitHub site for an example of code for Simon’s algorithm.
SECTION 8.5
Shor’s Algorithm
123
Let us now do a walkthrough of Shor’s algorithm using Cirq. The code
for this walkthrough is available on the book’s website. We ﬁrst import the
libraries we will use for this walkthrough.
import fractions
import math
import random
import numpy as np
import sympy
from typing import Callable, List, Optional, Sequence, Union
import cirq
Classical order ﬁnding
A function for classically computing the order r of an element a 2 Z=nZ is
provided below. This function simply computes the sequence
a2 mod n;
a3 mod n;
a4 mod n;
:::
until an integer r is found such that ar D 1 mod n. Since jZ=nZj D '.n/, this
algorithm for order ﬁnding has time complexity O.'.n// which is inefﬁcient.
(Speciﬁcally, the time complexity is roughly O.2L=2/ where L is the number
of bits in n.)
def classical_order_finder(a: int, n: int) -> Optional[int]:
"""Computes smallest positive r such that a**r mod n == 1."""
# Make sure a is both valid and in Z/nZ
if a < 2 or a >= n or math.gcd(a, n) > 1:
raise ValueError(f"Invalid a={a} for modulus n={n}.")
# Determine the order
r, x = 1, a
while x != 1:
x = (a**r) % n
r += 1
return r
An example of computing r for a given a 2 Z=nZ and given n is shown
in the code block below.
"""Example of (classically) computing the order of an element."""
n = 15 # The multiplicative group is [1, 2, 4, 7, 8, 11, 13, 14]
124
CHAPTER 8
The Canon: Code Walkthroughs
a = 8
r = classical_order_finder(a, n)
# Check that the order is indeed correct
print(f"a^r mod n = {a}^{r} mod {n} = {a**r % n}")
The output of this code block is:
a^r mod n = 8^4 mod 15 = 1
Quantum order ﬁnding
The quantum computing portion of Shor’s algorithm accomplishes the order
ﬁnding. Quantum order ﬁnding is essentially quantum phase estimation with
a unitary U that computes the modular exponential function fa.z/ for some
randomly chosen a 2 Z=nZ. The full details of how U is computed in terms
of elementary gates can be complex to unravel, especially on a ﬁrst reading.
In this tutorial, we’ll use arithmetic operations in Cirq which can implement
such a unitary U without fully delving into the details of elementary gates.
Below we ﬁrst show a simple example of an arithmetic operation in Cirq
(addition) then discuss the operation we care about (modular exponentiation).
Quantum arithmetic operations in Cirq
Here we discuss an example of deﬁning an arithmetic operation in Cirq. This
operation adds the value of the input register into the target register. More
speciﬁcally, this operation acts on two qubit registers as
jaiijbit 7! jaiija C b mod Ntit:
(8.8)
Here, the subscripts i and t denote input and target register, respectively,
and Nt is the dimension of the target register.
To deﬁne this operation, called Adder, we inherit from the pre-deﬁned
class cirq.ArithmeticOperation and override the four methods shown below.
The main method is the apply method which deﬁnes the arithmetic. Here, we
simply state the expression as aCb instead of the more accurate aCb mod Nt
above — the cirq.ArithmeticOperation class is able to deduce what we mean
by simply a C b since the operation must be reversible.
"""Example of defining an arithmetic (quantum) operation in Cirq."""
class Adder(cirq.ArithmeticOperation):
"""Quantum addition."""
def __init__(self, target_register, input_register):
SECTION 8.5
Shor’s Algorithm
125
self.input_register = input_register
self.target_register = target_register
def registers(self):
return self.target_register, self.input_register
def with_registers(self, *new_registers):
return Add(*new_registers)
def apply(self, target_value, input_value):
return target_value + input_value
Now that we have the class deﬁned, we can use it in a circuit. The cell
below creates two qubit registers, then sets the ﬁrst register to be j10i (in
binary) and the second register to be j01i (in binary) via X gates. Then, we
use the Adder operation, then measure all the qubits.
Since 10 C 01 D 11 (in binary), we expect to measure j11i in the target
register every time. Additionally, since we do not alter the input register, we
expect to measure j10i in the input register every time. In short, the only
bitstring we expect to measure is 1011.
"""Example of using an Adder in a circuit."""
# Two qubit registers
qreg1 = cirq.LineQubit.range(2)
qreg2 = cirq.LineQubit.range(2, 4)
# Define the circuit
circ = cirq.Circuit(
cirq.ops.X.on(qreg1[0]),
cirq.ops.X.on(qreg2[1]),
Adder(input_register=qreg1, target_register=qreg2),
cirq.measure_each(*qreg1),
cirq.measure_each(*qreg2)
)
# Display it
print("Circuit:\n")
print(circ)
# Print the measurement outcomes
print("\n\nMeasurement outcomes:\n")
print(cirq.sample(circ, repetitions=5).data)
The output of this code block is shown below:
Circuit:
0: ---X---#3------------------------------------------M---
|
1: -------#4------------------------------------------M---
|
2: -------<__main__.Adder object at 0x7fb5bb147be0>---M---
126
CHAPTER 8
The Canon: Code Walkthroughs
|
3: ---X---#2------------------------------------------M---
Measurement outcomes:
0 1 2 3
0 1 0 1 1
1 1 0 1 1
2 1 0 1 1
3 1 0 1 1
4 1 0 1 1
In the output, we ﬁrst see the circuit which shows the initial X gates, the
Adder operation, then the ﬁnal measurements. Next, we see the measurement
outcomes which are all the bitstring 1011 as expected.
It is also possible to see the unitary of the adder operation, which we do
below. Here, we set the target register to be two qubits in the zero state, i.e.
j00i. We specify the input register as the integer one which corresponds to
the qubit register j01i.
"""Example of the unitary of an Adder operation."""
cirq.unitary(
Adder(target_register=cirq.LineQubit.range(2),
input_register=1)
).astype(np.int32)
The output is:
array([[0, 0, 0, 1],
[1, 0, 0, 0],
[0, 1, 0, 0],
[0, 0, 1, 0]], dtype=int32)
We can understand this unitary as follows. The ith column of the unitary
is the state ji C 1 mod 4i. For example, if we look at the 0th column of
the unitary, we see the state ji C 1 mod 4i D j0 C 1 mod 4i D j1i. If we
look at the 1st column of the unitary, we see the state ji C 1 mod 4i D
j1 C 1 mod 4i D j2i. Similarly for the last two columns.
Modular exponential arithmetic operation
We can deﬁne the modular exponential arithmetic operation in a similar way
to the simple addition arithmetic operation, shown below. For the purposes
of understanding Shor’s algorithm, the most important part of the following
code block is the apply method which deﬁnes the arithmetic operation.
SECTION 8.5
Shor’s Algorithm
127
class ModularExp(cirq.ArithmeticOperation):
"""Quantum modular exponentiation.
This class represents the unitary which multiplies base raised to
exponent
into the target modulo the given modulus. More precisely, it
represents the
unitary V which computes modular exponentiation a**e mod n:
V|y>|e> = |y * a**e mod n> |e> 0 <= y < n
V|y>|e> = |y> |e>
n <= y
where y is the target register, e is the exponent register, a is
the base
and n is the modulus. Consequently,
V|y>|e> = (U**e|r>)|e>
where U is the unitary defined as
U|y> = |y * a mod n> 0 <= y < n
U|y> = |y>
n <= y
"""
def __init__(
self,
target: Sequence[cirq.Qid],
exponent: Union[int, Sequence[cirq.Qid]],
base: int,
modulus: int
) -> None:
if len(target) < modulus.bit_length():
raise ValueError(f’Register with {len(target)} qubits is
too small ’
f’for modulus {modulus}’)
self.target = target
self.exponent = exponent
self.base = base
self.modulus = modulus
def registers(self) -> Sequence[Union[int, Sequence[cirq.Qid]]]:
return self.target, self.exponent, self.base, self.modulus
def with_registers(
self,
*new_registers: Union[int, Sequence[’cirq.Qid’]],
) -> cirq.ArithmeticOperation:
if len(new_registers) != 4:
raise ValueError(f’Expected 4 registers (target, exponent,
base, ’
f’modulus), but got {len(new_registers)}’)
target, exponent, base, modulus = new_registers
if not isinstance(target, Sequence):
raise ValueError(
f’Target must be a qubit register, got {type(target)}’)
if not isinstance(base, int):
raise ValueError(
f’Base must be a classical constant, got {type(base)}’)
128
CHAPTER 8
The Canon: Code Walkthroughs
if not isinstance(modulus, int):
raise ValueError(
f’Modulus must be a classical constant, got
{type(modulus)}’)
return ModularExp(target, exponent, base, modulus)
def apply(self, *register_values: int) -> int:
assert len(register_values) == 4
target, exponent, base, modulus = register_values
if target >= modulus:
return target
return (target * base**exponent) % modulus
def _circuit_diagram_info_(
self,
args: cirq.CircuitDiagramInfoArgs,
) -> cirq.CircuitDiagramInfo:
assert args.known_qubits is not None
wire_symbols: List[str] = []
t, e = 0, 0
for qubit in args.known_qubits:
if qubit in self.target:
if t == 0:
if isinstance(self.exponent, Sequence):
e_str = ’e’
else:
e_str = str(self.exponent)
wire_symbols.append(
f’ModularExp(t*{self.base}**{e_str} %
{self.modulus})’)
else:
wire_symbols.append(’t’ + str(t))
t += 1
if isinstance(self.exponent, Sequence) and qubit in
self.exponent:
wire_symbols.append(’e’ + str(e))
e += 1
return
cirq.CircuitDiagramInfo(wire_symbols=tuple(wire_symbols))
In the apply method, we see that we evaluate (target * base**exponent)
% modulus. The target and the exponent depend on the values of the
respective qubit registers, and the base and modulus are constant — namely,
the modulus is n and the base is some a 2 Z=nZ.
Now we can see an example of the modular exponential unitary — or at
least part of it! The total number of qubits we will use is 3.L C 1/ where
L is the number of bits needed to store the integer n to factor. The size of
the unitary which implements the modular exponential is thus 43.LC1/. For
a modest n D 15, the unitary requires storing 230 ﬂoating point numbers in
memory which is out of reach of most current standard laptops.
SECTION 8.5
Shor’s Algorithm
129
and see the number of qubits needed for Shor’s algorithm.
"""
n = 15
L = n.bit_length()
# The target register has L qubits
target = cirq.LineQubit.range(L)
# The exponent register has 2L + 3 qubits
exponent = cirq.LineQubit.range(L, 3 * L + 3)
# Display the total number of qubits to factor this n
print(f"To factor n = {n} which has L = {L} bits, we need 3L + 3 =
{3 * L + 3} qubits.")
The output of this code block is:
To factor n = 15 which has L = 4 bits, we need 3L + 3 = 15 qubits.
As with the simple adder operation, this modular exponential operation
has a unitary which we can display (memory permitting) as follows.
"""See (part of) the unitary for a modular exponential operation."""
# Pick some element of the multiplicative group modulo n
a = 5
# Display (part of) the unitary. Uncomment if n is small enough
# cirq.unitary(ModularExp(target, exponent, a, n))
Using the modular exponential operation in a circuit
The quantum part of Shor’s algorithm is just phase estimation with the unitary
U corresponding to the modular exponential operation. The following cell
deﬁnes a function which creates the circuit for Shor’s algorithm using the
ModularExp operation we deﬁned above.
"""Function to make the quantum circuit for order finding."""
def make_order_finding_circuit(a: int, n: int) -> cirq.Circuit:
"""Returns quantum circuit which computes the order of a modulo n.
The circuit uses Quantum Phase Estimation to compute an
eigenvalue of
the unitary
U|y> = |y * a mod n> 0 <= y < n
U|y> = |y>
n <= y
"""
L = n.bit_length()
target = cirq.LineQubit.range(L)
exponent = cirq.LineQubit.range(L, 3 * L + 3)
return cirq.Circuit(
"""Create the target and exponent registers for phase estimation,
130
CHAPTER 8
The Canon: Code Walkthroughs
cirq.X(target[L - 1]),
cirq.H.on_each(*exponent),
ModularExp(target, exponent, a, n),
cirq.QFT(*exponent, inverse=True),
cirq.measure(*exponent, key=’exponent’),
)
We can visualize this circuit as follows.
"""Example of the quantum circuit for period finding."""
n = 15
x = 7
circuit = make_order_finding_circuit(a, n)
print(circuit)
As previously described, we put the exponent register into an equal super-
position via Hadamard gates. The X gate on the last qubit in the target register
is used for phase kickback. The modular exponential operation performs the
sequence of controlled unitaries in phase estimation, then we apply the inverse
QFT to the exponent register and measure to read out the result.
To illustrate the measurement results, we can sample from a smaller circuit.
"""Measuring Shor’s period finding circuit."""
circuit = make_order_finding_circuit(a=5, n=6)
res = cirq.sample(circuit, repetitions=8)
print("Raw measurements:")
print(res)
print("\nInteger in exponent register:")
print(res.data)
The output of this code block is:
Raw measurements:
exponent=00001010, 00000000, 00000000, 00000000, 00000000, 00000000,
00000000, 00000000, 00000000
Integer in exponent register:
exponent
0
0
1
0
2
0
3
0
4
256
5
0
6
256
7
0
SECTION 8.5
Shor’s Algorithm
131
We interpret each measured bitstring as an integer, but what do these
integers tell us? In the next section we look at how to classically post-process
to interpret them.
Classical post-processing
The integer we measure is close to s=r where r is the order of a 2 Z=nZ
and 0  s < r is an integer. We use the continued fractions algorithm to
determine r from s=r then return it if the order ﬁnding circuit succeeded, else
we return None.
def process_measurement(result: cirq.Result, a: int, n: int) ->
Optional[int]:
"""Interprets the output of the order finding circuit.
Specifically, it determines s/r such that exp(2 pi is/r) is an
eigenvalue
of the unitary
U|y> = |ay mod n> 0 <= y < n
U|y> = |y>
n <= y
then computes r (by continued fractions) if possible, and returns
it.
Args:
result: trial result obtained by sampling the output of the
circuit built by make_order_finding_circuit
Returns:
r, the order of a modulo n or None.
"""
# Read the output integer of the exponent register
exponent_as_integer = result.data["exponent"][0]
exponent_num_bits = result.measurements["exponent"].shape[1]
eigenphase = float(exponent_as_integer / 2**exponent_num_bits)
# Run the continued fractions algorithm to determine f = s / r
f = fractions.Fraction.from_float(eigenphase).limit_denominator(n)
# If the numerator is zero, the order finder failed
if f.numerator == 0:
return None
# Else, return the denominator if it is valid
r = f.denominator
if a**r % n != 1:
return None
return r
The next code block shows an example of creating an order ﬁnding circuit,
executing it, then using the classical postprocessing function to determine
132
CHAPTER 8
The Canon: Code Walkthroughs
the order. Recall that the quantum part of the algorithm succeeds with some
probability. If the order is None, the code needs to be run again.
"""Example of the classical post-processing."""
# Set n and a here
n = 6
a = 5
print(f"Finding the order of a = {a} modulo n = {n}\n")
measurement = cirq.sample(circuit, repetitions=1)
print("Raw measurements:")
print(measurement)
print("\nInteger in exponent register:")
print(measurement.data)
r = process_measurement(measurement, a, n)
print("\nOrder r =", r)
if r is not None:
print(f"a^r mod n = {a}^{r} mod {n} = {a**r % n}")
The output of a successful run of this code block is shown below:
Finding the order of a = 5 modulo n = 6
Raw measurements:
exponent=1, 0, 0, 0, 0, 0, 0, 0, 0
Integer in exponent register:
exponent
0
256
Order r = 2
a^r mod n = 5^2 mod 6 = 1
Quantum order ﬁnder
We can now deﬁne a streamlined function for the quantum version of order
ﬁnding using the functions we have previously written. The quantum order
ﬁnder below creates the circuit, executes it, and processes the measurement
result.
def quantum_order_finder(a: int, n: int) -> Optional[int]:
"""Computes smallest positive r such that a**r mod n == 1.
Args:
a: integer whose order is to be computed, must be greater than
one
and belong to the multiplicative group of integers modulo n
(which
consists of positive integers relatively prime to n),
n: modulus of the multiplicative group.
Returns:
SECTION 8.5
Shor’s Algorithm
133
Smallest positive integer r such that a**r == 1 mod n or None
if the
algorithm failed. The algorithm fails when the result of the
Quantum
Phase Estimation is inaccurate, zero or a reducible fraction.
Raises:
ValueError when x is 1 or not an element of the multiplicative
group of integers modulo n.
"""
# Check that the integer a is a valid element of the
multiplicative group modulo n
if a < 2 or n <= a or math.gcd(a, n) > 1:
raise ValueError(f’Invalid a={a} for modulus n={n}.’)
# Create the order finding circuit
circuit = make_order_finding_circuit(a, n)
# Sample from the order finding circuit
measurement = cirq.sample(circuit)
# Return the processed measurement result
return process_measurement(measurement, a, n)
This completes our quantum implementation of an order ﬁnder.
The complete factoring algorithm
We can use this quantum order ﬁnder (or the classical order ﬁnder) to complete
Shor’s algorithm. In the following code block, we add a few pre-processing
steps which
1. Check if n is even,
2. Check if n is prime,
3. Check if n is a prime power,
all of which can be done efﬁciently. Additionally, we add the last necessary
post-processing step which uses the order r to compute a non-trivial factor p
of n. This is achieved by computing y D ar=2 mod n (assuming r is even),
then computing p D gcd.y   1; n/.
"""Functions for factoring from start to finish."""
def find_factor_of_prime_power(n: int) -> Optional[int]:
"""Returns non-trivial factor of n if n is a prime power, else
None."""
for k in range(2, math.floor(math.log2(n)) + 1):
c = math.pow(n, 1 / k)
c1 = math.floor(c)
if c1**k == n:
return c1
c2 = math.ceil(c)
if c2**k == n:
return c2
134
CHAPTER 8
The Canon: Code Walkthroughs
return None
def find_factor(
n: int,
order_finder: Callable[[int, int], Optional[int]] =
quantum_order_finder,
max_attempts: int = 30
) -> Optional[int]:
"""Returns a non-trivial factor of composite integer n.
Args:
n: Integer to factor.
order_finder: Function for finding the order of elements of the
multiplicative group of integers modulo n.
max_attempts: number of random a’s to try, also an upper limit
on the number of order_finder invocations.
Returns:
Non-trivial factor of n or None if no such factor was found.
Factor k of n is trivial if it is 1 or n.
"""
# If the number is prime, there are no non-trivial factors
if sympy.isprime(n):
print("n is prime!")
return None
# If the number is even, two is a non-trivial factor
if n % 2 == 0:
return 2
# If n is a prime power, we can find a non-trivial factor
efficiently
c = find_factor_of_prime_power(n)
if c is not None:
return c
for _ in range(max_attempts):
# Choose a random number between 2 and n - 1
a = random.randint(2, n - 1)
# Most likely a and n will be relatively prime
c = math.gcd(a, n)
# If a and n are not relatively prime, we got lucky and found
a non-trivial factor
if 1 < c < n:
return c
# Compute the order r of a modulo n using the order finder
r = order_finder(a, n)
# If the order finder failed, try again
if r is None:
continue
# If the order r is even, try again
SECTION 8.6
Grover’s Search Algorithm
135
if r % 2 != 0:
continue
# Compute the non-trivial factor
y = a**(r // 2) % n
assert 1 < y < n
c = math.gcd(y - 1, n)
if 1 < c < n:
return c
print(f"Failed to find a non-trivial factor in {max_attempts}
attempts.")
return None
The function ﬁnd_factor uses the quantum_order_ﬁnder by default, in
which case it is executing Shor’s algorithm. As previously mentioned, due
to the large memory requirements for classically simulating this circuit, we
cannot run Shor’s algorithm for n  15. However, we can use the classical
order ﬁnder as a substitute.
"""Example of factoring via Shor’s algorithm (order finding)."""
# Number to factor
n = 184573
# Attempt to find a factor
p = find_factor(n, order_finder=classical_order_finder)
q = n // p
print("Factoring n = pq =", n)
print("p =", p)
print("q =", q)
The output of this code block is:
Factoring n = pq = 184573
p = 487
q = 379
We can easily check that n D pq, as claimed.
8.6
Grover’s Search Algorithm
In 1996, Lov Grover demonstrated that we can obtain a quadratic speedup
in algorithmic unstructured search on a quantum computer compared with a
classical one [130]. While this is not exponential speedup, it is still signiﬁcant.
The search problem can be set up as follows. Let say we have a dataset
of n numbers and where there is one number which we are searching for
136
CHAPTER 8
The Canon: Code Walkthroughs
which we will call n. We set up this search challenge as a algorithmic search
protocol which means that the method by which we know we have found the
number we want is to act upon that number with a certain function; if the
output of the function is 1 we have found our number; if the output is 0 we
have not yet found our number.
If we had to search for one unique number n amongst n numbers then
we would have to search on average n
2 times and an upper bound of the worst
case time complexity would be O.n/. However, on a quantum computer we
can achieve a better result; we can ﬁnd our desired unique element with a
time complexity of Opn with Grover’s algorithm. Furthermore, Bennett
et al. showed that any such algorithm which solves an algorithmic search
problem running on a quantum computer would query the oracle at best
O.pn/; Grover’s algorithm is therefore optimal [31].
Grover’s algorithm is more involved than DJ and BV. Here are the steps of
the algorithm:
 Initialize to Uniform Superposition: We initialize the input register
with a state of superposition in which all entries are equally likely. If
we have, for example, four numbers in our dataset we can initialize the
state of our quantum register of two qubits as follows
j i D 1
2 j00i C 1
2 j01i C 1
2 j10i C 1
2 j11i
(If we had eight elements in our dataset we would need three qubits,
etc). As you can see, since the amplitudes are all equal, the probability
of yielding any of the states upon measurement at this point in the
circuit is equal. As we recall, the probability of ﬁnding the state upon
measurement of the superposition of states is given by Born’s rule, i.e.,
the square of the modulus of the amplitude. In this case, since each
amplitude is 1
2, therefore each state has a probability of 1
4 or 25%. So
we must act upon this equal superposition in some way that increases
the amplitude of the state vector which represents our desired number
n while decreasing the other amplitudes and we must do so in a way
that still satisﬁes Born’s rule of normalizing to a total probability of
one. Let us recall that according to Born’s rule, if we have the state
j i D ˛ j0i C ˇ j1i
then the amplitudes ˛ and ˇ must satisfy the following condition
j˛j2 C jˇj2 D 1
SECTION 8.6
Grover’s Search Algorithm
137
 Phase Inversion: In this step we will apply a unitary operator to the
superposition of all states and ﬂip the sign of the amplitude in front of
the state which represents n, the number we are searching for. We call
this operator the oracle since we can consider it as a black box function.
 Inversion About the Mean (or Amplitude Ampliﬁcation): In this
step we apply another unitary operator which takes each amplitude
and adjusts it so that if it was y amount above the mean, now it is y
amount below the mean or vice-versa. We call this operator the Grover
diffusion operator. Recall from the phase inversion step that we inverted
the sign of the amplitude of the state vector associated with n so when
we now invert about the mean, we ﬂip this particular amplitude way
above the mean. To be more rigorous, it ﬂips it
2
pn over the mean. We
then iterate, repeating phase inversion and inversion about the mean so
that this amplitude keeps getting larger, while the other amplitudes get
progressively smaller.
 Measurement: When we measure after a series of iterations, the prob-
ability of collapsing the superposition to our desired result is now quite
high. At most we have to iterate pn times; we now see how Grover’s
algorithm has a worst case time complexity of O.pn/.
As with other algorithms we have examined, we prepare our data input
qubits in state j0i and our output qubit in state j1i. We then apply H to all
data input qubits in order to initialize them in an equal superstition state and
also apply a Hadamard to the output qubit. We then apply the unitary oracle
operator followed by iterations of the diffusion operator. Finally, we perform
the measurement and ﬁnd our answer.
Here is an example circuit diagram of Grover’s algorithm with two input
qubits:
j0i
H
oracle
H
X

X
H
j0i
H
H
X
H
H
X
H
j1i
H
Below are two examples of Grover’s algorithm – the comments in the
code give the reader guidance on which step is being implemented. Note
that in these simple examples, the code only implements one iteration of the
diffusion operator. Once larger quantum computers are available on the cloud,
we will post code examples on the companion online site of more realistic im-
plementations with larger datasets. See also [56] and [57] for a generalization
138
CHAPTER 8
The Canon: Code Walkthroughs
Figure 8.5: Plotting Grover’s algorithm as it closes in on the target
Source: Wikimedia
of Grover’s algo and can be classiﬁed as an amplitude ampliﬁcation algorithm.
See also Yoder, et al. for a dicussion of ﬁxed point quantum search [306]
code source: [81]
"""Grover’s algorithm in Cirq"""
# Imports
import random
import cirq
def set_io_qubits(qubit_count):
"""Add the specified number of input and output qubits."""
input_qubits = [cirq.GridQubit(i, 0) for i in range(qubit_count)]
output_qubit = cirq.GridQubit(qubit_count, 0)
return (input_qubits, output_qubit)
def make_oracle(input_qubits, output_qubit, x_bits):
"""Implement function {f(x) = 1 if x==x’, f(x) = 0 if x!= x’}."""
# Make oracle.
# for (1, 1) it’s just a Toffoli gate
# otherwise negate the zero-bits.
yield(cirq.X(q) for (q, bit) in zip(input_qubits, x_bits) if not
bit)
yield(cirq.TOFFOLI(input_qubits[0], input_qubits[1],
output_qubit))
yield(cirq.X(q) for (q, bit) in zip(input_qubits, x_bits) if not
bit)
def make_grover_circuit(input_qubits, output_qubit, oracle):
SECTION 8.6
Grover’s Search Algorithm
139
"""Find the value recognized by the oracle in sqrt(N) attempts."""
# For 2 input qubits, that means using Grover operator only once.
c = cirq.Circuit()
# Initialize qubits.
c.append([
cirq.X(output_qubit),
cirq.H(output_qubit),
cirq.H.on_each(*input_qubits),
])
# Query oracle.
c.append(oracle)
# Construct Grover operator.
c.append(cirq.H.on_each(*input_qubits))
c.append(cirq.X.on_each(*input_qubits))
c.append(cirq.H.on(input_qubits[1]))
c.append(cirq.CNOT(input_qubits[0], input_qubits[1]))
c.append(cirq.H.on(input_qubits[1]))
c.append(cirq.X.on_each(*input_qubits))
c.append(cirq.H.on_each(*input_qubits))
# Measure the result.
c.append(cirq.measure(*input_qubits, key=’result’))
return c
def bitstring(bits):
return ’’.join(str(int(b)) for b in bits)
def main():
qubit_count = 2
circuit_sample_count = 10
#Set up input and output qubits.
(input_qubits, output_qubit) = set_io_qubits(qubit_count)
#Choose the x’ and make an oracle which can recognize it.
x_bits = [random.randint(0, 1) for _ in range(qubit_count)]
print(’Secret bit sequence: {}’.format(x_bits))
# Make oracle (black box)
oracle = make_oracle(input_qubits, output_qubit, x_bits)
# Embed the oracle into a quantum circuit implementing Grover’s
algorithm.
circuit = make_grover_circuit(input_qubits, output_qubit, oracle)
print(’Circuit:’)
print(circuit)
# Sample from the circuit a couple times.
simulator = cirq.Simulator()
result = simulator.run(circuit, repetitions=circuit_sample_count)
140
CHAPTER 8
The Canon: Code Walkthroughs
frequencies = result.histogram(key=’result’, fold_func=bitstring)
print(’Sampled results:\n{}’.format(frequencies))
# Check if we actually found the secret value.
most_common_bitstring = frequencies.most_common(1)[0][0]
print(’Most common bitstring: {}’.format(most_common_bitstring))
print(’Found a match: {}’.format(
most_common_bitstring == bitstring(x_bits)))
if __name__ == ’__main__’:
main()
We now run the code and obtain this as an example output:
"""
=== EXAMPLE OUTPUT ===
Secret bit sequence: [1, 0]
Sampled results:
Counter({’10’: 10})
Most common bitstring: 10
Found a match: True
"""
Grover’s Algorithm in Qiskit
source: [228]
#initialization
import matplotlib.pyplot as plt
import numpy as np
# importing Qiskit
from qiskit import IBMQ, Aer, assemble, transpile
from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister
from qiskit.providers.ibmq import least_busy
# import basic plot tools
from qiskit.visualization import plot_histogram
n = 2
grover_circuit = QuantumCircuit(n)
def initialize_s(qc, qubits):
"""Apply a H-gate to ’qubits’ in qc"""
for q in qubits:
qc.h(q)
return qc
grover_circuit = initialize_s(grover_circuit, [0,1])
grover_circuit.draw()
grover_circuit.cz(0,1) # Oracle
SECTION 8.6
Grover’s Search Algorithm
 100
p
2
grover_circuit.draw()
# Diffusion operator (U_s)
grover_circuit.h([0,1])
grover_circuit.z([0,1])
grover_circuit.cz(0,1)
grover_circuit.h([0,1])
grover_circuit.draw()
sv_sim = Aer.get_backend(’statevector_simulator’)
qobj = assemble(grover_circuit)
result = sv_sim.run(qobj).result()
statevec = result.get_statevector()
from qiskit_textbook.tools import vector2latex
vector2latex(statevec, pretext="|\\psi\\rangle =")
grover_circuit.measure_all()
qasm_sim = Aer.get_backend(’qasm_simulator’)
qobj = assemble(grover_circuit)
result = qasm_sim.run(qobj).result()
counts = result.get_counts()
plot_histogram(counts)
3-Qubit Grover’s Algo
See [113] for a discussion of this algorithm. source: [228]
qc = QuantumCircuit(3)
qc.cz(0, 2)
qc.cz(1, 2)
oracle_ex3 = qc.to_gate()
oracle_ex3.name = "U$_\omega$"
def diffuser(nqubits):
qc = QuantumCircuit(nqubits)
# Apply transformation |s> -> |00..0> (H-gates)
for qubit in range(nqubits):
qc.h(qubit)
# Apply transformation |00..0> -> |11..1> (X-gates)
for qubit in range(nqubits):
qc.x(qubit)
# Do multi-controlled-Z gate
qc.h(nqubits-1)
qc.mct(list(range(nqubits-1)), nqubits-1) #
multi-controlled-toffoli
qc.h(nqubits-1)
# Apply transformation |11..1> -> |00..0>
for qubit in range(nqubits):
qc.x(qubit)
# Apply transformation |00..0> -> |s>
for qubit in range(nqubits):
qc.h(qubit)
142
CHAPTER 8
The Canon: Code Walkthroughs
# We will return the diffuser as a gate
U_s = qc.to_gate()
U_s.name = "U$_s$"
return U_s
n = 3
grover_circuit = QuantumCircuit(n)
grover_circuit = initialize_s(grover_circuit, [0,1,2])
grover_circuit.append(oracle_ex3, [0,1,2])
grover_circuit.append(diffuser(n), [0,1,2])
grover_circuit.measure_all()
grover_circuit.draw()
qasm_sim = Aer.get_backend(’qasm_simulator’)
transpiled_grover_circuit = transpile(grover_circuit, qasm_sim)
qobj = assemble(transpiled_grover_circuit)
results = qasm_sim.run(qobj).result()
counts = results.get_counts()
plot_histogram(counts)
See [228] for an example of using Grovers’ algorithm for a Sudoku puzzle.
Summary
In this chapter, we have explored the set of canonical quantum algorithms.
These breakthroughs from the 1980’s and 1990’s established the potential for
quantum advantage. While we still do not have the hardware to run Shor’s
and Grover’s algorithms with meaningful scale, they are powerful reminders
of what is to come.
In the next chapter we will cover a range of quantum computing methods
for NISQ regime quantum computers. We will also explore the emerging
understanding of the uniﬁcation of the various algos we discussed in this
chapter. While search, factoring and simulation may seem to be very different
types of algos, we can unify them in a common framework of quantum
singular value transforms (QSVT) which we shall explore in the next chapter.
CHAPTER9
Quantum Computing Methods
In this section we will walk through a range of quantum computing programs
that can be run on NISQ processors. We will cover methods in optimization,
chemistry, machine learning and other areas.
9.1
Variational Quantum Eigensolver
Let us ﬁrst examine a variational quantum eigensolver (VQE) [218]. We
can use a VQE to ﬁnd the eigenvalues of a large matrix that represents the
Hamiltonian of a system. In many cases, we are looking for the lowest
eigenvalue, which represents the ground state energy of the system. We can
also use VQE and VQE-type algorithms to calculate additional eigenvalues,
which represent excited state energies [193, 144]. VQE is a good example
of a hybrid classical/quantum approach to solving a problem (for more on
VQEs see [218, 293, 211, 288, 256]). While the VQE was initially developed
to ﬁnd ground states of Hamiltonians, we can use it to ﬁnd the minimum of
any given objective function that we can express in a quantum circuit. This
broadens the application space signiﬁcantly for this variational method. See
also [67] for a broad review of variational quantum algorithms.
In variational methods, we start with a best guess, or ansatz, for the ground
state. More speciﬁcally we parameterize a quantum state j ./i where  is a
set of parameters. The problem that VQE solves is as follows:
Given a Hamiltonian H, conventionally coming from
a physical system such as molecular hydrogen or
water, approximate the ground state energy (minimum
eigenvalue of H) by solving the following optimization
problem
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_9
143
144
CHAPTER 9
Quantum Computing Methods
min

h ./j H j ./i
(9.1)
By the variational principle of quantum mechanics, the quantity
h ./j H j ./i
can never be smaller than the ground state energy. So, by minimizing this
quantity, we get an approximation of the ground state energy.
In the VQE algorithm, j ./i is prepared on a quantum computer, so the
ansatz is typically developed from parametrized quantum gates; for example,
the rotation gates R.'/ where  is a Pauli operator, as well as other “static”
quantum gates like CNOT or controlled-Z.
For the purposes of VQE, we will assume our Hamiltonian is written as the
sum of tensor products of Pauli operators weighted by constant coefﬁcients as
per [193].
H D
m
X
iD1
ciHi
(9.2)
Note that the tensor products of Pauli operators form a basis for Hermitian ma-
trices, so in principle any Hamiltonian can be expressed in this way. However,
this may lead to a number of terms exponential in the system size. For this
general case, different representations are crucial for limiting the number of
terms in the Hamiltonian and thus limiting the number of resources required
for the quantum algorithm. For the present discussion, we will restrict our
attention to Hamiltonians of the form (9.2) where m grows at most polyno-
mially in the system size — that is, m D O.nk/ — which is a reasonable
assumption for many physical systems of interest.
The VQE algorithm computes expectation values of each term Hi using a
quantum circuit, then adds the total energy classically. The classical optimizer
changes the values of the ansatz wavefunction to minimize the total energy.
Once an approximate minimum is found, the VQE returns the ground state
energy as well as its eigenstate.
Another application of VQEs is error-mitigation. McClean et al. explore
this aspect of VQEs:
Here, we provide evidence for the conjecture that
variational approaches can automatically suppress even
non-systematic decoherence errors by introducing an
exactly solvable channel model of variational state
preparation.
Moreover, we show how variational
quantum-classical approaches ﬁt in a more general
SECTION 9.1
Variational Quantum Eigensolver
145
hierarchy of measurement and classical computation that
allows one to obtain increasingly accurate solutions with
additional classical resources [191].
We recommend the reader explore the use of subspace expansion to achieve
error-mitigation in the growing body of literature on this subject [190].
Below, we show a program implementing VQE for a simple Hamiltonian
using pyQuil and the Grove library [129].1 Let us walk through this program
in steps and explain each part. First, we import the necessary packages and
connect to the quantum virtual machine (QVM).
# Imports
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from pyquil.quil import Program
import pyquil.api as api
from pyquil.paulis import sZ
from pyquil.gates import RX, X, MEASURE
from grove.pyvqe.vqe import VQE
We then set up an ansatz, which in this case is the rotation matrix around
the x-axis with a single parameter.
# Function to create the ansatz
def small_ansatz(params):
"""Returns an ansatz Program with one parameter."""
return Program(RX(params[0], 0))
# Show the ansatz with an example value for the parameter
print("Ansatz with example value for parameter:")
print(small_ansatz([1.0]))
The output of this portion of the program showing the ansatz as a pyQuil
circuit is shown below.
Ansatz with example value for parameter:
RX(1.0) 0
Next, we set up a Hamiltonian; as we stated above, any Hamiltonian can
be expressed as a linear combination of tensor products of Pauli operators, as
these form a basis for Hermitian matrices. In practice, Hamiltonians must ﬁrst
1Note that in this book we show code examples in a range of QC frameworks; check the
book’s online site for code examples in other libraries as one can implement these methods
and algorithms in each of the frameworks.
146
CHAPTER 9
Quantum Computing Methods
be converted to qubit operators so that expectation values can be measured
using the quantum computer. If there are m non-trivial, distinct terms in the
Hamiltonian (9.2), then there are m distinct expectation values to compute.
Each quantum circuit in VQE computes one expectation value, so there are m
distinct quantum circuits to run.
For simplicity of instruction, we consider the simple case of one Pauli
operator H D Z (note that in this section H refers to a Hamiltonian and not
the Hadamard operator). We create an instance of the VQE algorithm using
the VQE class imported from Grove and compute the expectation value for an
example angle in the ansatz.
# Show the ansatz with an example value for the parameter
print("Ansatz with example value for parameter:")
print(small_ansatz([1.0]))
# Create a Hamiltonion H = Z_0
hamiltonian = sZ(0)
# Make an instance of VQE with a Nelder-Mead minimizer
vqe_inst = VQE(minimizer=minimize,
minimizer_kwargs={’method’: ’nelder-mead’})
# Check the VQE manually at a particular angle - say 2.0 radians
angle = 2.0
print("Expectation of Hamiltonian at angle = {}".format(angle))
print(vqe_inst.expectation(small_ansatz([angle]), hamiltonian,
10000, qvm))
To get a picture of the landscape of the optimization problem, we can
sweep over a set of values in the range Œ0; 2/. Here, since we have only one
parameter in our ansatz, this is computationally inexpensive to do. For larger
ansatzes with more parameters, implementing a grid search over all possible
values is not feasible, and so classical optimization algorithms must be used
to ﬁnd an approximate minimum.
# Loop over a range of angles and plot expectation without sampling
angle_range = np.linspace(0.0, 2.0 * np.pi, 20)
exact = [vqe_inst.expectation(small_ansatz([angle]), hamiltonian,
None, qvm)
for angle in angle_range]
# Plot the exact expectation
plt.plot(angle_range, exact, linewidth=2)
# Loop over a range of angles and plot expectation with sampling
sampled = [vqe_inst.expectation(small_ansatz([angle]), hamiltonian,
1000, qvm)
for angle in angle_range]
# Plot the sampled expectation
SECTION 9.1
Variational Quantum Eigensolver
147
Figure 9.1: Expectation value of the simple Hamiltonian H D Z at all angles  2 Œ0; 2/ in
the wavefunction ansatz j ./i D Rx./ j0i
plt.plot(angle_range, sampled, "-o")
# Plotting options
plt.xlabel(’Angle [radians]’)
plt.ylabel(’Expectation value’)
plt.grid()
plt.show()
The plot that this section of the program produces is shown in Figure 9.1.
Here, we can visually see that the minimum energy (in arbitrary units) of the
Hamiltonian appears around the angle  D  in the wavefunction ansatz. As
mentioned, for larger Hamiltonians that require more parameters in the ansatz,
enumerating the expectation values for all angles is not feasible. Instead, an
optimization algorithm must be used to traverse the optimization landscape
and ﬁnd, ideally, the global minima.
An example of the Nelder-Mead optimization algorithm, implemented in
the SciPy Optimize package, is shown below.
# Do the minimization and return the best angle
initial_angle = [0.0]
result = vqe_inst.vqe_run(small_ansatz, hamiltonian, initial_angle,
None, qvm=qvm)
print("\nMinimum energy =", round(result["fun"], 4))
print("Best angle =", round(result["x"][0], 4))
The output for this ﬁnal part of the program is
Minimum energy = -1.0
Best angle = 3.1416
148
CHAPTER 9
Quantum Computing Methods
As can be seen, the optimizer is able to ﬁnd the correct angle  D  for the
global minimum energy E D h j H j i D  1:0 (in arbitrary units).
VQE with Noise
The VQE algorithm is designed to make effective use of near-term quantum
computers. It is therefore important to analyze its performance in a noisy
environment such as a NISQ processor. Above, we used the noiseless QVM
to simulate circuits in VQE. Now, we can consider a QVM with a particular
noise model and run the VQE algorithm again.
A code block setting up a noisy QVM in pyQuil — and demonstrating it
is in fact noisy — is shown below. Note that this program is an extension of
the previous program and assumes all packages are still imported.
# Create a noise model which has a 10% chance of each gate at each
timestep
pauli_channel = [0.1, 0.1, 0.1]
noisy_qvm = api.QVMConnection(gate_noise=pauli_channel)
# Check that the simulator is indeed noisy
p = Program(X(0), MEASURE(0, 0))
res = noisy_qvm.run(p, [0], 10)
print(res)
The example output of this program (measuring the j1i state) demonstrates
that the simulator is indeed noisy — otherwise, we would never see the bit 0
measured!
"Outcome of NOT and MEASURE circuit on noisy simulator:"
[[0], [1], [1], [1], [0], [1], [1], [1], [1], [0]]
Now that we have a noisy simulator, we can run the VQE algorithm under
noise. Here, we modify the classical optimizer to start with a larger simplex
so we don’t get stuck at an initial minimum. Then, we visualize the same
landscape plot (energy vs. angle) as before, but now in the presence of noise.
# Update the minimizer in VQE to start with a larger initial simplex
vqe_inst.minimizer_kwargs = {"method": "Nelder-mead",
"options":
{"initial_simplex": np.array([[0.0],
[0.05]]),
"xatol": 1.0e-2}
}
# Loop over a range of angles and plot expectation with sampling
SECTION 9.1
Variational Quantum Eigensolver
149
Figure 9.2: Results of running VQE on a simulator with Pauli channel noise.
sampled = [vqe_inst.expectation(small_ansatz([angle]), hamiltonian,
1000, noisy_qvm)
for angle in angle_range]
# Plot the sampled expectation
plt.plot(angle_range, sampled, "-o")
# Plotting options
plt.title("VQE on a Noisy Simulator")
plt.xlabel("Angle [radians]")
plt.ylabel("Expectation value")
plt.grid()
plt.show()
An example plot produced by this part of the program is shown in Figure 9.2.
Here, we note that the landscape generally has the same shape but is slightly
distorted. The minimum value of the curve still occurs close to the optimal
value of  D , but the value of the energy here is vertically shifted — the
minimum energy here is approximately  0:6 (in arbitrary units) whereas in
the noiseless case the minimum energy was  1:0.
However, since the minimum value still occurs around  D , VQE
displays some robustness to noise. The optimal parameters can still be found,
and the vertical offset in the minimum energy can be accounted for in classical
postprocessing.
Pauli channel noise is not the only noise model we can consider. In the
book’s online site, this VQE program also demonstrates a noisy simulator
with measurement noise. We ﬁnd that the VQE is robust to measurement
noise in the same sense as above — the landscape curve has the same general
150
CHAPTER 9
Quantum Computing Methods
shape and the minimum value occurs again near  D . Our focus here is
on VQE, but further background on noisy quantum computations can allow
one to rigorously prove theorems about the noise resilience of variational
quantum algorithms. Topics related to noise such as Kraus operators (a
standard representation of noisy quantum channels) and puriﬁcation (writing
a mixed state  as the partial trace of a pure state j i over the environment,
 D TrEj ih j) can be found in additional resources such as [102, 206, 238].
For another approach to VQE, see [110].
More Sophisticated Ansatzes
As mentioned, larger Hamiltonians may require an ansatz with more parame-
ters to more closely approximate the ground state wavefunction. In pyQuil,
we can increase the number of parameters in our program by adding more
gates as follows:
# Function for an anstaz with two parameters
def smallish_ansatz(params):
"""Returns an ansatz with two parameters."""
return Program(RX(params[0], 0), RZ(params[1], 0))
print("Ansatz with two gates and two parameters (with example
values):")
print(smallish_ansatz([1.0, 2.0]))
# Get a VQE instance
vqe_inst = VQE(minimizer=minimize, minimizer_kwargs={’method’:
’nelder-mead’})
# Do the minimization and return the best angle
initial_angles = [1.0, 1.0]
result = vqe_inst.vqe_run(smallish_ansatz, hamiltonian,
initial_angles, None, qvm=qvm)
print("\nMinimum energy =", round(result["fun"], 4))
print("Best angle =", round(result["x"][0], 4))
In the program above, we create an ansatz with two gates and two parameters,
print it out, then run the VQE algorithm with this ansatz. An example outcome
of the program is
"Ansatz with two gates and two parameters (with example values):"
RX(1.0) 0
RZ(2.0) 0
Minimum energy = -1.0
Best angle = 3.1416
SECTION 9.2
Quantum Chemistry
151
Here, we see that the minimizer is able to ﬁnd the exact ground state energy
with the new ansatz. This is expected — since we know we can minimize the
expectation of the Hamiltonian with only one Rx gate, the second Rz gate is
superﬂuous. For larger, non-trivial Hamiltonians, however, this may not be
the case, and more parameters may be needed.
In this section, we use simple trial ansatzes for clarity of presentation. In
general, choosing both an appropriate ansatz and good initial starting point for
the parameters of the ansatz are critical for successful VQE implementations.
Randomly generated ansatzes are likely to have gradients that vanish for large
circuit sizes [189], thus making the optimization over parameters exceedingly
difﬁcult, if not practically impossible. For these reasons, structured ansatzes
such as unitary coupled cluster or QAOA (see Section 9.3) — as opposed to
parameterized random quantum circuits — are used in practice.
9.2
Quantum Chemistry
We will now explore an application of quantum chemistry, or more gener-
ally quantum simulation.2 Early work on quantum simulation was done by
Lloyd [174], and more recent developments include [37, 177, 21, 265]. In
quantum simulation we seek to model the dynamic evolution of the wavefunc-
tion under some Hamiltonian H as per Schrödinger’s equation
i @ j i
@t
D H j i
(9.3)
where we have set ℏD 1. It is easy to write the time evolution operator
U.t/ D exp . iHt/
(9.4)
which evolves the initial state j .0/i to a ﬁnal state at time t via
j .t/i D U.t/j .0/i
(9.5)
However, it is generally very difﬁcult to classically compute the unitary time
evolution operator U.t/ by exponentiating the Hamiltonian of the system, even
if the Hamiltonian is sparse. The problem is so important that many techniques
for approximating the time evolution have emerged, such as the Suzuki-Trotter
formula [206], also referred to as Trotterization. Techniques also emerged
2Note: this use of the term "quantum simulation" is distinct from the use of a program to
simulate the actions of a quantum computer as discussed in chapter 6.
152
CHAPTER 9
Quantum Computing Methods
to represent the Hamiltonian in a quantum computer such as qubitization
(see [178]). Quantum simulation on a QC give us the opportunity to more
easily compute unitary time evolution. Quantum simulation is useful in the
same respect that classical simulation of time-dependent processes is useful.
Namely, it allows us to analyze the behavior of a complex physical system,
compute observable properties, and use both of these to make new predictions
or compare them with experimental results. As an example, O’Malley et al.
demonstrated the use of VQE and quantum simulation with QPE to calculate
the potential energy surface of molecular hydrogen [211].
Quantum simulation of molecular Hamiltonians is useful for quantum
chemistry applications. In the following program, we use Cirq in conjuction
with OpenFermion — an open-source package for quantum chemistry that has
integration with Cirq [192]3 — to show how we can simulate the evolution
of an initial state under a Hamiltonian. In the sample code used here for
pedagogic purposes we randomly generate the Hamiltonian and its initial
state; we recommend against this in real world conditions for reasons outlined
in McClean et al. [189]. See also [146] for a broader discussion of running
quantum chemistry models on NISQ computers and [127] for a discussion of
Hartree-Fock on a NISQ superconducting quantum computer.
We now walk through the program in steps. First, we import the necessary
packages and deﬁne a few constants for our simulation. Namely, we deﬁne
the number of qubits n, the ﬁnal simulation time t, and a seed for the random
number generator which allows for reproducible results.
# Imports
import numpy
import scipy
import cirq
import openfermion
import openfermioncirq
# Set the number of qubits, simulation time, and seed for
reproducibility
n_qubits = 3
simulation_time = 1.0
random_seed = 8317
In the code block below, we generate a random Hamiltonian in matrix
form. In order to run any quantum circuits with this Hamiltonian, it ﬁrst must
be written in terms of quantum operators. The next few lines of code use the
functionality in OpenFermion to do this.
3Note that the package OpenFermion-Cirq is used as a bridge between OpenFermion and
Cirq.
SECTION 9.2
Quantum Chemistry
153
# Generate the random one-body operator
T = openfermion.random_hermitian_matrix(n_qubits, seed=random_seed)
print("Hamiltonian:", T, sep="\n")
# Compute the OpenFermion "FermionOperator" form of the Hamiltonian
H = openfermion.FermionOperator()
for p in range(n_qubits):
for q in range(n_qubits):
term = ((p, 1), (q, 0))
H += openfermion.FermionOperator(term, T[p, q])
print("\nFermion operator:")
print(H)
The output of this portion of the program is
Hamiltonian:
[[ 0.53672126+0.j
-0.26033703+3.32591737j 1.34336037+1.54498725j]
[-0.26033703-3.32591737j -2.91433037+0.j -1.52843836+1.35274868j]
[ 1.34336037-1.54498725j -1.52843836-1.35274868j 2.26163363+0.j ]]
Fermion operator:
(0.5367212624097257+0j) [0^ 0] +
(-0.26033703159240107+3.3259173741375454j) [0^ 1] +
(1.3433603748462144+1.544987250567917j) [0^ 2] +
(-0.26033703159240107-3.3259173741375454j) [1^ 0] +
(-2.9143303700812435+0j) [1^ 1] +
(-1.52843836446248+1.3527486791390022j) [1^ 2] +
(1.3433603748462144-1.544987250567917j) [2^ 0] +
(-1.52843836446248-1.3527486791390022j) [2^ 1] +
(2.261633626116526+0j) [2^ 2]
The ﬁrst section displays the Hamiltonian in matrix form, then the next section
displays the matrix in OpenFermion operator form. Here, the OpenFermion
notation [p^ q] is used to indicate the product of fermionic creation and
annihilation operators a
paq on sites p and q, respectively, which satisfy the
canonical commutation relations
fa
p; aqg D ıpq
(9.6)
fap; aqg D 0
(9.7)
Now that we have our Hamiltonian in a usable form, we can begin con-
structing our circuit. As is common in quantum simulation algorithms [206],
we ﬁrst rotate to the eigenbasis of the Hamiltonian. This is done by (classi-
cally) diagonalizing the Hamiltonian, then using OpenFermion to construct a
circuit that performs this basis transformation.
# Diagonalize T and obtain basis transformation matrix (aka "u")
eigenvalues, eigenvectors = numpy.linalg.eigh(T)
154
CHAPTER 9
Quantum Computing Methods
basis_transformation_matrix = eigenvectors.transpose()
# Initialize the qubit register
qubits = cirq.LineQubit.range(n_qubits)
# Rotate to the eigenbasis
inverse_basis_rotation = cirq.inverse(
openfermioncirq.bogoliubov_transform(qubits,
basis_transformation_matrix)
)
circuit = cirq.Circuit.from_ops(inverse_basis_rotation)
Now we can add the gates corresponding to evolution of the Hamiltonian.
Since we are in the eigenbasis of the Hamiltonian, this corresponds to a
diagonal operator of Pauli-Z rotations, where the rotation angle is proportional
to the eigenvalue and ﬁnal simulation time. Finally, we change bases back to
the computational basis.
# Add diagonal phase rotations to circuit
for k, eigenvalue in enumerate(eigenvalues):
phase = -eigenvalue * simulation_time
circuit.append(cirq.Rz(rads=phase).on(qubits[k]))
# Finally, change back to the computational basis
basis_rotation = openfermioncirq.bogoliubov_transform(
qubits, basis_transformation_matrix
)
circuit.append(basis_rotation)
The time evolution operator is now constructed in our quantum circuit.
Below, we ﬁrst obtain a random initial state. Note that this is program is for
demonstration purposes. In real world scenarios we will want to use a number
of non-random techniques to determine the initial state:
# Initialize a random initial state
initial_state = openfermion.haar_random_vector(
2 ** n_qubits, random_seed).astype(numpy.complex64)
Now we compute the time evolution numerically using matrix exponentia-
tion and then simulate it with a QC simulator. After obtaining the ﬁnal state
using both methods, we compute the ﬁdelity (overlap squared) of the two and
print out the value.
# Numerically compute the correct circuit output
hamiltonian_sparse = openfermion.get_sparse_operator(H)
exact_state = scipy.sparse.linalg.expm_multiply(
-1j * simulation_time * hamiltonian_sparse, initial_state
)
SECTION 9.2
Quantum Chemistry
155
# Use Cirq simulator to apply circuit
simulator = cirq.google.XmonSimulator()
result = simulator.simulate(circuit, qubit_order=qubits,
initial_state=initial_state)
simulated_state = result.final_state
# Print final fidelity
fidelity = abs(numpy.dot(simulated_state,
numpy.conjugate(exact_state)))**2
print("\nfidelity =", round(fidelity, 4))
The output of this section of the code
fidelity = 1.0
indicates that our quantum circuit evolved the initial state exactly the same as
the analytic evolution!
Of course, for larger systems the analytic evolution cannot be computed,
and we have to rely solely on a quantum computer. This small proof of
principle calculation indicates the validity of this method.
Lastly, we mention that Cirq has the functionality to compile this quantum
circuit for Google’s Xmon architecture quantum computers, as well as IBM’s
quantum computers. The code snippet below shows how this is done:
# Compile the circuit to Google’s Xmon architecture
xmon_circuit = cirq.google.optimized_for_xmon(circuit)
print("\nCircuit optimized for Xmon:")
print(xmon_circuit)
# Print out the OpenQASM code for IBM’s hardware
print("\nOpenQASM code:")
print(xmon_circuit.to_qasm())
Below, we include the OpenQASM code generated by Cirq for this circuit.
The complete circuit diagram and remaining output of this code can be seen
by executing this program, which can be found on the book’s GitHub site.
// Generated from Cirq v0.4.0
OPENQASM 2.0;
include "qelib1.inc";
// Qubits: [0, 1, 2]
qreg q[3];
u2(pi*-1.0118505646, pi*1.0118505646) q[2];
u2(pi*-1.25, pi*1.25) q[1];
u2(pi*-1.25, pi*1.25) q[0];
156
CHAPTER 9
Quantum Computing Methods
cz q[1],q[2];
u3(pi*-0.1242949803, pi*-0.0118505646, pi*0.0118505646) q[2];
u3(pi*0.1242949803, pi*-0.25, pi*0.25) q[1];
cz q[1],q[2];
u3(pi*-0.3358296941, pi*0.4881494354, pi*-0.4881494354) q[2];
u3(pi*-0.5219350773, pi*1.25, pi*-1.25) q[1];
cz q[0],q[1];
u3(pi*-0.328242091, pi*0.75, pi*-0.75) q[1];
u3(pi*-0.328242091, pi*-0.25, pi*0.25) q[0];
cz q[0],q[1];
u3(pi*-0.2976584908, pi*0.25, pi*-0.25) q[1];
u3(pi*-0.7937864503, pi*0.25, pi*-0.25) q[0];
cz q[1],q[2];
u3(pi*-0.2326621647, pi*-0.0118505646, pi*0.0118505646) q[2];
u3(pi*0.2326621647, pi*-0.25, pi*0.25) q[1];
cz q[1],q[2];
u3(pi*0.8822298425, pi*0.4881494354, pi*-0.4881494354) q[2];
u3(pi*-0.2826706001, pi*0.25, pi*-0.25) q[1];
cz q[0],q[1];
u3(pi*-0.328242091, pi*0.75, pi*-0.75) q[1];
u3(pi*-0.328242091, pi*-0.25, pi*0.25) q[0];
cz q[0],q[1];
u3(pi*-0.3570821075, pi*0.25, pi*-0.25) q[1];
u2(pi*-0.25, pi*0.25) q[0];
rz(pi*0.676494835) q[0];
cz q[1],q[2];
u3(pi*0.1242949803, pi*0.9881494354, pi*-0.9881494354) q[2];
u3(pi*-0.1242949803, pi*0.75, pi*-0.75) q[1];
cz q[1],q[2];
u2(pi*-0.0118505646, pi*0.0118505646) q[2];
u2(pi*-0.25, pi*0.25) q[1];
rz(pi*-0.4883581348) q[1];
rz(pi*0.5116418652) q[2];
9.3
Quantum Approximate Optimization
Algorithm (QAOA)
While the previous two quantum computing methods were geared towards
physics and chemistry applications, the quantum approximate optimization
algorithm (QAOA) is geared towards general optimization problems. Farhi
et al. introduced QAOA to handle these kinds of problems [104, 105]. See
also [106] for work on adiabatic algorithms which preceded QAOA. Here, the
goal is to maximize or minimize a cost function
C.b/ D
m
X
˛D1
C˛.b/
(9.8)
SECTION 9.3
Quantum Approximate Optimization Algorithm (QAOA)
157
written as a sum of m clauses C˛.b/ on bitstrings b 2 f0; 1gn, or equivalently
spins zi 2 f 1; C1gn as there is a bijective map between the bitstrings and
spins.4
MaxCut is an example of a problem for which we can use QAOA on a
regular graph [104]; the cost function can be written in terms of spins as
C.z/ D 1
2
X
hi;ji
.1   zizj /
(9.9)
Here, the sum is over edges hi; ji in a graph, and each clause .1   zizj /
contributes a non-zero term to the cost iff the spins zi and zj are anti-aligned
(i.e., have different values). A more general case of this problem considers
arbitrary weights wij between each edge [309]. This amounts to saying that
clauses with larger weights wij contribute a higher cost. We’ll consider this
case in the text that follows.
By promoting each spin to a Pauli-Z operator (which has eigenvalues ˙1),
the cost can be written as a cost Hamiltonian
C  HC D 1
2
X
hi;ji
wij .I   .i/
z .j /
z
/
(9.10)
where I is the identity operator and .i/
z
denotes a Pauli-Z operator on the
ith spin. This cost Hamiltonian can easily be seen to be diagonal in the
computational basis. This is the general input to the quantum approximate
optimization algorithm, as we discuss below.
The prescription for QAOA is as follows. Given a cost Hamiltonian
HC  H of the form (9.8), deﬁne the unitary operator
U.HC ; 
/ WD e i
HC D
m
Y
˛D1
e i
C˛
(9.11)
which depends on the parameter 
. Note that the second line follows because
each clause C˛ is diagonal in the computational basis, hence ŒC˛; Cˇ D 0
for all ˛; ˇ 2 f1; :::; mg. Further note that this can be interpreted, in light of
the previous section, as simulating (i.e., evolving with) the cost Hamiltonian
HC for a time 
. We can restrict the “time” to be between 0 and 2, however,
since C has integer values. Thus, we can equally think of the parameter 
 as
an angle of rotation.
4Bits b and spins z are related by the bijective mapping z D 1   2b () b D .1   z/=2
Thus, any problem on bits can be framed as a problem on spins and vice versa.
158
CHAPTER 9
Quantum Computing Methods
Next, deﬁne the operator B  HB, known as a mixer Hamiltonian, which
is conventionally taken to be
B  HB D
n
X
j D1
.j /
x
(9.12)
where .j /
x
is a Pauli-X operator on spin j. From this, we form the unitary
operator
U.HB; ˇ/ WD e iˇB D
n
Y
j D1
e iˇ.j/
x
(9.13)
Note that the second equality follows because all terms in the Hamiltonian
commute with one another. We can view the term e iˇ.j/
x
as a rotation about
the x axis on spin j by angle 2ˇ. Thus, we can restrict 0  ˇ < .
With these deﬁnitions, we can state the steps of the quantum part of the
QAOA:
1. Start with an initial state that is an equal superposition over all bitstrings
(spins)
jsi D
1
p
2n
X
b2f0;1gn
jbi
(9.14)
by applying a Hadamard to each qubit H ˝nj0i˝n.
2. Evolve with the cost Hamiltonian by implementing U.HC ; 
/ for an
angle 
.
3. Evolve with the mixer Hamiltonian by implementing U.HB; ˇ/ for an
angle ˇ.
4. Repeat steps (2) and (3) p times with different parameters 
i; ˇi at each
step i D 1; :::; p to form the state
j
; ˇi WD
p
Y
iD1
U.HB; ˇi/U.HC ; 
i/jsi
(9.15)
5. Measure in the computational basis to compute the expectation of HC
in this state:
Fp.
; ˇ/ WD h
; ˇjHC j
; ˇi
(9.16)
6. Use a (classical) optimization algorithm to (approximately) compute
the maximum or minimum value of Fp.
; ˇ/. Alternatively, if you have
other methods to determine the optimal angles, you may use these.
SECTION 9.3
Quantum Approximate Optimization Algorithm (QAOA)
159
7. Sample from the output distribution of the circuit (9.15) to get a set
of bitstrings b. The most probable bitstrings encode the approximate
optima for the cost function.
The full circuit diagram for the quantum circuit in the QAOA is shown below.
j0i
H
ei
1HC
eiˇ1X
  
ei
pHC
eiˇpX
j0i
H
eiˇ1X
  
eiˇpX
j0i
H
eiˇ1X
  
eiˇpX
j0i
H
eiˇ1X
  
eiˇpX
The adiabatic theorem states that a system remains in its eigenstate even
when subject to a perturbation, as long as that perturbation is slow and gradual
enough and there is a gap between the eigenvalue of that state and the rest
of the eigenvalues of the system (its spectrum) [48, 49, 148]. In other words
if we have a system in a measured state and that state has enough of gap
from other possible states of the system, then if we perturb the system slowly
enough, it will not jump to another eigenstate. It can be shown using the
adiabatic theorem that
lim
p!1 max

;ˇ Fp.
; ˇ/ D max
b
C.b/:
(9.17)
That is, given enough parameters 
; ˇ, we can be sure that the exact solution of
the problem is attainable. The parameter p can thus be considered a hyperpa-
rameter. One form of approximation in the quantum approximate optimization
algorithm is the ﬁnite cutoff for p. Another form of approximation is the
ability of the classical optimizer to ﬁnd the optimum.
However, in particular cases there are provable performance guarantees for
p D 1 layers. For example, for p D 1 on 3-regular graphs, the QAOA always
ﬁnds a cut that is at least 0.6924 times the size of the optimal cut [104]. Prov-
ing more worst-case or average-case performance guarantees is an interesting
line of research on the analytic side of QAOA, and developing better classical
optimization algorithms is an interesting area on the heuristic side of QAOA.
Example Implementation of QAOA
To get a better idea of how QAOA works, we now turn to an implementa-
tion. In this example, we consider the transverse ﬁeld Ising model as a cost
Hamiltonian:
160
CHAPTER 9
Quantum Computing Methods
HC D  X
hi;ji
Jij .i/
z .j /
z
 X
i
hi.i/
x
(9.18)
For simplicity of presentation, we set the transverse ﬁeld coefﬁcients to
zero (hi D 0) and set each interaction coefﬁcient to one (Jij D 1). The
Hamiltonian can be modiﬁed in a straightforward way to generalize it, but
these details are not important in a ﬁrst encounter with QAOA. Another
reason for this is that this system is trivial to solve analytically — thus, we
can compare the solution found by QAOA to the exact solution. By making
these simpliﬁcations, our cost Hamiltonian has the form
HC D  X
hi;j i
.i/
z .j /
z
(9.19)
The graph (i.e., the arrangement of spins) we will consider is in a nearest
neighbors conﬁguration on a 2D grid. We thus need a way to implement the
unitary operator
U.HC ; 
/ WD e iHC 
 D
Y
hi;ji
ei
ZiZj
(9.20)
For simplicity, from here on we will substitute Zi for .i/
z . In order to
implement this entire unitary, we need a sequence of gates for implementing
each ei
ZiZj term, where i and j are neighbors in the graph. It will be
convenient to rescale 
 and consider implementing the unitary ei
ZiZj . To
understand how to do this, note that the operator Z ˝ Z is diagonal in the
computational basis, hence ei
Z˝Z is just the exponential of each diagonal
element (multiplied by i
)
exp.i
Z ˝ Z/ D
2
664
ei
0
0
0
0
e i
0
0
0
0
e i
0
0
0
0
ei
3
775
(9.21)
To get some intuition about how to implement this operator in terms of
standard gates, note that the controlled-Z gate is diagonal with C.Z/ D
diag.1; 1; 1;  1/. Writing  1 D ei, we see that C.Z/ D diag.1; 1; 1; ei/,
hence
C.Z
/ D
2
664
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
ei
3
775
(9.22)
SECTION 9.3
Quantum Approximate Optimization Algorithm (QAOA)
161
This gives us one diagonal term in the ﬁnal unitary (9.21) that we want
to implement. To get the other terms, we can apply X operators on the
appropriate qubits. For example,
.I ˝ X/
2
664
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
ei
3
775 .I ˝ X/ D
2
664
1
0
0
0
0
1
0
0
0
0
ei
0
0
0
0
1
3
775
(9.23)
We can continue in this fashion to get all four diagonal elements, then get the
full unitary (9.21) by simply multiplying them together (using the fact that
the product of diagonal matrices is diagonal).
In the Cirq code below, we write a function which gives us a circuit for
implementing the unitary e i
ZiZj . We then test our function by printing
out the circuit for an example set of qubits i and j with an arbitrary value of

 and ensuring that the unitary matrix of this circuit is what we expect.
# Imports
import numpy as np
import matplotlib.pyplot as plt
import cirq
# Function to implement a ZZ gate on qubits a, b with angle gamma
def ZZ(a, b, gamma):
"""Returns a circuit implementing exp(-i \pi \gamma Z_i Z_j)."""
# Get a circuit
circuit = cirq.Circuit()
# Gives the fourth diagonal component
circuit.append(cirq.CZ(a, b)**gamma)
# Gives the third diagonal component
circuit.append([cirq.X(b), cirq.CZ(a,b)**(-1 * gamma), cirq.X(b)])
# Gives the second diagonal component
circuit.append([cirq.X(a), cirq.CZ(a,b)**-gamma, cirq.X(a)])
# Gives the first diagonal component
circuit.append([cirq.X(a), cirq.X(b), cirq.CZ(a,b)**gamma,
cirq.X(a), cirq.X(b)])
return circuit
#26.s.one
# Make sure the circuit gives the correct matrix
qreg = cirq.LineQubit.range(2)
zzcirc = ZZ(qreg[0], qreg[1], 0.5)
print("Circuit for ZZ gate:", zzcirc, sep="\n")
print("\nUnitary of circuit:", zzcirc.to_unitary_matrix().round(2),
sep="\n")
162
CHAPTER 9
Quantum Computing Methods
The output of this code is as follows:
Circuit for ZZ gate:
0: ---@-----------@------------X---@--------X---X---@-------X---
|
|
|
|
1: ---@^0.5---X---@^-0.5---X-------@^-0.5-------X---@^0.5---X---
Unitary of circuit:
[[0.+1.j 0.+0.j 0.+0.j 0.+0.j]
[0.+0.j 0.-1.j 0.+0.j 0.+0.j]
[0.+0.j 0.+0.j 0.-1.j 0.+0.j]
[0.+0.j 0.+0.j 0.+0.j 0.+1.j]]
As we can see by comparing with (9.21), this circuit indeed implements the
desired unitary operator. Note that the circuit is not optimal — a trivial simpli-
ﬁcation is removing sequential X operators on qubit 0, and other optimizations
are possible. Such optimizations will not concern us here, however.
In the next block of code, we deﬁne a 2x2 grid of qubits.
ncols = 2
nrows = 2
qreg = [[cirq.GridQubit(i,j) for j in range(ncols)] for i in
range(nrows)]
Then we write functions for implementing the operators U.HC ; 
/ and
U.HB; ˇ/.
# Function to implement the cost Hamiltonian
def cost_circuit(gamma):
"""Returns a circuit for the cost Hamiltonian."""
circ = cirq.Circuit()
for i in range(nrows):
for j in range(ncols):
if i < nrows - 1:
circ += ZZ(qreg[i][j], qreg[i + 1][j], gamma)
if j < ncols - 1:
circ += ZZ(qreg[i][j], qreg[i][j + 1], gamma)
return circ
# Function to implement the mixer Hamiltonian
def mixer(beta):
"""Generator for U(H_B, beta) layer (mixing layer)"""
for row in qreg:
for qubit in row:
yield cirq.X(qubit)**beta
These functions allow us to construct the entire QAOA circuit. The func-
tion below builds this circuit for an arbitrary number p of parameters.
# Function to build the QAOA circuit
def qaoa(gammas, betas):
SECTION 9.3
Quantum Approximate Optimization Algorithm (QAOA)
163
"""Returns a QAOA circuit."""
circ = cirq.Circuit()
circ.append(cirq.H.on_each(*[q for row in qreg for q in row]))
for i in range(len(gammas)):
circ += cost_circuit(gammas[i])
circ.append(mixer(betas[i]))
return circ
Now that we can build our QAOA circuit for a given set of parameters, we
can compute the expectation of the cost Hamiltonian in the ﬁnal state (9.16).
For simplicity we use Cirq’s ability to access the wavefunction to compute
this expectation rather than sampling from the circuit itself. The following
function shows how we can access the wavefunction after applying a circuit:
def simulate(circ):
"""Returns the wavefunction after applying the circuit."""
sim = cirq.Simulator()
return sim.simulate(circ).final_state
The next function evaluates the expectation using the wavefunction:
def energy_from_wavefunction(wf):
"""Computes the energy-per-site of the Ising Model from the
wavefunction."""
# Z is a (n_sites x 2**n_sites) array. Each row consists of the
# 2**n_sites non-zero entries in the operator that is the Pauli-Z
matrix on
# one of the qubits times the identites on the other qubits. The
(i*n_cols + j)th
# row corresponds to qubit (i,j).
Z = np.array([(-1)**(np.arange(2**nsites) >> i)
for i in range(nsites-1,-1,-1)])
# Create the operator corresponding to the interaction energy
summed over all
# nearest-neighbor pairs of qubits
ZZ_filter = np.zeros_like(wf, dtype=float)
for i in range(nrows):
for j in range(ncols):
if i < nrows-1:
ZZ_filter += Z[i*ncols + j]*Z[(i+1)*ncols + j]
if j < ncols-1:
ZZ_filter += Z[i*ncols + j]*Z[i*ncols + (j+1)]
# Expectation value of the energy divided by the number of sites
return -np.sum(np.abs(wf)**2 * ZZ_filter) / nsites
Finally, for convenience, we deﬁne a function that computes the energy/-
cost directly from a set of parameters. This function uses the parameters to
164
CHAPTER 9
Quantum Computing Methods
build a circuit, then gets the wavefunction of the ﬁnal state and lastly computes
the energy/cost using the previous function.
def cost(gammas, betas):
"""Returns the cost function of the problem."""
wavefunction = simulate(qaoa(gammas, betas))
return energy_from_wavefunction(wavefunction)
These functions provide the set up for QAOA, and we could now optimize the
parameters to minimize the cost. For instructional purposes, we implement
QAOA with p D 1 layers and perform a grid search, plotting the 2D cost
landscape for each parameter 
 and ˇ. The function for the grid search over a
range of parameters is given below:
def grid_search(gammavals, betavals):
"""Does a grid search over all parameter values."""
costmat = np.zeros((len(gammavals), len(betavals)))
for (i, gamma) in enumerate(gammavals):
for (j, beta) in enumerate(betavals):
costmat[i, j] = cost([gamma], [beta])
return costmat
Finally, here is the code for using this function within the main script and
plotting the cost landscape:
# Get a range of parameters
gammavals = np.linspace(0, 1.0, 50)
betavals = np.linspace(0, np.pi, 75)
# Compute the cost at all parameter values using a grid search
costmat = grid_search(gammavals, betavals)
# Plot the cost landscape
plt.imshow(costmat, extent=(0, 1, 0, np.pi), origin="lower",
aspect="auto")
plt.colorbar()
plt.show()
The output of this section of the program is shown in Figure 9.3. As
we can see, there is a signiﬁcant amount of symmetry in the cost landscape.
This phenomena is typical in variational quantum algorithms. Apart from the
symmetry which arises naturally from the Ising Hamiltonian, the symmetric
and periodic form of the cost landscape arises from symmetries in the ansatz
circuit. Exploiting these symmetries can help lead classical optimization
algorithms to a good solution more quickly.
SECTION 9.3
Quantum Approximate Optimization Algorithm (QAOA)
165
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.4
0.2
0.0
0.2
0.4
Figure 9.3: Cost landscape of the Ising Hamiltonian computed from one layer of QAOA
We can now obtain a set of optimal parameters by taking the coordinates
of a minimum in our cost landscape. The following short block of code does
this and prints out the numerical value of the cost at these parameters.
# Coordinates from the grid of cost values
gamma_coord, beta_coord = np.where(costmat == np.min(costmat))
# Values from the coordinates
gamma_opt = gammavals[gamma_coord[0]]
beta_opt = betavals[beta_coord[0]]
Now that we have the optimal parameters, we can run the QAOA circuit with
these parameters and measure in the computational basis to get bitstrings that
solve our original optimization problem. The function below runs the circuit
and returns the measurement results.
def get_bit_strings(gammas, betas, nreps=10000):
"""Measures the QAOA circuit in the computational basis to get
bitstrings."""
circ = qaoa(gammas, betas)
circ.append(cirq.measure(*[qubit for row in qreg for qubit in
row], key=’m’))
# Simulate the circuit
sim = cirq.Simulator()
res = sim.run(circ, repetitions=nreps)
return res
166
CHAPTER 9
Quantum Computing Methods
Finally, we use this function to sample from the circuit at the optimal pa-
rameters found above. Then, we parse the output and print out the two most
common bitstrings sampled from the circuit.
# Sample to get bits and convert to a histogram
bits = get_bit_strings([gamma_opt], [beta_opt])
hist = bits.histogram(key="m")
# Get the most common bits
top = hist.most_common(2)
# Print out the two most common bitstrings measured
print("\nMost common bitstring:")
print(format(top[0][0], "#010b"))
print("\nSecond most common bitstring:")
print(bin(top[1][0]))
A sample output of this portion of the code follows:
Most common bitstring:
0b000000000
Second most common bitstring:
0b111111111
These bitstrings are exactly the ones we would expect to minimize our cost
function! Recall the Ising Hamiltonian we considered, which when written
classically has the form
C.z/ D  X
hi;j i
zizj
(9.24)
where zi D ˙1 are spins. For our bitstring output, b D 0 corresponds to spin
up (z D 1) and b D 1 corresponds to spin down (z D  1). Since opposite
spins zi ¤ zj will produce a term with a positive contribution (don’t forget
the overall minus sign in front!) in the sum, the minimum value of the cost
function occurs when all spins are aligned. That is, zi D zj for all i; j. The
bitstrings that we measured correspond to all spins aligned down or all spins
aligned up, respectively. Thus, these bitstrings indeed produced the minimum
value for our cost function, and QAOA was able to successfully optimize the
cost.
For larger optimization problems with more complex cost functions, more
layers in the QAOA ansatz (i.e., p > 1) may become necessary. More layers
means more parameters in the variational quantum circuit, which leads to a
harder optimization problem. Such an optimization problem could not be
solved by a mere grid search over values, as this quickly becomes intractable.
SECTION 9.4
Machine Learning on Quantum Processors
167
Figure 9.4: Data types and processor types
Rather, gradient-based or gradient-free optimization algorithms must be used
to compute an approximately optimal set of parameters.
The complete program for this implementation of QAOA is available on
the book’s online site.
9.4
Machine Learning on Quantum
Processors
Several groups are exploring the use of QC for machine learning; it is natural to
ask whether QC affords us any advantage in this area. Speedup is not the only
advantage we should consider in quantum machine learning (QML). There
may be opportunities to use a QC to process data directly from a quantum
sensor that retains the full range of quantum information from that sensor.
Figure 9.4 points to the potential of matching quantum data with quantum
processing. Having a classiﬁer on the QC directly analyzing the datastream
for patterns may be better than piping the data to a classical computer.
A number of groups have published in this area, including:
1. Alan Aspuru-Guzik and colleagues have explored quantum machine
learning as well as hybrid classical-quantum models [64, 241].
168
CHAPTER 9
Quantum Computing Methods
2. The Rigetti team has worked on unsupervised machine learning on a
classical-quantum hybrid approach [212].
3. Farhi and Neven laid out an approach to classiﬁcation with neural
networks on a quantum processor (QNNs) [109].
4. Wittek and Gogolin explored Markov logic networks on quantum plat-
forms [299].
5. See [251] for a discussion of supervised learning on quantum comput-
ers.
6. See [39, 281, 282] for additional work in QML and [298] for an online
course in QML.
7. See [251] for a discussion of supervised learning on quantum comput-
ers.
8. See [7] for a review of quantum neural networks.
A standard benchmark problem in machine learning is MNIST classiﬁca-
tion. Here, we show how this can be done with a quantum machine learning
model. To do so, we follow an example in TensorFlow-Quantum (TFQ), a
library which allows machine learning practitioners to add quantum circuits
as layers in neural networks created with TensorFlow. Using TFQ to illustrate,
we show how to:
 Encode the data in a quantum state
 Process the data through a quantum circuit (quantum neural network)
 calculate a loss function
 minimize the loss function to accurately classify images
Our ﬁrst task is to encode the (classical) MNIST image data in a quantum
state. For completeness, we show the packages we will use below.
# Imports
import tensorflow as tf
import tensorflow_quantum as tfq
import cirq
import sympy
import numpy as np
import seaborn as sns
import collections
import matplotlib.pyplot as plt
TensorFlow has common datasets built-in; below, we load up the MNIST
dataset, which consists of images of handwritten digits (0-9).
(x_train, y_train), (x_test, y_test) =
tf.keras.datasets.mnist.load_data()
SECTION 9.4
Machine Learning on Quantum Processors
169
Figure 9.5: Example full-resolution (left) and downscaled (right) MNIST dataset image.
Each image is a 28  28 matrix (two-dimensional array) of ﬂoating point
values in the range Œ0; 255. Our ﬁrst step of the encoding is to map this range
to Œ0; 1 so the values are more easily stored in a quantum computer.
x_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[...,
np.newaxis] / 255.0
For simplicity, let’s consider the classiﬁcation task of distinguishing be-
tween 3’s and 6’s.
def filter_36(x, y):
keep = (y == 3) | (y == 6)
x, y = x[keep], y[keep]
y = y == 3
return x,y
x_train, y_train = filter_36(x_train, y_train)
x_test, y_test = filter_36(x_test, y_test)
Our dataset now only contains images of 3’s and 6’s. The last “pre-
processing” step is to downscale the images, as 28  28 is too large for NISQ
computers. A size of 4  4 is more reasonable.
x_train_small = tf.image.resize(x_train, (4,4)).numpy()
x_test_small = tf.image.resize(x_test, (4,4)).numpy()
An example of a full-resolution and downscaled image is shown in Fig-
ure 9.5.
Now that the “pre-processing” is done, we get to the fun part: encoding
the image data into a quantum circuit. There are many strategies to do so, and
several aspects of the learning procedure can vary depending on the encoding.
Here, we use the following strategy: each pixel (ﬂoating point value in the
range Œ0; 1) of the image will correspond to one qubit. If the pixel is below
170
CHAPTER 9
Quantum Computing Methods
0:5, we represent it as the j0i state; otherwise, we represent it as the j1i state.
(Intuitively, this can be thought of as making the image black and white.)
THRESHOLD = 0.5
x_train_bin = np.array(x_train_small > THRESHOLD, dtype=np.float32)
x_test_bin = np.array(x_test_small > THRESHOLD, dtype=np.float32)
def convert_to_circuit(image):
"""Encode truncated classical image into quantum datapoint."""
values = np.ndarray.flatten(image)
qubits = cirq.GridQubit.rect(4, 4)
circuit = cirq.Circuit()
for i, value in enumerate(values):
if value:
circuit.append(cirq.X(qubits[i]))
return circuit
x_train_circ = [convert_to_circuit(x) for x in x_train_bin]
x_test_circ = [convert_to_circuit(x) for x in x_test_bin]
At this point, for each image in the dataset, we have a corresponding
quantum circuit. In TFQ, all circuits must be converted to tensors.
x_train_tfcirc = tfq.convert_to_tensor(x_train_circ)
x_test_tfcirc = tfq.convert_to_tensor(x_test_circ)
Now that the encoding is done, we have to design a quantum neural
network (QNN) to propagate the encoded data through a series of trainable
(parameterized) quantum gates. Just like the encoding step, there are many
ways to design the QNN. Here, we follow the construction of Farhi [109] and
implement the following layered circuit ansatz.
class CircuitLayerBuilder():
def __init__(self, data_qubits, readout):
self.data_qubits = data_qubits
self.readout = readout
def add_layer(self, circuit, gate, prefix):
for i, qubit in enumerate(self.data_qubits):
symbol = sympy.Symbol(prefix + ’-’ + str(i))
circuit.append(gate(qubit, self.readout)**symbol)
demo_builder = CircuitLayerBuilder(data_qubits =
cirq.GridQubit.rect(4,1),
readout=cirq.GridQubit(-1,-1))
circuit = cirq.Circuit()
demo_builder.add_layer(circuit, gate = cirq.XX, prefix=’xx’)
SECTION 9.4
Machine Learning on Quantum Processors
171
Figure 9.6: A single layer circuit ansatz used for the quantum neural network classifying
MNIST images, as in [109]. The top qubit, indexed ( 1;  1), is the “readout qubit” that is
measured at the end of the circuit to get a prediction. The remaining qubits are the “data
qubits” which store the encoded MNIST image. Each two-qubit XX gate is parameterized by a
unique angle (the ith angle is labeled “xx-i”) which is optimized over in the training phase.
At this point, the circuit has a two-qubit XX gate between the readout qubit
and all data qubits. Each XX gate is parameterized by an independent angle,
and we use the naming convention “xx-i” for the ith angle. This particular
circuit is shown in Figure 9.6.
Next, we create the full model. This model includes the data encoding and
the QNN — which we have discussed — as well as the ﬁnal readout step.
To understand the readout step, consider the following: after propagating the
encoded data point through the QNN, we have some evolved quantum state,
and we need to get a label (here “the image is a 3” or “the image is a 6”)
from it. So, we need some way to map a quantum state to a label. This is the
purpose of the readout step. For binary classiﬁcation like we are considering
here, a very natural thing to do is measure a single qubit because this produces
a binary outcome. We adopt this strategy here and measure a single qubit
(which we have previously called the “readout qubit”). In particular, we will
measure the expectation of Pauli-Z on the readout qubit which produces a
value in the range Œ 1; 1. If the value is above zero, we assign it to one class;
else we assign it to the other class.
def create_quantum_model():
"""Create a QNN model circuit and readout operation to go along
with it."""
data_qubits = cirq.GridQubit.rect(4, 4) # a 4x4 grid.
readout = cirq.GridQubit(-1, -1) # a single qubit at [-1,-1]
circuit = cirq.Circuit()
172
CHAPTER 9
Quantum Computing Methods
# Prepare the readout qubit.
circuit.append(cirq.X(readout))
circuit.append(cirq.H(readout))
builder = CircuitLayerBuilder(
data_qubits = data_qubits,
readout=readout)
# Then add layers (experiment by adding more).
builder.add_layer(circuit, cirq.XX, "xx1")
builder.add_layer(circuit, cirq.ZZ, "zz1")
# Finally, prepare the readout qubit.
circuit.append(cirq.H(readout))
return circuit, cirq.Z(readout)
model_circuit, model_readout = create_quantum_model()
Now that we have the full quantum model, we can use it in TensorFlow
just like any other model.
# Build the Keras model.
model = tf.keras.Sequential([
# The input is the data-circuit, encoded as a tf.string
tf.keras.layers.Input(shape=(), dtype=tf.string),
# The PQC layer returns the expected value of the readout gate,
range [-1,1].
tfq.layers.PQC(model_circuit, model_readout),
])
The last step before training the model is to deﬁne the loss function. Since
our readout value is in the range Œ 1; 1, the hinge loss is a natural choice.
def hinge_accuracy(y_true, y_pred):
y_true = tf.squeeze(y_true) > 0.0
y_pred = tf.squeeze(y_pred) > 0.0
result = tf.cast(y_true == y_pred, tf.float32)
return tf.reduce_mean(result)
y_train_hinge = 2.0 * y_train - 1.0
y_test_hinge = 2.0 * y_test - 1.0
Note that we also map the labels to the range Œ 1; 1 in the last two lines
to use the hinge loss.
Now we can compile the model with a loss function, optimizer, and metric.
model.compile(
loss=tf.keras.losses.Hinge(),
optimizer=tf.keras.optimizers.Adam(),
SECTION 9.4
Machine Learning on Quantum Processors
173
metrics=[hinge_accuracy],
)
And ﬁnally train the model.
EPOCHS = 3
BATCH_SIZE = 32
NUM_EXAMPLES = len(x_train_tfcirc)
x_train_tfcirc_sub = x_train_tfcirc[:NUM_EXAMPLES]
y_train_hinge_sub = y_train_hinge[:NUM_EXAMPLES]
qnn_history = model.fit(
x_train_tfcirc_sub, y_train_hinge_sub,
batch_size=32,
epochs=EPOCHS,
verbose=1,
validation_data=(x_test_tfcirc, y_test_hinge)
)
qnn_results = model.evaluate(x_test_tfcirc, y_test)
In this example, we achieved 89.5% (hinge) accuracy. While this is not
up to par with state of the art classical machine learning models, it’s not bad
for a simple quantum model! The important part is to understand each major
step — pre-processing, data encoding, quantum neural network, readout, and
training — that makes up the quantum model. Can you change one or more
of these steps to improve the accuracy and get better results?
For this type of classiﬁcation problem, it remains to be seen whether a
QNN has an advantage over a classical model. More generally, advantages
in quantum machine learning seem to be elusive in the early stages of this
ﬁeld. For example, a quantum algorithm for recommendation systems was
proposed and assessed to have an exponential speedup over the best known
classical algorithm at the time of that work [151]. The general idea of a
recommendation system is as follows: Given an incomplete preference matrix
P of m users and their feedback on n products, output a good recommendation
for a particular user. Here, “incomplete” means that entries of the matrix are
missing — that is, not every user has provided feedback for every product.
Prior to the proposed quantum algo for a recommendation system, the best
classical algorithm had a runtime that scaled linearly in the matrix dimension
mn. The quantum recommendation algorithm scales polylogarithmically in
mn, speciﬁcally as
O.poly./polylog.mn//
where  is the condition number of P . However, a new classical algorithm,
inspired by the quantum one, also achieved polylogarithmic scaling in the
174
CHAPTER 9
Quantum Computing Methods
matrix dimension [273]. We say therefore that the quantum algo has been
“dequantized.” See the section on dequantization later in this chapter for
additional discussion of this important topic.
Much research continues to be done in quantum machine learning; see, for
example, [138, 250, 283], to explore the possibilities and prospects for this
relatively young ﬁeld.
9.5
Quantum Phase Estimation
Quantum phase estimation (QPE), also known as the phase estimation algo-
rithm (PEA), is an algorithm for determining the eigenvalues of a unitary
operator. QPE was initially proposed by Kitaev [152] (see also [83] for a
discussion of QPE). Eigenvalue problems, which have the form
Ax D x
(9.25)
where A 2 C2m2m, x 2 C2m and  2 C, are ubiquitous throughout math-
ematics and physics. In mathematics, applications range from graph theory
to partial differential equations. In physics, applications include computing
the ground state energy — the smallest eigenvalue of the Hamiltonian of
the system — for nuclei, molecules, materials and other physical systems.
Moreover, principal component analysis (PCA), an algorithm for reducing
the dimensionality of feature vectors in machine learning, has an eigenvalue
problem at its core. The applications of (9.25) range across a wide spectrum
of disciplines.
In the quantum case, we are concerned with ﬁnding the eigenvalues of
a unitary operator U . It follows immediately by the deﬁnition of unitarity
(U U D I) that eigenvalues of a unitary operator have modulus one: jj D 1.
Thus, any eigenvalue  of a unitary operator can be written in the form
 D e2i'
(9.26)
where 0  '  1 is called the phase. This is the same phase that appears in
the name of the algorithm — quantum phase estimation. By estimating ', we
get an estimate of the eigenvalue  via the equation above.
Suppose ' can be written exactly using n bits5
5This is the case where ' is rational. The general case of ' being irrational (requiring
inﬁnitely many bits) is similar, but for simplicity we won’t cover it here. See [206] for an
explanation.
SECTION 9.5
Quantum Phase Estimation
175
' D 0:'1'2    'n
(9.27)
This is a binary decimal representation of the phase '. Here, each 'k for
k D 1; :::; n is a binary digit 'k 2 f0; 1g. We can write this equivalently as
' D
n
X
kD1
'k2 k
(9.28)
The key to understanding QPE is to consider the action of controlling the
unitary operator on an eigenstate j i. Explicitly, let U be a unitary operator
which we take as input to the QPE algorithm such that
U j i D j i
(9.29)
Suppose for now we have the eigenstate j i. This is not a requirement
for QPE — in fact, it makes the algorithm trivial, for if we knew j i we
could just implement U j i on the quantum computer — it just simpliﬁes the
explanation. Now, suppose we prepare the equal superposition state (ignoring
normalization factors) in the ﬁrst register and the eigenstate of U in the second
register
.j0i C j1i/ ˝ j i D j0ij i C j1ij i
(9.30)
Now, as mentioned, we implement a controlled-U operation on this state,
which produces the following state:
j0ij i C j1iU j i D j0ij i C e2i0:'1'nj1ij i
D .j0i C e2i0:'1'nj1i/ ˝ j i
Note that the second register goes unchanged. Since j i is an eigenstate of
U , it is unaffected by the controlled operation. Why did we do this then? We
encoded the information about the phase into the ﬁrst region. Speciﬁcally, the
state in the ﬁrst register picked up the relative phase e2i0:'1'n.
Phase estimation now tells us to implement controlled-U 2k operations
for integers k D 0; :::; n   1. We already performed the k D 0 case above.
Consider now the effect of U 2; in particular,
U 2j i D 2j i D e2i.2'/j i D e2i0:'2'nj i
(9.31)
In the last step, we used the fact that e2i'1 D 1 for any '1 2 f0; 1g. Thus,
by preparing the equal superposition state in the ﬁrst register, the eigenstate
j i in the second register (9.30), then performing a controlled-U 2 operation,
we get the state
176
CHAPTER 9
Quantum Computing Methods
j0ij i C j1iU 2j i D j0ij i C e2i0:'2'nj1ij i
In general, using this same idea, we can see that
U 2kj i D 2kj i D e2i.2k'/j i D e2i0:'kC1'n
(9.32)
for k D 0; :::; n   1. Hence, we can transform (9.30) under a controlled-U 2k
as
j0ij iCj1ij i 7 ! j0ij iCj1iU 2kj i D .j0iCe2i0:'kC1'nj1i/˝j i
(9.33)
Equation (9.33) is at the heart of the QPE algorithm. In particular, the
algorithm says to implement this operation iteratively for k D 0; :::; n   1,
using n qubits in the top register with the eigenstate j i in the bottom register.
The full circuit for QPE is shown below:
j0i
H

QFT
j0i
H

:::
:::
j0i
H

j i
U 20
U 21
  
U 2n 1
After implementing the series of controlled-U 2k operations, the top regis-
ter is in the state
.j0i C e2i0:'1'nj1i/ ˝ .j0i C e2i0:'2'nj1i/ ˝    ˝ .j0i C e2i0:'nj1i/
(9.34)
To extract the phase information from this state, we use the inverse Fourier
transform, which transforms this state to a product state
j'1i ˝ j'2i ˝    ˝ j'ni
(9.35)
By measuring in the computational basis, we thus learn the bits '1; '2; :::; 'n,
which allow us to construct ' D 0:'1    'n and the eigenvalue
 D e2i'
(9.36)
SECTION 9.5
Quantum Phase Estimation
177
Implemention of QPE
We now turn to an example implementation of QPE using Cirq. Here, we
consider computing the eigenvalues of the unitary
U D X ˝ Z
(9.37)
We can see that the eigenvalues of U are, of course, ˙1. We will see that QPE
returns these eigenvalues as well.
First, we import the necessary packages
# Imports
import numpy as np
import cirq
and then deﬁne a helper function for converting from bitstrings in binary
decimal notation to numeric values:
def binary_decimal(string):
"""Returns the numeric value of 0babc... where a, b, c, ... are
bits.
Examples:
0b10 --> 0.5
0b01 --> 0.25
"""
val = 0.0
for (ind, bit) in enumerate(string[2:]):
if int(bit) == 1:
val += 2**(-1 -ind)
return val
Now we deﬁne the number of qubits in the bottom register of the QPE
circuit, create the unitary matrix and classically diagonalize it. We will use
these eigenvalues to compare to the ones found by QPE.
# Number of qubits and dimension of the eigenstate
m = 2
# Get a unitary matrix on two qubits
xmat = np.array([[0, 1], [1, 0]])
zmat = np.array([[1, 0], [0, -1]])
unitary = np.kron(xmat, zmat)
# Print it to the console
print("Unitary:")
print(unitary)
# Diagonalize it classically
evals, _ = np.linalg.eig(unitary)
178
CHAPTER 9
Quantum Computing Methods
The output of this portion of the code follows:
Unitary:
[[ 0 0 1 0]
[ 0 0 0 -1]
[ 1 0 0 0]
[ 0 -1 0 0]]
Now that we have our input to QPE, we can begin building the circuit.
As described above, we deﬁne the number of qubits in our top register to
determine the accuracy of the eigenvalues found. Here, we set this number
and deﬁne two registers of qubits. Next, we create a circuit and apply the
Hadamard gate to each qubit in the top (readout) register.
# Number of qubits in the readout/answer register (# bits of
precision)
n = 2
# Readout register
regA = cirq.LineQubit.range(n)
# Register for the eigenstate
regB = cirq.LineQubit.range(n, n + m)
# Get a circuit
circ = cirq.Circuit()
# Hadamard all qubits in the readout register
circ.append(cirq.H.on_each(*regA))
The next step in the QPE algorithm is to implement the series of controlled
U 2k operations. We show how this can be done in Cirq for arbitrary two-qubit
unitaries written as matrices. First, we create a TwoQubitMatrixGate from
the unitary matrix, then make a controlled version.
# Get a Cirq gate for the unitary matrix
ugate = cirq.ops.matrix_gates.TwoQubitMatrixGate(unitary)
# Controlled version of the gate
cugate = cirq.ops.ControlledGate(ugate)
Now that we have the controlled-U gate, we can implement the sequence of
transforms with the following code block:
# Do the controlled U^{2^k} operations
for k in range(n):
circ.append(cugate(regA[k], *regB)**(2**k))
SECTION 9.5
Quantum Phase Estimation
179
The last step in QPE is to implement the inverse quantum Fourier transform
and measure all qubits in the computational basis. This is done in the following
code block.
# Do the inverse QFT
for k in range(n - 1):
circ.append(cirq.H.on(regA[k]))
targ = k + 1
for j in range(targ):
exp = -2**(j - targ)
rot = cirq.Rz(exp)
crot = cirq.ControlledGate(rot)
circ.append(crot(regA[j], regA[targ]))
circ.append(cirq.H.on(regA[n - 1]))
# Measure all qubits in the readout register
circ.append(cirq.measure(*regA, key="z"))
Now that we have built our QPE circuit, we can run it and process the
results. The code below simulates the circuit and grabs the top two most
frequent measurement outcomes. We can obtain '1 and '2 from each of these
to compute our eigenvalues.
# Get a simulator
sim = cirq.Simulator()
# Simulate the circuit and get the most frequent measurement outcomes
res = sim.run(circ, repetitions=1000)
hist = res.histogram(key="z")
top = hist.most_common(2)
Even though we do not start the second register in an eigenstate of the
unitary U , we can think of starting the second register in a linear combination
of its eigenstates since the eigenstates of U form an orthonormal basis; in
other words, any vector can be expressed as a linear combination of the
eigenstates for some coefﬁcients. In particular, we started the second register
in the ground state j0i, which we can write as
j0i D
X
j
cj jji
(9.38)
where jji are the eigenstates of U . The most probable measurement outcomes
are thus those with large jcj j D jh0jjij.
Now that we have the most frequently measured bitstrings, we can convert
these to numerical values of the phase ' and then to eigenvalues. The code
below performs these operations and prints out the eigenvalues computed by
QPE and the eigenvalues computed by the classical matrix diagonalization
algorithm.
180
CHAPTER 9
Quantum Computing Methods
# Eigenvalues from QPE
estimated = [np.exp(2j * np.pi * binary_decimal(bin(x[0]))) for x in
top]
# Print out the estimated eigenvalues
print("\nEigenvalues from QPE:")
print(set(sorted(estimated, key=lambda x: abs(x)**2)))
# Print out the actual eigenvalues
print("\nActual eigenvalues:")
print(set(sorted(evals, key=lambda x: abs(x)**2)))
The output of this code, shown below, reveals that QPE ﬁnds the correct
eigenvalues within numerical roundoff precision.
Eigenvalues from QPE:
{(1+0j), (-1+1.2246467991473532e-16j)}
Actual eigenvalues:
{1.0, -1.0}
The complete program for this implementation can be found on the book’s
online website. One can change the unitary matrix to compute eigenvalues
and compare them to the ones found classically. Additionally, one can change
the number of qubits in the top register n to get more bits of precision for
more complex unitary operators.
9.6
Solving Linear Systems
The problem of solving a linear system of M equations with N variables is
ubiquitous in mathematics, science and engineering. The formal statement of
the problem is as follows:
Given an M  N matrix A and a solution vector b, ﬁnd a
vector x such that
Ax D b
(9.39)
Linear algebra tells us how to solve this problem classically in the case
that A is invertible:6
x D A 1b
(9.40)
However, although we can write down the solution immediately, numerically
computing x is intractable for large matrices.
6Please consult Part III: Toolkit for a review of these mathematical concepts.
SECTION 9.6
Solving Linear Systems
181
Explicitly computing the inverse of A is generally the most costly method.
In practice, most general-purpose numerical solvers use Gaussian elimination
and back-substitution, which runs in O.N 3/ time. In this discussion we
restrict ourselves to square matrices, i.e., M D N . Faster classical algorithms
are possible: if the matrix A has sparsity s and condition number , solving
the system to accuracy  can be done by the conjugate gradient algorithm
in O.Ns log.1=// time, which is a considerable speedup compared with
O.N 3/.
The quantum version of solving systems of linear equations — called the
quantum linear systems problem (QLSP) [87] — is similar to the classical
approach. Let A be an N  N Hermitian matrix with unit determinant. Let
b and x be N-dimensional vectors such that x D A 1b. Deﬁne the quantum
states jbi and jxi on n D log2 N qubits
jbi D
P
i bijii
jj P
i bijiijj2
(9.41)
and
jxi D
P
i xijii
jj P
i xijiijj2
(9.42)
Here, bi is the ith component of b, and similarly for xi.
The goal of the QLSP is as follows: given access to the matrix A (whose
elements are accessed by an oracle) and the state jbi, output a state j Nxi such
that
jjj Nxi   jxijj2  
(9.43)
with probability greater than 1=2.
A quantum algorithm for solving the QLSP in time O.log.N/s22=/ was
discovered by Harrow, Hassidim and Lloyd [135]. The algorithm is commonly
known as the HHL algorithm after its developers. Note that the order of HHL
is logarithmic, however, this is not always the case in practice. As Aaronson
has pointed out:
...the HHL algorithm solves Ax = b in logarithmic time,
but it does so with four caveats...each of which can be
crucial in practice. To make a long story short, HHL is
not exactly an algorithm for solving a system of linear
equations in logarithmic time. Rather, it’s an algorithm
for approximately preparing a quantum superposition
of the form jxi, where x is the solution to a linear
system Ax = b, assuming the ability to rapidly prepare
182
CHAPTER 9
Quantum Computing Methods
the state jxi, and to apply the unitary transformation
e iAt, and using an amount of time that grows roughly
like s.log.n//=, where n is the system size,  is the
system’s condition number, s is its sparsity and  is the
desired error [4].
In the remainder of this section, we explain the mathematics of HHL
and then turn to an example implementation. HHL uses several quantum
algorithms we have discussed — such as Hamiltonian and quantum phase
estimation — as subroutines.
Description of the HHL Algorithm
The HHL algorithm uses three registers of qubits, which we denote as A for
ancilla, W for work, and IO for input/output. The input to the algorithm is
the quantum state jbi, deﬁned above, which is input to the IO register. The
other registers start off in the j0i state, so the entire initial state input to HHL
can be written
j 0i WD j0iA ˝ j0iW ˝ jbiIO
(9.44)
We are also given the matrix A as input. There should be no confusion
between the matrix and the register, for we will refer to the former as “matrix
A” and the latter as “register A.” There are three main steps to the algorithm:
1. Quantum phase estimation with the unitary UA WD eiAt, controlled by
the W register and UA applied to the IO register.
2. Pauli-Y rotation for a particular angle  (discussed below) on the A
register controlled by the W register.
3. Implement the ﬁrst step in reverse (known as uncomputation) on the W
register.
If the A register is measured and one post-selects on the j1iA outcome,
then the state of the IO register will be close to jxi. We now walk through the
steps to show this.
Let the matrix A be written in its eigenbasis
A D
X
j
j juj ihuj j
(9.45)
For simplicity, we assume for the moment that jbi is one of the eigenvectors
of A. That is, jbi D juj i for some index j. This assumption will be relaxed
momentarily.
We can assume A is Hermitian without loss of generality, since if A is not
Hermitian, we can form a Hermitian matrix
SECTION 9.6
Solving Linear Systems
183
QA WD
0
A
A
0

and perform HHL with QA. Since A is Hermitian, the operator
UA WD eiAt
(9.46)
is unitary and has eigenvalues eij t and eigenstates juj i. After the ﬁrst step
of HHL, QPE brings us to the state
j 1i WD j0iA ˝ jQj iW ˝ juj iIO
(9.47)
Here, Qj is a binary representation of j up to a set precision. Note that we
used our assumption jbi D juj i when writing the result of QPE.
We now implement the second step of QPE, a controlled-Y rotation e iY
for the angle
 D arccos C
Q
(9.48)
Here, C is a hyperparameter set by the user of the algorithm. In the example
implementation below, we discuss setting the value of C. After this rotation
controlled on the O register, we have the state
j 2i WD
v
u
u
t1   C 2
Q2
j
j0iA ˝ jQj iW ˝ juj iIO C C
Qj
j1iA ˝ jQj iW ˝ juj iIO
(9.49)
We now relax the assumption that jbi is an eigenstate of A. That is, we
relax the assumption that jbi D juj i for some j. Note that we can write
without any assumptions, however, that
jbi D
X
j
ˇj juj i
(9.50)
where ˇj D huj jbi are complex coefﬁcients. We can write this because A is
Hermitian and so its eigenstates form an orthonormal basis.
We now perform the above analysis (9.45) — (9.49) with jbi expressed in
the eigenbasis (9.50). Doing so, we end up with the state
j 3i WD
N
X
jD1
ˇj
2
4
v
u
u
t1   C 2
Q2
j
j0iA C C
Qj
j1iA
3
5 ˝ jQj iW ˝ juj iIO
(9.51)
184
CHAPTER 9
Quantum Computing Methods
The next step of HHL is to uncompute the W register. Doing so sends
jQj iO ! j0iO. Since this state is the all zeros state, we can omit it and write
the state after uncomputing as
j 4i WD
N
X
j D1
ˇj
2
4
v
u
u
t1   C 2
Q2
j
j0iA C C
Qj
j1iA
3
5 ˝ juj iIO
(9.52)
This state is in a very useful form, though it may take a careful look to see
why. The reason is that
A 1jbi D
N
X
j D1
ˇj
j
juj i
(9.53)
Thus, if we measure the A register and post-select on the j1iA outcome, then
(9.52) becomes (ignoring the A register)
j 5i WD
N
X
j D1
ˇj
Qj
juj iIO  jxi:
(9.54)
Thus, the IO register contains an approximation to jxi D A 1jbi.
Note that this solves the quantum linear systems problem exponentially
faster than the best known classical algorithm. Like Shor’s algorithm and
QPE, HHL is a demonstration of potential quantum advantage.
However, note that only a quantum description of the solution vector is
output from HHL. For applications that need a full classical description of
x, this may not be satisfactory. Quantum state tomography — which is the
measurement and characterization of the wavefunction of a quantum system
— can be used to read out each amplitude of jxi, but this takes time that scales
exponentially in the number of qubits. Fortunately, there exists a number of
applications where only certain features of the solution x need to be computed,
for example the total weight of some subset of the indices.
Now that we have walked through the HHL algorithm, let us turn to an
example implementation written in Cirq:
Example Implementation of the HHL Algorithm
In this implementation7, we consider for simplicity a 2  2 system of linear
equations. In particular, the matrix A we consider is
7Adapted from https://github.com/quantumlib/Cirq/blob/master/examples/hhl.py
SECTION 9.6
Solving Linear Systems
185
A D
4:302134   6:015934  10 8i
0:235318 C 9:343861  10 1i
0:235318   9:343883  10 1i
0:583865 C 6:015934  10 8i

(9.55)
and we take the vector jbi as
jbi D Œ0:64510   0:47848i
0:35490   0:47848iT
(9.56)
Our goal is to use HHL to compute Pauli expectation values hxjjxi where
 2 fX; Y; Zg. We can easily compute these analytically (after classically
solving the system) to be
hxjXjxi D 0:144130
hxjY jxi D 0:413217
hxjZjxi D  0:899154
We will compare the expectation values obtained by HHL to these expectation
values.
In our program, we ﬁrst import the packages we will use:
import math
import numpy as np
import cirq
and then build up the HHL circuit. Here, we deﬁne classes which are new
gates in Cirq by inheriting from cirq.Gate or related objects. First, we
create a gate representing UA D eiAt which we will use in the QPE steps:
class HamiltonianSimulation(cirq.EigenGate, cirq.SingleQubitGate):
"""A gate that implements e^iAt.
If a large matrix is used, the circuit should implement actual
Hamiltonian
simulation, for example by using the linear operators framework
in Cirq.
"""
def __init__(self, A, t, exponent=1.0):
"""Initializes a HamiltonianSimulation.
Args:
A : numpy.ndarray
Hermitian matrix that defines the linear system Ax = b.
t : float
Simulation time. Hyperparameter of HHL.
"""
cirq.SingleQubitGate.__init__(self)
cirq.EigenGate.__init__(self, exponent=exponent)
186
CHAPTER 9
Quantum Computing Methods
self.A = A
self.t = t
ws, vs = np.linalg.eigh(A)
self.eigen_components = []
for w, v in zip(ws, vs.T):
theta = w*t / math.pi
P = np.outer(v, np.conj(v))
self.eigen_components.append((theta, P))
def _with_exponent(self, exponent):
return HamiltonianSimulation(self.A, self.t, exponent)
def _eigen_components(self):
return self.eigen_components
Next, we implement the series of controlled unitary operations in QPE,
known as the phase kickback portion of the circuit (please refer to chapter 8
for a discussion of phase kickback):
class PhaseKickback(cirq.Gate):
"""A gate for the phase kickback stage of Quantum Phase
Estimation.
Consists of a series of controlled e^iAt gates with the memory
qubit as
the target and each register qubit as the control, raised
to the power of 2 based on the qubit index.
"""
def __init__(self, num_qubits, unitary):
"""Initializes a PhaseKickback gate.
Args:
num_qubits : int
The number of qubits in the readout register + 1.
Note: The last qubit stores the eigenvector; all other
qubits
store the estimated phase, in big-endian.
unitary : numpy.ndarray
The unitary gate whose phases will be estimated.
"""
super(PhaseKickback, self)
self._num_qubits = num_qubits
self.U = unitary
def num_qubits(self):
"""Returns the number of qubits."""
return self._num_qubits
def _decompose_(self, qubits):
"""Generator for the phase kickback circuit."""
qubits = list(qubits)
SECTION 9.6
Solving Linear Systems
187
memory = qubits.pop()
for i, qubit in enumerate(qubits):
yield cirq.ControlledGate(self.U**(2**i))(qubit, memory)
Next, we create a quantum Fourier transform gate, the third and ﬁnal
component of QPE:
class QFT(cirq.Gate):
"""Quantum gate for the Quantum Fourier Transformation.
Note: Swaps are omitted here. These are implicitly done in the
PhaseKickback gate by reversing the control qubit order.
"""
def __init__(self, num_qubits):
"""Initializes a QFT circuit.
Args:
num_qubits : int
Number of qubits.
"""
super(QFT, self)
self._num_qubits = num_qubits
def num_qubits(self):
return self._num_qubits
def _decompose_(self, qubits):
processed_qubits = []
for q_head in qubits:
for i, qubit in enumerate(processed_qubits):
yield cirq.CZ(qubit, q_head)**(1/2.0**(i+1))
yield cirq.H(q_head)
processed_qubits.insert(0, q_head)
Now that we have the three major components of QPE, we can implement
the entire algorithm. As above, we make the QPE instance a gate in Cirq:
class QPE(cirq.Gate):
"""A gate for Quantum Phase Estimation."""
def __init__(self, num_qubits, unitary):
"""Initializes an HHL circuit.
Args:
num_qubits : int
The number of qubits in the readout register.
Note: The last qubit stores the eigenvector; all other
qubits
store the estimated phase, in big-endian.
unitary : numpy.ndarray
The unitary gate whose phases will be estimated.
188
CHAPTER 9
Quantum Computing Methods
"""
super(QPE, self)
self._num_qubits = num_qubits
self.U = unitary
def num_qubits(self):
return self._num_qubits
def _decompose_(self, qubits):
qubits = list(qubits)
yield cirq.H.on_each(*qubits[:-1])
yield PhaseKickback(self.num_qubits(), self.U)(*qubits)
yield QFT(self._num_qubits-1)(*qubits[:-1])**-1
With the unitary set as UA D eiAt, this instance of QPE will make up the
ﬁrst part of the HHL algorithm. The next step is to implement the controlled
Pauli-Y rotation, which the following class does:
class EigenRotation(cirq.Gate):
"""EigenRotation performs the set of rotation on the ancilla qubit
equivalent to division on the memory register by each eigenvalue
of the matrix.
The last qubit is the ancilla qubit; all remaining qubits are in
the register,
assumed to be big-endian.
It consists of a controlled ancilla qubit rotation for each
possible value
that can be represented by the register. Each rotation is an Ry
gate where
the angle is calculated from the eigenvalue corresponding to the
register
value, up to a normalization factor C.
"""
def __init__(self, num_qubits, C, t):
"""Initializes an EigenRotation.
Args:
num_qubits : int
Number of qubits.
C : float
Hyperparameter of HHL algorithm.
t : float
Parameter.
"""
super(EigenRotation, self)
self._num_qubits = num_qubits
self.C = C
self.t = t
self.N = 2**(num_qubits-1)
SECTION 9.6
Solving Linear Systems
189
def num_qubits(self):
return self._num_qubits
def _decompose_(self, qubits):
for k in range(self.N):
kGate = self._ancilla_rotation(k)
# xor’s 1 bits correspond to X gate positions.
xor = k ^ (k-1)
for q in qubits[-2::-1]:
# Place X gates
if xor % 2 == 1:
yield cirq.X(q)
xor >>= 1
# Build controlled ancilla rotation
kGate = cirq.ControlledGate(kGate)
yield kGate(*qubits)
def _ancilla_rotation(self, k):
if k == 0:
k = self.N
theta = 2*math.asin(self.C * self.N * self.t / (2*math.pi * k))
return cirq.Ry(theta)
Now that we have built up each component of the HHL algorithm, we can
write a function to build the entire circuit, shown below:
def hhl_circuit(A, C, t, register_size, *input_prep_gates):
"""Constructs the HHL circuit and returns it.
Args:
A : numpy.ndarray
Hermitian matrix that defines the system of equations Ax =
b.
C : float
Hyperparameter for HHL.
t : float
Hyperparameter for HHL
C and t are tunable parameters for the algorithm.
register_size is the size of the eigenvalue register.
input_prep_gates is a list of gates to be applied to |0> to
generate the
desired input state |b>.
"""
# Ancilla register
ancilla = cirq.GridQubit(0, 0)
# Work register
190
CHAPTER 9
Quantum Computing Methods
register = [cirq.GridQubit(i + 1, 0) for i in
range(register_size)]
# Input/output register
memory = cirq.GridQubit(register_size + 1, 0)
# Create a circuit
circ = cirq.Circuit()
# Unitary e^{iAt} for QPE
unitary = HamiltonianSimulation(A, t)
# QPE with the unitary e^{iAt}
qpe = QPE(register_size + 1, unitary)
# Add state preparation circuit for |b>
circ.append([gate(memory) for gate in input_prep_gates])
# Add the HHL algorithm to the circuit
circ.append([
qpe(*(register + [memory])),
EigenRotation(register_size+1, C, t)(*(register+[ancilla])),
qpe(*(register + [memory]))**-1,
cirq.measure(ancilla)
])
# Pauli observable display
circ.append([
cirq.pauli_string_expectation(
cirq.PauliString({ancilla: cirq.Z}),
key="a"
),
cirq.pauli_string_expectation(
cirq.PauliString({memory: cirq.X}),
key="x"
),
cirq.pauli_string_expectation(
cirq.PauliString({memory: cirq.Y}),
key="y"
),
cirq.pauli_string_expectation(
cirq.PauliString({memory: cirq.Z}),
key="z"
),
])
return circ
In this function, we deﬁne the three qubit registers for HHL and create an
empty circuit. Then we form the unitary UA D eiAt from the input matrix A
for a given time t and form a QPE circuit using that unitary. The next line
implements the state preparation circuit to prepare jbi from the ground state.
Note that while jbi is assumed as input to HHL, in practice we always need a
state preparation circuit.
SECTION 9.6
Solving Linear Systems
191
After this, the HHL circuit is constructed step-by-step; ﬁrst, we add the
QPE circuit, then the controlled-Y rotation, then the inverse QPE circuit. Note
that we are using the **-1 notation in Cirq which makes it very easy to get
the inverse of a quantum circuit. Finally, we append Pauli operators to make
it easy to compute expectation values.
Now that we have built the circuit for HHL, we can simulate it to run
the algorithm. The following function inputs an HHL circuit, simulates it,
and prints out the expectation values from the input/output (IO) register after
post-selecting the j1i outcome in the ancilla register.
Finally, we write our main function which deﬁnes the linear system A,
input state jbi and any hyperparameters needed for the HHL algorithm:
def main():
"""Runs the main script of the file."""
# Constants
t = 0.358166 * math.pi
register_size = 4
# Define the linear system
A = np.array([[4.30213466-6.01593490e-08j,
0.23531802+9.34386156e-01j],
[0.23531882-9.34388383e-01j,
0.58386534+6.01593489e-08j]])
# The |b> vector is defined by these gates on the zero state
# |b> = (0.64510-0.47848j, 0.35490-0.47848j)
input_prep_gates = [cirq.Rx(1.276359), cirq.Rz(1.276359)]
# Expected expectation values
expected = (0.144130 + 0j, 0.413217 + 0j, -0.899154 + 0j)
# Set C to be the smallest eigenvalue that can be represented by
the
# circuit.
C = 2*math.pi / (2**register_size * t)
# Print the actual expectation values
print("Expected observable outputs:")
print("X =", expected[0])
print("Y =", expected[1])
print("Z =", expected[2])
# Do the HHL algorithm and print the computed expectation values
print("\nComputed: ")
hhlcirc = hhl_circuit(A, C, t, register_size, *input_prep_gates)
expectations(hhlcirc)
if __name__ == "__main__":
main()
192
CHAPTER 9
Quantum Computing Methods
The output of this program is shown below:
Expected observable outputs:
X = (0.14413+0j)
Y = (0.413217+0j)
Z = (-0.899154+0j)
Computed:
X = (0.14413303+0j)
Y = (0.41321677+0j)
Z = (-0.89915407+0j)
As can be seen, HHL returns the correct (approximate) expectation values
for each Pauli operator, indicating that the ﬁnal state j Nxi is indeed close to the
solution vector jxi.
9.7
Quantum Random Number Generator
Generating random numbers is crucial for many applications and algorithms,
including Monte Carlo methods and cryptography. While classical computers
generate pseudorandom numbers, the random numbers generated by quan-
tum computers are guaranteed to be truly random by the laws of quantum
mechanics.
In this section, we consider a simple algorithm for generating truly random
numbers on current quantum processors. The algorithm applies a Hadamard
gate to a qubit in the ground state, then measures the state of the qubit in
the computational basis. As we have seen, the Hadamard gate acting on j0i
generates a superposition of computational basis states with equal amplitudes:
H j0i D
1
p
2
.j0i C j1i/
(9.57)
When this state is measured, there is therefore an equal probability of
obtaining the ground and excited states. This can be exploited computationally
as a random bit generator.
Let us now walk through an example program for generating random bits
in Cirq. This program creates a circuit with one qubit, applies the Hadamard
gate and then performs a measurement; the program then iterates the circuit
ten times in the simulator.
"""Program for generating random bits in Cirq."""
# Imports
import cirq
SECTION 9.7
Quantum Random Number Generator
193
# Helper function for visualizing output
def bitstring(bits):
return ’’.join(’1’ if e else ’0’ for e in bits)
# Get a qubit and quantum circuit
qbit = cirq.LineQubit(0)
circ = cirq.Circuit()
# Add the Hadamard and measure operations to the circuit
circ.append([cirq.H(qbit), cirq.measure(qbit, key="z")])
# Simulate the circuit
sim = cirq.Simulator()
res = sim.run(circ, repetitions=10)
# Print the outcome
print("Bitstring =", bitstring(res.measurements["z"]))
An example output of this program is shown below:
Bitstring = 0011001011
Note that this output can be interpreted in several ways, depending on
the context. One interpretation is a sequence of random bits, while another
is a random bitstring representing, for example, an integer. In base ten, the
integer produced in this example output is 203. In this sense, the program can
be interpreted as a uniform random number generator in the integer interval
Œ0; N / where N is the number of repetitions of the circuit.
It is also possible to generate a random number in the range Œ0; N/ by
using n D log2 N qubits. Here, instead of simulating a circuit with one qubit
many times, we apply a Hadamard gate on each of the n qubits, then measure.
Since there are 2n D N possible bitstrings for n qubits, this will also generate
a random bitstring that can be interpreted as an integer in the range Œ0; N/. A
program that implements this in Cirq is shown below:
"""Program for generating random numbers in Cirq."""
# Imports
import cirq
# Number of qubits
n = 10
# Helper function for visualizing output
def bitstring(bits):
return ’’.join(’1’ if e else ’0’ for e in bits)
# Get a qubit and quantum circuit
qreg = [cirq.LineQubit(x) for x in range(n)]
194
CHAPTER 9
Quantum Computing Methods
circ = cirq.Circuit()
# Add the Hadamard and measure operations to the circuit
for x in range(n):
circ.append([cirq.H(qreg[x]), cirq.measure(qreg[x])])
# Simulate the circuit
sim = cirq.Simulator()
res = sim.run(circ, repetitions=1)
# Print the measured bitstring
bits = bitstring(res.measurements.values())
print("Bitstring =", bits)
# Print the integer corresponding to the bitstring
print("Integer =", int(bits, 2))
Here, we use n D 10 qubits to generate a random number in the range
Œ0; 1024/. An example output of this program is shown below:
Bitstring = 1010011100
Integer = 668
Here, the integer is the bitstring in base 10. This program produces uniform
random numbers in the range Œ0; 1024/ and will produce different integers if
run successive times.
9.8
Quantum Walks
Quantum walks have been shown to have computational advantages over
classical random walks [10, 12, 74, 75, 76, 108, 149].
In the classical setting, a particle starts out in some initial position (vertex)
on a graph G D .V; E/ and “walks” to neighboring vertices in a probabilistic
manner; these are classical random walks. The ﬁnal probability distribution of
ﬁnding the particle at a given vertex V — as well as questions such as “how
long does it take the particle to reach a particular vertex?” — are interesting
and useful quantities to calculate. When certain problems are phrased in terms
of random walks, for example the 2-SAT problem, computing these quantities
can lead to novel solutions that may not have been known previously.
The simplest example of a classical random walk is a one-dimensional
walk on a line. Consider a particle starting at position x.t D 0/ D 0 at
time t D 0. At time t D 1, the particle moves to the right (x.1/ D 1)
or left (x.t D 1/ D  1) with equal probability. At time t D 1, we have
x.2/ D x.1/ ˙ 1 with equal probability, and in general x.t/ D x.t   1/ ˙ 1
SECTION 9.8
Quantum Walks
195
with equal probability. Because steps are made at only discrete increments of
time t D 1; 2; 3; :::, this is known as a discrete-time random walk. Continuous-
time random walks are another model which we discuss below.
Classical random walks are implemented by generating pseudo-random
number(s) at each time step. The particle’s position is updated at each iteration
according to the outcome of the random number generator. Alternatively, an
array of values representing probabilities for each position can be stored and
updated via a stochastic matrix which determines the time evolution of the
system.
In the case of a discrete-time quantum walk on a line, the idea is similar but
the implementation is different. The quantum walk consists of two registers
of qubits: a position register P and a coin register C. As the name suggests,
the position register tracks the probability distribution for the particle to be at
a particular position j0i, j1i, ..., jN   1i, where we impose periodic boundary
conditions jNi D j0i. The coin register is used to update the particle’s
position at each time step.
The update to the particle’s position is given by the shift operator
S WD j0ih0jC ˝
X
i
ji   1ihijP C j1ih1jC ˝
X
i
ji C 1ihijP
(9.58)
That is, if the coin register is in the j1i state, the particle shifts to the left, and
if the coin register is in the j0i state, the particle shifts to the right. That is,
Sj0iC ˝ jiiP D j0iC ˝ ji   1iP
(9.59)
and
Sj1iC ˝ jiiP D j1iC ˝ ji C 1iP
(9.60)
The coin is “ﬂipped” by applying a single qubit gate — for example the
Hadamard gate H to produce an equal superposition state, though “biased”
coins can also be used — and then the shift operator is applied. One step of
the quantum walk can thus be written
U D S.HC ˝ IP /
(9.61)
where HC is the Hadamard acting on the coin and IP refers to the identity
acting on the particle. T steps of the walk are given by U T .
In this simplest example of a random walk, numerous differences between
the classical and quantum cases can already be seen. For example, starting in
the initial state j0iC ˝ j0iP will cause the probability distribution to drift “to
the left” — that is, the particle is more likely to move to the left — whereas
196
CHAPTER 9
Quantum Computing Methods
in the classical case the distribution is symmetric. Starting the quantum walk
in the state j1iC ˝ j0iP will cause the distribution to drift to the right. The
reasons for this are constructive and destructive interference of amplitudes, a
distinctly quantum phenomena that is not possible in the classical case. Note
that the distribution for quantum walk can be made symmetric — by starting
in the state jCiC ˝ j0iP , for example, where jCi D .j0i C j1i/=
p
2.
This simple example is instructive for understanding both how quantum
walks work and how quantum walks are different that classical walks. Upon
further study, more differences can be seen. For example, the variance of the
classical distribution for a discrete-time random walk on a line is 2
c D T
after T time steps, but in the quantum case it is 2
q D T 2 [150]. Thus, the
quantum walker propagates quadratically faster than the classical one.
For a review of quantum walks, see [150] and [235] and the references
therein. For an example of quantum walks applied to graphs, see [10]. Farhi
et al. demonstrated speedup for NAND trees with quantum walks [103].
Let us now turn to an example implementation of a quantum walk to get
more experience.
Implementation of a Quantum Walk
In this section we provide an example implementation of a continuous time
quantum walk (CTQW). We ﬁrst discuss the transition from discrete to contin-
uous classical walk, as this will reveal how we perform a continuous quantum
walk. In a discrete-time classical walk, probability distributions are stored in
a vector p which is updated via a stochastic matrix
p.t C 1/ D Mp.t/
(9.62)
This operates only at discrete times. To make it continuous, we rewrite this
equation as a differential equation
dp.t/
dt
D  Hp.t/
(9.63)
where H is a time-independent matrix with elements given by
hijHjji D
8
ˆ<
ˆ:
 
i ¤ j and .i; j/ 2 E
0
i D j and .i; j/ 62 E
di
i D j
(9.64)
Here, 
 is the constant transition rate from vertex i to vertex j and di is the
degree (i.e., number of edges) of vertex i.
SECTION 9.8
Quantum Walks
197
The solution for this differential equation is known to be p.t/ D e Htp.0/.
The step to making this a continuous time quantum walk is to treat the matrix
H as a Hamiltonian which generates the unitary evolution
U.t/ D e iHt
(9.65)
which is deﬁned for a continuous, not discrete, spectrum of times t.
Let us now turn to the example implementation using pyQuil8. Here we
perform a continuous time quantum walk on a complete graph with four
vertices (nodes), commonly denoted K4. A complete graph is one in which
each vertex is connected to all vertices. We ﬁrst import the packages we
will use for this implementation. We highlight here the use of networkx, a
common Python package for working with graphs.
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from scipy.linalg import expm
import pyquil.quil as pq
import pyquil.api as api
from pyquil.gates import H, X, CPHASE00
We now create a complete graph on four nodes and visualize it:
# Create a graph
G = nx.complete_graph(4)
nx.draw_networkx(G)
The output of this portion of the program is shown in Figure 9.7. Note that
each vertex has an edge connecting it to all other vertices.
The spectrum of a complete graph (i.e., the eigenvalues of the adjacency
matrix of a complete graph) is quite simple. It is known from graph theory
that one eigenvalue is equal to N   1 (where N is the number of nodes) and
the remaining eigenvalues are equal to  1. In the following code block, we
get the adjacency matrix of the K4 graph and diagonalize it to verify the
spectrum is what we expect.
# Diagonalize the adjacency matrix
A = nx.adjacency_matrix(G).toarray()
eigvals, _ = np.linalg.eigh(A)
print("Eigenvalues =", eigvals)
8This implementation is adapted from open-source code which can be found at
https://github.com/rigetti/pyquil/blob/master/examples/quantum_walk.ipynb.
198
CHAPTER 9
Quantum Computing Methods
Figure 9.7: A complete graph on four vertices which we implement a continuous time quantum
walk on. This graph is denoted K4.
The output of this code block, shown below, veriﬁes our prediction of the
spectrum:
Eigenvalues = [-1. -1. -1. 3.]
For the CTQW, the usual Hamiltonian is the adjacency matrix A. We
modify it slightly by adding the identity, i.e., we take H D A C I. This will
reduce the number of gates we need to apply, since the eigenvectors with 0
eigenvalue will not acquire a phase. The code below deﬁnes our Hamiltonian:
# Get the Hamiltonian
ham = A + np.eye(4)
It turns out that complete graphs are Hadamard diagonalizable. This means
that we can write
H D QƒQ
(9.66)
where Q D H ˝ H and ƒ is the diagonal matrix of eigenvalues. Let’s check
that this works.
# Hadamard gate
hgate = np.sqrt(1/2) * np.array([[1, 1], [1, -1]])
# Form the matrix Q = H \otimes H to diagonalize the Hamiltonian
Q = np.kron(hgate, hgate)
# Print out the Q^\dagger H Q to verify it’s diagonal
diag = Q.conj().T.dot(ham).dot(Q)
print(diag)
SECTION 9.8
Quantum Walks
199
The output of this portion of the program, shown below, veriﬁes that QHQ
is indeed diagonal (note that numbers in the ﬁrst row are numerically zero):
[[ 4.00000000e+00 -4.93038066e-32 -4.93038066e-32 4.93038066e-32]
[ 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
[ 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
[ 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]
The time evolution operator U.t/ D e iHt is also diagonalized by the
same transformation. In particular, we have
Qe iHtQ D
0
BB@
e i4t
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
1
CCA
(9.67)
which is exactly a CPHASE00 gate in pyQuil with an angle of  4t. To
elaborate on this further, the CPHASE00, which we’ll denote as CZ00.'/
below, has the following action on the computational basis:
CZ00.'/j00i D ei'j00i
CZ00.'/j01i D j01i
CZ00.'/j10i D j10i
CZ00.'/j11i D j11i
The circuit to simulate the unitary evolution U.t/ D e iHt thus consists
of a Hadamard gate on each qubit, CZ00. 4t/, and then another Hadamard
gate on each qubit. The code snippet below deﬁnes a function for creating
this quantum circuit:
# Function for a the continuous time quantum walk circuit on a
complete graph
def k_4_ctqw(t):
"""Returns a program implementing a continuous time quantum
walk."""
prog = pq.Program()
# Change to diagonal basis
prog.inst(H(0))
prog.inst(H(1))
# Time evolve
prog.inst(CPHASE00(-4*t, 0, 1))
# Change back to computational basis
prog.inst(H(0))
prog.inst(H(1))
200
CHAPTER 9
Quantum Computing Methods
Figure 9.8: Time evolution of the continuous-time quantum and classical walks on a complete
graph with four vertices.
return prog
Let’s compare the quantum walk with a classical random walk. The
classical time evolution operator is e .M I/t where M is the stochastic
transition matrix of the graph. We obtain M from the adjacency matrix of the
graph below:
# Stochastic transition matrix for classical walk
M = A / np.sum(A, axis=0)
We choose as our initial condition j .0/i D j0i, so that the walker starts
on the ﬁrst node. Therefore, due to symmetry, the probability of occupying
each of the nodes besides j0i is the same. In the code below, we deﬁne the
ﬁnal times to simulate the random walks for and create arrays to store the
probability distributions at each ﬁnal time:
# Set up time to simulate for
tmax = 4
steps = 40
time = np.linspace(0, tmax, steps)
# Arrays to hold quantum probabilities and classical probabilities
at each time
quantum_probs = np.zeros((steps, tmax))
classical_probs = np.zeros((steps, tmax))
SECTION 9.8
Quantum Walks
201
We can now simulate the continuous-time quantum and classical walks
for each ﬁnal time we have chosen. The code block below performs this
simulation and stores the ﬁnal probability distributions:
# Do the classical and quantum continuous-time walks
for i, t in enumerate(time):
# Get a quantum program
prog = k_4_ctqw(t)
# Simulate the circuit and store the probabilities
wvf = qvm.wavefunction(prog)
vec = wvf.amplitudes
quantum_probs[i] = np.abs(vec)**2
# Do the classical continuous time walk
classical_ev = expm((M-np.eye(4))*t)
classical_probs[i] = classical_ev[:, 0]
Finally, the code below plots the probabilities for each node at all times:
_, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=True)
ax1.set_title("Quantum evolution")
ax1.set_ylabel("Probability")
ax1.plot(time, quantum_probs[:, 0], label=’Initial node’)
ax1.plot(time, quantum_probs[:, 1], label=’Remaining nodes’)
ax1.legend(loc=’center left’, bbox_to_anchor=(1, 0.5))
ax2.set_title("Classical evolution")
ax2.set_xlabel(’t’)
ax2.set_ylabel("Probability")
ax2.plot(time, classical_probs[:, 0], label=’Initial node’)
ax2.plot(time, classical_probs[:, 1], label=’Remaining nodes’)
The output of this code block is shown in Figure 9.8. Here, we see another
clear difference between the quantum and classical walks. Namely, in the clas-
sical cases, the probability for being in the initial node decays exponentially,
whereas in the quantum case it oscillates! This is what we should expect
based on our construction of the Hamiltonians for each case — namely, the
quantum cases has an i in the exponent e iHt which produces oscillatory
behavior, while in the classical case the exponent is real which produces
purely exponential decay.
202
CHAPTER 9
Quantum Computing Methods
9.9
Uniﬁcation Framework for Quantum
Algorithms (QSVT)
Researchers in the ﬁeld have developed a common framework for the three
major groups of known quantum algorithms: factoring, simulation and search.
This approach is based on the quantum singular value transform (QSVT) (see
[125] and [186]). The QSVT model recognizes that many different quantum
algos can be represented as sequences of unitary transforms and using this
technique we can represent a range of quantum algos with block-encoded
matrices.
 Category I:
– Factoring [254]
– Quantum Phase Estimation [152, 83]
– Linear Systems [135]
 Category II:
– Quantum Simulation [174]
– Quantum Walks [74]
– Linear Combination of Unitaries [37]
– Quantum Signal Processing [177]
 Category III
– Quantum Search [130]
– Amplitude Ampliﬁcation [56, 57]
– Adiabatic [106]
– Fixed Point Amplitude Ampliﬁcation [306]
All of the above algos can be represented in the QSVT framework. Since
operators in quantum computing can be represented as matrices, the goal
of the QSVT approach is to represent the matrix in question as a projected
unitary operator. Let’s take Q… and … to be orthogonal projectors and U to be
As we can see in Figure 9.9, a number of algorithms have been developed
in each of the three main categories of factoring, simulation and search.
Various lines of inquiry in different branches of quantum algos along with
work on pulse sequences for quantum computing on NMR platforms as well
as quantum signal processing led to the realization that algos across different
categories could all be built from common blocks (see [170, 61, 306, 177,
125]). Chuang [78] provides a helpful framework for categorizing the core set
of quantum algos:
SECTION 9.10
Dequantization
203
Figure 9.9: Uniﬁcation of quantum algorithms
Source: [78]
a unitary; we then can encode the matrix A as A WD Q…U …. This encoding is
termed symmetric if Q… D ….
A special case of projected unitary encoding of a quantum operator is
block encoding where we have Q… D … D .j0ih0j/˝a ˝ I (see [125]). In this
case, A can be represented as the upper left block of the unitary operator U ,
like so
A D
 h0j˝a ˝ I

U
 j0i˝a ˝ I

() U D
A




(9.68)
9.10
Dequantization
Researchers have also realized that quantum algorithms that were once thought
to represent classical-quantum separation are not in fact exponentially faster
than their classical counterparts. This line of work began with Tang [273]
and her dequantization of a quantum algo for recommendation systems [151].
Tang and others went on to dequantize additional quantum ML algos. Note that
when we refer to the dequantization of a quantum algo, we are not necessarily
saying that this exact algo has no quantum-classical exponential separation,
but that we can identify a classical algo that accomplishes roughly the same
As in [125], we leave out the dimensions projected out by j0ih0j˝a for
convenience; in other words, A and U have different dimensions, unlike in
projected encodings. For further details on the QSVT model and how a range
of standard quantum algos can be built from these blocks, see [178, 125, 186]
and this book’s companion website.
204
CHAPTER 9
Quantum Computing Methods
Figure 9.10: Sparsity-based QSVT and QRAM-based QSVT algos
Source: [272]
computational task as its quantum analogue and thus there is no quantum-
classical exponential separation. Often this classical algo is quantum-inspired
and we may not have developed it without ﬁrst considering its quantum
analogue. Also note that the quantum analogue may still provide a polynomial
speedup in practice.
In a talk on this subject [272], Tang distinguishes between QML algos in
the category of QRAM-based QSVT which Tang states are generally subject
to dequantization and those in the category of sparsity-based QSVT which are
unlikely to be dequantized (see also the paper with Chia et al. [71]. Quantum
random access memory (QRAM) is a form of memory that can store quantum
states such as states of superposition (see [126] and [68]). As we can see in
been dequantized include the quantum versions of these algos:
 Principal component analysis [273]
 Supervised clustering [273]
 Support vector machines [91]
 Semideﬁnite programming [72]
 Low-rank matrix decompositions [272]
Examples of algos in the sparsity-based QSVT category which are unlikely
to be dequantized include the quantum versions of these algos:
 Sparse linear programming [135]
 Data ﬁtting [296]
 Electromagnetic scattering [82]
 Topological data analysis [175]
 Gaussian process regression [308]
Figure 9.10, examples of algos in the QRAM-based QSVT category that have
SECTION 9.11
Summary
205
We expect to see additional developments in this area over the coming
years as we deepen our understanding of the common frameworks of different
quantum algorithms.
9.11
Summary
In this chapter, we have covered a range of quantum computing methods. We
have seen that a QC can be used for optimization, molecular simulation, true
random number generation and other techniques. For more algorithms please
see and contribute to the Quantum Algorithm Zoo [147]. Also see [183] for
an overview of several key quantum algos.
We have also covered the QSVT framework and developments in the
dequantization of quantum algos with classical analogues. In the coming
chapter we will turn to further developments in classical-quantum separation,
quantum error correction and the road ahead for quantum computing.
CHAPTER
10
As the quantum community eagerly seizes the
impending opportunity to experiment with
NISQ devices, we must not lose sight of the
essential longer-term goal: hastening the
onset of the fault-tolerant era.
—John Preskill
Applications and Quantum
Supremacy
In this work, we have taken a journey through the quantum computing
landscape; we have explored its theoretical foundation, discussed the key re-
search and milestones that advanced the ﬁeld and covered a range of hardware
approaches and quantum computing methods.
On the engineering front, there are still daunting challenges ahead of us
to scale to more than 106 qubits. Once we achieve fault-tolerant quantum
computing, more possibilities open up for applications. In the current NISQ
regime, there is plenty of work to be done to explore test cases and prepare
for the error-corrected machines.
10.1
Applications
As the quantum computing landscape evolves, a number of QC applications
are becoming clear. Check the online site and see [199] and [224] for updates
on QC applications.
Quantum Simulation and Chemistry
High-performance classical computers are used today to model new molecular
combinations. This work helps researchers develop new materials, novel
pharmaceuticals as well as compounds for other applications. Quantum
computers will likely give us new capabilities in this domain. Already, the
VQE and quantum chemistry simulation methods discussed in chapter 9
have shown promising results. See the following for additional examples:
[234, 293, 210]. See also [21] for work on simulating an SYK model through
asymmetric qubitization for an example of a physics simulation.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_10
207
208
CHAPTER 10
Applications and Quantum Supremacy
Sampling from Probability Distributions
We use distribution sampling in many applications such as pattern recognition
and probabilistic inference. With a quantum computer, we can sample from
a much larger distribution. This is one reason that distribution sampling is
being used to demonstrate quantum supremacy as we describe later in this
chapter.
Linear Algebra Speedup with Quantum Computers
There are many application of linear algebra in industry. Matrix inversion,
is a common technique that can be used, for example, in computing electro-
magnetic patterns to design an antenna [224]. The HHL technique which
we covered in chapter 9 is one method that may prove valuable for these
applications.
Optimization
There are numerous optimization applications in industry, including: delivery
truck routing, online ad bidding strategies and mixtures of different chemicals
for electric vehicle battery composition. It is becoming clear that quantum
computers can be used to optimize these kinds of systems.
Tensor Networks
One promising area of inquiry is the application of quantum computation
to tensor networks (TNs). This book’s online site contains references to a
number of good introductions to TNs.
Various tensor network architectures such as MERA, MPS, TTNs and
PEPS are proving to be useful tools to explore questions in physics as well as in
other ﬁelds such as deep learning networks. Several groups have demonstrated
a range of applications of tensor networks [284, 295, 247, 285, 140, 196, 240,
195, 124, 289].
10.2
Quantum Supremacy
The term quantum supremacy, ﬁrst coined by Preskill in 2012, refers to
a computational task that can be efﬁciently performed on a quantum com-
puter beyond the capabilities of state-of-the-art classical supercomputer can
efﬁciently implement [223]. We note immediately — as the strong term
SECTION 10.2
Quantum Supremacy
209
supremacy can generate confusion — that this refers to any computational
task which meets the criteria, not necessarily a task that is useful.
The algorithm used to demonstrate supremacy does not need to have wide
application, just a clear-cut ability to run efﬁciently on a quantum processor
compared with a classical computer where the algorithm is intractable. [45].
In this section, we discuss problems which researchers are considering
to demonstrate quantum supremacy. Regardless of the particular problem
used for the demonstration, quantum supremacy is a landmark achievement in
the history of physics and computer science. While many proof-of-principle
quantum computations have been performed on quantum processors, this will
be the ﬁrst to be executed at a large enough scale to reveal an experimentally
veriﬁable computational separation.
This has implications for verifying quantum mechanics through large-
scale computation. Indeed, we can think of supremacy experiments as the
computational analogue of Bell experiments [136]. Just as Bell experiments
refute local hidden variable models, supremacy experiments on a QC have
now refuted the Extended Church-Turing Thesis (ECTT) which, as discussed
in chapter 4, asserts that any algorithmic process can be simulated efﬁciently
using a probabilistic Turing machine PTM. This then leads us to the Quantum
Extended Church-Turing Thesis (QECTT). The ability to carefully control
quantum systems in this manner is a crowning achievement of engineering
and experimental physics.
In the remainder of the section, we discuss the some of the computational
tasks used in demonstrating quantum supremacy. We will begin with random
circuit sampling which was used in the paper by Arute, et al. [19]. See
also [302] for a subsequent implementation of random circuit sampling for
demonstration of quantum supremacy and see [201] for a guide to random
circuit sampling.
Random Circuit Sampling
Sampling from the output distribution of a quantum circuit is one of the most
natural problems to demonstrate quantum supremacy. To simulate this on a
classical computer, one must perform the linear algebra and matrix computa-
tion to determine the ﬁnal state of the wavefunction after the execution of the
quantum circuit (written as tensor products of unitary operators). However, a
quantum computer naturally performs this calculation by simply evolving in
time under the physical realizations of the unitary operators.
Classical methods for simulating quantum circuits generally scale expo-
nentially in the number of qubits. Speciﬁcally, for the most general, fully
210
CHAPTER 10
Applications and Quantum Supremacy
Name
Number of bytes
Number of qubits
Kilobyte (KB)
210  103
n D 6
Megabyte (MB)
220  106
n D 16
Gigabyte (GB)
230  109
n D 26
Terabyte (TB)
240  1012
n D 36
Petabyte (PB)
250  1015
n D 46
Exabyte (EB)
260  1018
n D 56
Zettabyte (ZB)
270  1021
n D 66
Table 10.1: Table of preﬁxes and the number of bytes they correspond to; the last column
shows the maximum number of qubits that can be stored for the given memory, assuming the
most general state of a qubit with amplitudes stored in double precision. One byte is 8 bits.
entangled, state of n qubits, there are 2n complex amplitudes to keep track
of in the wavefunction. Even for moderate values of n, this quickly reaches
current memory limitations on even the most capable supercomputers.
Each amplitude in the wavefunction is generally a complex number, which
entails storing two real ﬂoating-point numbers per amplitude. Suppose that
these ﬂoating-point numbers are stored in double precision format, i.e., 8
bytes per ﬂoating-point number. Under these assumptions, the total memory
required to store the wavefunction is
2n amplitudes  2 real numbers/amplitude  23 bytes/real number
i.e., 2nC4 bytes. Recall that one kilobyte is deﬁned as 210 bytes, one megabyte
is 220 bytes, and so on; see Table 10.1.
The leading supercomputers have RAM sizes of petabytes to exabytes.1
Based on our previous argument for memory requirements of storing wave-
functions, we can estimate the upper range of quantum circuit simulation for
any particular classical system.
This is the fundamental idea of using quantum circuit sampling as a candi-
date problem for demonstrating quantum supremacy [52, 200]. There are now
multiple methods for simulating quantum circuits — ranging from explicit
construction of the unitary for the circuit to tensor network contractions —
but all of them suffer from exponential complexity in the number of qubits.
Let us now consider in more detail random circuit sampling as a demon-
stration problem of quantum supremacy, following the work of Boixo et
al. [19, 205, 45, 184]. Here researchers are focused on the computational task
of sampling from the output distribution of random quantum circuits.
1See https://www.top500.org/ for an up-to-date list of supercomputers and their specs.
SECTION 10.2
Quantum Supremacy
211
The particular random circuits considered for supremacy experiments are
constructed via the following rules [19, 45]:2
1. Start with a Hadamard gate on each qubit.
2. Apply controlled-Z (CZ) operators between neighboring qubits in a two-
dimensional grid alternating between horizontal and vertical patterns.
Note that in any particular cycle, not all neighboring qubits will be
connected via a CZ, and the number of CZ gates can be different in
different cycles.3
3. Apply single-qubit operators from the set
fX1=2; Y 1=2; T g to the qubits which are not affected by CZ gates ac-
cording to the following criteria:
 If the previous cycle had a CZ gate on a given qubit, apply a
randomly-chosen non-diagonal unary gate to that qubit if possi-
ble.4
 If the previous cycle had a non-diagonal unary gate on a given
qubit, apply a T gate to that qubit if possible.
 Apply a T gate to a qubit if there are no unary gates in the previous
cycles on that qubit (except for the initial Hadamard gate). Note
that this rule is an if and not iff. That is, a T gate may follow
another unary gate; the rule simply states that if there is no unary
gate on that qubit in the previous cycles, then we must place
a T gate in the current cycle. The previous two criteria take
precedence over this one.
 If none of the above criteria are satisﬁed for a given qubit, then a
unary gate is not applied to that qubit for the current cycle.
4. Repeat steps (2) and (3) for a given number of cycles (which determines
the depth).
5. Measure in the computational or Hadamard (X) basis.
We can now incorporate these rules into a program for building supremacy
circuits. An example program in Cirq demonstrating this functionality is
provided below.
import cirq
# Number of rows in grid of qubits
2Note that we have incorporated Boixo’s updated instructions from his GitHub site:
https://github.com/sboixo/GRCS
3Note that the term cycle in this context refers to moments in the framework of Cirq. We
can think of cycles or moments as the set of operators that are applied simultaneously.
4“If possible" means if the circuit has not ended or if there is not a CZ gate on that qubit in
the current cycle.
212
CHAPTER 10
Applications and Quantum Supremacy
nrows = 4
# Number of columns in grid of qubits
ncols = 4
# Depth of CZ gates in supremacy circuit
depth = 5
# Generate the supremacy circuit
supremacy_circuit =
cirq.experiments.generate_supremacy_circuit_google_v2_grid(
nrows, ncols, depth, seed=123)
print(supremacy_circuit)
Here, the number of rows and number of columns are speciﬁed for the two-
dimensional grid of qubits; the depth, or number of cycles of CZ gates, is
speciﬁed to determine the overall depth of the supremacy circuit. The output
of this program is shown in Figure 10.1. Note that in any particular version
or implementation of a random circuit sampler, the code module may be
following rules that are slightly modiﬁed from those stated above.
This circuit of n D 16 qubits is easily handled by classical computers, but,
as we have argued, the difﬁculty of classical simulation scales exponentially
in n. For a sufﬁciently large number of qubits n, we outline the steps toward
demonstrating quantum supremacy [45]:
1. Generate a supremacy circuit U on n qubits and a given depth d as per
above.
2. Sample from the circuit m times with m  103   106 to get an output
distribution fx1; :::; xmg.
3. Compute log 1=pU .xj / for each j D 1; :::; m with a sufﬁciently pow-
erful classical computer. Here,
pU .xj / WD jhxj j ij2
(10.1)
where j i D U j0i is the ﬁnal state of the supremacy circuit.
4. Compute the quantity
˛ D H0   1
m
m
X
1
log
1
pU .xj /
(10.2)
where H0 D log.2n/ C 
 is the cross-entropy of an algorithm which
samples from bit strings uniformly. (Note that the logarithm is the
natural logarithm here.) Here, 
  0:577 is Euler’s constant.
Once the quantity ˛ is computed, it is then compared to a similar quantity
evaluated on the output distribution pA of the best classical algorithm A for
SECTION 10.2
Quantum Supremacy
213
Figure 10.1: Quantum supremacy circuit on a grid of qubits generated by Cirq. The ﬁnal cycle
of Hadamard gates is for measuring in the X basis, but Z basis measurements may be used as
well.
simulating quantum circuits. Note that the cross-entropy difference, which
gives a measure of how well the algorithm A can predict the outcome of a
typical random circuit U , is given by
H.pA/ D H0   H.pA; pU /
(10.3)
Now consider the expectation value of H.pA/ over an ensemble of
random circuits R and let C hold this value:
C WD ERŒH.pA/
(10.4)
In the Boixo et al. supremacy paper [45], it is shown that quantum advan-
tage is achieved in practice when
C  ˛  1
(10.5)
Note that C ! 0 for large enough circuits, and further that pU .xj / can
no longer be obtained numerically. This implies by deﬁnition that the quantity
214
CHAPTER 10
Applications and Quantum Supremacy
˛ can no longer be measured directly. However, it is possible to extrapolate ˛
for larger circuits in order to demonstrate quantum supremacy with random
circuit sampling.
Other Problems for Demonstrating Quantum Supremacy
While random circuit sampling is a very natural problem to consider for
demonstrating quantum supremacy, it is not the only one. A survey paper
by Harrow and Montanaro [136] provides a helpful discussion of additional
major problems being considered to show quantum-classical separation.
The problem of boson sampling is another candidate for demonstrating
quantum supremacy. Originally proposed in [5], boson sampling involves
sending n coincident photons into a randomly generated linear-optical network
of m  n modes (beam splitters); this generates a random unitary rotation.
Detectors are then used to sample from the distribution of photons, a process
which is believed to be classically hard. Boson sampling experiments have
been performed with up to ﬁve photons and nine modes [263]. Experimental
systems are challenged by non-trivial photon loss in the optical network.
Additionally, developing more efﬁcient classical sampling techniques is a
challenge for quantum supremacy via boson sampling.5
Quantum Advantage and Beyond Classical Computation
Researchers have coined several terms related to the distinction between clas-
sical and quantum computing, including quantum advantage, beyond classical
computation and quantum-classical separation. The ﬁeld now recognizes the
following distinctions:
 Quantum supremacy: a computational task which is proven to have
superpolynomial separation between classical and quantum platforms,
but is not necessarily a useful task. The random circuit sampling task is
an example (note that recently some have suggested possible practical
uses for random circuit sampling).
 Quantum advantage: a computational task which is useful and has
a proven superpolynomial separation between classical and quantum
platforms.
See W. Zeng’s article on terms and measures of quantum-classical com-
puting distinctions [307].
5This is true for any problem, P , used for quantum supremacy. Namely, if better classical
algorithms are developed for P , the threshold for quantum supremacy with P gets pushed
back further.
SECTION 10.3
Quantum Error Correction
215
Figure 10.2: Quantum computing roadmap
Source: Google
10.3
Quantum Error Correction
Context and Importance
While today’s quantum computers do not yet have sufﬁcient qubits to support
full quantum error correction (QEC), there is a growing body of research
on QEC with implications for both QC and beyond. Classical computation
admits straightforward error correction through the replication of a state across
many classical bits. The no-cloning theorem in quantum mechanics, however,
prevents us from taking this direct approach in a quantum computer.
A typical approach to QEC involves a surface code which encodes one
logical qubit into a topological state of several physical qubits [60, 86, 116].
When we measure these physical qubits we can see a pattern called a syndrome
which is the result of a particular sequence of errors; a decoder can then map
the syndrome to a particular error sequence. Such decoding may be amenable
to the use of machine learning (see, for example, [24]).
As discussed in the section on VQE, McClean et al. have explored the use
of subspace expansion for error mitigation [190]. See the work of Ofek et al.
for a discussion of the break-even point of QEC [209]).
Error correction schemes have also emerged from other branches of
physics; several researchers have been investigating QEC approaches that
derive from the duality framework of Anti-de Sitter/Conformal Field Theory
(AdS/CFT) [15]. QEC remains an active area of research and is critical in the
scaling quantum computing hardware devices.
A complete treatment of quantum error correction and fault tolerance is the
subject of an entire book itself. In the remainder of this section, we provide a
216
CHAPTER 10
Applications and Quantum Supremacy
pedagogical introduction to the essential elements of quantum error correction.
By mastering this material, you will be well-prepared to take on dedicated
books on the topic and being reading research or review articles.
Important Preliminaries
One of the most important facets of quantum error correction concerns discrete
vs. continuous errors. Consider a single-qubit state
j i D ˛j0i C ˇj1i:
(10.6)
As we have argued previously, there are many more operations one can do on
a qubit compared to a classical bit. This also means that there are more errors
that can occur on a qubit!
E WD e0I C e1X C e2Y C e3Z D e00 C e11 C e22 C e33
(10.7)
for complex coefﬁcients e0; : : : ; e3. We note that these coefﬁcients are con-
tinuous and can be in principle inﬁnitesimally small. How can we hope to
correct such continuous, potentially inﬁnitesimally small errors?
The answer is measurements. Suppose the error E occurs on our qubit
j i, leaving us with a superposition of possible Pauli errors. If we measure an
appropriate property of the state, we collapse the superposition7 into a state
with one discrete error. For concreteness, we can choose our measurement so
that
j i with error E 7! iij i
(10.8)
upon measurement for some i. Here i is the ith Pauli operator and i is a
complex coefﬁcient. Note the crucial fact that i can still be continuous but is
now irrelevant because it is a global phase. In this sense, measurement maps
a continuous error E to a discrete error ij i where i 2 f0; 1; 2; 3g.
6This is not the only error, however. Another classical error is erasure in which 0 7! 0 and
1 7! 0.
7Depending on your favorite interpretation of quantum mechanics, there are different ways
to say this statement, but they are all mathematically identical here.
Concretely, the primary classical error is a bit-ﬂip error.6 The quantum
analogue of this is of course a Pauli-X error, also loosely called a bit-ﬂip error
for obvious reasons: Xj0i D j1i and Xj1i D j0i. However, in the quantum
case there is also the phase-ﬂip error ZjCi D j i and Zj i D jCi, and the
combined bit- and phase-ﬂip error Y D iXZ. The Pauli operators (including
identity) form a basis for 2  2 matrices, so any error E can be written
SECTION 10.3
Quantum Error Correction
217
The discrete error state ij i is easy to correct as long as we know what
i is. If so, we simply apply the inverse operation, which here happens to
be just i since Pauli operators square to identity. For example, suppose
we measure and ﬁnd the state to be ij i D Xj i, i.e., a bit-ﬂip error has
occurred. Then, we simply apply the X operator to get back the original,
error-free state j i D XXj i.
The previous discussion highlights some important facts and major ideas
in quantum error correction. In the language of the ﬁeld, we say we perform
stabilizer measurements and obtain a syndrome. The syndrome tells us what
correction operators to apply to the state to return back to the original error
free state.
You may be wondering: Measurement seems useful to map continuous
errors to discrete errors, but doesn’t it destroy the information in the state?
How can we hope to perform computations if we repeatedly measure? The
answer to these questions is that we encode information in a logical qubit
and perform non-destructive measurements. The following example seeks to
introduce and deﬁne these concepts.
Motivating Example: The Repetition Code
Error correction has a long history in classical information theory, and it is
helpful to draw on its ideas. The simplest classical error correcting code is the
repetition code in which
0 7! 000
and
1 7! 111
(10.9)
That is, rather than a single physical bit, we make three copies of it and form
what we call a logical bit. We treat the logical bit as one unit of information,
even though we know it is composed of three physical bits. The repetition
code is very intuitive and occurs in everyday conversation: if Bob doesn’t hear
Alice, he may ask her to repeat herself. By hearing the phrase more than once,
Bob can use the multiple “copies” of the message to reconstruct her meaning.
The idea with bits is the same. Instead of sending just one physical bit
0 to Bob, Alice sends the logical bit 000. An error may occur on any of the
physical bits in the logical bit. When Bob receives the message, he takes a
majority vote — i.e., if there are more 0s than 1s, he interprets the message as
0, and vice versa — and takes this as the intended message. Note that if Alice
sends 000 and two bits ﬂip, for example Bob receives 101, then the majority
vote yields 1 as the message, which is incorrect. So, there is a logical error in
the repetition code when two or more physical bits ﬂip.
218
CHAPTER 10
Applications and Quantum Supremacy
Suppose each physical bit ﬂips independently with probability p. The
probability of a logical error, pe, is then
pe D 3p2.1   p/ C p3 D 3p2   2p3:
(10.10)
This example illustrates several concepts used in quantum error correction:
(i) redundantly using multiple physical (qu)bits to form a logical (qu)bit, and
(ii) physical errors vs. logical errors. Let’s see how these concepts transfer
concretely with the quantum version of the repetition code.
As stated earlier, the no-cloning theorem states that there does not exist a
unitary operation which can make multiple copies of an arbitrary state. So,
we cannot simply make three copies of a physical qubit j i D ˛j0i C ˇj1i
to j ij ij i as our logical qubit. However, the basis states j0i and j1i are
orthogonal, and so we can make copies of them. In particular, we can map
j i WD ˛j0i C ˇj1i 7! j N i WD ˛j000i C ˇj111i:
(10.11)
Note that we use the common notation j N i to indicate a logical qubit instead
of a physical qubit.
Just like the classical repetition code, there are three physical qubits in
the state j N i WD ˛j000i C ˇj111i, but we treat this as one unit of quantum
information, and so call it a logical qubit. Formally, a logical qubit is a
simply two-dimensional subspace of a larger (physical) Hilbert space. In
this example, the physical Hilbert space is the space of three physical qubits
C2 ˝ C2 ˝ C2 ' C8, and the logical qubit is the subspace spanned by
j000i and j111i. The basis vectors of a logical subspace are also known as
codewords.
As we have seen in the classical repetition code, we are able to correct
errors that occur on one or fewer physical bits. The same will be true in
the quantum case. However, we cannot simply “peek” at the state to take a
majority vote. What we instead do is measure particular operators which tell
us which physical qubit has ﬂipped.
There are four correctable errors which can occur: no physical qubit has
ﬂipped, the ﬁrst physical qubit has ﬂipped, the second physical qubit has
ﬂipped, or the third physical qubit has ﬂipped. To determine which error has
occurred, we measure the projection operators in Table 10.2. To understand
these operators, suppose a bit-ﬂip error occurred on the second qubit. This
means that our logical qubit is j N ei D Xj N i D ˛j010i C ˇj101i. It is not
hard to see that the following holds:
The ﬁrst term 3p2.1   p/ is the probability that any two bits ﬂip, and the
second term p3 is the probability that all three bits ﬂip. We have an advantage
using the repetition code when pe < p, which occurs when p < 1=2. (This
value is found by solving the inequality p < 3p2   2p3.)
SECTION 10.3
Quantum Error Correction
219
Operator
Syndrome
P0 D j000ih000j C j111ih111j
No qubit has ﬂipped
P1 D j100ih100j C j011ih011j
The ﬁrst qubit ﬂipped
P2 D j010ih010j C j101ih101j
The second qubit ﬂipped
P3 D j001ih001j C j110ih110j
The third qubit ﬂipped
Table 10.2: Projection operators for the three-qubit repetition code.
h N ejP0j N ei D 0;
(10.12)
h N ejP1j N ei D 0;
(10.13)
h N ejP2j N ei D 1;
(10.14)
h N ejP3j N ei D 0
(10.15)
In words, we measure P2 with probability 1.
In the previous example, we started with the error state and realized what
operator we would measure. In practice we of course do not know the error
state, but we perform the same procedure. Namely, we implement a circuit
which measures P0, ..., P3 and end up with a syndrome. The syndrome tells
us either no bit has ﬂipped, the ﬁrst bit has ﬂipped, etc. After this, we apply
the appropriate correction operator. For example, if the syndrome was that
the second bit ﬂipped, we would apply Pauli-X to the second physical qubit.
A difference in the quantum case is that bit-ﬂip errors are not the only
errors that can occur! While the three-qubit repetition code can correct
single-qubit bit-ﬂip errors, it cannot correct arbitrary single-qubit errors. For
example, phase-ﬂip errors are undetectable by this code.8 The ﬁrst quantum
error correcting code to correct arbitrary single-qubit errors was Shor’s 9-qubit
code, which actually concatenates two three-qubit repetition codes.
The Stabilizer Formalism
In the previous sections, we have seen important elements of error correction.
We now introduce the standard language for talking about codes, namely
the stabilizer formalism, which was introduced by Gottesman in his PhD
8It is easy to modify the repetition code to correct for phase-ﬂip errors instead of bit-ﬂip
errors, but we skip this presentation for brevity. The key insight is that bit-ﬂip and phase-ﬂip
errors are related by Z D HXH where H is the Hadamard gate.
Analyzing when the quantum repetition code is advantageous to implement
is analogous to analyzing when the classical repetition code is advantageous
to use from above. We assume qubits ﬂip independently with probability p
and again we have advantage in using the repetition code when p < 1=2.
220
CHAPTER 10
Applications and Quantum Supremacy
thesis [128]. The stabilizer formalism leans heavily on group theory (see
chapter 12 for background/review) and may seem cumbersome at ﬁrst, but it
is the de facto standard because it is such a powerful framework.
We say that a state j i 2 C2n is stabilized by operator S 2 C2n2n if
Sj i D j i. (In other words, if j i is a +1 eigenvector of S.) The set of all
such operators is known as the stabilizer of j i. The stabilizer is sometimes
referred to as the stabilizer group for reasons we will see shortly, and its
elements are referred to as stabilizer elements. We will use the notation S to
represent a stabilizer group.
What does this have to do with error correcting codes? Previously, we
speciﬁed a code by its codewords, e.g. j N i D ˛j000i C ˇj111i for the
three-qubit repetition code. In the stabilizer formalism, we specify a code
by its stabilizer group. There is a one-to-one mapping between the two. The
stabilizer group is the set of all operators S such that Sj i D j i for each
codeword j i.
As an example, the stabilizer group for the three-qubit repetition code is
S D fI; Z1Z2; Z1Z3; Z2Z3g. You are encouraged to check that Sj000i D
j000i and Sj111i D j111i for each S 2 S. In this simple example, specifying
the stabilizer group instead of codewords does not seem to provide much
advantage, but for larger, non-trivial codes the utility is unquestionable.
Now that we have motivated stabilizers and connected them to our previ-
ous understanding of quantum error correcting codes, let us introduce them
formally. The Pauli group on n qubits is deﬁned as
Pn WD fp1 ˝    ˝ n W p 2 f˙i; ˙1g and i 2 fI; X; Y; Zg 8i 2 Œng:
(10.16)
That is, Pn is n-fold products of Pauli operators with a phase p. One can
show that Pn is a group, and we encourage the reader to try this exercise.
We now deﬁne a stabilizer group as an abelian subgroup of Pn which does
not contain  I. Why do we require these two conditions? Remember that
we care about the stabilizer group for the purpose of deﬁning quantum error
correcting codes — the codewords j i are the states such that Sj i D j i
for all stabilizer elements S. Suppose that  I was a stabilizer element. Then,
any codeword must satisfy  Ij i D j i, which immediately gives us a
trivial code j i D 0! This explains why we require that  I 62 S. What about
the second condition, that S is abelian? Suppose that that S is non-abelian.
Recall that Pauli operators either commute or anti-commute. Let S1; S2 2 S
such that S1S2 D  S2S1. Then,
S1S2j i D  S2S1j i H) j i D  j i:
(10.17)
SECTION 10.3
Quantum Error Correction
221
The implication follows by deﬁnition of a stabilizer: i.e., S1j i D j i and
S2j i D j i. Thus we again arrive at a trivial codespace. So, the conditions
that S be abelian and  I 62 S ensure the codespace of the stabilizer is
non-trivial.
We now introduce the concept of stabilizer generators. In a previous
example, we stated that the stabilizer group of the three-qubit repetition code
is S D fI; Z1Z2; Z1Z3; Z2Z3g. We would like to write out the group
as succinctly as possible, rather than list all elements explicitly. To this
end, we write S D hZ1Z2; Z2Z3i. Here, the elements Z1Z2 and Z2Z3
are generators and the notation hi means “the set generated by all possible
products” of generators. We can generate the the stabilizer elements from
these generators as follows: the identity I is either generator squared, Z1Z2
is the ﬁrst generator, Z1Z3 is the product of both generators, and Z2Z3 is
the second generator.
Generating sets are not unique. We invite the reader to verify that
hZ1Z3; Z2Z3i D fI; Z1Z2; Z1Z3; Z2Z3g
(10.18)
as well.
Notice that for the three-qubit repetition, regardless of which generators
we pick, there are always two of them. Is it a coincidence that two stabilizer
generators deﬁned on three physical qubits leads to one logical qubit? It is
not! One can show that an n-qubit code with k stabilizer generators has n   k
logical qubits. The ratio k=n is known as the rate of the code. We defer the
proof of this theorem to other texts and instead continue to emphasize the
concepts through examples.
Let us now see another quantum error correcting code which we will
specify using the stabilizer formalism. The ﬁve-qubit code has stabilizer
generators listed below:
S1 D XZZXI
S2 D IXZZX
S3 D XIXZZ
S4 D ZXIXZ
Each generator acts on ﬁve qubits, so n D 5, and there are k D 4 stabilizer
generators. Thus we know that this code deﬁnes n   k D 1 logical qubit. We
invite the reader to verify that S WD hS1; S2; S3; S4i is indeed abelian. The
ﬁve-qubit code can correct arbitrary errors on any physical qubit. One can
show that ﬁve is the smallest number of physical qubits needed in order to
correct arbitrary single-qubit errors, so in this sense the code is optimal.
222
CHAPTER 10
Applications and Quantum Supremacy
So far we have been concerned with how to store information in logical
qubits, but how do we process this information to perform computations?
Similar to how we perform physical operations on physical qubits, we perform
logical operations on logical qubits. Consider the three-qubit repetition
code which has jN0i D j000i. Suppose we want to perform a logical bit-ﬂip
operation NX on this logical qubit. We can do this by applying X to each
physical qubit since
XXXj000i D j111i DW jN1i
(10.19)
Thus, the logical operation is NX D XXX. We invite the reader to verify that
we can take the logical phase-ﬂip operation as NZ D ZZZ.
We reasoned above by thinking about actions on codewords — how do we
treat logical operations in the stabilizer formalism? The key is to think about
commutation relations of operators. For physical Pauli operators, we have
fX; Zg D 0 where f; g denotes the anti-commutator. The logical operations
should also obey this rule. We encourage the reader to check that this is true
for the three-qubit repetition code, i.e., f NX; NZg D fXXX; ZZZg D 0. This
is only half of the story, however: we also need logical operations to commute
with each stabilizer generator. The reason for this is by construction — we will
soon see that correctable errors anti-commute with stabilizer generators. So
requiring logical operations to commute with stabilizer generators means that
they are not errors. Continuing with the guiding example of the three-qubit
repetition code, we invite the reader to verify that Œ NX; S D Œ NZ; S D 0 for
each S 2 hZ1Z2; Z2Z3i.
Back to the ﬁve-qubit code, this provides us with a way to ﬁnd logical
operations without ever knowing what the codewords are. Namely, we need
to ﬁnd operations NX and NZ such that f NX; NZg D 0 and Œ NX; S D Œ NZ; S D 0
for each S 2 S. Can you ﬁnd such operators? One choice9 is NX D XXXXX
and NZ D ZZZZZ. We encourage you to verify that these operators satisfy
the previously stated conditions. If you try to analyze these operators on
the codewords of the ﬁve-qubit code, listed below for reference, you will
immediately appreciate the advantage of the stabilizer formalism!
jN0i D 1
4Œj0000i C j10010i C j01001i C j10100i
C j01010i   j11011i   j00110i   j11000i
  j11101i   j00011i   j11110i   j01111i
  j10001i   j01100i   j10111i C j00101i
9Note that the choice of logical operators is generally not unique.
SECTION 10.3
Quantum Error Correction
223
jN1i D 1
4Œj1111i C j01101i C j10110i C j01011i
C j10101i   j00100i   j11001i   j00111i
  j00010i   j11100i   j00001i   j10000i
  j01110i   j10011i   j01000i C j11010i
We now have two main pieces of the puzzle: stabilizer generators and
logical operations. What about errors? How do we know which errors a given
code can correct? The answer comes in the following theorem, known as the
Knill-Laﬂamme conditions: Given a stabilizer code S, set of errors fEj g is
correctable if E
j Ek 62 N.S/   S for all j and k. Here N.S/ denotes the
normalizer of S in Pn, i.e., the set of all P 2 Pn such that PSP  2 S for all
S 2 S. The proof of this theorem is also outside the scope of our treatment
here but can be found in standard textbooks on quantum error correction.
To complete the discussion on errors and the last major piece of the puzzle,
we introduce the important concept of code distance. So far, we have seen the
three-qubit repetition code and the ﬁve-qubit code, and described them by the
number of physical qubits n and the number of logical qubits k. The former
can only correct single-qubit bit-ﬂip errors, but the latter can correct arbitrary
single-qubit errors. This difference, not captured in n and k, is the purpose
of deﬁning and stating the distance d of a code. The distance d is deﬁned
as the minimum weight error (element) in N.S/   S, where the weight of a
Pauli string is the number of non-identity terms. For example, the weight of
XXI is two. Intuitively, the larger the distance of a code, the more errors it
can correct. Speciﬁcally, a code with distance d D 2t C 1 can correct errors
with weight less than or equal to t.
Now that we have deﬁned distance, we can introduce the standard notation
for codes, which is ŒŒn; k; d, and use it to summarize the concepts we have
seen so far.10 First, we have motivated and deﬁned the difference between
physical qubits and logical qubits. The number of physical qubits in an
ŒŒn; k; d code is n, and the number of logical qubits is k. The rate of a code
is k=n. We introduced the stabilizer formalism and saw that the number of
stabilizer generators is n   k. For each of the k logical qubits, we deﬁne
logical operators NX1; NZ1; :::; NXk; NZk which satisfy f NXi; NZig D 0 for all i and
Œ NXi; NZj  D 0 for all i ¤ j, and further that Œ NXi; S D Œ NZi; S D 0 for each
10Note that classical codes are presented with a single square bracket and quantum codes
with two square brackets by convention.
224
CHAPTER 10
Applications and Quantum Supremacy
i 2 Œk and S 2 S. A distance d D 2t C 1 code can correct errors with
weight less than or equal to t.
10.4
Doing Physics with Quantum
Computers
The key principle here is that in quantum computing we are not merely
modeling a superposition or entangled state and pointing to it from a classical
computer; we are in fact implementing these states, and as such can ask
questions about their dynamics. See also [21] for work on simulating an SYK
model through asymmetric qubitization.
Conclusion
We anticipate a fast pace of development in this ﬁeld — both in hardware
and software — and predict that many more universities and companies will
explore how these platforms can impact their work. We invite the reader to
access the book’s online companion site
http://www.github.com/jackhidary/quantumcomputingbook
for more resources and updates as the ﬁeld progresses. As we drive to a fully
error-corrected quantum computer, it will certainly be an interesting journey.
This exposition reveals only the fundamental concepts of the rich ﬁeld of
quantum error correction. Further topics include the threshold theorem, magic
state distillation (or simply distillation) and fault tolerance, as well as different
classes of error correction codes. For more on these advanced topics, see [62].
We hope this introductory section will help you delve further into the ﬁeld and
perhaps develop the new leading quantum error correction code in the future.
As we mentioned in the preface of this book, one of the most interesting po-
tential uses of quantum computers is to probe open questions in physics. The
duality framework of AdS/CFT gives us an initial mapping between general
relativity and quantum mechanics. Susskind and others have speculated on
the use of quantum computers to explore this duality [268]. While we are
years away from building QCs of sufﬁcient scale to run such experiments, it
is still useful to consider what we might learn from such explorations.
Part III
Toolkit
CHAPTER
11
“To those who do not know mathematics it is
difﬁcult to get across a real feeling as to the
beauty, the deepest beauty, of nature ... If
you want to learn about nature, to
appreciate nature, it is necessary to
understand the language that she speaks in.”
—Richard Feynman
Mathematical Tools for Quantum
Computing I
11.1
Introduction and Self-Test
One of the most important discoveries of quantum mechanics in the twentieth
century was the observation by John von Neumann in his Mathematical
Foundations of Quantum Mechanics that all of quantum mechanics can be
described by linear algebra [286].
Conﬁdent readers may feel that they need not read this chapter and might
instead ﬂip through it for formulas, equations and the like. We provide readers
with the following provocative questions whose answers we will provide
directly afterward.
11.1
Exercise
Self-Test
1. Is the function
T W R ! R
T .x/ WD x C 1
a linear transformation?
2. Does a binary operation have anything to do with binary code?
3. Which space has a bigger dimension: R4 or C2?
4. Of these expressions:
a. h0j1i
b. j0ih1j
c. h0j1i j0i
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_11
227
228
CHAPTER 11
Mathematical Tools for Quantum Computing I
d. hijAjji where A is a matrix and i and j are numbers
Which is a number? A vector? A matrix?
5. Give an example of a Hermitian operator whose eigenvalues are not
real numbers.
The answers:
1. No, it’s hopelessly not linear.
2. No, a binary operation is a special type of function.
3. R4 and C2 have the same dimension over R.
4. Here is the classiﬁcation of the four expressions in Dirac notation:
a. The expression h0j1i is a number. In fact, h0j1i D 0.
b. The expression j0ih1j is a matrix, speciﬁcally, the matrix
 1
0
0
0

c. The expression h0j1i j0i is a vector. You can see that h0j1i is
the number 0 and we mentioned earlier in the book that j0i is the vector
j0i WD
 1
0

so we can write
h0j1i j0i D .h0j1i/ j0i D .0/ j0i D .0/
 1
0

D
 0  1
0  0

D
 0
0

d. This is a number. In particular, it’s a clever way to write the
entry in the ith row and jth column of the matrix A.
5. There is no such thing! Hermitian operators always have real eigenval-
ues; try to prove this statement – we will in this section.
Linear algebra can become very complicated. We don’t want to discourage
you, rather the opposite! We would like to be your ambassadors on a journey
into linear algebra and the supporting abstract mathematics that underlies
quantum computing. We will develop all of the prerequisite mathematics and
offer several examples from the book to relate the tools of linear algebra to
quantum computing. That being said, feel free to begin wherever it is that you
feel comfortable.
SECTION 11.2
Linear Algebra
229
 1
2

1
2
(a) Vector in the plane
1
2
(b) Length of a vector
 1
2

1
2
(c) Angle subtended by a vec-
tor
Figure 11.1
11.2
Linear Algebra
Vectors
The vector is one of the central objects of linear algebra. There are several
ways to conceptualize a vector. First, we may think of a vector as an ordered
collection of numbers (a 1-dimensional array). For example, the following
vector is the ordered collection of the numbers 1 and 2:
 1
2

(11.2)
A vector can be thought of as a geometric object as well. For example, we
may plot the vector
 1
2

in the 2-dimensional plane, as in Figure 11.1a.
Thinking geometrically, a vector can have a magnitude (length) and a di-
rection. For example, by the Pythagorean Theorem, the vector in Figure 11.1a
has length
p
12 C 22
(11.3)
as indicated by the depiction in Figure 11.1b. We can describe the direction
by giving the angle subtended by the arc from the x-axis to the head of the
arrow, as in Figure 11.1c.
We often denote a vector with a lowercase bold letter, like v, or, when
writing by hand, with an arrow, like Ev. We’ll use the simple notation v to
denote a vector when context makes it clear that v is a vector.
230
CHAPTER 11
Mathematical Tools for Quantum Computing I
Sometimes, we write a vector more explicitly, like so:
v D
0
BBB@
v1
v2
:::
vn
1
CCCA
(11.4)
to indicate the number of entries in the vector. So, this vector has n entries.
Some people like to denote vectors with square brackets, as below
v D
2
6664
v1
v2
:::
vn
3
7775
(11.5)
but it doesn’t matter which you use. We prefer round brackets in this discus-
sion.
We have already encountered the qubit, the quantum analog of the classical
bit, earlier in the book. We used the example earlier in the book of a qubit that
represents the polarization of light, which can be vertical or horizontal, or in
some superposition of these states. As discussed in chapter 1, we can denote
the state “vertical polarization” with j0i and the state “horizontal polarization”
with j1i, or equivalently, j"i and j!i.
Vectors offer a convenient mathematical representation for these states.
For instance, we denote the state “vertical polarization” j0i D j"i by the
vector
 1
0

and the state “horizontal polarization” j1i D j!i by the vector
 0
1

.
Introduction to Dirac Notation
Paul Dirac developed a notation for vectors as well as vector and matrix
operations that we use extensively in quantum mechanics and quantum com-
puting [92]. We refer to a vector of the form j'i as a “ket” in Dirac’s bra-ket
notation. We will describe additional uses of Dirac notation as we build our
mathematical toolkit. Dirac notation and its uses are then summarized in
chapter 14.
SECTION 11.2
Linear Algebra
231
Basic Vector Operations
Now that we covered vector notation let’s discuss what we can do with vectors.
There are two quite natural operations to consider. The ﬁrst of these is addition.
We can add two vectors in the way you would expect – just add the entries!
For example, we add the vectors
 1
0

and
 0
1

as follows:
 1
0

C
 0
1

WD
 1 C 0
0 C 1

D
 1
1

(11.6)
The notation “WD” is used to denote equality that is true by deﬁnition
and not by happenstance. For example, we would write 1 C 1 D 2 and not
1C1 WD 2, since the fact that the sum of 1 and 1 is 2 is not by deﬁnition and is
a consequence of other facts. However, we would write N WD f0; 1; 2; 3; :::g to
indicate the set of natural numbers, denoted N, is equal to the set f0; 1; 2; 3; :::g
by deﬁnition and not as a consequence of other facts.
11.7
Exercise
Find the sum of the vectors
 1
2

and
 4
2

.
Note that we cannot add vectors with different numbers of entries. For
example, the expression
0
@
1
0
0
1
A C
 1
0

(11.8)
does not make sense. A moment’s thought convinces us this should be the
case: How would you reasonably add these?1
Let us now consider the next operation: a special kind of multiplication
called scalar multiplication. This operation allows for the multiplication of a
vector by a number, also called a scalar because it scales the vector. Scalar
multiplication also works in the way you might expect – just multiply each
entry of the vector by the number. For example, we would multiply the vector
 1
2

by the scalar 3 as in Equation 11.9.
1Occasionally, computer scientists “pad” the vector which has fewer entries with extra
zeros to make the addition sensible, but we won’t get into this idea here.
232
CHAPTER 11
Mathematical Tools for Quantum Computing I
 1
2

 4
2

 5
4

D
 1
2

C
 4
2

1
4 5
2
4
Figure 11.2: Addition of vectors
3 
 1
2

WD
 3  1
3  2

D
 3
6

(11.9)
11.10
Exercise
Perform the scalar multiplication 4 
 5
6

The operations of vector addition and scalar multiplication have natural
geometric interpretations as well. Perhaps you recall the “head to tail” method
of vector addition from a previous course. It turns out that the vector addition
described above algebraically encodes precisely this approach; see Figure 11.2.
Scalar multiplication by a number corresponds to “scaling” or “stretching”
the vector by that number; see Figure 11.3.
After becoming comfortable with each of these operations, we may mix
and match. For example, we can consider multiplying the vector
 1
0

by
the scalar 3
5, yielding the vector
3
5 
 1
0

D
 3
5  1
3
5  0

D
 3
5
0

(11.11)
and then multiplying the vector
 0
1

by the scalar 4
5, yielding the vector
4
5 
 0
1

D
 4
5  0
4
5  1

D
 0
4
5

(11.12)
and then adding the resulting vectors, yielding
SECTION 11.2
Linear Algebra
233
 5
6

 20
24

D 4 
 5
6

5
20
6
24
Figure 11.3: Scalar multiplication of a vector
 3
5
0

C
 0
4
5

D
 3
54
5

(11.13)
In sum, we refer to the expression
3
5 
 1
0

C 4
5 
 0
1

D
 3
54
5

(11.14)
as a linear combination of the vectors
 1
0

and
 0
1

. We’ll revisit linear
combinations in greater detail later. Using Dirac notation, we can express this
as
3
5 j0i C 4
5 j1i
(11.15)
Having read some of chapter 3, you might recognize this expression as
a superposition of the states j0i and j1i. The numbers 3
5 and 4
5 in the linear
combination above are often called coefﬁcients, or in the language of quantum
mechanics, amplitudes, in the superposition of the states j0i and j1i. The
square of their absolute values,
ˇˇ 3
5
ˇˇ2 and
ˇˇ 4
5
ˇˇ2, are the probabilities of observ-
ing each of the states, j0i and j1i, upon measurement. So, in this example, j1i
234
CHAPTER 11
Mathematical Tools for Quantum Computing I
is more likely to be observed than j0i. Note that in quantum mechanics, we
often use complex numbers as the amplitudes, not real numbers.
11.16
Exercise
You’re invited to verify expression 11.15 is a bona ﬁde
superposition of states in the sense that, as per Born’s rule, the sum of the
squares of the absolute values of the coefﬁcients (or amplitudes, in the lan-
guage of quantum mechanics) 3
5 and 4
5 is in fact 1:
ˇˇˇˇ
3
5
ˇˇˇˇ
2
C
ˇˇˇˇ
4
5
ˇˇˇˇ
2
D 1
11.17
Exercise
Can you ﬁnd a superposition of the states j0i and j1i so
that each of the states j0i and j1i has equal probability of measurement? Be
careful, you have to make sure the squares of the coefﬁcients, or amplitudes,
adds up to exactly 1! If you read carefully, you’ll ﬁnd the answer in previous
chapters.
11.18
A superposition is a linear combination
This leads to an important observation linking quantum mechanics and
linear algebra: a superposition of states can be represented as a linear
combination of the vectors representing their states.
The Norm of a Vector
It is quite natural to ask what the length of a vector is given the geometric
interpretation described above. For vectors in two-dimensional space, our
answer is given by the Pythagorean Theorem. To see this, consider the vector
 3
4

. We may plot this vector in the plane, as in Figure 11.4.
So, by the Pythagorean Theorem, we see that the length of this vector is
the square root of the sum of the squares of the entries, i.e.,
p
32 C 42 D 5.
SECTION 11.2
Linear Algebra
235
3
4
Figure 11.4: Another vector in the plane
11.19
Exercise
Plot the vector
 5
12

in the plane. What is the length
of this vector?
What about vectors with three entries? Let’s imagine the three-dimensional
analog of our previous discussion about lengths of two-dimensional vectors.
Consider the vector
0
@
1
2
2
1
A. We may plot this vector in three-dimensional
space, as in Figure 11.5.
The computation of the length of this vector reduces to two applications of
the Pythagorean Theorem, and ultimately, in the expression
p
12 C 22 C 22 D
p
1 C 4 C 4 D
p
9 D 3
(11.20)
for the length of the vector
0
@
1
2
2
1
A.
11.21
Exercise
Convince yourself that the computation of the length of
a three-dimensional vector reduces to two applications of the Pythagorean the-
orem using the following technique: Think of a point P in three-dimensional
236
CHAPTER 11
Mathematical Tools for Quantum Computing I
x
y
z
F
P D
0
@
1
2
2
1
A
O
Figure 11.5: Vector in space
space. Now, draw the line segment OP whose endpoints are the origin O and
P . From P drop a perpendicular to the ﬂoor. Refer to the point beneath P on
the ﬂoor by the name F . Draw the line segment PF whose endpoints are P
and F . Then, draw the line segment OF whose endpoints are O and F . Now,
you have formed a triangle with vertices OP; PF , and OF .
To compute the length of the vector pointing from the origin to the point
P , we must compute the length of the hypotenuse of the triangle we’ve just
created. We can’t do that just yet. We focus attention on the line segment
OF . It is a line segment on the ﬂoor, a two-dimensional space. Thus, we
can compute the length of the line segment OF using the usual Pythagorean
Theorem. The length of the line segment PF is simply the height off of the
ﬂoor of the point P . So, we know the length of two of the sides of our triangle.
We can ﬁnd the third using a second application of the usual Pythagorean
Theorem. Draw the picture, and compare to the one in Figure 11.5!
So, we should expect that the length of any vector
0
BBB@
v1
v2
:::
vn
1
CCCA is given by a
generalization of the Pythagorean Theorem to n dimensions as
q
v2
1 C v2
2 C ::: C v2n
(11.22)
Remarkably, this is the case! You’re invited to think about why. The reason is
similar to the above exercise. Recalling that the square root is equivalent to
the 1
2 power, and rewriting this expression as
SECTION 11.2
Linear Algebra
237
q
v2
1 C v2
2 C ::: C v2n D
 v2
1 C v2
2 C ::: C v2
n
 1
2
(11.23)
reveals an interesting pattern. What is so special about the number 2? Why
not do this for other numbers? For example, exchange 2 for 3, yielding
3q
v3
1 C v3
2 C ::: C v3n D
 v3
1 C v3
2 C ::: C v3
n
 1
3
(11.24)
These variations on the theme of the length of a vector are known as norms.
More generally, given a vector v D
0
BBB@
v1
v2
:::
vn
1
CCCA, the Lp-norm2 of v is given by
11.25
Deﬁnition
Lp-norm of a vector
jjvjjp WD
 vp
1 C vp
2 C ::: C vp
n
 1
p
We call the usual length of a vector the L2-length to emphasize its calcula-
tion via the L2 norm. Likewise, we refer to the length of a vector calculated
using the Lp norm as the Lp-length. In what follows, we exclusively consider
the L2 norm, and we will make clear when we are using another norm.
The Dot Product
During our discussion of the addition and scalar multiplication operations we
can perform with vectors, the curious reader might have wondered whether
there is a natural way to multiply two vectors. Interestingly enough, the
natural approach where we multiply the corresponding entries is not desirable.
One reason we would like to avoid this type of multiplication for vectors is
that this “product” of two non-zero vectors can often be zero.3 For example,
if we take the two non-zero vectors
 1
0

and
 0
1

we would have:
 1
0


 0
1

D
 1  0
0  1

D
 0
0

(11.26)
2We sympathize with the observant reader who notices that the p value of the norm in the
deﬁnition is noted with a subscript. However, when writing Lp, we use a superscript.
3Note that this component-wise product is known as the Hadamard product. While this
product is named after Jacques Hadamard, it has no relation to the Hadamard operator beyond
the eponymous connection.
238
CHAPTER 11
Mathematical Tools for Quantum Computing I
Who cares? One argument for why this is not desirable is that it lacks
geometric interpretation.4 There is a way, however, of multiplying two vectors
with a useful geometric interpretation that we will now explore.
Curiously, the product we will focus our attention on takes two vectors of
the same number of components as input and yields, not a vector, but rather a
number. As stated earlier, we can refer to a number as a scalar quantity or
scalar to remind us that numbers “scale” vectors. To motivate the deﬁnition
we are about to give, let us return to the notion of the length of a vector. Given
a vector v D
0
BBB@
v1
v2
:::
vn
1
CCCA, we determined that its L2-norm is
jjvjj2 WD
 v2
1 C v2
2 C ::: C v2
n
 1
2
(11.27)
and so the square of its length is the square of its L2 norm
jjvjj2
2 D
 v2
1 C v2
2 C ::: C v2
n
 1
2
2
D v2
1 C v2
2 C ::: C v2
n
(11.28)
Inspecting this expression further, we can express
jjvjj2
2 D v2
1 C v2
2 C ::: C v2
n
(11.29)
equivalently as
v1  v1 C v2  v2 C ::: C vn  vn
(11.30)
So
jjvjj2 D .v1  v1 C v2  v2 C ::: C vn  vn/
1
2
(11.31)
Squaring both sides, we have
jjvjj2
2 D v1  v1 C v2  v2 C ::: C vn  vn
(11.32)
At the very least, this instructs how we could multiply a vector by itself:
multiply the corresponding entries by themselves and then sum these products.
4Another argument is that in most number systems of interest, the product of two numbers
is zero iff at least one of the numbers is zero. Number systems enjoying this property are called
integral domains and are an important class of objects in the study of abstract algebra. We
exploit this property of numbers when solving equations. For example, we solve the equation
x2   1 D 0 by factoring the left-hand side as x2   1 D .x C 1/.x   1/, and then realize that
the only way the product .x C 1/.x   1/ could be zero is if either x C 1 D 0 or x   1 D 0.
We then conclude x D  1 or x D 1.
SECTION 11.2
Linear Algebra
239
More precisely, for a vector v D
0
BBB@
v1
v2
:::
vn
1
CCCA, we’re inspired to deﬁne the product
of v with itself v  v by
v  v WD v1  v1 C v2  v2 C ::: C vn  vn
(11.33)
This is simply the square of the L2 norm of v! Interesting. So, if we deﬁne
our multiplication like this, we recover the square of the length, or L2 norm
of a vector, in the special case that we multiply the vector by itself. Guided by
this example, we deﬁne the dot product of two vectors with an equal number
of entries
u D
0
BBB@
u1
u2
:::
un
1
CCCA ; v D
0
BBB@
v1
v2
:::
vn
1
CCCA
in the manner below.
11.34
Deﬁnition
The dot product of two vectors
u  v WD u1  v1 C u2  v2 C ::: C un  vn
We sometimes refer to the dot product of two vectors as the scalar product
to emphasize that the dot product produces a scalar.
11.35
Exercise
The reader is invited to verify that in the case that u D v,
we recover the square of the usual length or L2, the norm of the vector u.
11.36
Deﬁnition
The L2 norm of a vector as a dot product
v  v D v1  v1 C v2  v2 C ::: C vn  vn D jjvjj2
2
So, the multiplication of two vectors that we’ve deﬁned is convenient
geometrically in that multiplying a vector by itself in this fashion yields a
240
CHAPTER 11
Mathematical Tools for Quantum Computing I
number that can be interpreted as the square of the length of the vector! We’ll
also see later that vectors whose dot product is zero can be thought of as being
orthogonal to one another.5
11.3
The Complex Numbers and the Inner
Product
Complex Numbers
The more experienced reader might be familiar with complex numbers. Com-
plex numbers arise naturally when solving equations. For example, we have
no trouble solving the equation x2   1 D 0. However, solving the equation
x2 C 1 D 0 is more confusing. If we subtract 1 from both sides, we have
the equation x2 D  1, and we are now faced with the question of ﬁnding a
number whose square is negative. There is no real number whose square is
negative, so we introduce a new number, named i, whose square is  1:
i2 D  1
(11.37)
In essence, i is the solution to the equation x2 C 1 D 0, along with  i, of
course (as you should check).
We refer to i as the imaginary number.6 We can then form complex
numbers by combining a real part and an imaginary part, like so:
0 C i;
1 C i;
2 C 3i; and
1
p
2
C 1
p
2
i
(11.38)
Note that the complex in complex numbers is due to the fact that these
numbers consist of two constituent parts, and thus form a complex, not because
they’re complicated (although they might be to some). In fact, every complex
number can be uniquely expressed in the form a C bi for some real numbers
a and b, and so we might as well deﬁne them as such. We usually denote
complex numbers with symbols like z and w, and for a complex number of
5The reader may wonder what the difference is between the terms orthogonal and perpen-
dicular. The term orthogonal is more general than the term perpendicular in that it covers the
case where one of the vectors is the zero vector. In this case, we must use the term orthogonal
and not perpendicular. Let us recall that the dot product of any vector with the zero vector
is zero. However, it doesn’t really make sense to say that a vector and the zero vector are
perpendicular to each other, since there is really no angle formed between a vector and the
zero vector.
6We, along with Gauss, lament this unfortunate nomenclature.
SECTION 11.3
The Complex Numbers and the Inner Product
241
a C bi
a
b
Figure 11.6: Complex number in the plane
the form z D a C bi, we refer to the real part as Re.z/ D R.z/ D a and the
imaginary part as Im.z/ D I .z/ D b. The set of real numbers, R, may be
recovered from the set of complex numbers, C, as the numbers a C bi such
that b D 0, i.e., numbers of the form a C 0i D a.
In an earlier chapter, we discussed Born’s Rule which states that the square
of the modulus of the amplitude of a state is the probability of that state
resulting after measurement. Let us now review the concept of modulus and
the need for it in this context. With real numbers, there is no need to invoke the
modulus, since squaring the number automatically makes it positive. However,
when squaring a complex number, the result can be a negative number. Since
it is not possible to have a negative probability, we require that we apply the
modulus ﬁrst before squaring. The modulus of a complex number a C bi is
deﬁned to be
p
a2 C b2
(11.39)
Thus, the modulus squared is simply a2 C b2, which is always a real, positive
number, which is what we require for a probability in the measurement of
quantum systems.
Now that we are armed with complex numbers, let us examine the proper-
ties of vectors whose components are complex, as in Expression 11.40.
 1 C i
1   i

(11.40)
It is a bit more difﬁcult to develop a geometric interpretation for such
vectors. How would we plot this vector in space? It has four dimensions!7
7We admit that we have not yet given a deﬁnition of dimension. For now, it is ﬁne to
think of dimension in the intuitive sense: a point is 0-dimensional, a line is 1-dimensional, a
plane is 2-dimensional, space is 3-dimensional, etc. We will give a mathematical deﬁnition of
dimension later!
242
CHAPTER 11
Mathematical Tools for Quantum Computing I
However, each single complex number has a natural geometric interpre-
tation as a vector in the plane. To see this, realize that any complex number
a C bi may be identiﬁed with the vector
 a
b

, as in Figure 11.6.
11.41
Exercise
Where do the real numbers “live” in the complex plane?
Can you draw the real line as it is embedded in the complex plane?
The Inner Product as a Reﬁnement of the Dot Product
We have discussed how to determine the square of the norm, or the square of
the length, of such a vector
 a
b

: we compute
p
a2 C b2
2
(11.42)
Naturally, this is how we deﬁne the norm of the complex number a Cbi so
that it corresponds to the square of the length of its corresponding vector when
plotted in the plane. That is, we deﬁne the square of the norm of a complex
number a C bi, which may be thought of as a vector
 a
b

, as a2 C b2.
This deﬁnition of the norm of a complex number might confuse you, and
rightfully so! If you are not confused, consider this. Every number may be
thought of as a vector with only one entry. This is a subtle, but important
philosophical point – every number is a vector, in fact.8 Thinking of a complex
number a Cbi then as a vector v D
 v1

D
 a C bi

with only one entry
and following the deﬁnition of the squared norm of a vector given earlier, we
compute the square of the norm of a C bi via
jjvjj2
2 WD
q
v2
1
2
D v2
1 D .a C bi/2 D .a C bi/.a C bi/
(11.43)
D a2 C a  bi C bi  a C .bi/2 D .a2   b2/ C .2ab/i
(11.44)
remembering that we multiply complex numbers .a C bi/; .c C di/ using the
usual distributive law of multiplication, and that i2 D  1. So, for example,
8... and every vector is a matrix! See the discussion of matrices in Section 12.4.
SECTION 11.3
The Complex Numbers and the Inner Product
243
by the distributive property of multiplication, the product of complex numbers
1 C 2i and 3 C 4i is
.1C2i/.3C4i/ D .13C14iC2i3C2i4i/ D 13C.14/iC.23/iC.24/i2
(11.45)
D 13C.14C23/i C.24/. 1/ D .13 24/C.14C23/i D  5C10i
(11.46)
The reader can regard i as x, replacing any x2 encountered in the com-
putation instead with  1. In fact, this is exactly how algebraists think of
this.
11.47
Exercise
Practice multiplying complex numbers by multiplying
.3 C 4i/.4 C 5i/
Then, multiply
.3 C 4i/.3   4i/:
Does the second multiplication remind you of something we discussed
previously in the Norms section?
In any case, what we have now for the square of the norm
jjvjj2
2 D .a2   b2/ C .2ab/i
(11.48)
does not look at all like what we called the square of the norm a2 C b2 of the
complex number a C bi. Even more disturbing is that .a2   b2/ C .2ab/i
is potentially an imaginary number, since 2ab is likely nonzero (if both a
and b are nonzero). This is disturbing because such an expression cannot be
as easily interpreted as a length and we would like the norm of a complex
number to be interpretable as its length.9
What is going on here?
Let us inspect the expression a2 C b2 a bit more carefully. Clever readers
(who have done the previous exercise) might recognize expression 11.49
(check it using the distributive property).
9Refer to the later section on Hermitian Operators where we discuss why we can’t measure
with complex numbers.
244
CHAPTER 11
Mathematical Tools for Quantum Computing I
R
i
Figure 11.7: The complex plane
a2 C b2 D .a C bi/.a   bi/
(11.49)
The number a   bi is known as the complex conjugate of the number
a C bi. We can also simply call it the conjugate when we’re well-aware that
we’re working with complex numbers. We often denote a complex number
by z, and the conjugate of z by z. For example, if we denote a C bi by z,
then z D a   bi. The act of changing a complex number into its conjugate
is known as conjugating. You might wonder how to conjugate a complex
number like 1   i. The answer is 1 C i. That is, plus becomes minus and vice
versa.
11.50
Exercise
Conjugate the following: 3 C 4i; 3   4i;  1   i.
What if we have a real number a and think of it as a vector
 a
 with
only one entry? Then, the square of the norm of this vector, following the
previous deﬁnition, is simply a2. We may recover the norm of a by taking
(the positive) square root, which makes sense since the length of a should be
simply a, and we do not encounter any difﬁculties as we did with complex
numbers.
One explanation for this peculiarity is that perhaps we should have been
considering real numbers as complex numbers all along. In fact, a previous
exercise asking you to realize the real line as “living” in the complex plane
makes this explicit. You can check your solution to that exercise by referencing
Figure 11.7. What we mean by this is that a real number a can be thought of
as a complex number by expressing it as
a D a C 0i
(11.51)
Writing a as a C 0i reveals that the complex conjugate of a is simply a, since
a C 0i is a   0i D a. Then, the product of a and its complex conjugate is
.a   0i/.a C 0i/ D a  a D a2
(11.52)
SECTION 11.3
The Complex Numbers and the Inner Product
245
In other words, our deﬁnition of the square of the norm for real numbers
remains the same even if we involve the conjugate!
This observation makes us realize that we should deﬁne the square of the
norm of any number, complex or not, as the product of that number with its
conjugate. More precisely, using the previously introduced notation,
jjzjj2
2 WD zz
(11.53)
We often omit this extensive notation when the context makes clear that
we are using the L2 norm, replacing it with jzj2 D zz.
This discussion of how to deﬁne the square of the norm of any number
instructs our deﬁnition of the square of the norm of any vector, complex
entries or not. For any vector v D
0
BBB@
v1
v2
:::
vn
1
CCCA, we deﬁne the square of the norm
of v to be:
11.54
Deﬁnition
Squared norm of a vector
jvj2 WD v1  v1 C v2  v2 C ::: C vn  vn
11.55
Exercise
You’re encouraged to verify that this deﬁnition recovers
the previous deﬁnition for vectors whose entries are exclusively real numbers,
and also that it effectively computes the square of the norm of any real or
complex number.
Generalizing this, we deﬁne the inner product of two vectors (again, with
equal number of entries):
u D
0
BBB@
u1
u2
:::
un
1
CCCA ; v D
0
BBB@
v1
v2
:::
vn
1
CCCA
to be
246
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.56
Deﬁnition
Inner product of two vectors
hu; vi WD u1v1 C u2v2 C ::: C unvn
11.57
Exercise
Similarly, you’re invited to verify that this deﬁnition
recovers the square of the norm of any real or complex number in the case
u D v is a real or complex number, i.e., for any real or complex number z,
hz; zi D jzj2. You should also verify that the deﬁnition of the square of the
norm is recovered for a vector whose entries are all real numbers.
It should be noted that the dot product and the inner product agree only
if the entries of the vectors we are considering are exclusively real numbers.
The inner product generalizes this operation to vectors with complex entries.
Recalling Dirac notation, you will likely notice the uncanny resemblance
of the notations hu; vi and hujvi – the only difference is the line in the middle!
Paul Dirac was probably inspired by the inner product notation when he
decided on his notation. That being said, if we have two vectors, written in
Dirac notation as huj and jvi, we write their inner product as
hujvi WD hu; vi
(11.58)
So, the notation hujvi literally means “the inner product of the vectors u
and v.”
So, in what follows, we will assume that our vectors may include complex
entries. We will revisit the inner product from a more formal and abstract
point of view later on in our description of a Hilbert space.
The Polar Coordinate Representation of a Complex Number
We now explore a remarkable connection between complex numbers and
geometry. Every complex number can be thought of as living in the two-
dimensional plane depicted in Figure 11.7.
We have discussed how a complex number of the form z WD aCbi may be
thought of as a vector
 a
b

. Thinking of z as a vector in two-dimensional
space should convince us that it’s reasonable to assign a complex number an
L2-norm, i.e., a length. Speciﬁcally, the complex number z D aCbi, thought
SECTION 11.3
The Complex Numbers and the Inner Product
247
0
=2

3=2
p
a2 C b2

a C bi
Figure 11.8: Angle subtended by radius
of as a vector
 a
b

, has L2 norm given by the Pythagorean Theorem:
p
a2 C b2
(11.59)
Visualizing complex numbers in this fashion should also make it reasonable
to assign a complex number an angle. Speciﬁcally, we assign the complex
number a C bi the angle, , subtended by the arc from the positive real
axis to the head of the vector
 a
b

in the counterclockwise direction, as in
Figure 11.8.
We can use some simple trigonometry to describe this angle. Let’s refer to
the angle by . Then,  lives in the right triangle depicted in Figure 11.9
and so satisﬁes the equation
tan./ D b
a
(11.60)
If you’re shaky on trigonometry, remember that the tangent of an angle 
in a right triangle is the opposite over the adjacent. Let’s recall the inverse
tangent function, known by the name tan 1, or sometimes arctan. We’ll use
248
CHAPTER 11
Mathematical Tools for Quantum Computing I
0
=2

3=2
p
a2 C b2

a C bi
a
b
Figure 11.9: Right triangle where the angle lives
the name arctan.10 The role of the inverse tangent function is to take a number
as input and give an angle as output. For example, arctan.y/ means “the angle
that makes tangent equal to y.” Since
tan./ D b
a
(11.61)
applying arctan to both sides yields the equation
arctan .tan.// D arctan
b
a

(11.62)
So, we can express the angle, , in terms of a and b as
 D arctan
b
a

(11.63)
All this being said, we can refer to the L2-norm of the complex number as
its radius, and refer to the angle as its... well, angle. In summary, a complex
number a C bi has a radius
10tan 1 could be confused with the reciprocal function of tangent, known as cotangent,
which is expressed .tan/ 1 D
1
tan D cot.
SECTION 11.3
The Complex Numbers and the Inner Product
249
r WD
p
a2 C b2
(11.64)
and an angle
 WD arctan
b
a

(11.65)
We can also describe a complex number by simply giving a radius and
angle. For example, let’s try to ﬁgure out which complex number is described
by the radius 1 and the angle  WD 
4 radians (45 degrees). If we write the
complex number temporarily as a C bi, our problem reduces to determining
the numbers a and b given the radius r and the angle .
We know from the previous paragraph that the radius r of a complex
number of the form a C bi is given by
r D
p
a2 C b2
(11.66)
and we know that the angle  is given by
 D arctan
b
a

(11.67)
If we now apply tan to both sides of 11.67, we obtain
tan./ D b
a
(11.68)
Since  D 
4 ,
tan./ D tan

4

D 1
(11.69)
So,
1 D tan

4

D tan./ D b
a
(11.70)
and we see that
1 D b
a
(11.71)
Multiplying by a on both sides reveals a D b. Great, so now we know that
a D b!
Knowing that a D b, we look toward the equation
r D
p
a2 C b2
(11.72)
We are told that the radius r is equal to 1, so we know that
1 D r D
p
a2 C b2
(11.73)
250
CHAPTER 11
Mathematical Tools for Quantum Computing I
and since a D b, we may replace b with a yielding instead
1 D r D
p
a2 C b2 D
p
a2 C a2 D
p
2a2
(11.74)
Squaring both sides yields 1 D 2a2, and dividing both sides by 2 reveals that
1
2 D a2. Then, taking the square root of both sides reveals that:
a D
r
1
2
(11.75)
It turns out that
r
1
2 D
p
1
p
2
D
1
p
2
(11.76)
so we now know that
a D b D
1
p
2
(11.77)
Fantastic — we now know our complex number a C bi described by the
radius 1 and the angle 
4 is actually
a C bi D
1
p
2
C 1
p
2
i
(11.78)
So, we have converted the polar expression of a complex number given by
radius 1 and angle  D 
4 to what we refer to as its rectangular expression
1
p
2 C
1
p
2i:
11.79
Deﬁnition
Rectangular (or Cartesian) and polar form of a
complex number
In general, we say that a complex number of the form a C bi is given in
rectangular or Cartesian form, and that a complex number described by a
radius r and an angle  as .r; / is given in polar form.
11.80
Exercise
Check that z D
1
p
2 C
1
p
2i enjoys the above properties,
i.e., z has radius 1 and angle 
4 .
11.81
Exercise
Try to convert the Cartesian expression
p
3
2 C 1
2i to polar
form by determining its radius and angle.
SECTION 11.3
The Complex Numbers and the Inner Product
251
0
=2

3=2
r D 1
 D 
4
z
Figure 11.10: Unit complex number with angle 
4
11.82
Exercise
To see that Cartesian expressions can sometimes have
“ugly” polar expressions, convert 1 C 2i to polar coordinates and check that
the radius is
p
5 and the angle is arctan
  2
1

, which, unfortunately, cannot be
expressed in any more familiar way.11
Recalling a bit of trigonometry, we could have determined this already!
Given the radius r D 1 and the angle  D 
4 radians, we have the picture in
Figure 11.10.
Recalling the deﬁnition of sine and cosine, we see that the value for a
should be
a D r  cos./
(11.83)
and the value for b should be
b D r  sin./
(11.84)
We can reformulate a as
11In fact, the arctan of any natural number is irrational.
252
CHAPTER 11
Mathematical Tools for Quantum Computing I
0
=2

3=2
r D 1
 D 
4
z
sin
  
4

D
p
2
2
cos
  
4

D
p
2
2
Figure 11.11: The trigonometry of a complex number
a D 1  cos

4

D
p
2
2
(11.85)
and b as
b D 1  sin

4

D
p
2
2
(11.86)
11.87
Exercise
Check that
p
2
2 D
1
p
2 by cross-multiplying.
The above exercise makes us realize we could have determined a and b all
along just having recalled some basic trigonometry!
So, we’ve discussed how a complex number gives rise to a radius and
an angle, and conversely, how a radius and an angle give rise to a complex
number. We realize now that the descriptions are equivalent!
11.88
Exercise
Convince yourself that giving a complex number in the
form a C bi is equivalent to giving a radius and an angle .r; /. Recall
SECTION 11.3
The Complex Numbers and the Inner Product
253
0
=2

3=2
1

ei D cos./ C isin./
Figure 11.12: Euler’s formula
that we say that the form a C bi is rectangular or Cartesian (for Cartesian
coordinates) and that the form .r; / is polar.
We can take this one step further with Euler’s formula (pronounced “oiler”).
Euler’s formula states that a complex number z with radius 1 and angle , i.e.,
a complex number living on the “unit circle” can be expressed as
z D ei D cos./ C isin./
(11.89)
That z D cos./ C isin./ should not be surprising, as it follows our
previous discussion, but rather that the number e comes into play here. If
you’re not familiar with the number e, check out our explanation in chapter
13. This formula is one of the most remarkably beautiful formulas in all of
mathematics for many reasons, including the deep connection it illuminates
between complex numbers — a priori, an algebraic phenomenon — and
geometry. We discuss this equation further and prove its validity in our
chapter about additional mathematical topics. For now, we’d like for you to
take away the following idea:
254
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.90
Euler’s formula
A complex number z with radius 1 and angle  can be expressed as
z D ei
That being said, if our complex number z has radius r instead of 1, and
angle , we may write as z D rei, following Euler.
11.91
Exercise
Convince yourself that a complex number z whose radius
is r and whose angle is  can be expressed z D rei using Euler’s formula as
above.
We’ll exploit this idea later to deﬁne a special class of transformations that
rotate space by any speciﬁed angle!
11.4
A First Look at Matrices
Basic Matrix Operations
Having read chapter 3, you encountered matrices, which a priori resemble
rectangular grids of numbers, e.g., the Pauli X operator x (also known as
the NOT operator):
x D X WD
 0
1
1
0

(11.92)
and the Pauli Z operator z:
z D Z WD
 1
0
0
 1

(11.93)
from chapter 3.
Matrices might even involve complex numbers, like the Pauli Y operator
y:
y D Y WD
 0
 i
i
0

(11.94)
We can multiply matrices by numbers, like you’ve seen with the Hadamard
operator from chapter 3. Just multiply all of the entries by that number, like
so:
SECTION 11.4
A First Look at Matrices
255
H WD
1
p
2
 1
1
1
 1

WD
 
1
p
2  1
1
p
2  1
1
p
2  1
1
p
2  . 1/
!
(11.95)
D
 
1
p
2
1
p
2
1
p
2
  1
p
2
!
(11.96)
We can also add two matrices by adding their corresponding entries, like
so:
 1
2
3
4

C
 5
6
7
8

WD
 1 C 5
2 C 6
3 C 7
4 C 8

D
 6
8
10
12

(11.97)
A matrix is a representation of a more fundamental object called a linear
transformation. In fact, so is a vector, since we may think of a vector as an
n  1 matrix. The term transformation begs the question transformation of
what?
11.98
Matrices transform space
A matrix does not merely transform a particular vector or set of vectors; it
transforms an entire vector space.
To gain an understanding of how a matrix might be thought of as a trans-
formation of space, we will consider a few geometric examples. However,
before we can explain these geometric examples, we will need to learn how to
multiply a vector by a matrix. This will seem a bit weird at ﬁrst, but we will
explain why it’s deﬁned this way later on in this text.
Let
 e
f

(11.99)
be a vector and
 a
b
c
d

(11.100)
be a matrix.
We multiply the vector
 e
f

by the matrix
 a
b
c
d

like so:
 a
b
c
d
  e
f

WD
 a  e C b  f
c  e C d  f

(11.101)
If you’re observant, you might recognize a  e C b  f resembles the dot
product described earlier. In fact, each of the expressions a  e C b  f and
256
CHAPTER 11
Mathematical Tools for Quantum Computing I
c  e C d  f (respectively) are literally the dot products of the vectors
 a
b

and
 e
f

, and of
 c
d

and
 e
f

(respectively).12
In other words, for vectors with exclusively real entries, the multiplication
we’ve just deﬁned can be thought of as a sequence of dot products!
Recall the Pauli Z matrix:
Z WD
 1
0
0
 1

(11.102)
and consider the vector:
 0
1

(11.103)
Recall from the previous description that we write the product of the matrix
 1
0
0
 1

(11.104)
and the vector
 0
1

(11.105)
as
 1
0
0
 1
  0
1

;
(11.106)
and we multiply them as follows
 1
0
0
 1
  0
1

WD

1  0 C 0  1
0  0 C . 1/  1

D

0
 1

(11.107)
We could say then that the matrix has transformed the vector
 0
1

into
the vector

0
 1

!
11.108
Exercise
Figure out where the vector
 1
0

is sent via this
transformation. More precisely, multiply the vector
 1
0

by the matrix
12Actually, if all of a; b; c; d; e; f are strictly real numbers, then these dot products are
literally the inner products of the vectors.
SECTION 11.4
A First Look at Matrices
257
 1
0
0
 1

and see where the vector
 1
0

ends up! Do you see a relation-
ship between the transformed vectors and the columns of the matrix? Try to
formulate a conjecture!
In fact, we may use Dirac notation to express the relationship between the
matrix Z and the states j0i and j1i like so:
Z jji D . 1/j jj i
(11.109)
for all j 2 f0; 1g, as you should check!
11.110
Exercise
Learn why the NOT operator X D
 0
1
1
0

is also
known as the “bit ﬂip” operator by showing that multiplying the vector
 1
0

,
which represents the state j0i, by the matrix X “ﬂips the state” to j1i, i.e., the
vector
 0
1

.
We can likewise multiply two matrices as follows:
 a
b
c
d
  e
f
g
h

WD
 ae C bg
af C bh
ce C dg
cf C dh

(11.111)
A helpful graphic depicting matrix multiplication is given in Figure 11.13.
11.112
Exercise
Realize that the multiplication of a vector by a matrix
described earlier can in fact be thought of as the multiplication of two matrices,
where we think of the vector as an n  1 matrix.
Extending the idea of realizing matrix-vector multiplication as simply
matrix-matrix multiplication as described in the exercise above, we can also
multiply matrices of different dimensions. For example, consider the matrices
0
@
a
b
c
d
e
f
1
A
(11.113)
258
CHAPTER 11
Mathematical Tools for Quantum Computing I
a11
a12
a21
a22
a31
a32
a41
a42
2
6666666666664
3
7777777777775
2
6666666666664
3
7777777777775
b11
b12
b13
b21
b22
b23
2
6664
3
7775
A
B
a11b12 C a12b22
a31b13 C a32b23
Figure 11.13: Matrix multiplication
and
 g
h
i
j
k
l

(11.114)
where the matrix entry i is simply the letter, not the imaginary number i.
Then, we may multiply these matrices like so:
0
@
a
b
c
d
e
f
1
A
 g
h
i
j
k
l

WD
0
@
ag C bj
ah C bk
ai C bl
cg C dj
ch C dk
ci C dl
eg C fj
eh C f k
ei C f l
1
A
(11.115)
It should be noted that the product of a 32 matrix and 23 matrix yields
a 3  3 matrix!
Now that we have seen how to multiply a vector by a matrix, we can see
the method for multiplying two matrices, as follows:
SECTION 11.4
A First Look at Matrices
259
m
n
A
n
p
B
D m
p
AB
Figure 11.14: Size of the matrix product
 g
h
i
j
k
l
 0
@
a
b
c
d
e
f
1
A WD
 ga C hc C ie
gb C hd C if
ja C kc C le
jb C kd C lf

(11.116)
This should be a surprise – we got a 2  2 matrix by multiplying the two
matrices in the opposite order!
11.117
Size of the matrix product
In general, if A is an m  n matrix and B is an n  p matrix, then AB is
an m  p matrix.
The picture in Figure 11.14 depicts this phenomenon, exempliﬁed by
equations 11.115 and 11.116.
So, when multiplying two matrices, the “inner” dimensions must agree,
and the resulting matrix will have dimensions equal to the “outer dimensions”
of the two matrices.
When faced with the following expression
 1
0
0
 1
  0
1
1
0
  1
0
0
 1

(11.118)
we might wonder how to compute it given that we’ve not discussed how to
multiply three matrices at a time. Well, think of an analogous scenario. When
asked to compute the product 2  3  4 what do we do? Well, you have probably
never met anyone capable of multiplying three numbers at a time (at least, we
have never met anyone like that). However, we know that it’s fair to do either
of the following things in an effort to compute the product:
Either we compute:
.2  3/  4
(11.119)
or we compute:
260
CHAPTER 11
Mathematical Tools for Quantum Computing I
2  .3  4/
(11.120)
That is, we either multiply 2 and 3 ﬁrst, and then multiply the result (6) by
the remaining 4, or we multiply 3 and 4 ﬁrst, and then multiply the result (12)
by the remaining 2. What is remarkable is that we get the same answer (24)
either way!
We can hope that the same is true for matrices. Explore this idea in the
following exercise:
11.121
Exercise
Recognize
 1
0
0
 1
  0
1
1
0
  1
0
0
 1

as the product ZXZ of the matrices Z D
 1
0
0
 1

and X D
 0
1
1
0

.
Then, compute the product ZXZ by ﬁrst computing ZX and then multiplying
the result by the remaining Z on the right. More precisely, compute
.ZX/Z
Then, compute
Z.XZ/
and check that you get the same answer either way! So, you’ve shown that, at
least for these three matrices, matrix multiplication is an associative operation.
We’ll revisit associative operations later on when we discuss the formal
deﬁnition of a vector space. We won’t prove that matrix multiplication is
associative in general here. When we say that matrix multiplication is asso-
ciative, we mean that for all matrices A; B; C, .AB/C D A.BC/ whenever
the product makes sense, i.e., the dimensions agree. In fact, we won’t bring
the issue up again until after we’ve shown that multiplication of matrices is
equivalent to composition of functions, at which point the fact that matrix
multiplication is associative will become an obvious fact!
So, when faced with a product like
ZX j0i D
 1
0
0
 1
  0
1
1
0

j0i
(11.122)
we ﬁrst think of the vector on the right as a 2  1 matrix. We then realize that
we may ﬁrst compute the product of the two matrices on the left, i.e.,
SECTION 11.4
A First Look at Matrices
261
ZX D
 1
0
0
 1
  0
1
1
0

D

0
1
 1
0

(11.123)
then apply the resulting matrix to the vector (2  1 matrix) j0i, like so:

0
1
 1
0

j0i D

0
1
 1
0
  1
0

D

0
 1

(11.124)
Or we could iteratively apply the matrices to the vector j0i, ﬁrst applying
the matrix X
X j0i D
 0
1
1
0
  1
0

D
 0
1

D j1i
(11.125)
then applying the matrix Z to the resulting vector, like so
Z.X j0i/ D Z.j1i/ D Z
 0
1

D
 1
0
0
 1
  0
1

D

0
 1

(11.126)
Either way, we get the same result!
Please note that when applying a sequence of matrices to a vector, we go
from right to left, sometimes said “inside-out,” so in this case, we apply X
ﬁrst, then Z. However, when multiplying the matrices, we go from left to
right. What we are realizing is that:
11.127
Multiplying several matrices
The result of applying the composition of two transformations to a space
is equivalent to applying them iteratively.
Again, we won’t prove this here, but we promise, it will become obvious
once we’ve established the correspondence between matrix multiplication
(and thus, multiplication of vectors by matrices) and composition of functions.
The Identity Matrix
Thinking of matrices as transformations of space leads us to believe that there
should be a matrix that does not transform the space at all. In other words, it
is natural to ask if there is a matrix that has no effect on any of the vectors it
multiplies. In two dimensions, the answer is the matrix
I2 WD
 1
0
0
1

(11.128)
262
CHAPTER 11
Mathematical Tools for Quantum Computing I
The subscript 2 is there to inform us that this is the identity matrix for 2-
dimensional space. We refer to this matrix as the identity matrix because it
preserves the identity of the vectors it acts on.
Let’s see that the matrix I2 deserves its name. Let’s multiply the vector
 1
0

by the matrix I2 to see what we get:
I2 
 1
0

WD
 1
0
0
1


 1
0

D
 1  1 C 0  0
0  1 C 1  0

D
 1
0

(11.129)
It’s the same vector! We leave it to you to check that the vector
 0
1

is also
preserved under multiplication by I2.
11.130
Exercise
Verify that the vector
 0
1

is unchanged under multi-
plication by I2.
However, the matrix just described is the identity matrix for 2-dimensional
space. What if we want an identity matrix for three-dimensional space? No
problem – just make the matrix a little bigger:
I3 WD
0
@
1
0
0
0
1
0
0
0
1
1
A
(11.131)
We also call this the identity matrix, although it is speciﬁc to three-dimensional
space.
11.132
Exercise
Multiply each of the vectors
0
@
1
0
0
1
A ;
0
@
0
1
0
1
A ;
0
@
0
0
1
1
A
by this new identity matrix for three-dimensional space and check that it
deserves its name.
In fact, each dimension has its own identity matrix, as you might now
expect! To build the identity matrix for n-dimensional space, simply create an
n  n matrix with 1’s along the diagonal and 0’s elsewhere, like so
SECTION 11.4
A First Look at Matrices
263
In WD
0
BBB@
1
0
: : :
0
0
1
: : :
0
:::
:::
:::
0
0
0
0
1
1
CCCA
(11.133)
This matrix has the property that it preserves any vector of n-dimensions that
it acts on, as you should verify.
Transpose, Conjugate and Trace
We have now seen that not all matrices have to be square. For example,
consider
A WD
0
@
a
b
c
d
e
f
1
A ; B WD
 g
h
i
j

(11.134)
The matrix A has 3 rows and 2 columns and B has 2 rows and 2 columns,
so B is square, while A is not. A very natural operation to consider when
thinking of matrices as rectangular grids of numbers is the transpose. Here
are the transposes, denoted AT and BT , of the above matrices:
AT WD
 a
c
e
b
d
f

; BT WD
 g
i
h
j

(11.135)
What happened? We could describe this operation as turning rows into
columns and vice versa. Visual learners might recognize that transposing a
matrix is reﬂecting its entries over an imaginary line extending from the upper
left-hand corner of the matrix to the lower right-hand corner. We can just
as well transpose the transposed matrices, i.e., compute .AT /T and .BT /T .
You’re invited to check that .AT /T is just A again in an exercise below.
11.136
Exercise
Find the transpose of the following matrix:
0
BB@
1
2
3
4
5
6
7
8
9
10
11
12
1
CCA
11.137
Exercise
Check that transposing the already transposed matrix
AT yields A. More precisely, check that .AT /T D A. So, the operation of
transposing inverts itself!
264
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.138
Exercise
For which matrices A is A equal to AT ? We call such
matrices symmetric (for good reason). If a matrix is symmetric and all of its
entries are real numbers, we call it real symmetric. Real symmetric matrices
are very special matrices that deserve quite a bit of attention, as we’ll see later.
We can transpose vectors just as well as matrices. To see this, we recognize
that we can think of any vector as a matrix. For example, think of the vector
 1
0

as a 2  1 matrix, so its transpose is the 1  2 matrix
 1
0
T
D
 1
0

(11.139)
11.140
Exercise
Find the transpose of the vector
 0
1

, i.e., ﬁnd
 0
1
T
We’d like to mention now why it is that we consider the vector
 1
0

and its transpose
 1
0
T
D
 1
0

different objects. Of course, they are
visually different, but the difference is more than just in their presentation.
For starters, we cannot multiply the vector
 1
0

by itself, i.e., the ex-
pression
 1
0
  1
0

(11.141)
does not make sense, as you should recall from our previous discussion about
appropriate dimensions for the product of two matrices. Explicitly, the vector
 1
0

has dimensions 2  1, and we see that the product of a 2  1 matrix
SECTION 11.4
A First Look at Matrices
265
with a 2  1 matrix does not make sense since the “inner” dimensions, 1 and
2, do not agree.
However, the expression
 1
0
T  1
0

D
 1
0
  1
0

(11.142)
makes perfect sense, since the inner dimensions are both 2, as you should
verify. Thinking of each of
 1
0
T
and
 1
0

as matrices and following
the description of matrix multiplication described above, we have
 1
0
T  1
0

D
 1
0
  1
0

D 1  1 C 0  0 D 1
(11.143)
This result makes sense since we multiplied a 1  2 matrix by a 2  1 matrix
and got a number, i.e., a 1  1 matrix!
Another fun operation we can perform on matrices is conjugation. Yes, the
same conjugation from earlier! Let’s see how this works. Consider the matrix
C WD
 1 C i
0
0
1   i

(11.144)
We denote by C the conjugate of the matrix C.
C WD
 1 C i
0
0
1   i

D
 1   i
0
0
1 C i

(11.145)
To conjugate a matrix, just conjugate each of its entries.
11.146
Exercise
Figure out when a matrix is equal to its conjugate. Hint:
When is a 1  1 matrix, i.e., a number, equal to its conjugate?
Of course, viewing a vector as a matrix allows us to conjugate any vector.
For example, the conjugate of the vector
 1 C i
1   i

is the vector
 1 C i
1   i

WD
 1 C i
1   i

D
 1   i
1 C i

(11.147)
Let’s tie a few concepts together now by expressing the inner product of
two vectors using the notation we’ve developed.
266
CHAPTER 11
Mathematical Tools for Quantum Computing I
Given two vectors u D
0
BBB@
u1
u2
:::
un
1
CCCA and v D
0
BBB@
v1
v2
:::
vn
1
CCCA, we would express
their inner product via
hu; vi WD u1  v1 C u2  v2 C ::: C un  vn
(11.148)
using our earlier notation. Notice that
u1 v1 Cu2 v2 C:::Cun vn D
 u1;
u2;
: : : ;
un

0
BBB@
v1
v2
:::
vn
1
CCCA (11.149)
which we may express using our recently developed notation for transpose as
follows:
 u1;
u2;
: : : ;
un

0
BBB@
v1
v2
:::
vn
1
CCCA D
0
BBB@
u1
u2
:::
un
1
CCCA
T 0
BBB@
v1
v2
:::
vn
1
CCCA
(11.150)
and we may further adorn this expression with our new notation for the
conjugate of a matrix (and thus a vector) as follows:
0
BBB@
u1
u2
:::
un
1
CCCA
T 0
BBB@
v1
v2
:::
vn
1
CCCA D uT v
(11.151)
All in all, we may compactly express the inner product of vectors u and v as
simply uT v – fantastic! So, in summary, we have the following equivalent
expressions for the inner product of two vectors u and v:
hu; vi D hujvi D uT v
(11.152)
By convention, we can refer to uT as u, so we have
hu; vi D hujvi D uT v D uv
(11.153)
SECTION 11.4
A First Look at Matrices
267
We can go even further to relate this to Dirac notation! What the above
string of equalities reveals is that we may think of the “bra” huj of a vector u
as the conjugate-transpose of the vector u, i.e., for a vector
0
BBB@
u1
u2
:::
un
1
CCCA,
huj jvi D uT v D
0
BBB@
u1
u2
:::
un
1
CCCA
T 0
BBB@
v1
v2
:::
vn
1
CCCA
(11.154)
D
 u1;
u2;
: : : ;
un

0
BBB@
v1
v2
:::
vn
1
CCCA
(11.155)
Given that we now have the operations of transposition and conjugation
at our disposal, we may apply both to a matrix to yield what is known as its
conjugate transpose.
11.156
Exercise
First, conjugate the Pauli Y operator Y , i.e., the matrix
Y D
 0
 i
i
0

. Then, transpose the result. We call this the conjugate trans-
pose of the matrix Y . Do you notice something special about the relationship
between Y and its conjugate transpose?
We’ll show now that you could equivalently transpose the matrix Y and
then conjugate.
First, we transpose Y , yielding
Y T D
 0
 i
i
0
T
D
 0
i
 i
0

(11.157)
Then, we conjugate Y T :
.Y T / D
 0
i
 i
0

D
 0
i
 i
0

D
 0
 i
i
0

(11.158)
268
CHAPTER 11
Mathematical Tools for Quantum Computing I
Check to see that you got the same thing in the exercise above!
So, in general, we may compute the conjugate transpose of a matrix, and
we may compute it in either order: conjugate ﬁrst, then transpose, or transpose
ﬁrst, then conjugate.
We will see later that the idea of the conjugate transpose is important for
deﬁning a class of operators called unitary operators. A unitary operator is an
operator whose inverse is its conjugate transpose, as we will consider further
in this section. This is important because quantum states are represented as
vectors with norm 1 living in something called a Hilbert space (which we will
deﬁne rigorously later in this exposition). It turns out that unitary operators
have the special property that they preserve the norm of the vectors on which
they operate. So, the application of a unitary operator to a vector whose norm
is 1 is a vector whose norm is also 1.
There is another important operation that we can perform on a matrix,
known as taking the trace. Let us recall that the primary diagonal of interest
to us in analyzing matrices is the one that runs from the upper left-hand corner
to the lower right-hand corner; we refer to this as the main diagonal. Given a
matrix A, we can ﬁnd the sum of entries of the main diagonal of A like so:
A D
 1
3
8
4

7! 1 C 4 D 5
(11.159)
We notate this as Tr.A/ D 5, which can be read, “The trace of A is 5.”
11.160
Exercise
Compute the trace of the matrix B D
 5
6
7
8

. Then,
check that the trace of B is the trace of BT . That the trace of a matrix and its
transpose are always the same is a theorem of advanced linear algebra, so we
content ourselves with this example for now.
The trace has a number of interesting properties including the following:
11.161
Invariance of the Trace
The trace remains invariant among matrices that are similar.
Similar matrices can be thought of as matrices that represent the same
linear transformation viewed from different perspectives.13 Now we are
13Unfortunately, we won’t be able to discuss the idea of similar matrices in this exposition,
but the name should give some indication of the idea. Two matrices are similar if they are in
SECTION 11.4
A First Look at Matrices
269
equipped to verify the list of equalities of products of matrices stated in
chapter 3.
Recall from earlier chapters that H is the Hadamard operator deﬁned as
H WD
1
p
2
 1
1
1
 1

X is the Pauli X operator x, also known as the NOT (or “(qu)bit ﬂip”)
operator deﬁned as
X WD
 0
1
1
0

Z is the Pauli Z operator z deﬁned as
Z WD
 1
0
0
 1

Y is the Pauli Y operator y deﬁned as
Y WD
 0
 i
i
0

and I is the identity operator, which we’ll take to be two-dimensional, so
I WD
 1
0
0
1

Recall also that for any matrix A, A denotes the conjugate transpose of
A.
11.162
Exercise
Verify the following list of equalities of matrices:
 HXH D Z
 HZH D X
 H YH D  Y
 H  D H
some sense the same. We can make this idea more precise when we understand the notion
of a basis, and what it means to change a basis. The idea is that two matrices are similar iff
they differ only by a change of basis. This invariance is useful for distinguishing classes of
quantum operators.
270
CHAPTER 11
Mathematical Tools for Quantum Computing I
So, we can say that the Hadamard operator H is unitary,14 since its inverse
is its conjugate transpose,
H  1 D H 
Matrix Exponentiation
Let us now discuss the exponentiation of matrices. To do this, ﬁrst we discuss
powers of matrices. We can apply the same matrix successively to a vector, as
in
X.Xj i/ D X2j i
The notation on the right hand side of this equation suggests we are applying
the square of the X operator (matrix) to the vector. Indeed, powers of linear
operators (matrices) are deﬁned in this way. In general, the notation Ak for
any operator A and any positive integer k means k successive applications of
A.
This leads naturally to the notion of the exponential of a matrix, denoted
exp A or eA. The way this is deﬁned is through the Taylor series of ex, namely
ex D
1
X
nD0
1
nŠxn:
In the same way, we use this to deﬁne the exponential of a matrix
exp A  eA WD
1
X
nD0
1
nŠAn
(11.163)
We deﬁne A0 D I, the identity matrix, for any matrix A. Although we will
not prove it here, it can be shown that the inﬁnite sum in (11.163) converges
for any matrix A and thus is well-deﬁned.
11.164
Exercise
Please verify the following.
 X2 D I
 Y 2 D I
 Z2 D I
 H 2 D I
14We will discuss unitary operators later on in this text.
SECTION 11.5
The Outer Product and the Tensor Product
271
11.165
Exercise
Verify that for any operator A such that A2 D I, the following identity
holds:
eiA D cos./I C i sin./A
(11.166)
For this exercise, it will be useful to recall the Taylor series for cosine and
sine. Use this expression for the Pauli matrices X, Y and Z.
11.5
The Outer Product and the Tensor
Product
The Outer Product as a Way of Building Matrices
Now we’ll demonstrate an operation that builds a matrix from two vectors.
Consider the vectors
 1
0
T
D
 1
0

and
 1
0

:
As discussed earlier, it is perfectly sensible to compute the product
 1
0
  1
0

(11.167)
since it is the product of a 2  1 matrix with a 1  2 matrix, i.e., a 2  2 matrix.
11.168
Exercise
Practice your matrix computation skills and compute
the above matrix product.
If you completed the above exercise, you now know that
 1
0
  1
0

D
 1  1
1  0
0  1
0  0

D
 1
0
0
0

(11.169)
We call the resulting matrix the outer product of the vector
 1
0

with
itself. We’d like to emphasize that this is just fancy terminology! The outer
product is simply the matrix product of two vectors. We can think of the ﬁrst
 100e
CHAPTER 11
Mathematical Tools for Quantum Computing I
vector as a 2  1 matrix and the second vector as a 1  2 matrix, remembering
that the the product of an mn matrix and an np matrix is an mp matrix.
So, in this case, the product of a 2  1 matrix and a 1  2 matrix yields a 2  2
matrix.
In Dirac notation, we can express the outer product above as j0ih0j. Like-
wise, we may construct the outer products j0ih1j, j1ih0j, and j1ih1j. We leave
it to you to compute these in the following exercise:
11.170
Exercise
Find the outer products j0ih1j, j1ih0j, and j1ih1j.
To help you out, the ﬁrst of these is:
j0ih1j WD
 1
0
  0
1

D
 1  0
1  1
0  0
0  1

D
 0
1
0
0

(11.171)
Hopefully, you’ve completed the rest!
With the computation we completed for you above and recalling how it is
that we add two matrices, you can now conﬁrm a result written in chapter 3:
X WD j0ih1j C j1ih0j D
 0
1
1
0

(11.172)
The outer product of two vectors is a speciﬁc case of the more general
concept of a tensor product, as we will see now. Before we move on to the
tensor product, we would like to discuss how the outer product relates to Dirac
notation. Recall that the inner product hujvi of two vectors u and v is in fact
expressible as uv, where u denotes the conjugate transpose of u. It should
not be too much of a surprise then that the outer product juihvj is expressible
as uv!
11.173
Exercise
Check that all of the above computations of outer prod-
ucts juihvj for vectors u and v could have been thought of as computations of
uv.
In summary:
SECTION 11.5
The Outer Product and the Tensor Product
273
11.174
Inner and Outer Product and Their Relationships with the
Conjugate Transpose
For any two vectors u and v,
hujvi D uv
and
juihvj D uv
The Tensor Product
Before introducing the operation of a tensor product, let’s discuss some
terminology for talking about tensor products. As we have seen earlier, a
scalar is simply a number.
We can refer to scalars as 0-tensors, meaning a tensor of order 0. Please
note that we discourage the use of the word “rank” when referring to the order
of tensors, as rank is a term reserved for another term in linear algebra. We
can refer to a vector as a 1-tensor. Similarly, we can refer to a matrix as a
2-tensor. A 3-tensor would in fact be a rectangular prism of numbers! Beyond
the 3-tensor, we no longer have the ability to give a geometric interpretation.
However, many applications in the real world call for tensors of higher order
— sometimes in the thousands or millions, as in the case of neural networks.
We summarize this terminology in Figure 11.15.
11.175
Exercise
Why is it appropriate to say that the table in Figure
11.15 is a 2-tensor?
Let us now return to our discussion of the tensor product as a generalization
of the outer product. The outer product is the product of two 1-tensors, which
produces a matrix (a 2-tensor). However, what happens if we wish to take the
tensor product of two tensors of any arbitrary order? We can generalize the
outer product of two 1-tensors to the tensor product of any two tensors, A and
B, of arbitrary order, like so:
A ˝ B
To see what we mean by this, consider the two vectors u WD
 a
b

and
v WD
 c
d
e

. Their tensor product is the matrix
274
CHAPTER 11
Mathematical Tools for Quantum Computing I
Tensors
Terminology
Example
0-tensor
scalar
3
1-tensor
vector
 1
0

2-tensor
matrix
1
p
2
1
1
1
 1

Figure 11.15: Tensor terminology
u ˝ v D
 a
b

˝
 c
d
e

WD
 a  c
a  d
a  e
b  c
b  d
b  e

(11.176)
This might remind you of the previously deﬁned outer product of two
vectors, and it should, because the tensor product of the two vectors here is
the outer product.
Let’s see the tensor product of two column vectors:
0
@
r
s
t
1
A ˝
 x
y

WD
0
BBBBBB@
r  x
r  y
s  x
s  y
t  x
t  y
1
CCCCCCA
(11.177)
You may have read earlier in the book that we sometimes write j00i to
denote the vector
0
BB@
1
0
0
0
1
CCA, or j11i to denote the vector
0
BB@
0
0
0
1
1
CCA. We’d like to
point out now that the notation j00i is shorthand for j0i ˝ j0i, which is
j00i WD j0i ˝ j0i D
 1
0

˝
 1
0

D
0
BB@
1  1
1  0
0  1
0  0
1
CCA D
0
BB@
1
0
0
0
1
CCA
(11.178)
It should be noted that the tensor product cares about whether the vectors
are column vectors or row vectors, as we can see from contrasting the previous
two examples of tensor products. A quick perusal of the previous examples
should convince you that the tensor product of a column vector with a row
SECTION 11.5
The Outer Product and the Tensor Product
275
vector yields a matrix, whereas the tensor product of two column vectors
yields another column vector.
11.179
Exercise
What do you think the tensor product of two row vectors
yields?
If you thought about the exercise above, you’re hopefully convinced that
the tensor product of two row vectors is another row vector.
11.180
Size of tensor product of matrices
The tensor product of an .a  b/ matrix with a .c  d/ matrix is an
.a  c/  .b  d/ matrix.
11.181
Exercise
Check that the aforementioned formula for the dimen-
sion of the tensor product of an a  b matrix with a c  d matrix specializes,
as examples, to the following cases:
 the tensor product of two row vectors
 the tensor product of two column vectors
Note that a row vector can be thought of as matrix whose dimensions are
1  m and a column vector can be thought of as a matrix whose dimensions
are n  1.
Now, we invite you to check that j11i is in fact j1i ˝ j1i, as claimed!
11.182
Exercise
Check that j11i WD j1i˝j1i is in fact the vector
0
BB@
0
0
0
1
1
CCA.
Then, check that the intermediate vectors are what they’re supposed to be, i.e.,
j01i D
0
BB@
0
1
0
0
1
CCA and j10i D
0
BB@
0
0
1
0
1
CCA.
276
CHAPTER 11
Mathematical Tools for Quantum Computing I
We won’t dwell on the idea of the tensor product now. We’d just like to
introduce it and get you thinking about it. We’ll revisit this idea once we’ve
established the formal deﬁnition of vector spaces and linear transformations
between them. At that point, we’ll realize the tensor product of two vectors is
actually the tensor product of the two linear transformations they represent. It
will also become clear at that point why it is that the dimension of the tensor
product works the way it does.
11.6
Set Theory
The Basics of Set Theory
Now we’ll spend a bit of time developing some of the prerequisite set theory
necessary to continue with this chapter. In particular, we’ll need to have an
idea of what a function is (this requires a bit of work!) and then we can
ascertain what it means for a function to be invertible. Invertible functions are
of utmost importance in quantum computing, since the quantum gates we use
to construct quantum circuits have to be reversible.
The curious thing is that these gates are represented by matrices, and we’ll
learn that these matrices are actually just representations of transformations
of space, and so are functions themselves. Once we believe that matrices are
actually just functions, we have a reasonable notion of what it means for a
matrix to be invertible, and thus for a quantum gate to be reversible!
So, don’t let the following passages discourage you. A ﬁrm mathematical
understanding of basic set theory and function theory will guide you well on
your way to grasping the underlying ideas of quantum computing.
First, we want to have a notation expressing the notion of the containment
of an element in a set. We say that x is an element of a set S, and denote this
by x 2 S. The symbol “2” resembles an “e” for “element,” which might help
us remember – some people like mnemonics.
We will often abbreviate the phrase “if and only if” with “iff.” So, any
time you see “iff,” think “if and only if.” We also often discuss the notion of
set containment, or set inclusion when doing mathematics, so let’s make sure
we have an idea of what this is.
Given two sets A and B, we say that A is a subset of B, denoted A  B,
iff for all elements a 2 A, a 2 B. In other words, everything in A is also in
B.
SECTION 11.6
Set Theory
277
Sometimes people indicate the inclusion of a set A into a set B by A ,! B,
and we say that “A includes into B.” Other ways to say this include “A embeds
into B” and “A injects into B.”
The terminology “A injects into B” is hinting at a property of functions
called injectivity that we’ll investigate soon.
11.183
Exercise
Check that f0g  f0; 1g.
We will ﬁnd ourselves talking about a few important sets in this chapter,
so let us deﬁne them. The set of natural numbers,15 denoted N, is
N WD f0; 1; 2; 3; :::g
(11.184)
The set of integers,16 denoted Z, is
Z WD f:::;  3;  2;  1; 0; 1; 2; 3; :::g
(11.185)
The set of rational numbers,17 denoted Q, is
Q WD
p
q W p; q 2 Z; q ¤ 0

(11.186)
The set of real numbers, denoted R, is a bit more complicated to deﬁne
formally, but it is acceptable to think of a real number as any number that can
be approximated to any level of precision by a sequence of rational numbers.
Examples of real numbers include
0; 1;  1; 3
4;
p
2; e; 
(11.187)
Non-examples of real numbers include
15Some people prefer to deﬁne the natural numbers excluding 0, like so: N D f1; 2; 3; :::g.
It makes no difference, really, although computer scientists are inclined to include 0. Some
have suggested that the symbol N be reserved for the set f1; 2; 3; :::g and that the symbol N0
be reserved for the set f0; 1; 2; 3; :::g. It is a good idea, but it has not yet caught on as far as we
know.
16You are probably wondering why the set of integers, beginning with the letter “i” is
denoted with a “z.” It turns out that the word “number” is “zahlen” in German, and the
Germans are responsible for much of the notation found in number theory and algebra.
17Can you ﬁgure out why the rational numbers are denoted with a “q”? The root of the
word “rational” is “ratio,” and another word for ratio is “quotient”!
278
CHAPTER 11
Mathematical Tools for Quantum Computing I
i; 1 C i; 1   i; 1
p
2
C 1
p
2
i
(11.188)
By including the imaginary number i, we get the complex numbers, de-
noted C. The set C is deﬁned as
C WD fa C bi W a; b 2 Rg
(11.189)
11.190
Exercise
Check the following set inclusions are actually true
N  Z  Q  R  C
You might want to think of each integer x as the rational number x
1 to establish
the second inclusion in the chain. Then, you might want to think of each real
number a as the complex number a C 0i to establish the fourth inclusion.
We can also say things like “B contains A,” emphasizing B’s containment
of A rather than A’s containment in B.
We will also begin to use set-builder notation18 to express and describe
sets. For example, we can describe the set of all integers whose square is 1
with the notation
S WD fx 2 Z W x2 D 1g
(11.191)
We read this as “The set of elements x in the integers (recall, Z denotes the
set of integers) such that the square of x is 1.” So, the colon (:) indicates that
we should say “such that.”
11.192
Exercise
How many elements are in the set S above? Can you
write in set-builder notation the set of all integers whose cube is 1? How many
elements are in that set?
11.193
Exercise
Can you express the set of all complex numbers whose
square is  1 in set-builder notation? What about the set of all complex
numbers whose fourth power is 1? The set of all complex numbers whose
third power is 1? How many are there of each? The answer to the third
question is not 1! Remember, you’re dealing with complex numbers. This
18Note that this notation is also called a set comprehension, which is the origin of the term
of list comprehension in Python and other high-level languages.
SECTION 11.6
Set Theory
279
is hinting at a classical theorem about the complex numbers known as De
Moivre’s theorem.19
The Cartesian Product
In the development of the deﬁnition of vector space, we’ll have a desire to
understand the notion of a Cartesian product of two sets.
The idea is that if we have two sets, say S and T , we’d like to create one
uniﬁed set from which each of S and T may be identiﬁed unambiguously. We
call this set the Cartesian product of the sets S and T and denote it by S  T .
The notation  was likely chosen to remind us that the number of elements in
the Cartesian product of S and T is the number of elements of S times the
number of elements of T .
Formally, the Cartesian product of the sets S and T is deﬁned
S  T WD f.s; t/ W s 2 S; t 2 T g
(11.194)
For example, if we take our two sets to be S D f1; 2; 3g and T D f4; 5g,
then, the Cartesian product of S and T is the new set
S  T D f.s; t/ W s 2 S; t 2 T g D f.1; 4/; .2; 4/; .3; 4/; .1; 5/; .2; 5/; .3; 5/g
(11.195)
Notice that S  T has the promised 2  3 D 6 elements.
11.196
Exercise
Think about what the Cartesian product of the set R
with itself is. We call this R2, i.e., R2 WD R  R. Similarly, we say that
C2 WD C  C, and more generally, that
Rn WD R  :::  R
„
ƒ‚
…
n times
and
Cn WD C  :::  C
„
ƒ‚
…
n times
19De Moivre’s theorem states that the complex numbers z satisfying the equation zn D 1 for
a natural number n form the vertices of a regular n-gon in the complex plane. The n complex
numbers satisfying the equation zn D 1 are known as the nth roots of unity, since 1 is also
known as unity, and they are the nth roots of 1. For example, the complex numbers satisfying
the equation z3 D 1 are the 3rd roots of unity, and form an equilateral triangle in the complex
plane. Speciﬁcally, they are 1 D e0 2
3 ;   1
2 C
p
3
2 i D e1 2
3 ; and   1
2  p
3
2 i D e2 2
3 , as
you can verify with Euler’s formula!
280
CHAPTER 11
Mathematical Tools for Quantum Computing I
Relations and Functions
Before we venture into the concept of a function, we’ll want to discuss
relations. The following discussion of relations could accidentally convince
you that relations are only interesting in that they are a stepping stone on the
path to the deﬁnition of a function. This is far from the truth!20
Maybe you remember writing and seeing things like
f .x/ D x2
(11.197)
while in school. People would say f .x/ is a function. What really is a function
though? Intuitively, a function is an unambiguous assignment of each element
of one set to an element of another set. Of course, we’ll want to make this
mathematically precise, since it is the backbone of nearly all of the concepts
that follow. Actually, we’ll learn soon that matrices themselves are functions!
Let’s look at some examples and non-examples of functions to get an idea
of what they are. Consider the sets X WD f1; 2g and Y WD f3; 4g. We can
assign each element of X to an element of Y like so
1 7! 3
2 7! 4
Introducing notation, we denote the assignment described above by the
letter f (for “function”) and write
f W X ! Y
to indicate that f assigns elements of X to elements of Y . We sometimes say
“f maps X to Y ”
as well. We refer to the set X as the domain of the function f and to the set
Y as the codomain (sometimes called the “range”) of f .
We can concisely describe the assignment above with the notation
20Curious readers should investigate category theory, where the category of sets and relations,
denoted Rel, is an interesting mathematical object in its own right and enjoys an intimate
connection with the category of ﬁnite-dimensional Hilbert spaces, denoted Hilb. For example,
every relation R  X  Y of ﬁnite sets is naturally associated to a matrix Rij , where Rij D 1
iff .xi; yj / 2 R and is 0 otherwise. Viewing these matrices as having coefﬁcients in the
complex numbers C leads to an interpretation of a relation as a linear transformation between
ﬁnite-dimensional Hilbert spaces. Category theory offers an enticing framework for computing
and other sciences. To learn more about category theory as a unifying language for all of
mathematics and science (and even understanding language!), check out Tai-Danae Bradley’s
blog Math3ma [54]. More advanced readers might enjoy the n-Category CafKe.
SECTION 11.6
Set Theory
281
g.1/ D 3
and
g.2/ D 4
(11.198)
and say that
“g assigns (i.e., maps) the element 1 to the element 3”
and that
“g assigns (i.e., maps) the element 2 to the element 4.”
Now, for a non-example of a function. Let X and Y be the sets as before, but
consider instead the assignment, denoted by the letter h, described by
1 7! 3
1 7! 4
2 7! 3
Note that the symbol 7! denotes “is assigned to” or “maps to.” What is strange
is that 1 is now being assigned not only to 3, but also to 4. So, the rule of
assignment given by h is ambiguous. It is exactly this ambiguity that we seek
to preclude in our formal deﬁnition of a function.
We won’t give the formal deﬁnition of a function just yet though. We
would like to emphasize that a function is a special case of a more general
phenomenon known as a relation. This defers our discussion to that of
relations.
More precisely, we would like to deﬁne the notion of a relation between
two sets. The deﬁnition of a relation is quite brief and might surprise you:
11.199
Deﬁnition
Deﬁnition of a relation
A relation on two sets X and Y is a subset of their Cartesian product.
That’s all? Yes, that’s what a relation is. Let’s see a few examples of
relations to get an idea of what they are.
Consider the subset
R WD f.x; x2/ W x 2 Rg  R  R D R2
(11.200)
By deﬁnition, R is a relation, since it is a subset of the Cartesian product
R  R of the set of real numbers R with itself. But what is it?
The following are examples of points which are elements of R, as you
should check
.1; 1/; .2; 4/; .3; 9/; .4; 16/; .5; 25/; .
p
2; 2/; .; 2/; :::
(11.201)
282
CHAPTER 11
Mathematical Tools for Quantum Computing I
See what’s going on?
These are not elements of R
.0; 1/; .1; 2/; .e; /; .7; 89/; .
p
2;
p
3/;
1
2; 1
3

:::
(11.202)
Notice that we say that a point is in the relation because we’re indicating
membership in the set deﬁned by the relation.
11.203
Exercise
Plot the points of R2 in the relation R.
If you tried the above exercise, you see now that the set of points in the
relation R is a parabola! What is interesting about R is that it is even better
than a relation – it’s a function! In fact, you might recognize it as being
described compactly by the equation f .x/ D x2. However, the equation
f .x/ D x2 hides so much of the character of the function f that it’s virtually
useless to say.
To see what we mean by this, ﬁrst realize that to say f .x/ D x2 gives
no indication of the domain or codomain of f . For all we know, f could be
mapping from the set C to f0; 1g or whichever pair of sets you like. In fact,
if the domain of f .x/ D x2 is taken to be C and the codomain is taken to
be f0; 1g, f isn’t a function at all – we’ll see why when we give the formal
deﬁnition of a function!
11.204
Exercise
Check that the set f.0; 0/; .1; 1/; .2; 2/; .3; 3/g  ZZ
is a relation. Can you describe this relation using set-builder notation?
The point we’re trying to make is that we need to give the domain and
codomain when specifying a function, not just the rule of assignment. For
example:
A function f from a set X to a set Y is a relation on X
and Y satisfying a special criterion...
Before we give the special criterion, a comment: Recall that to say that
f is a relation on X and Y is to say that f is a subset of the Cartesian
product X  Y , so we can reasonably talk about elements of the function
(thus, relation) f . This type of terminology may sound odd. You’ve likely
not heard of someone saying “.x; y/ is an element of the function f ” and
SECTION 11.6
Set Theory
283
are probably more accustomed to the phrase f .x/ D y. However, to say
f .x/ D y is literally to say that .x; y/ is in the function (and thus, relation)
f .
After much deliberation, the special criterion that the points .x; y/ in the
relation f must satisfy is
11.205
Deﬁnition
Deﬁnition of a function
A relation f on a Cartesian product X  Y of sets X and Y is a function
iff f satisﬁes
for all .x1; y1/; .x2; y2/ 2 f;
x1 D x2 H) y1 D y2
The symbol H) denotes “implies.” In other words, if the ﬁrst coordinates
are equal, then the second coordinates have to be equal! We could also phrase
this as
for all x1; x2 2 X;
x1 D x2 H) f .x1/ D f .x2/
Let’s revisit the non-example we gave earlier and conﬁrm that it is not a
function. Recall that we had deﬁned a mapping h W f1; 2g ! f3; 4g earlier by
the rule of assignment
1 7! 3
1 7! 4
2 7! 3
Let’s express this as a relation. h is the relation
h D f.1; 3/; .1; 4/; .2; 3/g  f1; 2g  f3; 4g
(11.206)
11.207
Exercise
Use the deﬁnition of a function to check that h, as given,
is not a function. What rule of the deﬁnition of a function does it violate?
Next, recall that we claimed earlier that if we take the mapping f .x/ D x2
and assign its domain to be C and its codomain to be the set f0; 1g, then f is
not a function. Explain why this is true. Hint: Think of f as a relation ﬁrst –
is it even a relation?
284
CHAPTER 11
Mathematical Tools for Quantum Computing I
Hopefully, you realized that h above is not a function because .1; 3/ and
.1; 4/ are in h and yet despite the fact that their ﬁrst coordinates are equal
(both equal 1), their second coordinates differ (they’re each 3 and 4). This is
exactly why h violates the deﬁnition of a function.
This example, when analyzed alongside the formal deﬁnition of function
we’ve given, hopefully illuminates exactly what type of examples the def-
inition is precluding. To request that if the ﬁrst coordinates of two points
in a relation are equal that then their second coordinates must be equal is
essentially to ask that no element in the domain is mapped by f to more than
one element in the codomain. In other words, the assignment that describes f
should be unambiguous.
Let’s turn our attention to the second part of the exercise. If we insist that
the domain of the mapping f .x/ D x2 is truly all of C and not any proper
subset, then we see that there exists an element in the domain of f whose
image under f is not an element of f0; 1g, e.g. x D 2, because if x D 2, then
f .x/ D f .2/ D 22 D 4 … f0; 1g. So, the codomain cannot be just f0; 1g and
would have to be expanded to accommodate all of the potential images of
elements of C under f . Then, the codomain would have to be taken be all of
C!
A clever way of dealing with this issue is to “restrict the domain” of f .
By this we mean, instead of taking the domain of the function f to be all of
C, take it instead to be the subset of C consisting only of complex numbers
whose squares are in f0; 1g, i.e. f 1; 0; 1g. Making this restriction of the
domain from C to f 1; 0; 1g ensures that each element in the domain maps to
an element in the codomain f0; 1g, solving the previously discussed issue. So,
you could take f to be the relation
f WD f. 1; 1/; .0; 0/; .1; 1/g  C  f0; 1g
(11.208)
Then, f is in fact a relation on C  f0; 1g as per our deﬁnition, and it is
also a function on C  f0; 1g as per our deﬁnition. Some might argue that this
new f that we’ve constructed in an effort to remedy the problem about the
codomain f0; 1g not being large enough to accommodate all possible images
of elements of C under f is not really the f given at the outset. If we were to
take the set of inputs to just be f 1; 0; 1g all along and not actually consider
all of C, then why wouldn’t we just say so originally?
People do this occasionally, i.e. state a domain that is larger than what is
actually intended. A relation of this sort is known in the literature as a “partial
SECTION 11.6
Set Theory
285
function.” As we do not have necessity to develop this concept here, we will
encourage interested readers to check this out on their own.
The curious reader might wonder why we restrict our focus to functions
and seldom mention relations. A quick answer is that relations are such a
broad class of objects that it’s difﬁcult to classify and study them at all. This is
not to say that they aren’t interesting, just to say that the theory for functions
is better understood.
11.209
Exercise
Consider the function f W R ! R described by the rule
of assignment
f .x/ WD x2
We are inclined to say that the inverse function of f is the function
f  1.x/ WD px
Ah, but if f  1 is to be a function, it has to have a speciﬁed domain and
codomain! What are its domain and codomain? Describe the relation that
the rule of assignment describing f  1 deﬁnes after ﬁnding the domain and
codomain. Is it even a function?
This exercise likely confused you, and rightly so! What is confusing is
that trying to formulate an inverse for the function f W R ! R deﬁned by
f .x/ D x2 demands a bit more care. To see what goes wrong, observe that
f maps two different elements of the domain R to the same element of the
codomain R:
f .1/ D 12 D 1
(11.210)
f . 1/ D . 1/2 D 1
(11.211)
So, in the process of “inverting” f (which we may think of as “undoing”
the squaring operation), we’ll have to determine what to do with the number
1 in the codomain. In other words, the question of “Who in the domain did f
map to the number 1 in the codomain?” is ambiguous, since both 1 and  1
get mapped to 1 by f . You could answer with either 1 or  1 and you’d be
correct! Do you recall now why taking square roots demands that we include
“plus or minus”? It’s not just dogma!
This phenomenon manifests itself geometrically as the failure of the
parabola (the graph of f ) to pass the “horizontal line test” as you’re invited to
explore in the next exercise...
286
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.212
Exercise
Draw the graph of the function f and check that it is in
fact a parabola. Having done so, see that any horizontal line drawn above the
x-axis hits the curve twice. In particular, ﬁnd the geometric manifestation of
the fact that f .1/ D 1 and f . 1/ D 1 by drawing a horizontal line at height
1 on your plot.
11.213
Exercise
You might be wondering now “But wait – then, f isn’t
even a function because it sends two different things to the same place!”
Careful, f is a function. The deﬁnition of a function disallows the possibility
that one thing map to two different things, which is different from what is
going on here. Check this using the formal deﬁnition of a function!
We’ll see that if we impose a condition on the functions that we consider,
we’ll never encounter this problem while trying to invert them. You can
probably already guess that we will require that our functions do not map two
different things to the same thing in what follows.
From now on, we’ll revert to our usual notation for functions and will
likely not need to mention relations. We simply wanted to emphasize the idea
that a function is just a speciﬁc kind of relation, which instructs its precise
mathematical deﬁnition.
Important Properties of Functions
We now describe three desirable attributes of functions:21
injectivity, surjectivity, bijectivity
We grappled with the idea of a non-injective function earlier while discussing
the parabola. Intuitively, an injective function is a function that never maps
two different things to the same thing. You might know this by the name
one-to-one.
21The terminology injective, surjective and bijective can be traced back to Nicolas Bourbaki,
the pseudonym of a secret society of mainly French mathematicians, including André Weil,
Jean-Pierre Serre and Alexander Grothendieck, who sought out to reformulate mathematics on
an abstract, yet self-contained basis around 1935.
SECTION 11.6
Set Theory
287
11.214
Deﬁnition
Injective function
A function f W X ! Y is injective iff
for all x1; x2 2 X;
f .x1/ D f .x2/ H) x1 D x2:
In other words, we ask that f maps two elements to the same element only
if those elements are equal. You should check that the function f W R ! R
deﬁned by f .x/ D x2 earlier is not injective, despite being a function.
11.215
Exercise
Prove that if a function f is injective, then for all
x1; x2 2 X, f .x1/ D f .x2/ iff x1 D x2. Recall the deﬁnition of a function!
Sure, f W R ! R deﬁned by f .x/ D x2 is not injective. However, if we
restrict the domain of f to the smaller set Œ0; 1/ WD fx 2 R W x  0g and
consider instead the function
f jŒ0;1/ W Œ0; 1/ ! R
(11.216)
(read this “the restriction of f to the set Œ0; 1/”), we realize that f jŒ0;1/ is
suddenly injective! Geometrically, this corresponds to chopping off the left
side of the parabola and considering only the right side. Graph this to see
what we mean. We can express this by saying that we’re only considering a
single “branch.”
Now, let’s try to invert this new restricted version of f . Having attempted
a previous exercise, you might have struggled to determine the domain and
codomain for the inverse function, which we at least know should be some-
thing like px. Since the restriction f jŒ0;1/ W Œ0; 1/ ! R is a function from
the domain Œ0; 1/ to the codomain R, it seems reasonable to request that the
inverse function have opposite domain and codomain, i.e., that the domain
of the inverse function
 f jŒ0;1/
 1 should be R and the codomain should be
Œ0; 1/.
No worries about the codomain: the inverse
 f jŒ0;1/
 1 D px certainly
has codomain Œ0; 1/ since the square root of any real number is either 0 or
positive, i.e., is an element of the set Œ0; 1/, as you should check. However,
we have an interesting dilemma arising from the issue of what the domain
should be.
As you may have already noticed, we cannot take the domain of
288
CHAPTER 11
Mathematical Tools for Quantum Computing I
 f jŒ0;1/
 1 D px
(11.217)
to be all of R. If we did, we would have to declare how it is that
 f jŒ0;1/
 1
operates on numbers like  1... In other words, we would have to take the
square root of  1 (and all of the other negative numbers!), which would take
us into the imaginary domain.
Since there is no real number whose square is  1, we’ll have to restrict
the domain of the inverse function
 f jŒ0;1/
 1 D px just as well, or else
the inverse won’t be a function as per our previous deﬁnition! Speciﬁcally, we
remedy our problem of inverting
 f jŒ0;1/

by restricting
 f jŒ0;1/
 1 D px
only to the non-negative numbers, i.e., Œ0; 1/.
In other words, we needed for the function to map onto every element of
the codomain. In this particular example, we see that the function f .x/ D x2
and even its restriction f jŒ0;1/ to Œ0; 1/ does not “hit” everything on the
other side. For instance, it misses the number  1, since there is no real number
whose square is  1. You can see this geometrically by recognizing that the
parabola, and even its right-hand branch, do not ever encroach into negative
territory.
We now state a deﬁnition clarifying all of this discussion and giving a
precise mathematical meaning to the word onto.
11.218
Deﬁnition
Surjective function
A function f W X ! Y is surjective iff for each element y 2 Y there
exists an element x 2 X such that f .x/ D y.
Colloquially, we could say that every element in Y is “hit” by some element
of X.
Now, the punchline. We set out to make the idea of a function precise and
then to give attributes of a function, but we never stated explicitly why we
care about these attributes. This next deﬁnition is truly a theorem, but we state
it as a deﬁnition:
11.219
Deﬁnition
Bijective function
A function f W X ! Y is bijective, equivalently, invertible by a function
f  1 W Y ! X, iff f is injective and surjective.
So, now we know that a function is invertible by another function precisely
when f is bijective. Invertible functions play an important role in deﬁning
what it means for two mathematical objects to be the same. For example, for
SECTION 11.6
Set Theory
289
most purposes, it is often sufﬁcient to consider two sets to be the same iff
they have the same number of elements. One way of expressing that two sets
have the same number of elements is by stating that there exists an invertible
function between them (think about why!). If the sets that we’re interested in
happen to have a bit more structure and we’d like to assign a meaning to the
statement that they’re the same, it seems only natural to request that there be
an invertible function between them that preserves their structure, i.e., it is
not sufﬁcient only to have a bijective map between the sets. We’ll see later
that this idea of a structure-preserving map manifests itself in linear algebra
as an invertible linear transformation.
A few comments: If f is not injective, it is fair to restrict the domain of
the function to impose injectivity on it, as we did above with the function
f W R ! R deﬁned by f .x/ D x2. But then the function might not be
surjective, so we have to invert the function f only on its image, where by the
image of f we mean
f .X/ WD ff .x/ W x 2 Xg;
(11.220)
i.e., the elements f .x/ 2 Y for some x 2 X. If we don’t invert the function
only on its image, the inverse we desire might not end up being a function, as
was the case for the squaring function and its presumed inverse, the square
root!
11.221
Exercise
You should check that the image of the squaring func-
tion f W R ! R deﬁned by f .x/ D x2 is in fact Œ0; 1/.
For a moment, let’s recall how it is that we compose two functions.
If you have two functions, say
f W R ! R deﬁned via f .x/ D x2
(11.222)
and
g W R ! R deﬁned via g.x/ D x C 1;
(11.223)
it’s natural to ask what their composition is, i.e., the result of applying them
iteratively.
11.224
Deﬁnition
The composition of two functions
In general, we deﬁne the composition of two functions
f W X ! Y
and
g W Y ! Z
to be the function .g ı f / W X ! Z deﬁned via g.f .x//.
290
CHAPTER 11
Mathematical Tools for Quantum Computing I
So, you apply g ﬁrst, then f . In the case we have above, the composition
of f .x/ D x2 and g.x/ D x C 1 is the function
.f ı g/.x/ WD f .g.x// D f .x C 1/ D .x C 1/2
(11.225)
11.226
Exercise
Check to see that g ı f is not the same function and is
in fact
.g ı f /.x/ D x2 C 1
In general, it’s quite rare that f ı g D g ı f for two functions f and g.
For any set X, there is a special function called the identity function,
denoted IX such that for all x 2 X, IX.x/ D x. Given a function f W X !
Y , it’s natural to ask if there is another function f  1 W Y ! X such that the
following two properties hold: (1) f  1 ı f W X ! Y ! X is equal to the
identity function IX on X and (2) f ı f  1 W Y ! X ! Y is the identity
function IY on Y . If we can ﬁnd such a function f  1, we say that we have
found an inverse function for the function f .
Now, we have a more reﬁned notion of an invertible function:
11.227
Characterization of invertible functions
A function f W X ! Y is invertible iff there exists a function f  1 W Y !
X such that
f  1 ı f D IX
(11.228)
and
f ı f  1 D IY
(11.229)
That is, f is invertible iff we can ﬁnd a function f  1 whose composition
with f in either order yields the identity function! This idea will manifest
itself later in our discussion of matrices, when we discuss what it means for a
matrix to be invertible.
SECTION 11.7
The Deﬁnition of a Linear Transformation
291
11.7
The Deﬁnition of a Linear
Transformation
You might have wondered since the beginning of this chapter why it is that
we call linear algebra “linear algebra.”
So far, it’s likely not clear at all, since we have not discussed lines or
anything obviously linear in any sense. Interestingly, the matrices that have
occupied our previous discussion are in fact what we refer to as linear trans-
formations. This demands some explanation.
So, what is a linear transformation then? The next (non)example might
startle you...
Consider the function (transformation) T from R ! R described by
x 7! x C 1
(11.230)
In words, T is the transformation of 1-dimensional space that maps a vector
(a number, in this case) to that vector plus one. For example, T .0/ D 0C1 D 1
and T .1/ D 1 C 1 D 2. Those familiar with function notation might describe
the transformation T as
T W R ! R
(11.231)
T .x/ D x C 1
(11.232)
and draw its graph as in Figure 11.16
Well, you might say, “Then T is obviously linear – it’s a line!”
Ah, but it’s not actually! In fact, the relation above is afﬁne, not linear.22
The reason why this example goes against our intuition is because we have
been trained to analyze the graph of a function. Typically, the information
encoded by the graph of a function guides our intuition. However, in this case,
the graph of T is the set of points
f.x; T .x// W x 2 Rg
(11.233)
which does indeed resemble a line when plotted. However, we are not
asking that the graph of the function be a line – we’re asking that the function
22An afﬁne transformation is a transformation of the form Ax WD T x C b, where T is a
linear transformation and b is a vector. In other words, an afﬁne transformation is the result of
a linear transformation (i.e., a composition of rotations and dilations) and then a translation. In
fact, the reason the transformation T .x/ WD x C 1 is not linear is because it translates by 1!
It’s a good exercise to determine exactly what A and b are in the deﬁnition of T f and to realize
that an afﬁne transformation is linear exactly when the translation vector b is zero.
292
CHAPTER 11
Mathematical Tools for Quantum Computing I
x
y
T .x/ D x C 1
Figure 11.16: The graph of T .x/ D x C 1
itself be linear! You might want to revisit this example after learning the
deﬁnition of a vector space to see if you can ﬁnd more reason for why this
should not be included in our set of linear functions. Try to wrap your head
around this for a moment (or three).
Here is the deﬁnition of linear that we will take, at least for the case of a
transformation that maps from R to R:
11.234
Deﬁnition
The deﬁnition of a linear transformation
A transformation T W R ! R (interchangeably, function) is linear iff
1. for all x; y 2 R, T .x C y/ D T .x/ C T .y/
2. for all a 2 R and for all x 2 R, T .a  x/ D a  T .x/
So, we’re requiring that applying T to a sum is the same as applying T
to each of the summands and then summing, and that applying T to a scalar
multiple is the same as applying T to the multiplicand and then multiplying
the result by the scalar. In other words, we require that T preserve addition and
scalar multiplication. Often people will say things like structure-preserving
map. By structure, they mean algebraic structure, i.e., the way things are
added and multiplied.
SECTION 11.8
How to Build a Vector Space From Scratch
293
Let’s see why the previously deﬁned map T .x/ D x C 1 is not linear. If
T were linear, it would be the case that for all x and y in R, T .x C y/ D
T .x/ C T .y/. We see, however, that this is not true:
T .x C y/ WD .x C y/ C 1
(11.235)
and
T .x/CT .y/ WD .x C1/C.y C1/ D .x Cy/C2 ¤ .x Cy/C1 (11.236)
So, T .x C y/ ¤ T .x/ C T .y/ in general, and so T does not satisfy our
deﬁnition of linear!
11.237
Exercise
Check that T also violates the second property speciﬁed
in the deﬁnition of linear with a speciﬁc choice of a and x in R. So, T is
hopelessly not linear!
We still claim that the matrices shown earlier are in fact linear transforma-
tions. However, to explain this correctly, we’ll need to understand in what
sense a matrix is a transformation from a more careful and mathematical
perspective. In particular, we’ll need to turn our attention toward exactly
which spaces the matrices are transforming. These spaces are known as vector
spaces.
11.8
How to Build a Vector Space From
Scratch
The name vector space likely brings to mind a space of vectors. That is pretty
much what a vector space is – a space where a bunch of vectors live. We want
this space to facilitate all of the usual operations we perform on vectors, such
as addition of vectors and scalar multiplication of vectors.
We’ll state the precise deﬁnition of a vector space and then unpack the
terminology.
11.238
Deﬁnition
Vector space
A vector space V over a ﬁeld F is an abelian group V equipped with an
action of the ﬁeld F on V .
294
CHAPTER 11
Mathematical Tools for Quantum Computing I
There are a lot of unfamiliar terms in this deﬁnition which we will explain
step by step.
First of all, what is a ﬁeld? It turns out that it will be best to deﬁne an
abelian group ﬁrst, since we’ll see that any ﬁeld is an augmentation of an
abelian group. Later on in this section we will deﬁne the word action.
Groups
Recall that the notation
x 2 G
(11.239)
denotes “x is an element of G.” We also write things like
x; y; z 2 G
(11.240)
meaning “x; y and z are elements of G.”
We’ll state the deﬁnition of a group precisely now. After its statement,
we’ll delve into what all of this really means.
A group .G; ?/ is a set G satisfying the following properties:
 Closure: There exists a function
? W G  G ! G
which we call the binary operation of G.
Instead of writing the application of the operation ? to a pair of elements
g1; g2 each in G as ?.g1; g2/, as we usually do when describing the
application of a function to a pair of elements, we write g1 ? g2 to be
succinct.
For now, it’s advisable to think of ? as being a familiar operation like
addition or multiplication. For example, it would be weird, but correct,
to say that addition is a binary operation that accepts two numbers as
the input and outputs a new number. We could write such a thing like
this, where we replace the name ? for the binary operation with the
symbol C
C.x; y/ WD x C y:
This idea shouldn’t be too unfamiliar to computer scientists and pro-
grammers – it’s like deﬁning a function or method!
SECTION 11.8
How to Build a Vector Space From Scratch
295
 Associativity: For any triplet x; y; z of elements of G,
.x ? y/ ? z D x ? .y ? z/
This is a reasonable assumption that we often make in mathematics.
You should be aware of the fact that not every binary operation is
associative!
11.241
Exercise
Can you think of an example of a binary operation
that isn’t associative? Think of the usual subtraction of two numbers, e.g.,
3   2 D 1. Interestingly, this simple operation is not associative! To see
this, consider the ambiguous difference 1   2   3. Is it .1   2/   3 or is it
1   .2   3/? Check to see that it does matter!
 Identity: There exists an element e 2 G, called the identity element
such that for any element x 2 G,
x ? e D e ? x D x
Again, it’s advisable to think of a familiar scenario: you’re likely already
familiar with the number 0 in the integers equipped with the operation
of addition.
11.242
Exercise
You should check that 0 has the identity property for
any set of numbers equipped with addition, i.e., adding any number x and
zero in either order yields the number x.
 Inverse: For each element x 2 G, there exists an inverse element
x 1 2 G, with the property that
x ? x 1 D x 1 ? x D e
Inverses in any set of numbers equipped with the operation of addition are
the “negatives.” To see what we mean, try to ﬁnd a number such that when
you add it to 1 on either side, you get 0. A moment’s thought makes us realize
that the number we’re looking for is  1! So, we would say that  1 is the
inverse of the number 1 with respect to addition.
296
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.243
Deﬁnition
The deﬁnition of a group
A group .G; ?/ is a set G satisfying the following properties:
 Closure: There exists a function
? W G  G ! G;
which we call the binary operation of G.
 Associativity: For any triplet x; y; z of elements of G,
.x ? y/ ? z D x ? .y ? z/
 Identity: There exists an element e 2 G, called the identity element
of G, such that for any element x 2 G,
x ? e D e ? x D x
 Inverse: For each element x 2 G, there exists an inverse element
x 1 2 G, with the property that
x ? x 1 D x 1 ? x D e
Now, let’s regroup (sorry, intended...) and think about what this deﬁnition
really says.
The idea of a group is simple. We begin with a set of things. Then, we
deﬁne a single binary operation on that set of things, i.e., an operation that
accepts two inputs from your set of things and produces another thing in your
set. This property is also known as closure of the set under the operation.
Next, we require that the operation be associative. So, if we have three
things to operate on, we may operate on any two ﬁrst, and then the third (you
can’t operate on all three at once because you started with a binary operation!).
Afterward, we require that there be a special element in our set called the
identity element. If we apply the binary operation to this special element and
any other element, we get that element back.
Finally, we ask that each element in our set have an inverse element, also
in the set, so that when we apply the binary operation to that element and its
inverse element (in either way), we get the identity element.
To make the group abelian,23 we ask that the operation be commutative,
in the sense that the operation yields the same result regardless of the order.
23The name “abelian” honors the mathematician Niels Henrik Abel.
SECTION 11.8
How to Build a Vector Space From Scratch
297
11.244
Deﬁnition
Abelian group
A group .G; ?/ is called abelian iff ? is a commutative operation, i.e.,
for all x; y 2 G; x ? y D y ? x
This deﬁnition is fairly abstract, so let’s pin it down with an example: the
integers, denoted Z. This example will hopefully solidify a few of the remarks
made during the course of the deﬁnition. The integers Z are the numbers
Z WD f:::;  3;  2;  1; 0; 1; 2; 3; :::g
(11.245)
There is a natural choice of binary operation ? W Z  Z ! Z. It’s addition!
We deﬁne ?.x; y/ WD x Cy (we saw this earlier when explaining the meaning
of a binary operation). In other words, ? is just the binary operation of usual
integer addition. Then the claim is that this operation does in fact satisfy all
of the other properties listed.
Well, associativity is certainly satisﬁed, since addition of integers is asso-
ciative (although some might take issue with this, since in some sense associa-
tivity is satisﬁed by ﬁat). For example, if asked to ﬁnd the sum 1 C 2 C 3, we
know that we may either compute 1 C 2 ﬁrst, then add 3, or compute 2 C 3
ﬁrst, then add 1. In symbols, .1 C 2/ C 3 D 1 C .2 C 3/, and in fact, this is
true for all triplets of integers.
The next question is as to what the identity element is here. The answer is
0 because adding 0 and any other integer yields that integer.
Finally, we ask that every integer have an inverse, and this is true, since
any integer a has inverse  a. Check that for any integer a 2 Z,
a C . a/ D . a/ C a D 0
(11.246)
11.247
Exercise
Try to ﬁgure out why the set of natural numbers
N WD f0; 1; 2; 3; :::g
(11.248)
is not a group with the operation of usual addition of natural numbers, e.g.,
1 C 2 D 3.
In any case, it turns out that the rational numbers (Q), the real numbers (R)
and the complex numbers (C) each form abelian groups under usual addition
of numbers.
298
CHAPTER 11
Mathematical Tools for Quantum Computing I
Now, we have the following observation:
11.249
Key idea
Vectors of numbers form an abelian group!
To see what we mean, let’s revisit our deﬁnition of the addition of two
vectors. Let’s focus on what happens in two dimensions. In fact, let’s check
to see that the set of all vectors with two complex components, denoted
V D C2 WD
 x
y

W x; y 2 C

(11.250)
does in fact form an abelian group under usual vector addition.
First, we need to verify that this set is closed with respect to the operation
of vector addition, i.e., if we take two elements of this set and we add them,
that we do in fact get another element of this set (closure). So, let
 x
y
T
and
 z
w
T be any two elements in the set V . We need to check that their
sum
 x
y

C
 z
w

D
 x C z
y C w

(11.251)
is in fact another element of the set V .
Since each of x; y; z; w are complex numbers, the sums x C z and y C w
are in fact complex numbers too. So, the resulting vector is an element of V
after all. Then, V is closed with respect to vector addition.
The next question is as to whether this addition is associative. You’re
invited to check this and we’ll set it up for you. Let
 a
b

;
 c
d

;
 e
f

(11.252)
be elements of V .
11.253
Exercise
Check the associativity of vector addition, i.e., that
 a
b

C
 c
d

C
 e
f

D
 a
b

C
 c
d

C
 e
f

SECTION 11.8
How to Build a Vector Space From Scratch
299
So now we need an identity element. We can take the vector that has only
zero entries
e D
 0
0

(11.254)
11.255
Exercise
Check that the vector e is in fact an identity element for
the set V , i.e., adding e to any vector on either side yields the same vector.
We then ask whether any given vector in V has an inverse element in
V with respect to addition. This isn’t too bad. If we’re given a vector
v WD
 x
y
T in V , the inverse element is  v WD
  x
 y
T , as you
might have expected.
11.256
Exercise
Check to see that it is in fact the case that
v C . v/ D . v/ C v D e
(Remember that “e” is the all-zero vector deﬁned above!)
Now, what about the abelian part? Let us recall that abelian refers to
commutativity, so let’s see if the group of vectors has commutativity for the
operation of addition.
11.257
Exercise
You should check that adding vectors in V in either
order yields the same result, i.e., that they commute.
 a
b

;
 c
d

;
 e
f

(11.258)
This is true due to the fact that adding complex numbers in either order
yields the same result, as you’ll see while completing this exercise.
We have now veriﬁed that C2, the set of all two-dimensional vectors with
complex entries is an abelian group!
300
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.259
Exercise
Prove that R2 is an abelian group by similar means.
Then, prove that the n-dimensional analogue Rn is also a group. Last, prove
the n-dimensional analogue Cn of C2 is also an abelian group with respect to
usual vector addition.
A comment about notation: After assuming the group is abelian, it’s fair
to revert to calling the abstract operation ? by the name C. This is because
the binary operation for every abelian group can be realized as some sort of
addition of integers! We won’t have time to discuss this idea here, however.
The inverse of an element x in the abelian group setting is thus fairly referred
to by  x.
11.260
Exercise
To see an example of a non-abelian group, consider the
Pauli matrices X; Y; Z from earlier, and the identity matrix I WD I2. Check
that the set of sixteen matrices
P WD f˙I; ˙iI; ˙X; ˙iX; ˙Y; ˙iY; ˙Z; ˙iZg
equipped with the operation of matrix multiplication is a group by verifying
the axioms listed above. We call this group the Pauli group. It is non-abelian
because the matrix product is not commutative in general. For example,
XY ¤ YX.
You should now be wondering what we mean by an action of a ﬁeld F
on V , and in particular, what a ﬁeld is. We can jump directly from abelian
groups to ﬁelds, but we might as well take a more leisurely route and visit the
notion of a ring along the way. We’ll save the idea of an action for last.
Rings
A ring is an abelian group with an additional operation, which is often called
“multiplication,” that satisﬁes some additional conditions. Rings are not too
abstract, actually. You’ve been working with rings all of your life! Examples
include the integers, Z, rational numbers Q, the real numbers R, the complex
numbers C, and more exotic ﬁnite rings like Z=2Z D f0; 1g equipped with
addition and multiplication modulo 2 (also known as F2), as we will see. In
fact, most of these rings turn out to be ﬁelds.
SECTION 11.8
How to Build a Vector Space From Scratch
301
More formally, a ring R is an abelian group (so, it’s a set satisfying all of
the properties of an abelian group deﬁned earlier – review these!), where we
denote the commutative binary operation playing the role of addition by C,
equipped with an additional binary operation, denoted by by , playing the
role of multiplication, and satisfying the following properties:
 Closure: There exists a binary operation
 W R  R ! R
additional to C, which we usually call multiplication.
Similar to the scenario with groups, we often write the application of
 to a pair of elements .x; y/ of R as x  y as opposed to .x; y/. As
before, you’re advised to think of usual multiplication of numbers as you
read through these axioms. For example, the rational numbers Q (all
ratios of integers, i.e., fractions) are closed under usual multiplication.
11.261
Exercise
Verify that the rational numbers
Q WD
na
b W a; b 2 Z; b ¤ 0
o
are in fact closed under usual multiplication. So, let
a
b ; c
d 2 Q
(so, a; b; c; d 2 Z) and check that their product
a
b  c
d WD a  c
b  d
is actually another rational number. This amounts to conﬁrming the numerator
and denominator of the result are each integers and that the product of two
integers is an integer (which might even be considered deﬁning properties of
the integers!).
We also want for this multiplication to be associative so that multiplying
“three things at once” is well-deﬁned. To see what we mean by this, we dare
you to multiply 3 numbers at once, e.g. 2  3  4. Pause for a moment and try
to genuinely multiply all 3 numbers at once, i.e. no “cheating” by multiplying
two of them, and then proceeding from there. Give up?
Given a product of 3 numbers, e.g. 2  3  4, we must compute their product
by either associating 2 and 3, computing their product: 2  3 D 6, and then
associating this with 4 afterward or by associating 3 and 4 ﬁrst, computing
302
CHAPTER 11
Mathematical Tools for Quantum Computing I
their product: 12, and then associating this with 2. The fact that we can
compute this product in either way and get the same result is the miracle of
associativity of multiplication of numbers.
Here is the formal axiom requesting associativity:
 Associativity: For all x; y; z in R,
.x  y/  z D x  .y  z/
This is similar to the axiom for groups, but we’d like it to be true for
multiplication.
11.262
Exercise
Verify that multiplication of rational numbers is asso-
ciative. So, take three arbitrary rational numbers, say
a
b ; c
d ; e
f 2 Q
and show that
a
b  c
d

 e
f D a
b 
 c
d  e
f

This will amount to the fact that integer multiplication is associative!
In usual arithmetic, we are familiar with the idea that we may distribute
multiplication “over” addition. So, we also require an axiom of distributivity:
 Distributivity: For all x; y; z 2 R
x  .y C z/ D x  y C x  z
Also,
.y C z/  x D y  x C z  x
This is a bit subtle. We want the distributivity to “work” on both sides.
We won’t dwell on it though.24
11.263
Exercise
Verify that multiplication of rational numbers does in
fact distribute over addition. So, take three rational numbers, say
a
b ; c
d ; e
f
24Technically, we could request the axiom of distributivity before the axiom of commutativ-
ity, or even request the axiom of distributivity and not the axiom of commutativity. Do you see
that if we request the axiom of distributivity and not the axiom of commutativity that we really
need both equalities in the axiom of distributivity?
SECTION 11.8
How to Build a Vector Space From Scratch
303
and show that:
a
b 
 c
d C e
f

D a
b  c
d C a
b  e
f
Fortunately, the multiplication of rational numbers is commutative, so
there’s no need to verify the distributive property “on the other side”. You can
if you want to practice, however!
We’ll get you started: Realize that because we are verifying the distributiv-
ity of multiplication, we’re not yet allowed to use it. So, we have to ﬁrst add
the two parenthesized rational numbers (by giving the common denominator
of df and get a single rational number, like so:
c
d C e
f D c
d C e
f D c  1
d  1 C e  1
f  1 D c
d  1
1 C e
f  1
1 D c
d  f
f C e
f  d
d
Continuing...
D c  f
d  f C e  d
f  d D c  f
d  f C e  d
d  f D c  f C e  d
d  f
So, we may now freely multiply this result (sum of c
d and e
f by a
b. Go
ahead and do this and see what you get.
Then, perform the addition on the right-hand side and see what you get.
Are these the same?
It might surprise you how much work goes into verifying something as
seemingly simple as this!
So, a ring is an abelian group (and therefore comes equipped with a
binary operation which is fairly called “addition”) equipped with another
binary operation, called “multiplication,” which is associative and satisﬁes a
distributive law compatible with addition (on both the right and the left).
Most rings of interest also come with a special element called the (multi-
plicative) identity element that “preserves the identity” of the elements that it
multiplies. For example, you will be able to verify for yourself shortly that
the (multiplicative) identity element in each of the rings Z; Q; R; and C is in
fact simply the number 1, as we would hope. This (multiplicative) identity
element is almost always different from the (additive) identity element that
the underlying abelian group of the ring already came with. To see this, notice
that the underlying abelian groups of each of the rings Z; Q; R; and C has
(additive) identity element 0, which is drastically different from 1. A ring
304
CHAPTER 11
Mathematical Tools for Quantum Computing I
equipped with a (multiplicative) identity element is known as a ring with unity,
where “unity” is meant to evoke the notion of the number 1, which it often is.
Here is the axiom requesting the existence of a multiplicative identity
element:
 Identity: There exists an element, denoted by 1, in R such that for any
element x in R,
1  x D x  1 D x
Equationally, this is strikingly similar to the group property. The identity
element of a ﬁeld is often literally the number 1, as opposed to e or 0, however.
Notice that we haven’t yet required multiplication to be commutative, so it
is truly necessary to request that the multiplicative identity “works” on both
sides.
11.264
Exercise
Verify that the set of 2  3 matrices with integer entries
is in fact a ring, i.e. is an abelian group (what should be the binary operation
likened to “addition” be?) equipped with the additional binary operation of
matrix multiplication satisfying: closure, associativity, and distributivity. Is
there an identity element (as per our stringent axiom above) in this ring? If
you think so, discover it, and prove that it really is an identity element. If you
think not, prove that there cannot be an identity element in this ring.
Many rings of interest are even better: they are commutative, i.e. their
multiplication enjoys a commutative property, as in the case of abelian groups.
In fact, almost of the examples above, e.g. Z; Q; R; and C, are commutative
rings. The example in the exercise above consisting of matrices is a non-
commutative ring. Nearly all rings of matrices are not commutative. Once
we show that matrix multiplication corresponds to composition of linear
transformations (which are themselves functions), that matrix multiplication
is not commutative is quite believable.
Here is the axiom requesting that multiplication be commutative:
 Commutativity: For all x; y 2 R,
x  y D y  x
So, multiplication in either order should yield the same product.
11.265
Exercise
Verify for yourself that the following set of matrices
surprisingly constitutes a commutative ring with unity:
SECTION 11.8
How to Build a Vector Space From Scratch
305
 1
0
0
1

;
 0
 1
1
0

;
  1
0
0
 1

;

0
1
 1
0

Figure out the relationship between this ring of matrices and the following
ring:
Z=4Z equipped with addition modulo 4 and multiplication modulo 4.
Are they “the same”?
Rings which enjoy a multiplicative identity and whose multiplication is
commutative are called commutative rings with unity.
It turns out a ﬁeld can be obtained from a commutative ring with unity
by simply requiring one more axiom: that each nonzero element admits a
multiplicative inverse in the ring. We’ll see this in what follows.
Fields
A ﬁeld F is a commutative ring with unity (so, an abelian group with an
additional binary operation satisfying all of the axioms above – review these
carefully!), where we denote the binary operation of the underlying abelian
group by C and the binary operation of multiplication by , satisfying the
following salient property that requires each nonzero element to have a multi-
plicative inverse:
 Inverse: For each nonzero element x 2 F, there exists an element x 1
in F such that
x  x 1 D x 1  x D 1
25
We are ﬁnally able to state the deﬁnition of a ﬁeld once and for all:
11.266
Deﬁnition
The deﬁnition of a ﬁeld
A ﬁeld is a commutative ring with unity satisfying the additional axiom
that for each nonzero element x 2 F, there exists an element x 1 in F
such that
x  x 1 D x 1  x D 1
25If we demand all of the axioms of a ﬁeld except for the commutativity axiom, we have
what is referred to as a division ring. The quaternions are a famous example (especially in
physics!) of a division ring, i.e. a non-commutative ring with unity and where each nonzero
element has a multiplicative inverse.
306
CHAPTER 11
Mathematical Tools for Quantum Computing I
Here’s an interesting example of a ﬁeld not quite like those explored in the
previous exercise.26 Consider the set F2 WD f0; 1g. We equip F2 with addition
and multiplication modulo-2, as indicated by the addition and multiplication
tables
+
0
1
0
0
1
1
1
0
*
0
1
0
0
0
1
0
1
So, a quick look reveals interesting things like 1 C 1 D 0 happen in F2.
The addition and multiplication tables clearly indicate that closure of each is
satisﬁed. Let’s take for granted that associativity is satisﬁed – it’s not difﬁcult
to check, just tedious. We can see from the addition and multiplication tables
that the (additive) identity element is 0 and that the (multiplicative) identity
element is 1.
11.267
Exercise
Check the inverse property of each of addition and
multiplication for F2 given the addition and multiplication tables.
The ﬁnal piece in the “deﬁnition of a vector space” puzzle is the deﬁnition
of an action of a ﬁeld on an abelian group, which we’ll discuss now.
An action of a ﬁeld on an abelian group is an abstraction of the “scalar
multiplication” of vectors by numbers deﬁned earlier in the chapter. Here it is
formally:
An action of a ﬁeld F on an abelian group V is a function  W F  V ! V
satisfying some properties.
Before listing the properties, we’d like to mention that we’ll not write
.a; v/ when expressing the application of the function (action)  to a pair
.a; v/, with a in F and v in V . Instead, we’ll write a  v to be succinct. it’s
advisable to think of an action as scalar multiplication of a vector v by a
number a.
Here are the deﬁning properties of an action:
 Distributivity I: For all a 2 F and all u; v in V ,
26This is called the Galois ﬁeld, abbreviated GF2. Galois was a startlingly brilliant mathe-
matician who lived for only twenty one years, from 1811 to 1832. During his short life, he
essentially founded all of Galois theory and group theory in an effort to determine precisely
when the solutions of a polynomial equation could be written down explicitly. To understand
the issue he was interested in, try to ﬁnd the exact solutions to the equation x5   x C 1 D 0.
SECTION 11.8
How to Build a Vector Space From Scratch
307
a  .u C v/ D a  u C a  v
 Distributivity II: For all a; b 2 F and all v in V ,
.a C b/  v D a  v C b  v
These distributivity axioms are subtle. The addition on the left is
happening in the ﬁeld F, while the addition on the right is happening in
the group V !
To see what we mean, try this exercise:
11.268
Exercise
Let a D 2 and u D
 1
0

; v D
 0
1

. Check that
a  .u C v/ D a  u C a  v, i.e.,
2 
 1
0

C
 0
1

D 2 
 1
0

C 2 
 0
1

 Compatibility of the action with multiplication in the ﬁeld F: For all
a; b 2 F and v in V
.ab/  v D a  .b  v/
This is also subtle. We’re asking that multiplying two elements of the
ﬁeld and then applying the resulting element of the ﬁeld to a vector be
the same as iteratively applying those elements of the ﬁeld. Try this
exercise to get a feel for this idea:
11.269
Exercise
Let a D 2; b D 3 and v D
 1
0

. Check that .ab/v D
a  .b  v/, i.e.,
.2  3/ 
 1
0

D 2 

3 
 1
0

 Identity: For all v in V ,
1  v D v
This is to say that the (multiplicative) identity element of the ﬁeld
should act on the group V in such a way as to not disturb V .
308
CHAPTER 11
Mathematical Tools for Quantum Computing I
11.270
Exercise
Check that the (multiplicative) identity 1 of the ﬁeld C
acts on the vector
 1
0

in the way that is requested in the identity clause,
i.e., check that
1 
 1
0

D
 1
0

11.271
Deﬁnition
Action of a ﬁeld on an abelian group
An action of a ﬁeld F on an abelian group V is a function  W F  V ! V
satisfying:
 Distributivity I: For all a 2 F and all u; v in V ,
a  .u C v/ D a  u C a  v
 Distributivity II: For all a; b 2 F and all v in V ,
.a C b/  v D a  v C b  v
 Compatibility of the action with multiplication in the ﬁeld F For all
a; b 2 F and v in V :
.ab/  v D a  .b  v/
 Identity: For all v in V ,
1  v D v
The Deﬁnition of a Vector Space
Congratulations! If you’ve made it to this point (and you’ve completed and
understood the exercises), you understand the mathematician’s deﬁnition of
a vector space V over a ﬁeld F. You’ll likely have to review the deﬁnitions
above and play around with a few examples to get a solid understanding of
them, so here are some examples to play with:
11.272
Exercise
To solidify your understanding of the deﬁnition of a
vector space, verify that each of the following are examples of vector spaces:
 Q over itself (yes, it’s a vector space!)
 R over R
 C over C
SECTION 11.8
How to Build a Vector Space From Scratch
309
 Q2 over Q
 R2 over R
 C2 over C
 Qn over Q
 Rn over R
 Cn over C
You’re now invited to contrast the succinct sentence “A vector space V over
a ﬁeld F is an abelian group V equipped with an action of F on V ” with the
deﬁnition found in most standard texts on linear algebra:
A vector space V over a ﬁeld F is a set satisfying the following properties:
 Associativity of addition: For all u; v; w 2 V ,
.u C v/ C w D u C .v C w/
 Commutativity of addition: For all u; v 2 V ,
u C v D v C u
 Identity element of addition: There exists an element 0 2 V such that
for all v 2 V ,
v C 0 D v
 Inverse element of addition: For every v 2 V , there exists an element
 v 2 V such that
v C . v/ D 0
 Compatibility of scalar multiplication and ﬁeld multiplication: For any
a; b 2 F and any v 2 V ,
.ab/v D a.bv/
 Identity element of scalar multiplication: For any v 2 V ,
1v D v
where 1 is the multiplicative identity of the ﬁeld F
 Distributivity of scalar multiplication with respect to vector addition:
For all a 2 F and any u; v 2 V , a.u C v/ D au C av
 Distributivity of scalar multiplication with respect to ﬁeld addition: For
all a; b in F and any v 2 V , .a C b/v D av C bv
310
CHAPTER 11
Mathematical Tools for Quantum Computing I
Even after requiring the properties above, we would still have to deﬁne a
ﬁeld axiomatically as a set F together with two operations, called addition
(denoted C) and multiplication (denoted ), where by an operation we mean an
association of any pair of elements of F to some other element of F. Thus, the
deﬁnition of a vector space in this approach still requires additional properties:
 Associativity of addition and multiplication: For all a; b; c 2 F,
a C .b C c/ D .a C b/ C c
and
a  .b  c/ D .a  b/  c
 Commutativity of addition and multiplication: For all a; b 2 F,
a C b D b C a
and
a  b D b  a
 Additive and multiplicative identity: There exist two elements 0 and 1
in F such that for all a 2 F,
a C 0 D a
and
a  1 D a
 Additive inverses: For every a 2 F, there exists an element  a in F
such that
a C . a/ D 0
 Multiplicative inverses: For every nonzero element a in F, there exists
an element a 1 in F such that
a  a 1 D 1
 Distributivity of multiplication over addition: For all a; b; c 2 F,
a  .b C c/ D a  b C a  c
The choice is up to you! Since many linear algebra textbooks do not wish
to cover set theory, they are forced to delineate an exhaustive list of properties
to deﬁne a vector space. With set theory in hand, we state the deﬁnition of a
vector space once more to remind you:
11.273
Deﬁnition
Vector Space
A vector space V over a ﬁeld F is an abelian group V equipped with an
action of the ﬁeld F on V .
SECTION 11.8
How to Build a Vector Space From Scratch
311
Subspaces
Now that we know what a vector space is (make sure you do!), we can deﬁne
the notion of a subspace of a vector space. It’s pretty much what it sounds
like. You have a space of vectors and then you consider a subcollection of
vectors that also satisﬁes the properties of a vector space.
11.274
Deﬁnition
Subspace of a Vector Space
A subset S  V of a vector space V over a ﬁeld F is a subspace of V iff
S is a vector space over F.
The remarkable thing is that instead of having to verify all of the properties
of a vector space for S after having done so for V , the veriﬁcation of the
subspace property of a subcollection of a vector space can be checked in three
easy steps:
11.275
The subspace lemma
S  V is a subspace of a vector space V over a ﬁeld F iff
 Identity: 0 2 S, where this 0 is the additive identity that comes as
part of the data of the vector space V
 Additive closure: For all u; v 2 S, u C v 2 S
 Closure under scalar multiplication: For all a 2 F, v 2 S, a  v 2 S
Let’s see an example. Consider the vector space R2 and the x-axis, which
we may think of as the set of points
X WD
 x
0

W x 2 R

 R2
(11.276)
We claim that X  R2 is a subspace. We’ll apply the subspace lemma to
prove this.
First of all, the zero vector is in fact part of the x-axis, so the identity
property is satisﬁed.
Second, it’s easy to see geometrically that the sum of two vectors on the
x-axis is another vector on the x-axis. However, here is a rigorous proof. Let
 x1
0

;
 x2
0

(11.277)
each be elements of the x-axis (these are like our u and v in the additive
closure clause of the subspace lemma). Then
312
CHAPTER 11
Mathematical Tools for Quantum Computing I
 x1
0

C
 x2
0

D
 x1 C x2
0 C 0

D
 x1 C x2
0

(11.278)
This is another element of the x-axis.
Lastly, we see that if we take a scalar multiple of a vector on the x-axis
that it will again be a vector on the x-axis. We give a rigorous proof.
Let a be in R and v WD
 x
0

in X. Then, we need to check that a  v is
again in X (here, a and v are like the a in the v in the closure under the scalar
multiplication clause in the subspace lemma). To see that this is the case, look
at
a  v D a 
 x
0

D
 a  x
0

(11.279)
This is another element of the x-axis.
So, it is true that the x-axis is a subspace of R2.
11.280
Exercise
You’re invited to verify that the y-axis, and in fact, any
line through the origin in the vector space R2 over R is in fact a subspace of
R2. By a line through the origin in R2, we mean the set of points
L D

x
a  x

W x 2 R

for a ﬁxed a 2 R. Use the subspace lemma like we did above to make your
life easier, or else you’ll have to verify all of the properties of a vector space
again. Why do we emphasize that the line be “through the origin”? What
happens if it doesn’t pass through the origin?
We close this section with the deﬁnition of a linear transformation between
arbitrary vector spaces:
11.281
Deﬁnition
Deﬁnition of a linear transformation between
vector spacesa
A linear transformation T between vector spaces V and W , each over a
ﬁeld F, is a function T W V ! W satisfying the following properties:
 For all x; y 2 V , T .x C y/ D T .x/ C T .y/.
 For all x 2 V and all a 2 F, T .a  x/ D a  T .x/.
aIt should be noted that the addition of x and y in the deﬁnition above is occurring
in the vector space V , whereas the addition of T .x/ and T .y/ is occurring in W , and V
and W may have different deﬁnitions of the operation C. A similar remark holds for the
scalar multiplication occurring in the second criterion.
SECTION 11.9
Span, Linear Independence, Bases and Dimension
313
11.9
Span, Linear Independence, Bases and
Dimension
Span
A quick comment about some of the notation that follows: We will often write
a column vector like
 1
0

as a transposed row vector like so
 1
0
T for
simplicity of presentation.
The deﬁnition of a vector space provided above is helpful, but it would be
nice to have a more concrete description. Let’s think about the vector space
R2 over R for a moment so that we can motivate a new deﬁnition.
Let’s say we want to make a vector in R2. In other words, we would like
to construct any vector in two-dimensional space. A moment’s thought brings
forth the realization that we can build any vector in two-dimensional space
with just two vectors. Let’s make this idea more precise. Visualize the vector
 1
1

in R2.
Suppose we’d like to get from the origin to the point in space to which this
vector points. We can travel along the x-axis 1 step, and then travel upward 1
step. We could also travel upward along the y-axis ﬁrst, and then rightward 1
step. Here is the algebraic manifestation of this concept:
 1
1

D 1 
 1
0

C 1 
 0
1

In other words, the vector . 1
1 /T is 1 of the vector . 1
0 /T plus 1
of the vector . 0
1 /T . So, we can build the vector . 1
1 /T using the
“building blocks” . 1
0 /T and . 0
1 /T . Can we make every vector in
R2 in this way?
11.282
Exercise
Try to build the vector . 2
3 /T using some combi-
nation of the building blocks . 1
0 /T and . 0
1 /T . Can you make the
vector . 1
2
1
3 /T ? Nobody said we can’t use fractional parts of our building
blocks! What about the vector .
p
2
2
p
2
2
/T ?
 100
CHAPTER 11
Mathematical Tools for Quantum Computing I
What we’re getting at here is the notion of a spanning set of vectors. We
say that the vectors . 1
0 /T and . 0
1 /T span the vector space R2 over
R.
We do have to be a bit more precise here though. What if we were
asked to build the vector . 1
2
1
3 /T and we weren’t allowed to use fractional
amounts of our building blocks . 1
0 /T and . 0
1 /T ? So, imagine we
can only use integer multiples of the building blocks. Well, then we can’t
make the vector . 1
2
1
3 /T , at least not with integer multiples of . 1
0 /T
and . 0
1 /T . So, we really should say a bit more about in what sense
the building blocks span the space. Here is the formal deﬁnition and some
terminology that will be useful for the discussion which follows:
11.283
Deﬁnition
Spanning set of vectors
We say that a set of vectors v1; :::; vm spans a vector space V over a ﬁeld
F iff for each vector v in V , there exist elements a1; :::; am of the ﬁeld F
such that
a1  v1 C a2  v2 C ::: C am  vm D v
Then, we deﬁne the span of a set of vectors:
11.284
Deﬁnition
Span of a set of vectors
The span of a set of vectors v1; :::; vm over a ﬁeld F is deﬁned to be:
span.fv1; :::; vmg/ WD fa1v1 C ::: C amvm W a1; :::; am 2 Fg :
We often abbreviate the phrase “span of a set of vectors v1; :::; vm over
a ﬁeld F” to simply “span of v1; :::; vm” when the ﬁeld F is understood.
We say that a vector v is in the span of some vectors v1; :::; vm iff v 2
span.fv1; :::; vmg/. We call a1; :::; am the coefﬁcients of the F-linear com-
bination of the vectors v1; :::; vm. Furthermore, we say that the set of vectors
spanned by the vectors v1; :::; vm (equivalently, the set of all F-linear combi-
nations of the vectors v1; :::; vm) is the span of the vectors v1; :::; vm. So, we
can rephrase our deﬁnition above to say that a set of vectors fv1; :::; vmg spans
a vector space V over a ﬁeld F iff
span.fv1; :::; vmg/ WD fa1v1 C ::: C amvm W a1; :::; am 2 Fg D V
In other words, every vector in V can be constructed from a sum of F-
multiples of the “building blocks” (spanning set) of vectors v1; :::; vm.
SECTION 11.9
Span, Linear Independence, Bases and Dimension
315
11.285
Exercise
Can you ﬁnd another spanning set of vectors for R2?
How many such sets do you think there are? Can you ﬁnd a spanning set for
R3?
Linear Independence
Having thought about the previous exercise a bit, you’re likely convinced that
there are several – in fact, inﬁnitely many – spanning sets for R2, R3, any Rn
as vector spaces over R, and likewise, any vector space Cn over C. This is
wonderful in that we know that there are several sets of vectors that sufﬁce
to build the vector spaces we care about. What is interesting to consider is
whether there is a minimum set of vectors we can use to build the space we
are considering.
For example, we can build all of the vectors in R2 using the set of three (!)
vectors
 1
2

;
 3
4

;
 5
6

(11.286)
We’ll try to convince you here by showing that we can at least build the
vector
 10
10
T using these three vectors. Before you see what we do
you should think of how many of each of these vectors you need to construct
 10
10
T . It’s not that easy, actually...
 10
10

D 1 
 1
2

C . 7/ 
 3
4

C 6 
 5
6

(11.287)
11.288
Exercise
Check that the equality above is true!
This probably seems a bit magical if you tried to ﬁgure it out yourself
before you looked. It turns out that it’s nothing more than solving a bunch of
equations though!
Of course, you’re probably thinking “...we had a perfectly good set of two
vectors that we could use to build everything already. Why bother with this
set of three more complicated vectors?” Fair question. First, we’ll show you
that we don’t need three vectors at all – only two. In other words, these three
vectors span R2 over R, but we can do with just two. Later on, we’ll try to
316
CHAPTER 11
Mathematical Tools for Quantum Computing I
argue that it is occasionally advantageous to use different sets of building
blocks at different times – sort of like a change in perspective.
To see that we only need two of these vectors, we’ll show that one of them
can be built from the others and then we’ll revisit the previous example and
drive the point home.
Observe that
 5
6

D . 1/ 
 1
2

C 2 
 3
4

(11.289)
You might have seen this without any calculation. The point is that we can
replace any instance of the vector
 5
6
T with this combination
. 1/ 
 1
2

C 2 
 3
4

(11.290)
So, we revisit the previous example and apply this substitution:
 10
10

D 1 
 1
2

C . 7/ 
 3
4

C 6 
 5
6

D 1 
 1
2

C . 7/ 
 3
4

C 6 

. 1/ 
 1
2

C 2 
 3
4

(11.291)
and then reorganize:
D 1 
 1
2

C . 7/ 
 3
4

C . 6/ 
 1
2

C 12 
 3
4

D 1 
 1
2

C . 6/ 
 1
2

C . 7/ 
 3
4

C 12 
 3
4

D . 5/ 
 1
2

C 5 
 3
4

(11.292)
So, we can express our original vector
 10
10
T using only the building
blocks
 1
2
T and
 3
4
T .
In sum, we say that
 5
6
T is R-linearly dependent (even Z-linearly
dependent, since the coefﬁcients are all integers!) on the vectors
 1
2
T
and
 3
4
T and that
 10
10
T is in the span of the vectors
 1
2
T
and
 3
4
T . Using our previous deﬁnition of span, we can state precisely
what it means for a vector to be linearly dependent on a set of vectors.
SECTION 11.9
Span, Linear Independence, Bases and Dimension
317
11.293
Deﬁnition
Linear dependence
A vector v in a vector space V over a ﬁeld F is F-linearly dependent on
the vectors v1; :::; vm iff v is in spanfv1; :::; vmg. We often just use the
term linear dependence rather than F-linear dependence where the ﬁeld is
understood.
Naturally then, we have a notion of linear independence of vectors!
11.294
Deﬁnition
Linear independence of a set of vectors
We say that a set of vectors fv1; :::; vmg is linearly independent if none of
the vectors in the set are linearly dependent on the others. More precisely,
we say that fv1; :::; vmg is a linearly independent set of vectors iff for each
i 2 f1; :::; mg, vi is not in spanfv1; :::; vi 1; viC1; :::vmg.
11.295
Exercise
Do we need two vectors to span all of R2? Can we get
away with just having one?
Bases and Dimension
Finally, we can deﬁne a basis of a vector space. This deﬁnition will unlock a
matrix-centric view of linear algebra, as we’re about to see.
You might be wondering why we bothered deﬁning all of the previous
terms. It turns out that the precise deﬁnition of a matrix that we desire depends
(linearly? We couldn’t resist...) on the precise deﬁnition of a basis of a vector
space. In fact, you might have been frustrated by the idea of a matrix this
whole time, wondering, “Where are the numbers in the grid coming from?” If
you answered a previous exercise asking you to conjecture what the columns
of a matrix tell us, what follows will be quite satisfying!
First, the deﬁnition of a basis:
11.296
Deﬁnition
The deﬁnition of a basis
A basis of a vector space V over a ﬁeld F is a linearly independent
spanning set of vectors.
318
CHAPTER 11
Mathematical Tools for Quantum Computing I
That’s it? Well, quite a bit is packed into the deﬁnitions of linearly
independent and spanning, so you might want to review those!
11.297
Exercise
Check that the set
 1
0

;
 0
1

is in fact a basis
for R2.
We keep saying a basis, which implies there are more than one. Hopefully,
you convinced yourself earlier that there are inﬁnitely many spanning sets
of vectors for R2. It thus follows that there’s an inﬁnite number of linearly
independent spanning sets for R2 as a result. Here are a bunch more:
 2
0

;
 0
1

;
 1
2

;
 3
4

;
  1
0

;
 0
1

(11.298)
All of these bases have the same number of vectors! With enough playing
around (don’t just believe us!), we might eventually be convinced that all
bases have the same number of vectors, and in fact they do.
This is what many call the fundamental theorem of linear algebra:
11.299
Invariance of basis number
The number of basis vectors for a vector space is the same no matter which
basis we choose.
This calls for a deﬁnition!
11.300
Deﬁnition
Dimension
The dimension of a vector space V is the number of vectors for any basis
for V .
Observe that we need the theorem to have a well-deﬁned notion of di-
mension. If we didn’t know that all bases have the same length, we could
potentially have two different answers for what the dimension of a vector space
is! Believing the theorem, we have a natural notion of ﬁnite-dimensionality: a
vector space is ﬁnite-dimensional iff it has a ﬁnite basis.
We won’t prove the theorem here, and instead will offer some slight
indication for why it should be true. It is likely believable that any set of
linearly independent vectors in a ﬁnite-dimensional vector space should have
size less than or equal to the size of any spanning set of vectors. This fact
SECTION 11.9
Span, Linear Independence, Bases and Dimension
319
is known as the Exchange Lemma. It is of fundamental importance in linear
algebra. You can quickly ascertain the power of the Exchange Lemma, since
knowing it and that any two bases are spanning sets already implies they must
be of the same length.
11.301
Exercise
Convince yourself that any two bases for a
ﬁnite-dimensional vector space have the same size.
This idea has major implications. Suppose we already know the dimension
d of a vector space V and that we’d like to ﬁnd a basis for this vector space.
Imagine we already have a set of d vectors that spans the vector space V .
Well, then the previous discussion guarantees that this list of d spanning
vectors is linearly independent and thus already a basis for V ! Likewise, if
we have a list of d linearly independent vectors in V , they must span V and
are a basis as well.
So if we want to ﬁnd a basis for a vector space V of dimension d, it
sufﬁces to discover a list of d vectors that are either all linearly independent
or that spans the space V . One or the other is enough, so long as we have
d vectors. Unless otherwise mentioned, we will restrict our attention to the
ﬁnite-dimensional case.
We write the dimension of a vector space V over a ﬁeld F as dimF.V /, but
often shorten this to dim.V / when the ﬁeld is understood. So, relating this
idea to a question asked in the self-test at the outset of the chapter, we would
write dimC.C2/ to indicate the dimension of the vector space C2 considered
as a vector space over the ﬁeld C and dimR.C2/ to indicate the dimension
of the vector space C2 considered as a vector space over the ﬁeld R. The
ﬁrst of these, dimC.C2/, is equal to 2, while the second, dimR.C2/, is equal
to 4. They are therefore not the same thing, and we now appreciate that the
subscript is occasionally quite important.27
11.302
Exercise
Try to ﬁnd a basis of length 2 for the vector space C2
over C and then a basis of length 4 for the vector space C2 over R. This
should convince you that the subscript is occasionally necessary. Then, think
about the question from the self-test again, and ﬁgure out why it’s a trick
question.
27This point relates to the question in the self test at the opening of the chapter which asks
the reader to compare R4 and C2.
320
CHAPTER 11
Mathematical Tools for Quantum Computing I
Orthonormal Bases
Once we have a basis, we can ask that it satisfy certain conditions. Let’s take
our cue from a basis we know and love at this point
 1
0

;
 0
1

D fj0i ; j1ig
(11.303)
What’s so nice about this particular basis is that each of the vectors are of unit
length (i.e., have L2 norm 1) and they are what is referred to as orthogonal to
each other. “Orthogonal” is essentially a fancy word for perpendicular.28
11.304
Deﬁnition
Orthogonality of vectors
More precisely, we say that two vectors u; v in a vector space V equipped
with an inner product h; i are orthogonal (to each other) iff their inner
product is zero, i.e., hu; vi D 0.
In Dirac notation, we say that two vectors u and v are orthogonal iff
hujvi D 0. A basis such that each of the elements have unit length and such
that they are pairwise orthogonal, is known as an orthonormal basis. In the
context of quantum computing, we choose a set of orthonormal basis vectors
to be our preferred computational basis vectors. We can say this a bit more
mathematically:
11.305
Deﬁnition
Orthonormal basis
A set of vectors B D fv1; :::; vng in a vector space V is called an ortho-
normal basis iff B is a basis, and for all i; j 2 f1; :::; ng,
(
hvi; vj i D 1
if i D j
hvi; vj i D 0
if i ¤ j
We’ll learn later that what is written above is an instance of what is known
as the Kronecker Delta function ı. An important theorem of linear algebra
28Perpendicular and orthogonal are interchangeable except in the case that either vector is
the zero vector, since it does not make sense to say that a vector is perpendicular to the zero
vector as there is no angle between them. When comparing two vectors when at least one of
the vectors is the zero vector, we use the word orthogonal.
SECTION 11.9
Span, Linear Independence, Bases and Dimension
321
is that every spanning set of vectors in a ﬁnite-dimensional vector space can
be pruned to form an orthonormal basis. The process yielding the desired
orthonormal basis is known as the Gram-Schmidt process in honor of Jørgen
Pedersen Gram and Erhard Schmidt.
11.306
Gram-Schmidt Orthonormalization
Every spanning set of vectors for a ﬁnite-dimensional vector space can be
modiﬁed to form an orthonormal basis.
Let’s relate these ideas to some of the quantum computing you’ve read
earlier. You’ve read by now that any state is a superposition of the states j0i
and j1i. The ﬁrst thing you should check is that the states j0i and j1i are
orthogonal to one another:
11.307
Exercise
Verify that the vectors j0i and j1i are in fact orthogonal
to one another, i.e., h0j1i D 0.
Then, you should check that they’re even orthonormal!
11.308
Exercise
Verify that the vectors j0i and j1i are even orthonormal,
i.e., h0j0i D 1 and h1j1i D 1.
We already know that fj0i ; j1ig is a basis for R2. It’s not too difﬁcult to
believe that fj0i ; j1ig is also a basis for C2.
CHAPTER
12
Mathematical Tools for Quantum
Computing II
12.1
Linear Transformations as Matrices
We claimed earlier that every linear transformation has an associated matrix,
and vice versa. We aim to demonstrate for you that, in fact, a linear trans-
formation and a matrix are the same thing. This justiﬁes the study of linear
algebra as the study of matrices and operations on them.
Equipped with the deﬁnition of a basis, we are prepared to deﬁne the
matrix associated to a linear transformation between two vector spaces.
Let V and W be vector spaces over some ﬁeld F. Yes, they have to be
vector spaces over the same ﬁeld – you’ll see why! Then, V and W each
have a basis, say BV D fv1; :::; vng and BW D fw1; :::; wmg. So, V has
dimension n and W has dimension m.
Let T be a linear transformation between the vector spaces V and W . So,
T W V ! W is a linear transformation, and thus a function. Since T is a
linear transformation from V to W , each basis element vj for j 2 f1; :::; ng
is mapped into W , i.e., T vj 2 W . Since W is spanned by the vectors
w1; :::; wm and T vj 2 W , there exist coefﬁcients a1;j ; :::; am;j such that
T vj D a1;j w1 C ::: C am;j wm, i.e., we can express each T vj as a linear
combination of the basis elements w1; :::; wm.
So, to each basis element vj of V , we have an associated list of elements
a1;j ; :::; am;j of the ﬁeld F. Maybe you see where we’re going with this...
We deﬁne the matrix M .T W V ! W /BV ;BW associated to the linear
transformation T W V ! W with respect to the bases BV and BW entrywise:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_12
323
324
CHAPTER 12
Mathematical Tools for Quantum Computing II
12.1
Deﬁnition
Matrix associated to a linear transformation
 M .T W V ! W /BV ;BW

i;j WD ai;j
where ai;j is the coefﬁcient of the ith basis element of BW in the linear
combination T vj D a1;j w1 C ::: C am;j wm expressing the image T vj
of the j th basis element of BV .
12.2
Exercise
You should wonder why we can’t play the same game
with any transformation between vector spaces. More speciﬁcally, why do we
only deﬁne a matrix for a linear transformation between vector spaces? Can
you ﬁgure this out?
Let’s see this construction of a matrix in action. Consider the linear
transformation T from R2 ! R2 described by
T
 x
y

WD
 x C y
x   y

where we take the basis for R2 to be the standard basis
 1
0

;
 0
1

in each case. So far, we have T as deﬁned, each of V and W are R2, and each
of BV and BW are
 1
0

;
 0
1

.
12.3
Exercise
Check that the transformation T given above is actually a
linear transformation.
Now that you’ve veriﬁed that the transformation T is linear, we can proceed
to ﬁgure out the coefﬁcients ai;j as in the construction above. This amounts
to ﬁguring out where T sends the two basis elements. Following the deﬁnition
of T , we have
T
 1
0

WD
 1 C 0
1   0

D
 1
1

(12.4)
Then, we decompose this into a linear combination of the basis vectors like so
SECTION 12.1
Linear Transformations as Matrices
325
 1
1

D 1 
 1
0

C 1 
 0
1

(12.5)
At this point, we see that
 M .T W V ! W /BV ;BW

1;1 WD a1;1 D 1
(12.6)
and that
 M .T W V ! W /BV ;BW

2;1 WD a2;1 D 1;
(12.7)
We’ll abbreviate the fancy notation
 M .T W V ! W /BV ;BW

(12.8)
with M .T / when the vector spaces V and W and their bases BV and BW
are understood.
12.9
Exercise
Verify that a1;2 D 1 and a2;2 D  1 by determining
where the second basis vector
 0
1

is mapped to and decomposing the
resulting vector into a linear combination of the basis vectors
 1
0

and
 0
1

.
After all of this computation, we realize that the matrix associated to the
linear transformation T W V D R2 ! W D R2 with respect to the bases
BV D BW D
 1
0

;
 0
1

(12.10)
is
M .T / D
 1
1
1
 1

(12.11)
12.12
Exercise
To practice building matrices from the description of a
linear transformation, construct the matrix associated to the linear transforma-
tion
T W V D R3 ! W D R3
deﬁned by
326
CHAPTER 12
Mathematical Tools for Quantum Computing II
T
0
@
x
y
z
1
A WD
0
@
1x C 2y C 3z
4x C 5y C 6z
7x C 8y C 9z
1
A
with respect to the standard basis
BV D BW D
8
<
:
0
@
1
0
0
1
A ;
0
@
0
1
0
1
A ;
0
@
0
0
1
1
A
9
=
;
for each. You should ﬁrst check that this is a linear transformation. Then,
ﬁgure out where each of the basis vectors is mapped. Once you know this,
decompose those into a linear combination of the basis vectors and use the
coefﬁcients of those linear combinations to build the matrix.
Unless we mention otherwise, we will assume that the vector spaces Rn
and Cn have the standard bases. You may be frustrated by the deﬁnition of
matrix multiplication that we gave earlier on. We’d like to demonstrate why
it is deﬁned the way it is. It has to do with the intimate relationship between
the composition of two transformations (as functions) and the product of the
matrices as we have deﬁned it.
Consider the following two transformations R W R2 ! R2 and S W R2 !
R2 deﬁned by
R
 x
y

WD

y
 x

(12.13)
and
S
 x
y

WD
 y
x

:
(12.14)
12.15
Exercise
First, try to understand these transformations geometri-
cally. Then, ﬁgure out matrices M .R/ and M .S/ representing each of these
transformations (using the standard bases). Afterward, recall how it is that we
multiply two matrices, and compute each of the matrix products
M .R/  M .S/
and
M .S/  M .R/
(12.16)
They should not be equal, which might be clear from your geometric interpre-
tation!
If you did the above exercise, you now know that R and S have matrices
SECTION 12.1
Linear Transformations as Matrices
327
M .R/ D

0
1
 1
0

and M .S/ D
 0
1
1
0

(12.17)
and that the products are
M .R/M .S/ D
 1
0
0
 1

and M .S/M .R/ D
  1
0
0
1

(12.18)
Ok, now we’re prepared to ﬁnd the composition of the transformations
(functions) R and S. Remember that we work “inside-out.” So, S is applied
ﬁrst and R is then applied after S. Thus, we can compute R ı S as
.R ı S/
 x
y

D R

S
 x
y

D R
 y
x

D

x
 y

(12.19)
Next, let us compute S ı R:
.S ı R/
 x
y

D S

R
 x
y

D S

y
 x

D
  x
y

(12.20)
They’re not the same! We’re getting closer to the analogy. Now, we ﬁnd
the matrix associated to the transformation R ı S. Note where the two basis
vectors end up and then express each result as a linear combination of the basis
vectors on the other side. Then, the coefﬁcients in these linear combinations
help us build the matrix.
.R ı S/
 1
0

D

1
 0

D 1 
 1
0

C 0 
 0
1

(12.21)
and
.R ı S/
 0
1

D

0
 1

D 0 
 1
0

C . 1/ 
 0
1

(12.22)
and
.S ı R/
 1
0

D
  1
0

D . 1/ 
 1
0

C 0 
 0
1

(12.23)
and
.S ı R/
 0
1

D
  0
1

D 0 
 1
0

C 1 
 0
1

(12.24)
Now, we have everything we need to build the matrices M .R ı S/ and
M .S ı R/!
328
CHAPTER 12
Mathematical Tools for Quantum Computing II
12.25
Exercise
Using the above computations, determine the matrices
M .R ı S/ and M .S ı R/.
Having done the above exercise, you now see that the matrices associated
to the transformations R ı S and S ı R are
M .R ı S/ D
 1
0
0
 1

and M .S ı R/ D
  1
0
0
1

(12.26)
and these are the matrices M .R/  M .S/ and M .S/  M .R/ we computed
earlier! This isn’t just a happy coincidence, by the way. This is the motivation
for deﬁning the matrix product the way we did.
12.27
The Matrix of a Composition of Linear Transformations
The matrix of a composition of linear transformations is the product of
their matrices, for all linear transformations S and T ,
M .S ı T / D M .S/  M .T /
12.2
Matrices as Operators
We refer to a linear transformation that maps from a vector space to itself
as a linear operator on that space. In fact, all of the linear transformations
we’ve considered (except for one non-square matrix that we used to explain
the transpose operation on matrices) are linear operators! We pay special
attention to operators in quantum mechanics.
More particularly, we care about invertible linear operators because we
can use them to create quantum gates and thus quantum circuits. Next, we
introduce the determinant, a numerical invariant of a matrix encoding the
invertibility of the transformation of space that it represents!
An Introduction to the Determinant
Each square matrix has an associated determinant, which speaks for the
geometry of the linear transformation of space represented by that matrix. It
subsequently describes the invertibility of the linear transformation, whether
the transformation can be undone.
SECTION 12.2
Matrices as Operators
329
What do we mean by this idea that a transformation can be undone? Let’s
see an example of a transformation that can be undone, and then an example
that can’t.
First, an example of a transformation that can be undone:
Consider the linear transformation R from R2 to R2 described by the
matrix
R WD
 1
0
0
ei

(12.28)
which can be found in chapter 3.
12.29
Exercise
Review Euler’s formula from earlier in this chapter and
check that
ei D cos ./ C isin ./ D  1 C 0  i D  1
The above exercise alerts us to the fact that the matrix R above can
actually be expressed as
R WD
 1
0
0
ei

D
 1
0
0
 1

(12.30)
12.31
Exercise
You’re invited to investigate and ﬁgure out which trans-
formation of two-dimensional space is described by this matrix. Figure out
where the basis vectors are mapped and you should get an idea!
If you tried the above exercise, you probably ﬁgured out that the matrix
R describes a reﬂection of two-dimensional space over the x-axis. Believing
this, it is fairly clear geometrically that this transformation can be undone –
just reﬂect back! Or you could just reﬂect again, since two reﬂections over an
axis does nothing. You might wonder then why  appears in the name of this
matrix. We’ll give you a little idea of why. The  is there to signify that this
matrix has the effect of rotating the larger space C2 about the z-axis1 by 
radians, although we’d prefer not to get into this discussion right now. We’d
like for you to take away the fact that this is a linear transformation of space
that can be undone – that’s good for now!
1Refer to the Bloch sphere in chapter 3.
330
CHAPTER 12
Mathematical Tools for Quantum Computing II
You might have also noticed that the matrix R is a special case of the
matrix
R' WD
 1
0
0
ei'

(12.32)
mentioned in chapter 3, where the angle ' is .
Now, let us consider a transformation that cannot be undone. Consider the
transformation Projx from R2 to R2 described by the matrix
Projx WD
 1
0
0
0

(12.33)
12.34
Exercise
As before, you should try and ﬁgure out which transfor-
mation of space is described by the matrix
Projx WD
 1
0
0
0

above. The name might give it away...
So, if you tried the above exercise, you’re hopefully convinced that this
matrix describes the projection of a vector onto the x-axis. For example,
applying Projx to the vector
 1
1
T yields
 1
0
T . Since many vectors
also get mapped to
 1
0
T by Projx we lose information as to what the
input was if we just look at the output. For example, this vector
 1
2
T
also gets mapped to
 1
0
T (and actually, any vector whose ﬁrst component
is 1 will as well!). Since we cannot recover the input from the output and this
transformation cannot be undone, it is not invertible.
In essence, this transformation collapses the whole space to the x-axis.
This mental image should give the impression that we cannot undo this
transformation. In some sense, a higher dimensional space is collapsed into a
smaller dimensional one, and so information must be lost.
We don’t like such transformations in quantum computing! We’d like for
the matrices that represent the quantum logic gates we use to preserve all of
the information given to them, and we’d like for these transformations to be
reversible. We’ll see that we even ask for more of these matrices in what
follows.
So, at this point, hopefully you’re convinced that not all linear transfor-
mations can be undone. It would be nice to have a numerical invariant that
encodes the invertibility of a linear transformation. Enter the determinant.
SECTION 12.2
Matrices as Operators
331
Let’s see how to compute the determinant of a 22 matrix with an example.
Consider the matrix
A WD
 1
2
3
4

(12.35)
We compute its determinant like so
det.A/ WD
1
2
3
4
D 1  4   2  3 D 4   6 D  2
(12.36)
12.37
Exercise
You’re invited to investigate and describe the transforma-
tion of two-dimensional space described by this transformation. Figure out
where the basis vectors end up and that should give you an idea!
The previous exercise hopefully convinced you that the transformation of
two-dimensional space represented by the matrix A is invertible.
Let’s follow this example and compute the determinants of the matrices
described above
det.R 
2 / WD
1
0
0
-1
D 1  . 1/   0  0 D  1
(12.38)
det.Projx/ WD
1
0
0
0
D 1  0   0  0 D 0
(12.39)
If you’re observant, you may notice that the matrices corresponding to
the invertible transformations have nonzero determinant, while the matrix
corresponding to the non-invertible transformation has zero determinant.
12.40
Determinant of a 2  2 matrix
The determinant of a 2  2 matrix
 a
b
c
d

is
ad   bc
332
CHAPTER 12
Mathematical Tools for Quantum Computing II
You might wonder how to compute the determinant of a 3  3 matrix.
Again, we’ll demonstrate by example
det
0
@
1
2
3
4
5
6
7
8
9
1
A WD
1
2
3
4
5
6
7
8
9
(12.41)
D 1  5
6
8
9   2  4
6
7
9 C 3  4
5
7
8
(12.42)
D 1  .5  9   8  6/   2  .4  9   7  6/ C 3  .4  8   7  5/
(12.43)
D 1  .45   48/   2  .36   42/ C 3  .32   35/
(12.44)
D 1  . 3/   2  . 6/ C 3  . 3/
(12.45)
D  3 C 12   9 D 9   9 D 0
(12.46)
12.47
Equivalent formulations of invertibility
The following are equivalent for a linear transformation T :
 det.T / D 0
 T is not invertible
 The rows of the matrix representing T are linearly dependent
 The columns of the matrix representing T are linearly dependent
Unfortunately, we will not prove this here. This set of equivalences relating
the determinant of a matrix, and thus a linear transformation, to geometric
attributes such as invertibility of the transformation should be surprising given
the seemingly combinatorial description of the determinant. It might not seem
geometric at all from the deﬁnition we’ve given.
The Geometry of the Determinant
There is a more geometric deﬁnition of the determinant and we would like to
give you a small taste of it. Consider the matrix
 1
4
2
2

SECTION 12.2
Matrices as Operators
333
 0
1

 1
0

Figure 12.1: Unit square in the plane
 1
2

 4
2

Figure 12.2: Parallelogram
again and observe that it sends the ﬁrst basis vector
 1
0

to the vector
 1
2

and it sends the second basis vector
 0
1

to the vector
 4
2

. This
should convince us that the unit square formed by the basis vectors in the ﬁrst
quadrant, as in Figure 12.1, is mapped to the parallelogram whose coordinates
are at the points
 0
0

;
 1
2

;
 4
2

and
 5
4

as depicted in Figure 12.2.
12.48
Exercise
You should draw the parallelogram!
334
CHAPTER 12
Mathematical Tools for Quantum Computing II
You’ll realize that in some sense that the orientation of the unit square
“ﬂips” upon being transformed to the parallelogram, since the vectors
 1
0

and
 0
1

, which constitute the unit square in the ﬁrst quadrant, exchange
places to form the parallelogram.
The unit square has area 1, while the parallelogram has area 2. What
does this have to do with the determinant of this matrix, which is -2? Well,
that the orientation of the unit square “ﬂips” is encoded in the fact that
the determinant is negative. Transformations that preserve orientation have
positive determinant, while transformations that ﬂip orientation have negative
determinant. Then, the magnitude of the determinant (absolute value) encodes
the factor by which the unit parallelogram is magniﬁed by the transformation.
In general, the determinant of the matrix associated to a linear transfor-
mation encodes whether that transformation is orientation preserving and
then by which factor it “stretches” the unit parallelepiped (higher-dimensional
analog of a parallelogram) – quite geometric, actually! Figure 12.2 depicts
the parallelogram.
Matrix Inversion
We previously discussed how the determinant of a matrix encodes its invert-
ibility. But how do we actually ﬁnd the inverse transformation?
First off, when we say an inverse transformation, what do we mean? Well,
transformation is just another word for function, so presumably what we seek
is an inverse function of the transformation we’re given. It might not be so
simple to ﬁgure out what an inverse function is though. To see what we
mean, try to ﬁnd the inverse transformation for the transformation T of space
described by
T W R3 ! R3
(12.49)
T
0
@
x
y
z
1
A WD
0
@
1  x C 2  y C 3  z
4  x C 5  y C 6  z
7  x C 8  y C 10  z
1
A
(12.50)
Is it even clear that we can invert this transformation? We saw that not all
transformations are invertible.
12.51
Exercise
Find the matrix M .T / associated to the transformation
T and then compute its determinant. Make a claim about the invertibility of
SECTION 12.2
Matrices as Operators
335
the transformation T based on your computation of the determinant. Can we
invert T ?
12.52
Exercise
Can we give a reasonable geometric description of this
transformation of three-dimensional space from this deﬁnition of T ?
Recall from our earlier discussion about functions and attributes of func-
tions that we say that a function f W X ! Y is invertible iff there exists an
inverse function f  1 W Y ! X such that the compositions f  1ıf W X ! X
and f ı f  1 W Y ! Y are the respective identity functions IX and IY . So,
we seek a transformation (function) T  1 whose composition with the trans-
formation T is the identity on either side.
If we have such a T  1, we’ll know that
T ı T  1 D IR3
(12.53)
and
T  1 ı T D IR3
(12.54)
Now that we know that every linear transformation of space is a matrix
and that the composition of two transformations is analogous to the multipli-
cation of their corresponding matrices, we might hope to express the given
transformation T as a matrix M .T /. We can then use this matrix description
to come up with an inverse matrix to multiply M .T / to get the identity matrix
(which corresponds to the identity function).
In other words, we apply the “matrix operation” to the compositions above
yielding
M .T ı T  1/ D M .IR3/
(12.55)
and
M .T  1 ı T / D M .IR3/
(12.56)
The question then is what these matrices are. However, like we said earlier,
the matrix of a composition of transformations is the product of their matrices,
so we have
M .T /M .T  1/ D M .IR3/
(12.57)
336
CHAPTER 12
Mathematical Tools for Quantum Computing II
and
M .T  1/M .T / D M .IR3/
(12.58)
We need to ﬁnd a matrix whose product with the matrix M .T / on either side
is the identity matrix. This is an enormous observation!
We now focus our attention on what it means to ﬁnd such a matrix. Con-
sider the matrix
A D
 1
2
3
4

(12.59)
Let
B D
 x
y
z
w

(12.60)
be any other matrix, and now suppose that B has the desired inverse property,
that AB D I2 D BA.
Well, then to start, we would have
AB D
 1
2
3
4
  x
y
z
w

D I2
(12.61)
Let’s extract the meaning of
 1
2
3
4
  x
y
z
w

(12.62)
Recall the deﬁnition of matrix multiplication. Before we compute the product,
you should give it a try!
 1
2
3
4
  x
y
z
w

D
 1  x C 2  z
1  y C 2  w
3  x C 4  z
3  y C 4  w

(12.63)
Now, we want this to be equal to the identity matrix, so we need for the
following equations and inequations to be satisﬁed
1x C 2z ¤ 0
1y C 2w D 0
3x C 4z D 0
3y C 4w ¤ 0
1x C 2z D 3y C 4w
SECTION 12.2
Matrices as Operators
337
The second and third equations are self-evident, but the ﬁrst, fourth and ﬁfth
desire some explanation. We want for the ﬁrst, fourth and ﬁfth entries to
be nonzero and equal so that we can divide by them and force the diagonal
entries to be equal to 1 each.
12.64
Exercise
Try to solve the system of equations and inequations
above.
With enough effort, you’ll ﬁnd that the appropriate choice of x; y; z; w is
x D 4; y D  2; z D  3; w D 1
(12.65)
So, the matrix we seek is
B D

4
 2
 3
1

(12.66)
Let’s compute the product
AB D
 1
2
3
4
 
4
 2
 3
1

D
  2
0
0
 2

(12.67)
That’s not the identity matrix! It’s not so bad, actually. Remember that we
demanded that the diagonal entries would be nonzero and equal? Now, you’ll
see why.
Sure, the matrix B doesn’t work, but the matrix
  1
 2

 B does (remember
how we multiply a matrix by a number)
A
 1
 2

B

D
 1
2
3
4
  1
 2



4
 2
 3
1

D
 1
2
3
4
  1
 2
 
4
 2
 3
1

D
 1
 2
  1
2
3
4
 
4
 2
 3
1

D
 1
 2
   2
0
0
 2

D
   1
 2

 . 2/
  1
 2

 0
  1
 2

 0
  1
 2

 . 2/

D
 1
0
0
1

(12.68)
338
CHAPTER 12
Mathematical Tools for Quantum Computing II
So, we have good reason to call the matrix
  1
 2

B by the name A 1!
12.69
Exercise
We should check that multiplying on either side yields
the identity matrix – do this! That is, verify that
    1
2

B

A D I.
Then, we have found the matrix inverting the matrix A, so it’s just a matter
of determining what linear transformation this inverse matrix represents.
Recalling that the entries in a matrix are exactly expressing the coefﬁcients of
the images of the basis elements, we can recover the transformation without
too much trouble:
12.70
Exercise
Try to describe the linear transformation corresponding
to the matrix
  1
 2

 B using function notation. Fill in the details in the
unﬁnished equation
 1
 2

 B
 x
y

WD
Hopefully, you ﬁgured out that what should be on the right side of the
equation is the vector
 1
 2


 4  x C  3  y
 2  x C 1  y

(12.71)
We’ve got it! The inverse transformation of the linear transformation
A
 x
y

WD
 1  x C 3  y
2  x C 4  y

(12.72)
which you might recall has the geometric effect of transforming the unit
square in the ﬁrst quadrant into the parallelogram described in the section
about determinants, is the transformation
 1
 2

 B
 x
y

WD
 1
 2


 4  x C  3  y
 2  x C 1  y

(12.73)
That’s wonderful and all, but is there a pattern here? Can we extrapolate a
general technique from this example? Perhaps you noticed this matrix from
our earlier discussion about determinants. Maybe you even got the impression
that the  2 appearing in the fraction
1
 2 in front of B has something to do
with the previous discussion, since  2 is the determinant of the matrix B!
You’d be correct!
SECTION 12.2
Matrices as Operators
339
In general, given a 2  2 matrix
A D
 a
b
c
d

(12.74)
its inverse matrix is the matrix
A 1 WD
1
det.A/
 d
 b
 c
a

(12.75)
12.76
Deﬁnition
Adjugate matrix
The matrix
 d
 b
 c
a

(12.77)
is known as the adjugate of A.
In general, given a matrix A, we denote the adjugate of A by adj.A/. What
we discovered above can then be phrased as
A 
 det.A/ 1adj.A/

D det.A/ 1.Aadj.A// D det.A/ 1.det.A/I/
D
 det.A/ 1det.A/

I D 1I D I
(12.78)
So, the inverse of a matrix A is the matrix described by det.A/ 1  adj.A/.
This makes us realize that we need to ﬁnd a way to determine the adjugate
matrix. Remarkably, the adjugate matrix is equal to the transpose of what is
known as the cofactor matrix of A.
So, what is the cofactor matrix of a matrix? We’ll deﬁne it and then
invite you to verify that the adjugate we built above does in fact satisfy this
deﬁnition.
Let us ﬁrst deﬁne Ai;j to be the .n   1/  .n   1/ submatrix of A formed
by removing the ith row and j th column of A. The i; j th entry of the cofactor
matrix of A is then Ci;j WD . 1/iCj det.Ai;j /. We call Ci;j the i; j th cofactor
of A. We’ll demonstrate with an example. Consider the matrix
A D
0
@
1
2
3
4
5
6
7
8
9
1
A
(12.79)
Let’s compute the 1; 1 cofactor C1;1. Removing the 1st row and 1st column
of A yields the smaller 2  2 matrix
340
CHAPTER 12
Mathematical Tools for Quantum Computing II
 5
6
8
9

(12.80)
The cofactor C1;1 is then determined as follows
C1;1 WD . 1/1C1  5
6
8
9
D 5  9   8  6 D 45   48 D  3
(12.81)
12.82
Exercise
Compute the cofactors C1;2 and C1;3.
If you tried the above the exercise, you found that
C1;2 WD . 1/1C2  4
6
7
9
D . 1/Œ4  9   7  6 D 42   36 D 6
(12.83)
and
C1;3 WD . 1/1C3  4
5
7
8
D 4  8   5  7 D 32   35 D  3
(12.84)
12.85
Exercise
The ambitious reader should go forth and compute the
remaining 6 cofactors C2;1; C2;2; C2;3; C3;1; C3;2; C3;3 and then construct
the cofactor matrix C. Further, transpose this matrix. You’ll run into a
problem if you try to create the inverse though!
If you try to construct the inverse of this matrix, you’ll run into an issue, as
stated in the above exercise. Everything works swimmingly until you compute
the determinant – it’s 0!
There are several ways to discern that the determinant of A is 0 and seeing
them will tie some ideas together.
First of all, the ﬁrst column
0
@
1
4
7
1
A
(12.86)
of the matrix A is linearly dependent on the other two.
12.87
Exercise
Can you ﬁnd the dependence?
SECTION 12.3
Eigenvectors and Eigenvalues
341
With a bit of computation, you’ll ﬁnd that 2 times the second column
minus 1 times the third column yields the ﬁrst column
0
@
1
4
7
1
A D 2 
0
@
2
5
8
1
A C . 1/ 
0
@
3
6
9
1
A
(12.88)
If we apply the theorem stated earlier about all of the equivalent ways of
expressing that the determinant of a linear transformation is 0, we see that the
linear dependence of the ﬁrst column on the other two is equivalent to the fact
that the determinant of A is 0.
Now, we’ll perform a computation that will lead to a interesting observation
connecting the inverse of a matrix, its determinant, and the cofactor matrix
we described above. It will start off rather unusual, but if you follow each
equality, you’ll see the connection!
det.A/ D 0 D .1/  . 3/ C .2/  .6/ C .3/  . 3/
D .1/  C1;1 C .2/  C1;2 C .3/  C1;3
D A1;1  C1;1 C A1;2  C1;2 C A1;3  C1;3
(12.89)
As you can see, we can compute the determinant by multiplying the cofactors
by their respective entries of the matrix along that row and then summing the
products.
Hopefully, this example convinces you this might be true. You should try
to compute the determinant of A along the second row instead. In fact, you
should try to compute the determinant along any row or column you like and
see that they all yield the same answer!
12.3
Eigenvectors and Eigenvalues
To motivate this next topic, let’s play around with the matrix
A WD
 2
0
0
3

(12.90)
12.91
Exercise
First of all, can you describe the transformation of space
it represents?
342
CHAPTER 12
Mathematical Tools for Quantum Computing II
You’ll realize that the ﬁrst basis vector is stretched by a factor of 2, while
the second basis vector is stretched by a factor of 3. So, these vectors don’t
move under this transformation; they are simply scaled. You should contrast
this example with the previous example
 1
2
3
4

(12.92)
Can you ﬁnd a similar set of vectors that are only stretched by the transforma-
tion corresponding to B for the transformation corresponding to this matrix?
Good luck – we bet you can’t ﬁnd them!
The vectors in the ﬁrst example, the standard basis vectors, are referred
to as eigenvectors of the matrix A. In general, a nonzero vector v is an
eigenvector of a linear transformation T iff there exists  2 F such that
T v D v
(12.93)
In other words, the operation of T on v simply scales the vector v by the scalar
.
Where did the name eigenvector come from? Well, the Germans are
responsible for quite a bit of the terminology and notation of algebra, and
“eigen” means “own,” as in “one’s own” or “characteristic” in German. So, an
eigenvector is a vector characteristic to the matrix A. This implies that we
could very well be able to identify the matrix A given all of its characteristic
vectors, eigenvectors.
This is true for the most part. We know from earlier that a linear transfor-
mation is described entirely by how it acts on a basis for the space. For the
example A above, we’re fortunate to know immediately from the description
of A how it acts on the standard basis, and so we pretty much know everything
about A already.
What if the basis was different though? We saw earlier that there are
several bases for a vector space, so could it be that we understand the behavior
of a matrix on a different basis than the standard basis? Well, sure. Here’s an
example of such a scenario:
Consider the matrix
B D
 2
1
0
3

(12.94)
Let’s apply this transformation to the vectors v1 WD
 1
0

and v2 WD
 1
1

:
SECTION 12.3
Eigenvectors and Eigenvalues
343
Bv1 D
 2
1
0
3
  1
0

D
 2
0

D 2 
 1
0

D 2  v1
(12.95)
and
Bv2 D
 2
1
0
3
  1
1

D
 3
3

D 3 
 1
1

D 3  v2
(12.96)
Interesting: so, Bv1 D 2  v1 and Bv2 D 3  v2.
Now, let’s try this out on the vector v WD
 0
1

. Then, we have
Bv WD B
 0
1

D
 2
1
0
3
  0
1

D
 1
3

(12.97)
and notice that if there were a number  such that
 1
3

D 
 0
1

D
   0
  1

D
 0


(12.98)
then 1 D 0. So, there is no such number . This is equivalent to saying the
vector v is not like the vectors v1 and v2, since the transformation does not
simply stretch v by some factor.
The vectors v1 and v2 are the eigenvectors of the matrix B, while v is not.
We refer to the scale factors 2 and 3 as the eigenvalues of the eigenvectors v1
and v2, respectively. In general, the eigenvalue associated to an eigenvector v
is the scalar  appearing in the deﬁnition of an eigenvector from earlier
T v D v
(12.99)
Collectively, we refer to the eigenvectors and eigenvalues of a transformation
as its eigenstuff.2 There can only be two of these eigenvectors for an operator
on two-dimensional space, and in general, there could be as many as n for an
operator on n-dimensional space.
12.100
Exercise
Find the eigenstuff for the matrices
Projx WD
 1
0
0
0

(12.101)
and
2The eigenstuff is sometimes called the eigensystem of the transformation. We acknowledge
(and embrace) the informal choice of terminology.
344
CHAPTER 12
Mathematical Tools for Quantum Computing II
M WD

2
0
 1
2
3

(12.102)
Finding the eigenstuff for Projx shouldn’t be too hard, while M might pose a
bit more of a challenge.
Change of Basis
Let’s keep running with the matrix B from our discussion of eigenstuff.
Although the standard basis is great, we’d like to demonstrate that changing
our basis to the eigenvectors of B might be advantageous for several reasons.
Well, we should probably check that the eigenvectors of B do indeed form a
basis for the vector space R2.
12.103
Exercise
Verify that the eigenvectors
v1 D
 1
0

; v2 D
 1
1

(12.104)
actually form a basis for R2. Remember that this means we have to check
that v1 and v2 are linearly independent and that v1 and v2 span R2. You
should do this by brute force ﬁrst. Then, think about how you could apply the
theorem about the length of any basis for a vector space of a certain dimension
afterward.
We know the standard basis consisting of two vectors is indeed a basis for
R2, so that theorem ensures the length of any list of basis elements for R2 is
also two. Knowing this and that the eigenvectors are either linearly indepen-
dent or spanning therefore guarantees they form a basis by the theorem.
We know from earlier that the matrix describing a linear transformation
depends on the bases we choose for the space. Let’s change the basis we’re
using from the standard basis to the basis of eigenvectors and see how the
matrix B changes as a result!
To perform this operation, we need to know how the transformation T
corresponding to the matrix B acts on the eigenvectors v1 and v2 in terms of
v1 and v2 only, but we’ve already ﬁgured this out! Recall that Bv1 D 2  v1 D
2  v1 C 0  v2 and that Bv2 D 3v2 D 0  v1 C 3  v2.
So, we can make the matrix for the transformation T with respect to the
basis
SECTION 12.3
Eigenvectors and Eigenvalues
345
BR2 WD fv1; v2g
(12.105)
of eigenvectors v1 and v2 following the procedure described in an earlier
section

M .T W R2 ! R2/BR2;BR2

D
 2
0
0
3

(12.106)
Let’s abbreviate all of that with M .T / like we have before. We can see that
this matrix is diagonal; diagonal matrices are quite useful. To see why, try the
following exercise.
12.107
Exercise
Find the square of the matrix M .T / D
 2
0
0
3

formed with respect to the basis of eigenvectors v1 and v2 (sometimes called
an eigenbasis), compute
M .T /2 WD M .T /  M .T /
Then, compute
M .T /3 WD M .T /  M .T /  M .T /
Then, compute
M .T /4 WD M .T /  M .T /  M .T /  M .T /
What’s going on? Can you compute M .T /100?
12.108
Exercise
Now, try this for the original matrix B WD
 2
1
0
3

formed with respect to the standard basis vectors. That is, compute the square
of B. Then, compute B3. Then, compute B4. Can you compute B100?
We’re willing to bet you ﬁgured out how to do the exercise regarding
M .T / and gave up while trying the exercise regarding B — anybody would.
The difference between M .T / and B is that M .T / is diagonal, while B is
not. The failure of B to be diagonal causes considerable difﬁculties when
multiplying, as you learned in the exercise.
The question then becomes: Can we always change our basis to one where
the matrix we start with ends up a diagonal matrix?
Actually, no, not always.
346
CHAPTER 12
Mathematical Tools for Quantum Computing II
12.109
Exercise
Think about why we can’t ﬁnd a basis of eigenvectors
for the transformation described by the matrix
T WD
 0
1
0
0

Matrices that have this special property are aptly named diagonalizable.
It is not so simple to give an explicit characterization of matrices that are
diagonalizable, so we won’t delve into this right now.
Here we make a connection between quantum mechanics and eigenstuff.
Linear operators with eigenvalues that are real numbers form a special class
of operators. In our review of quantum mechanics in a previous chapter, we
reviewed the measurement postulate of quantum mechanics; this postulate
states that any operator associated with a physically measurable property will
be Hermitian. We have not seen Hermitian operators just yet, but we will ﬁnd
that they are a class of operators whose eigenvalues are always real numbers,
and so will be measurable. We’ll see what Hermitian operators are and why
they have such a remarkable property in a little bit!
12.4
Further Investigation of Inner Products
Further investigation of the previously deﬁned inner product reveals some
salient properties. The ﬁrst of which is that of conjugate symmetry, which
states for all vectors u; v, hu; vi D hv; ui. So, exchanging the vectors does not
yield the same result, but the conjugate instead. This might seem odd at ﬁrst
glance, but an example should help convince us this should be the case.
Let u WD
 i
1

and v WD
 2
i

. Then
hu; vi D
 i
1

;
 2
i

D i  2 C 1  i D . i/  2 C 1  i D  2i C i D  i
(12.110)
and
hv; ui D
 2
i

;
 i
1

D 2  i C i  1 D 2  i C . i/  1 D 2i   i D i
(12.111)
SECTION 12.4
Further Investigation of Inner Products
347
So, unless we conjugate, we don’t get the same number. We should conjugate
if we exchange the vectors in the inner product.
12.112
Exercise
Let u WD
 i
2

and v WD
 1
i

. Find the inner
product hu; vi. Now, ﬁnd the inner product hv; ui. Do you see why we have
to conjugate?
Conjugate symmetry also guarantees that the inner product of any vector
with itself is a real number. To see this, observe that for any vector v, conjugate
symmetry ensures that hv; vi D hv; vi (we exchanged v with itself!), so the
inner product of v with itself needs to be equal to its conjugate. A complex
number a C bi is equal to conjugate a   bi iff a C bi D a   bi, which occurs
iff b D  b. But b D  b only in the case when b D 0, i.e., our complex
number a C bi was a real number all along.
The inner product is also linear in the ﬁrst argument. So, ﬁrst of all, for
any two vectors u; v and w, the inner product satisﬁes the following equation:
hu C v; wi D hu; wi C hv; wi
(12.113)
Let’s see an example of this phenomenon. Let
u D
 1
0

; v D
 0
1

; w D
 1
2

(12.114)
Let’s compute hu C v; wi:
hu C v; wi D
 1
0

C
 0
1

;
 1
2

D
 1
1

;
 1
2

WD 1  1 C 1  2 D 1  1 C 1  2 D 3
(12.115)
12.116
Exercise
Now, you compute
hu; wi C hv; wi D
 1
0

;
 1
2

C
 0
1

;
 1
2

and check that it equals 1 C 2 D 3.
348
CHAPTER 12
Mathematical Tools for Quantum Computing II
We said that the inner product is linear in the ﬁrst argument, so you should
be wondering how scalar multiplication manifests itself here. The second part
of this linearity in the ﬁrst argument is that for any scalar a and any vectors u
and v, we have
ha  u; vi D a  hu; vi
(12.117)
To see this property in action, let
a D 2; u D
 1
2

; v D
 3
4

(12.118)
Let’s compute the left-hand side
ha  u; vi D

2 
 1
2

;
 3
4

D
 2
4

;
 3
4

D 2  3 C 4  4 D 6 C 16 D 22
(12.119)
12.120
Exercise
Compute the right-hand side, i.e., a  hu; vi and check
that it’s equal to 22.
You might wonder if hu; a  vi D a  hu; vi for all scalars a and vectors u
and v. Check that this is not true in the following exercise.
12.121
Exercise
Check that it is not true that hu; a  vi D a  hu; vi for all
scalars a and vectors u and v by ﬁnding a scalar a and two vectors u and v such
that hu; a  vi ¤ a  hu; vi. In fact, you can prove using the axioms listed later
on that for all scalars a and vectors u and v, it is true that hu; a  vi D ahu; vi.
This property of the inner product is sometimes called conjugate homogeneity
in the second argument.
The ﬁnal property of the inner product is known as positive-deﬁniteness:
for all vectors v, hv; vi  0 and hv; vi D 0 iff v D 0.
Let’s see why this is true from a more abstract perspective. Let v D
 v1
v2
: : :
vn
T be a vector.
Then
hv; vi WD v1  v1 C v2  v2 C ::: C vn  vn
(12.122)
Recall that the product of a complex number with its conjugate is always
a non-negative real number (i.e., greater than or equal to 0), so vi  vi is a
SECTION 12.4
Further Investigation of Inner Products
349
real number for each i 2 f1; :::; ng. Well, then the sum of a (ﬁnite) bunch of
non-negative real numbers is a real number, so the sum
v1  v1 C v2  v2 C ::: C vn  vn
(12.123)
is still a non-negative real number, i.e., is greater than or equal to 0.
12.124
Exercise
Convince yourself that the expression above is 0 iff the
vector v is the zero vector, i.e., has all zero entries.
So, we’ve shown that for any vector v, hv; vi  0 and that hv; vi D 0 iff
v D 0.
12.125
Exercise
Notice that we never said that the inner product satisﬁes
linearity in the second argument! It is true, however, and your task is to prove
it using the axioms that we’ve already listed and veriﬁed. Linearity in the ﬁrst
argument will be crucial for your proof.
The Kronecker Delta Function as an Inner Product
While discussing the idea of an orthonormal basis for a vector space earlier, we
mentioned the idea that the mathematical description of an orthonormal basis
could be described in terms of the Kronecker delta function ı. The Kronecker
delta function ı has such a simple description that you can’t imagine why it
could possibly be named after anyone – although Kronecker isn’t complaining.
12.126
Deﬁnition
The Kronecker delta function
For a set f1; 2; :::; ng
(
ı.i; j/ D 1
if i D j
ı.i; j/ D 0
if i ¤ j
This should look familiar! Recall our deﬁnition of orthonormal basis
vectors from earlier. So, we can refer to an orthonormal set S of vectors
as a set of vectors such that the restriction of the inner product to S is the
Kronecker delta function!
12.127
Exercise
Check that the entries of the identity matrix can be
350
CHAPTER 12
Mathematical Tools for Quantum Computing II
described by the Kronecker delta function. More precisely, check that we can
deﬁne the identity matrix I entry-wise as Iij WD ı.i; j/.
12.5
Hermitian Operators
A linear operator (interchangeably, matrix) is referred to as Hermitian iff it
is equal to its conjugate transpose (deﬁned above). Hermitian operators are
crucial in quantum mechanics. We often want to measure a quantity associated
with some state. Often the quantity we’re after is the eigenvalue of an operator,
and so we’d like for that eigenvalue to be a real number in order to effectively
measure it.
Why We Can’t Measure with Complex Numbers
A subtle point worth mentioning is that we can’t perform a measurement with
complex numbers. Suppose you wanted to ﬁgure out which of the numbers 0
or i is bigger? Then, you would either have to decide that i D 0, 0 < i or that
i < 0. Of course, we won’t choose i D 0 because, if we do, then we could
multiply each side of the equation i D 0 by  i and ﬁnd that 1 D 0! Let’s try
the choice 0 < i.
If we choose that 0 < i, then multiplication by i on each side yields the
inequality
i  0 < i  i
(12.128)
But then
0 < i  i D i2 D  1
(12.129)
In summary, we have deduced that 0 <  1 from our assumption that 0 < i.
Since 0 <  1 does not make sense, we are left to conclude that our original
assumption that 0 < i does not make sense.
So, what if we choose that i < 0? Subtract i from both sides, revealing
i   i < 0   i
(12.130)
So,
0 <  i
(12.131)
Multiplying both sides by  i reveals
SECTION 12.5
Hermitian Operators
351
0  . i/ <  i  . i/
(12.132)
But
  i  . i/ D . 1/  i  . 1/  i
(12.133)
. 1/. 1/  i  i D 1  i2 D  1
(12.134)
So, the inequality 0  . i/ <  i  . i/ really means 0 <  1. This does not
make sense either, and so we must conclude that the original hypothesis that
i < 0 does not make sense. So, what we’re discovering is that neither choice
of order, i.e., neither 0 < i nor i < 0 makes sense. Assuming either such
order reveals that 0 <  1, which is simply not true. We are left to conclude
that there is no acceptable way of ordering the complex numbers.
To make this more precise, suppose you want to come up with an order for
the complex numbers that satisﬁes the following reasonable conditions that
we are familiar with from our experience with usual real numbers:
 Trichotomy: For all complex numbers x; y, we have a trichotomy:
either x < y, x > y or x D y.
 Additivity property: For all complex numbers x; y; z, if x < y, then
x C z < y C z.
 Multiplicative property: For all complex numbers x; y; z, if 0 < z, then
x < y implies xz < yz.
Now, let’s very carefully prove that the choice 0 < i contradicts at least
one of the above axioms. Since 0 < i, using the third (multiplicative) property
stated above, we see that
0 < i H) 0  i < i  i D i2 D  1
(12.135)
Although the fact that we have deduced that 0 <  1 from our assumption
that 0 < i is disturbing, it technically does not violate any of the axioms
above. Using the second (additivity) property above, the inequality 0 <  1
allows us to deduce
0 <  1 H) 0 C 1 <  1 C 1 D 0
(12.136)
So, 1 < 0. This is also disturbing, but is not a direct violation of any of
the axioms above. Now, we invoke the third (multiplicative) property a ﬁnal
time yielding
1 < 0 H) 1  i < 0  i D 0
(12.137)
352
CHAPTER 12
Mathematical Tools for Quantum Computing II
Then, we have i < 0, but our original assumption was that 0 < i. This
violates the ﬁrst (trichotomy) property above!
12.138
Exercise
Try to ﬁgure out what goes wrong with deﬁning i < 0
in a similar fashion.
So, we really want these measurements to be real numbers, or else we have
no way of making sense of them. The question then becomes: Why is it that
Hermitian matrices have real eigenvalues?
Hermitian Operators Have Real Eigenvalues
Despite the ease with which we may verify that a matrix is equal to its
conjugate transpose, a different but equivalent deﬁnition of Hermitian makes
the proof that Hermitian matrices have real eigenvalues quite simple. We
could also deﬁne a linear operator T W V ! V to be Hermitian iff it is
self-adjoint, where we say:
12.139
Deﬁnition
Adjoint of a matrix
The adjoint of a matrix T is a matrix T  such that for all vectors u; v 2 V
hT u; vi D hu; T vi.
Then, that T is self-adjoint means that T D T  and so for all vectors
u; v 2 V , hT u; vi D hu; T vi. Notice that the adjoint of an operator is precisely
its conjugate transpose! So, to say that an operator is self-adjoint is to say that
it equals its conjugate transpose.
12.140
Adjoint is conjugate transpose
The adjoint of a linear operator is its conjugate transpose.
Now, if v is an eigenvector with eigenvalue  of a Hermitian matrix T ,
then we may further conclude that
hT v; vi D hv; T vi
(12.141)
That v is an eigenvector of T with eigenvalue  means that T v D v, so we
may substitute v for T v in the equation hT v; vi D hv; T vi, yielding
hv; vi D hv; vi
(12.142)
SECTION 12.6
Unitary operators
353
Our intuition now is to factor out , which causes something interesting
to happen. By homogeneity of the inner product in the ﬁrst argument and
conjugate homogeneity in the second argument, “factoring out ” yields the
new equation
hv; vi D hv; vi
(12.143)
Now, we’d like to “divide by hv; vi” on both sides and call it a day, but how do
we know that it isn’t zero? Well, we started by assuming v was an eigenvector
of T , and the deﬁnition of an eigenvector requests that v not be zero. Then,
we apply the deﬁniteness property of the inner product to ensure that, since
v is not zero, the inner product of v with itself, i.e., hv; vi, is not zero either.
Thus,
 D 
(12.144)
Remember what it means for a complex number to equal its complex conju-
gate? Exactly — then it’s a real number after all!
12.145
Exercise
It’s worth noting that any real symmetric matrix (sym-
metric matrix with real entries only) is Hermitian. Can you see why?
Hopefully, this foray into the proof of this theorem makes you realize that
all of the axioms we request in these deﬁnitions are truly necessary! Do you
see where we used each and every one of the axioms?
12.6
Unitary operators
You’ve surely encountered the zoo of matrices describing quantum logic gates
within chapter 3 and listed in chapter 15. Those matrices are special because
they are unitary. Unitary matrices preserve the length of vectors. They are
so named because unit vectors remain unit vectors after an application of a
unitary matrix.
More precisely,
12.146
Deﬁnition
Unitary Operator
A linear operator U W V ! V is unitary iff for all vectors x; y 2 V ,
hx; yi D hUx; Uyi
i.e., U preserves inner products.
354
CHAPTER 12
Mathematical Tools for Quantum Computing II
It is interesting to note that we could equivalently deﬁne a unitary operator
(matrix) U W V ! V to be a matrix whose conjugate transpose is its inverse,
i.e.,
U  D U  1
(12.147)
So,
U U D U U  D I
(12.148)
Nothing we’ve said so far directly implies that a unitary operator preserves
the length of any vector it operates on, so let’s verify this in an exercise.
12.149
Exercise
Verify that a unitary operator U preserves the length
of any vector v that it operates on by using the deﬁnition given above. Use
the fact that the length of the vector v is equal to
p
hv; vi, and compare the
original length of v, i.e.,
p
hv; vi, to the length of v after U has operated, i.e.,
p
hU v; U vi.
12.7
The Direct Sum and the Tensor Product
We might want to build bigger vector spaces from collections of vector spaces
we already have. There are two popular ways of doing this. One way is
the direct sum, the other the tensor product. We’ve already seen the tensor
product in the context of taking the tensor product of two vectors, or even two
matrices. There is a relationship between that previously deﬁned operation
and the operation we’re about to describe.
The Direct Sum
The ﬁrst point we’d like to emphasize is that the direct sum operates on vector
spaces, not numbers, vectors or matrices (although the astute reader already
considers all of these to be simply linear transformations!). The same is true
for the tensor product. Speciﬁcally, the direct sum is a binary operation that
accepts two vector spaces as input and yields another, usually “bigger,” vector
space as output. Let’s see an example.
Consider the one-dimensional vector space R and a copy of itself – so, two
copies of R.
The direct sum of R with itself is written
R ˚ R
(12.150)
SECTION 12.7
The Direct Sum and the Tensor Product
 113
and is deﬁned to be
R ˚ R WD
 x
y

W x 2 R; y 2 R

(12.151)
Hold on – that’s just the Cartesian product R  R of R and R!
12.152
Exercise
Recall the deﬁnition of the Cartesian product R  R
and compare it to the direct sum R ˚ R.
Why do we have two names for the same thing? Well, they’re not actually
the same thing because the direct sum R˚R has more structure. In particular,
the direct sum R ˚ R is a vector space (over R), where the Cartesian product
R  R is simply a set.
To further elaborate on this idea, consider the sets A WD fa; b; cg and
B WD fd; eg. We may create their Cartesian product
A  B WD f.x; y/ W x 2 A; y 2 Bg
D f.a; d/; .a; e/; .b; d/; .b; e/; .c; d/; .c; e/g
(12.153)
Now, we ask the question: What is the direct sum A ˚ B? Well, at ﬁrst,
we have
A ˚ B WD
 x
y

W x 2 A; y 2 B

(12.154)
simply following the deﬁnition from above. A cursory glance might convince
us that the issue is that the Cartesian product has row vectors and the direct
sum has column vectors, but when discussing sets, it doesn’t matter if we’re
using row or column vectors. The question is illuminated by a further question:
How is A ˚ B a vector space? And over which ﬁeld?
You see, in order to deﬁne A ˚ B as a vector space, you have to give it
additional algebraic structure so that it is an abelian group and has a ﬁeld
action by some speciﬁed ﬁeld. Currently, it has neither of these. You’re
invited to ponder this for a moment. How would you deﬁne the addition of
two elements of A ˚ B?
Whatever way you decide has to allow for the addition of the speciﬁc
elements
 a
d
T and
 b
e
T . Sure, we could say
 a
d

C
 b
e

D
 a C b
d C e

(12.155)
356
CHAPTER 12
Mathematical Tools for Quantum Computing II
but what do a C b and d C e even mean? Remember, a; b; d and e are just
letters. They have no numerical meaning whatsoever.
So, the big idea is that it’s perfectly reasonable to form Cartesian products
of any two sets you like. For example, the Cartesian product of the set
COLORS WD fchartreuse; magenta; periwinkleg
(12.156)
and the set
ANIMALS WD fcat; dog; aardvarkg
(12.157)
is the new set
COLORS  ANIMALS WD f.x; y/ W x 2 COLORS; y 2 ANIMALSg
(12.158)
which ends up being
8
<
:
(chartreuse, cat), (chartreuse, dog), (chartreuse, elephant),
(magenta, cat), (magenta, dog), (magenta, elephant),
(periwinkle, cat), (periwinkle, dog), (periwinkle, elephant)
9
=
;
(12.159)
which is just a set, and not a vector space. Whereas the direct sum R ˚ R is,
a priori, the set
R ˚ R WD
 x
y

W x 2 R; y 2 R

(12.160)
which has a natural interpretation as being a vector space over the ﬁeld R,
since we can deﬁne the addition (to get the abelian group structure) to be
usual vector addition and the ﬁeld action to be usual scalar multiplication.
We would also like to mention that since the direct sum of two vector
spaces is itself a vector space, it has a dimension. It turns out that the dimen-
sion of the direct sum of two vector spaces is the sum of their dimensions,
hence the name direct “sum.” More speciﬁcally, for any two vector spaces V
and W both over the same ﬁeld F,
dim.V ˚ W / D dim.V / C dim.W /
(12.161)
12.162
Deﬁnition
The direct sum of two vector spaces
The direct sum of vector spaces V and W is the vector space
V ˚ W WD
 v
w

SECTION 12.7
The Direct Sum and the Tensor Product
357
12.163
Dimension of direct sum
dim.V ˚ W / D dim.V / C dim.W /
(12.164)
The Tensor Product
Now, we’ll discuss the tensor product. What is remarkable about the tensor
product of two vector spaces is that it is entirely described by the “tensor
product” of the basis elements for each. What we mean by this is that if we
have two vectors spaces V and W both over the same ﬁeld F and with bases
BV D fv1; :::; vmg and BW D fw1; :::; wng, respectively, then the tensor
product of V and W , denoted V ˝ W is a vector space with basis given by
BV ˝W WD
fv1 ˝ w1; v1 ˝ w2; :::; v1 ˝ wn; : : : ; vm ˝ w1; vm ˝ w2; : : : ; ˝vm ˝ wng
(12.165)
You might recognize that all of the possible pairs of tensor products of the
basis vectors of V with the basis vectors of W appear in the “tensor product”
of the bases of V and of W . In fact, this is always the case! That is, if we
have two vector spaces V and W both over the same ﬁeld F and we’d like
to ﬁnd their tensor product V ˝ W , we can at least determine the basis of
V ˝ W by tensoring all of the possible pairs of basis vectors of V and W .
Although this is a convenient fact, it does require a little bit of work to prove;
it is often referred to as The Tensor Product Basis Theorem, emphasizing that
it is a result to be proved.
What we would have to show is that, in fact, the proposed basis for V ˝W
consisting of all possible tensor products of pairs of basis vectors from V and
W is actually a basis, i.e., all of those tensor products are linearly independent
and span V ˝ W . This is believable, given that the basis vectors from each
of V and W are linearly independent amongst themselves and they span
their respective spaces. However, we would like you to think about why this
requires a proof!
Anyway, we’ll avoid deﬁning the tensor product of two vectors from an
axiomatic perspective here, and instead give a basis-centric view of the idea.
If you’ve never seen the tensor product of two vector spaces before, it’s fair to
think like this for the moment:
Given two vector spaces V and W with bases BV and BW , respectively,
we deﬁne their tensor product V ˝ W to be the vector space with basis equal
358
CHAPTER 12
Mathematical Tools for Quantum Computing II
to the “tensor product” of their bases, i.e., the basis BV ˝W consisting of
the tensor products of all possible pairs of basis elements of V and W , as
described above.
Mathematicians reading this might be frustrated, but we’re simply trying
to give a working deﬁnition. Readers interested in why there might be any
fuss about this deﬁnition of a tensor product are invited to investigate more
mathematical treatments of the tensor product from the perspective of bilinear
maps, and even further, category theory!3
Now, a word about notation. You might come across some fancy ways of
writing the direct sum or tensor product of several copies of the same vector
space. For example,
H
L n
(12.166)
is just a fancy way of writing
H ˚ ::: ˚ H
„
ƒ‚
…
n times
(12.167)
Another way to state this is: the direct sum of n copies of the vector space H .
This can also be written as
M
n
H
(12.168)
A similar notation is in place for tensor products
H
N n WD H ˝ ::: ˝ H
„
ƒ‚
…
n times
(12.169)
and
O
n
H
(12.170)
Both of these notations refer to the n-fold tensor product of H , i.e., the
tensor product of n copies of the vector space H . These notations for the
tensor product of several H s arise when notating a quantum register, i.e., a
collection of qubits, as we’ll see at the culmination of this chapter with the
deﬁnition of a Hilbert space.
3Category theory deﬁnes the tensor product of two vector spaces via its universal property.
This deﬁnition offers an elegant and sophisticated view of the tensor product which eases
proofs and offers insight into the behavior of the tensor product as it interacts with other vector
spaces. This construction also instructs the deﬁnition of the tensor product of more general
objects than vector spaces, like modules!
SECTION 12.7
The Direct Sum and the Tensor Product
359
We can also take the tensor product of two operators. Thinking of each
operator as a linear transformation, and thus a matrix, allows us to take their
tensor product as we did earlier in this chapter! For example, given the
operators on two-dimensional complex space C2,
I WD
 1
0
0
1

(12.171)
and
X WD
 0
1
1
0

(12.172)
we may form their tensor product
I ˝ X WD
 1
0
0
1

˝ X D
 1  X
0  X
0  X
1  X

D
0
BB@
1 
 0
1
1
0

0 
 0
1
1
0

0 
 0
1
1
0

1 
 0
1
1
0

1
CCA
D
0
BB@
 1  0
1  1
1  1
1  0

 0  0
0  1
0  1
0  0

 0  0
0  1
0  1
0  0

 1  0
1  1
1  1
1  0

1
CCA
D
0
BB@
 0
1
1
0

 0
0
0
0

 0
0
0
0

 0
1
1
0

1
CCA D
0
BB@
0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0
1
CCA
(12.173)
Summarizing the result of our computation, we’ve created an operator
that now operates on four-dimensional complex space C4 with the following
effect
.I ˝ X/.j00i/ D .I ˝ X/.j0i ˝ j0i/
D
0
BB@
0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0
1
CCA
0
BB@
1
0
0
0
1
CCA D
0
BB@
0
1
0
0
1
CCA
D j0i ˝ j1i D j01i
(12.174)
12.175
Exercise
Find the tensor product X ˝I. Is it the same as I ˝X?
360
CHAPTER 12
Mathematical Tools for Quantum Computing II
Note that we constructed a 4  4 matrix as the tensor product of two 2  2
matrices, which makes sense given our earlier discussion of how the tensor
product of an .a  b/ matrix and a .c  d/ matrix is a .a  c/  .b  d/ matrix.
Curious readers might try to build the quantum operators discussed in
chapter 3 by taking tensor products of two-dimensional operators. Beware:
it’s not easy! In fact, you cannot build CNOT by taking the tensor product of
two two-dimensional operators.
12.8
Hilbert Space
The modern deﬁnition of a quantum computer laid out in the text postulates
that a qubit is modeled by a two-dimensional complex Hilbert space, so we
include the formal deﬁnition here. However, before we can do so, we need to
have a brief discussion of the notions of a metric on a vector space, Cauchy
sequences, and ﬁnally, the idea of completeness of a vector space.
Metrics, Cauchy Sequences and Completeness
By a metric on a vector space, we mean that we can measure (hence, “metric”)
the distance between two vectors u and v by computing the norm of their
difference, i.e.,
hu   v; u   vi
(12.176)
12.177
Exercise
Check that the distance between two vectors u and v as
deﬁned above is zero iff u D v, equivalently, u   v D 0.
At ﬁrst glance, a Cauchy4 sequence can be thought of as “a sequence of
numbers whose terms get as close as we like, given that we look far enough.”
Before we investigate Cauchy sequences, we should understand the notion
of convergence of a sequence of numbers. Here is an example of a Cauchy
sequence of real numbers f W N ! R:
4This sequence is named after Augustin-Louis Cauchy.
SECTION 12.8
Hilbert Space
361
f .1/
1
2
f .2/
1
4
f .3/
1
8
f .4/
1
16
:::
:::
f .n/
1
2n
We can see that these numbers are getting closer and closer to the number
0, and in fact, we can make them as close to 0 as we like by choosing terms
sufﬁciently far along in the sequence.
12.178
Exercise
Verify for yourself that we can make the numbers in the
sequence above as close to the number 0 as we like by ﬁnding the number n
such that f .n/ D
1
2n is close to 0 within an error of 0:000001. In other words,
ﬁgure out how far we need to go in the sequence before the terms differ from
0 by only 0:000001 or less. It might help to think of 0:000001 as
1
106 .
We say that this sequence of numbers converges to the number 0. We can
express that the above sequence f W N ! R of real numbers deﬁned by
f .n/ D
1
2n converges to 0 more mathematically by saying,
for all  > 0, there exists N 2 N such that for all n 2 N, if n > N, then
jf .n/   0j < .
You actually found the “N” in the previous mathematical description of
the convergence of a sequence to 0 in the above exercise!
Of course, there is nothing special about this particular sequence, nor the
number 0. We can play this game with any sequence and number. In general,
we say that a sequence f W N ! R converges to a number L (L stands for
“limit”) iff
for all  > 0 there exists N 2 N such that for all n 2 N, if n > N, then
jf .n/   Lj < .
Again, intuitively, we should think of this as saying “A sequence of num-
bers converges to a number L iff we can make those numbers get as close to
L as we like simply by taking terms further along in the sequence.”
Now, a Cauchy sequence is a special type of sequence whose terms, as we
said earlier, “get as close as we like, given that we look far enough.” Let’s
revisit the previous sequence
362
CHAPTER 12
Mathematical Tools for Quantum Computing II
f .1/
1
2
f .2/
1
4
f .3/
1
8
f .4/
1
16
:::
:::
f .n/
1
2n
Observe the following phenomenon: How far apart are the terms f .1/ and
f .2/? Well,
jf .1/   f .2/j D
ˇˇˇˇ
1
2   1
4
ˇˇˇˇ D
ˇˇˇˇ
1
4
ˇˇˇˇ D 1
4
so they’re 1
4 apart. How far apart are the terms f .2/ and f .3/? You should
check that they’re 1
8 apart. f .3/ and f .4/? Now, they’re
1
16 apart! Hmm...
So, it seems that as we go further along in the sequence, pairs of terms get
closer and closer together. Let’s make this mathematically precise.
We say that a sequence f W N ! R is Cauchy iff
for all  > 0, there exists N 2 N such that for all m; n 2 N, if m; n > N ,
then jf .m/   f .n/j < .
12.179
Exercise
Convince yourself that this precise mathematical formu-
lation of a Cauchy sequence agrees with our intuitive statement about terms
getting closer further along in the sequence. Then, check that the sequence
f .n/ D
1
2n is in fact a Cauchy sequence.
So, we’re convinced that the above sequence is Cauchy. Now, we can
ask the question as to whether a sequence of numbers converges to a number
already in our set. You might be wondering how a sequence of numbers could
ever possibly converge to something not already in our set... Well, let’s look
at the sequence f .n/ D
1
2n once more.
Suppose we’re only considering numbers greater than 0 for the moment.
In other words, we’re working with the numbers
.0; 1/ WD fx 2 R W x > 0g
So, the numbers in the set we’re considering cannot be equal to 0. Well,
then the previous sequence has the curious property that it is Cauchy and
it converges, but not to any number in the space we’re working in, because
SECTION 12.8
Hilbert Space
363
it converges to 0, which is not a number in our set! This is a failure of the
set .0; 1/ to be what we refer to as complete. Formally, we say that a set
is complete iff any Cauchy sequence of elements in the set converges to
an element in the set. We often relax this statement to “Cauchy sequences
converge” when the context is understood.
Now, we can generalize all of these ideas by allowing ourselves to measure
the distance between two objects using a more general notion of an abso-
lute value. The absolute value is actually a special case of a more general
phenomenon known as a metric:
12.180
Deﬁnition
Deﬁnition of a metric
A metric is a binary function d W S  S ! R from the Cartesian product
of two copies of a set S to the real numbers satisfying the following
properties:
 For all x; y 2 S, d.x; y/  0
 Deﬁniteness: For all x; y 2 S, d.x; y/ D 0 iff x D y
 Triangle inequality:
For all x; y; z
2
S,
d.x; z/

d.x; y/ C d.y; z/
The absolute value j  j W R ! R is in fact a metric, as we invite you to
check:
12.181
Exercise
Check that the absolute value j  j W R ! R is actually a
metric by verifying that it satisﬁes the above three properties. Also, convince
yourself that the third property really should be called the triangle inequality!
But we said that a metric is a more general notion! What’s a more general
metric? Well, consider the following way of measuring the “distance” between
two vectors: Given two vectors u and v in R2, we could measure the distance
between them by taking the (L2) norm of their difference (as vectors, that is),
i.e., d.u; v/ WD jju   vjj2: But a moment’s thought makes us realize that this
is simply taking the square root of the inner product of the vector difference
with itself, i.e.,
jju   vjj2 D
p
hu   v; u   vi
There we have it! A very general notion of a metric. We can now deﬁne a
metric on any vector space with an inner product in the following way. For
any vector space V with inner product h; i, deﬁne the metric
364
CHAPTER 12
Mathematical Tools for Quantum Computing II
d W V  V ! R
(12.182)
via
d.u; v/ WD
p
hu   v; u   vi
(12.183)
We invite you to check that this deﬁnition does in fact satisfy the criteria for a
metric from earlier.
12.184
Exercise
Check that the metric we deﬁned above
d W V  V ! R
via
d.u; v/ WD
p
hu   v; u   vi
is actually a metric by checking that it satisﬁes the criteria laid out earlier.
We say then that the inner product induces a metric on the vector space V ,
or that the metric is the metric induced by the inner product.
An Axiomatic Deﬁnition of the Inner Product
Let us touch brieﬂy upon the idea of an axiomatic deﬁnition of an inner
product. The deﬁnition given previously in this text serves us well, but it
turns out that there are more exotic ways of taking the inner product of two
vectors. In fact, we could possibly be working in a vector space where the
vectors are... matrices or something some other mathematical object. There
are vector spaces, for example, whose “vectors” consist of all continuous
functions f W R ! R, where the inner product of two “vectors” (really,
functions!) f and g is deﬁned to be
hf; gi WD
Z 1
0
f .x/g.x/dx
(12.185)
Yes, inner products, and vectors spaces for that matter, can be quite other-
worldly. So, we want an axiomatic framework for them that captures the
essence.
SECTION 12.8
Hilbert Space
365
12.186
Deﬁnition
Axiomatic deﬁnition of an inner product
An inner product h; i on a vector space V over a ﬁeld F (where F is either
R or C) is a binary function
h; i W V  V ! F
satisfying the following properties:
 Conjugate symmetry: For all u; v 2 V ,
hu; vi D hv; ui
 Linearity in the ﬁrst argument: For all a 2 F; u; v; w 2 V ,
ha  u; vi D a  hu; vi
and
hu C v; wi D hu; wi C hv; wi
 Positive-deﬁniteness: For all v 2 V ,
hv; vi  0
and
hv; vi D 0 iff v D 0
12.187
Exercise
With the axioms laid out above, you’re invited to verify
that the inner product which we have been using does in fact satisfy these
axioms.
Finally, we’re able to state the precise deﬁnition of a Hilbert space!
The Deﬁnition of Hilbert Space
12.188
Deﬁnition
Deﬁnition of Hilbert space
A Hilbert space is a vector space H over either the ﬁeld of real or complex
numbers equipped with an inner product h; i that is a complete metric
space with respect to the metric induced by the inner product.
366
CHAPTER 12
Mathematical Tools for Quantum Computing II
The deﬁnition of a Hilbert space is the culmination of this chapter, and
we’d like to mention brieﬂy how Hilbert spaces arise in quantum computing.5
We care about Hilbert spaces over the ﬁeld C of complex numbers, and the
following discussion refers only to such spaces.
12.9
The Qubit as a Hilbert Space
One of the central concepts of quantum computing is that a qubit can be
represented as a two-dimensional complex Hilbert space.6 We denote a
Hilbert space with the letter H, or sometimes more fancily with H .
12.189
A qubit is a vector space
A qubit is represented by a vector space — more speciﬁcally, a Hilbert
space!
We sometimes call the Hilbert space representing the qubit the state space.
A state in the state space is a vector in the state space with L2 norm 1. So,
a state is on the Bloch sphere – refer to chapter 3. For example, the familiar
vectors j0i and j1i are states in the 2-dimensional Hilbert space (state space)
H , which represents one qubit. In fact, j0i and j1i are an orthonormal basis
for H , as you veriﬁed earlier in our discussion of orthonormal bases, and so
every vector in the state space is a linear combination of these two vectors
with L2 norm 1. In the terminology of quantum mechanics, every state is a
superposition of these two states!
We refer to a collection of n qubits as a quantum register and often notate
it as
H ˝ H ˝ : : : ˝ H
„
ƒ‚
…
n times
(12.190)
It turns out that the tensor product of Hilbert spaces is another Hilbert space,
although it will have a greater dimension in general!
12.191
Exercise
Think about why the dimension of the tensor product of
two two-dimensional Hilbert spaces is 4. Then, think about why the dimension
of the tensor product of three two-dimensional Hilbert spaces is 8 – not 6!
Then, ﬁgure out why the dimension of the tensor product of n two-dimensional
5The name Hilbert space honors the mathematician David Hilbert.
6We take our motivation from page 15 of Yuri Manin’s seminal paper [182].
SECTION 12.9
The Qubit as a Hilbert Space
367
Hilbert spaces is in fact 2n. It might help to think of what the basis for the
tensor product space is.
For example, if we have two qubits H and H , each with orthonormal
basis
BH D fj0i ; j1ig
(12.192)
then the quantum register
H ˝ H
(12.193)
is a Hilbert space of dimension 4 (as you checked in the exercise above) with
basis
BH ˝H D fj0i ˝ j0i ; j0i ˝ j1i ; j1i ˝ j0i ; j1i ˝ j1ig
D fj00i ; j01i ; j10i ; j11ig;
(12.194)
where we recall from earlier that j00i is just a convenient renaming of the
tensor product j0i ˝ j0i, i.e.,
j00i WD j0i ˝ j0i D
 1
0

˝
 1
0

D
0
BB@
1
0
0
0
1
CCA
(12.195)
and the vectors j01i ; j10i ; j11i are deﬁned similarly.
12.196
Exercise
Figure out the basis BH ˝3 for H ˝3 where H has
basis BH D fj0i ; j1ig: It should consist of 23 D 8 vectors by the previous
exercise. Can you ﬁgure out the basis for H ˝n?
12.197
A quantum register is a tensor product
A quantum register consisting of n qubits is a 2n-dimensional tensor
product of vector spaces!
368
CHAPTER 12 Mathematical Tools for Quantum Computing II
12.198
Summary of the relationship between quantum computing
and linear algebra
 A qubit can be represented by a two-dimensional complex Hilbert
space. The state of the qubit is represented by a vector in the Hilbert
space.
 More speciﬁcally, the vectors representing the states of a qubit have
an L2 norm of 1.
 The quantum register consisting of n qubits is a 2n-dimensional
complex Hilbert space composed of the n-fold tensor product of the
two-dimensional Hilbert spaces representing the qubits.
 A computational basis vector of the space represents a deﬁnite
computational state of the qubit.
 A superposition of states is a linear combination of the computa-
tional basis vectors.
 Quantum logic gates are unitary operators, which act on the state
space. Since unitary operators preserve the norm of the vectors they
act on, states are transformed to new states and not to just any vector
in the space. Thus, we can build quantum circuits using unitary
operators.
 Measurement is performed by projection operators
And so our review of linear algebra culminates with the table in Figure 12.3.
Readers wanting a deeper understanding of linear algebra are encouraged to
read:
 Sheldon Axler’s Linear Algebra Done Right [20], where emphasis is
placed on the theory and proof technique common to linear algebra, a
sophisticated vantage point of matrices, the spectral theorem (which
determines when the eigenvectors of a linear operator form a basis), and
the determinant are presented.
 Michael Artin’s Algebra [18] caters to a more mathematical audience. It
begins with a wonderful discussion of elementary matrices and their role
in Gaussian elimination and offers great perspective for group theory.
 Gilbert Strang’s textbook Linear Algebra and its Applications [264] is
helpful for applications of linear algebra to the sciences and is supple-
mented by free MIT OpenCourseWare material.
 See Appendix A and B of Rieffel and Polak’s textbook, Quantum Com-
puting, A Gentle Introduction, for connections between quantum me-
SECTION 12.9
The Qubit as a Hilbert Space
369
Quantum Computing
Linear Algebra
Example
qubit
two-dimensional complex Hilbert space
H D spanCfj0i ; j1ig
n-qubit quantum register
n-fold tensor product of two-dimensional complex Hilbert spaces
H ˝n
state space
vectors with L2 norm 1 (Bloch sphere)
fv 2 H W hvjvi D 1g
deﬁnite state
orthonormal basis vector
j0i ; j1i
superposition of states
linear combination of orthonormal basis vectors with L2 norm 1 (on the Bloch sphere)
1
p
2 j0i C
1
p
2 j1i
quantum logic gates
unitary operators
X D
 0
1
1
0

measurement operators
projection operators
 1
0
0
0

Figure 12.3: Relationship of quantum computing to linear algebra
chanics and probability theory as well as a treatment of the Abelian
hidden subgroup problem [236].
 For readers interested in learning more about abstract algebra, here are
additional texts:
- John Fraleigh’s A First Course in Abstract Algebra [117]
- Dummit and Foote’s Abstract Algebra [97]
- Joseph Rotman’s Advanced Modern Algebra [243]
 A whimsical introduction to category theory can be found in the text
Conceptual Mathematics: A First Introduction to Categories [166] by F.
William Lawvere and Stephen H. Schanuel.
 Readers interested in acquiring a reﬁned deﬁnition of the tensor product
should look up Tai-Danae Bradley’s blog Math3ma [54].
 More adventurous readers are invited to read Bradley’s book What is
Applied Category Theory? [55] to see that category theory is not simply
a rephrasing of mathematics and is in fact quite useful for things like
chemistry and natural language processing!
 Emily Riehl’s Category Theory in Context [237] is a helpful introduction
to category theory for advanced undergraduates and graduate students.
CHAPTER
13
Mathematical Tools for Quantum
Computing III
13.1
Boolean Functions
13.1
Deﬁnition
Boolean function
A Boolean function f is a function from a Cartesian product
f0; 1gn ! f0; 1gm
where f0; 1gn denotes the n-fold Cartesian product of the set f0; 1g with
itself, i.e.,
f0; 1gn WD f0; 1g  :::  f0; 1g
„
ƒ‚
…
n times
Cartesian products and functions are discussed in chapter 11, if you want
to brush up on these terms. Boolean functions arise naturally in computing.
For example, the Deutsch-Jozsa Algorithm discussed in chapter 7 involves
the four Boolean functions
f0; f1; fx; fx
(13.2)
each with domain and codomain f0; 1g. So, in the case of these four functions,
n D 1 and m D 1 when we compare them to the deﬁnition of a Boolean
function given above.
Other examples of Boolean functions include NOT , AND, OR and XOR.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_13
371
372
CHAPTER 13
Mathematical Tools for Quantum Computing III
Interest Rate
Number of Times Compounded Annually
Amount in One Year
100
1 percent
1 time per year
2 dollars
100
2
D 50 percent
2 times per year
2:25 dollars
100
3 percent
3 times per year
 2:37 dollars
100
4 percent
4 times per year
 2:44 dollars
100
5 percent
5 times per year
 2:49 dollars
100
100 percent
100 times per year
 2:70481 dollars
100
1 percent
1 times per year
e dollars
Figure 13.1: The Banker’s experiment
13.2
Logarithms and Exponentials
We recall some of the basic facts about logarithms here. We ﬁrst consider the
natural logarithm with a base of the number e. The number e is approximately
2:71 and appears in a menagerie of mathematical and scientiﬁc contexts. One
interesting way to acquire e is via the following banker’s thought experiment:
Invest 1 dollar in a bank account at an interest rate of 100
1 percent annually
(once per year) and in one year, you’ll earn 1 dollar, and thus have a total of
2 dollars after one year. Now, allow yourself to accrue interest twice a year,
but at the compromised rate of 100
2
D 50 percent. After the ﬁrst six months,
you’ll earn 50 cents, since 50 cents is 50 percent of the invested 1 dollar. After
another six months, you’ll earn 50 percent of the 1:50 dollars you have, and
end up with 2:25 dollars. This is more than if you compounded only once per
year. If you compound three times per year, you have the compromised rate
of 100
3 percent. You earn a little bit more at this rate, as you can check. If you
allow yourself to compound “inﬁnitely many times” at the compromised rate
of “ 100
1 ” percent, you’ll end up with e dollars!
This is summarized in the table in Figure 13.1.
ln WD loge W .0; 1/ ! . 1; 1/
(13.3)
where “ln” is the natural logarithm.
The following equivalence guides all of the facts to come:
SECTION 13.2
Logarithms and Exponentials
373
 5
5
10
 5
5
10
y D ex
y D ln.x/
Figure 13.2: Graphs of the logarithm and exponential
13.4
Equivalence of logarithm and exponential
ln.y/ D x () ex D y
Some ﬁrst properties of the natural logarithm include
ln.ex/ D x
and
eln.x/ D x
(13.5)
So, the functions ln.x/ and ex are inverse to one another.
Further, for any two positive real numbers a and b,
ln.a  b/ D ln.a/ C ln.b/
(13.6)
effectively because
eaeb D eaCb
(13.7)
So, logarithms turn products into sums!
Using the above property, we can deduce the following for all positive real
numbers a and b and any real number c:
 ln

b
a

D ln.b/   ln.a/
because eb
ea D eb a
 ln.ac/ D c  ln.a/
because .ea/c D eac
We also have a logarithm log2 with a base of 2. Similar to the natural loga-
rithm,
374
CHAPTER 13
Mathematical Tools for Quantum Computing III
log2.y/ D x () 2x D y
(13.8)
In fact, the natural logarithm could be written ln.x/ D loge.x/, i.e., with
an explicitly mentioned base of e. There are logarithms for every positive real
number base, so it’s nice to be able to base change. To change the base of a
logarithm from one base b to another base c, we can use the formula
logb.a/ D logc.a/
logc.b/
(13.9)
One trick to remember this is that the a is above the b on the left-hand
side, and that after changing the base to c, the a remains above the b on the
right-hand side.
13.3
Euler’s Formula
We will give an inkling of a proof via power series (avoiding issues of conver-
gence for brevity) of the mysterious and wonderful formula of Euler, relating
the polar coordinates of a complex number of unit norm to the number e, as
mentioned in the previous chapter and depicted in Figure 13.3.
13.10
Euler’s formula
ei D cos./ C isin./
The power series of the (complex-valued) cosine and sine functions are
cos.z/ D 1   z2
2Š C z4
4Š   z6
6Š C :::
(13.11)
and
sin.z/ D z   z3
3Š C z5
5Š   z7
7Š C :::
(13.12)
The power series of the (complex-valued) exponential function ez WD exp .z/
is
ez D 1 C z C z2
2Š C z3
3Š C z4
4Š C :::
(13.13)
Then, evaluating the power series for the exponential function at the complex
number iz yields
SECTION 13.3
Euler’s Formula
375
0
=2

3=2
1

ei D cos./ C isin./
Figure 13.3: Euler’s formula
eiz D 1 C .iz/ C .iz/2
2Š
C .iz/3
3Š
C .iz/4
4Š
C : : :
D 1 C iz C i2z2
2Š
C i3z3
3Š
C i4z4
4Š
C : : :
(13.14)
and remembering that i2 D  1; i3 D  i and i4 D 1 further yields
1 C iz C i2z2
2Š Ci3z3
3Š
C i4z4
4Š
C : : :
D 1 C iz C  1  z2
2Š
C  iz3
3Š
C z4
4Š C : : :
D 1 C iz   z2
2Š   iz3
3Š C z4
4Š C : : :
(13.15)
and after a little reorganization, we have
D

1   z2
2Š C z4
4Š   :::

C

iz   iz3
3Š C iz5
5Š   :::

(13.16)
Now, multiplying the power series for sin.z/ by i yields
376
CHAPTER 13
Mathematical Tools for Quantum Computing III
isin.z/ D i

z   z3
3Š C z5
5Š   z7
7Š C : : :

D iz   i z3
3Š C i z5
5Š   i z7
7Š C : : :
D iz   iz3
3Š C iz5
5Š   iz7
7Š C : : :
(13.17)
Adding the power series of cos.z/ and the newly acquired expression for
isin.z/ yields
cos.z/ C isin.z/ D

1   z2
2Š C z4
4Š   :::

C

iz   iz3
3Š C iz5
5Š   :::

(13.18)
which is exactly the expression we found for eiz earlier! Therefore,
eiz D cos.z/ C isin.z/
(13.19)
Using Euler’s formula, we have Euler’s identity:
13.20
Euler’s Identity
ei C 1 D 0
For further resources and additional mathematical tools, please refer to the
book’s GitHub site.
CHAPTER
14
Dirac Notation
Paul Dirac developed a notation for vectors as well as vector and matrix
operations that we use extensively in quantum mechanics and quantum com-
puting [92]. Now that we have built up our mathematical toolkit we can
describe the correspondence between traditional linear algebra notation and
Dirac notation.
14.1
Vectors
A vector in linear algebra is typically represented as a column vector or as the
row vector which is the conjugate transpose of the column vector. In Dirac
notation we denote a column vector as a ket and the conjugate transpose of
the column vector as a bra – thus the bra-ket framework developed by Dirac.
So the column vectors we represent as j0i and j1i are as follows:
j0i WD
 1
0

j1i WD
 0
1

The symbol used inside the ket or bra is called the label and it is important
to note that the choice of a particular label is arbitrary. We could have named
the above two vectors jbluei and jgoldi as easily as j0i and j1i. It is also
important to note the difference between j0i and the zero vector of traditional
linear algebra notation which is denoted as 0. That zero vector represents the
origin point in a vector space and has no relationship with the Dirac notation
of j0i. The reason for labeling the ﬁrst basis vector with the numeral 0 is
likely due to the way that computer scientists like to count: starting from 0
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_14
377
378
CHAPTER 14
Dirac Notation
(then 1; 2, etc.) Using Dirac notation, we can signify that a particular vector is
in a Hilbert space like so
jui  H
In chapters 11 and 12, we covered the core operations in linear algebra
including inner and outer products as well as the tensor product. Here will
outline how to denote these operations using Dirac notation.
14.2
Vector operations
˛ j0i
where ˛ is a scalar. We can represent the linear combination or superposition
of states with their normalization coefﬁcients as in this example
j i D
1
p
2
j0i C 1
p
2
j1i
(14.1)
Inner and Outer Products
If we wish to take the inner product of two vectors in Dirac notation, u and v,
we can represent this operation as
huj jvi
(14.2)
Or, we can contract this notation as follows
hujvi
(14.3)
Note that since in the inner product we take the conjugate transpose of the
ﬁrst vector, we denote this vector with a bra. The second vector remains a
column vector so it is a ket.
We recall that the result of the inner product is a scalar. So if we take the
inner product of j0i and j1i we obtain 0 since the two vectors are orthogonal
to each other, as shown here
h0j1i D 0
(14.4)
We can represent vector operations using Dirac notation. We denote a vector
multiplied by a scalar as follows
SECTION 14.3
Tensor Products
379
We can also represent the outer product (or matrix multiplication) of two
vectors (where we treat them as matrices) as
juihvj
(14.5)
Note that in the outer product we keep the ﬁrst vector in column form and
take conjugate transpose of the second vector. Thus we represent the ﬁrst
vector as a ket and the second as a bra. The outer product yields a matrix.
So, for example, if we take the outer product of j0i and j1i we obtain the
following matrix
j0ih1j D
 0
1
0
0

(14.6)
We can also string several operations together. For example, the following
notation represents an inner product operation followed by taking the inner
product of u and v and multiplying the result as a scalar with w.
hujvi jwi
(14.7)
hu; vi
(14.8)
14.3
Tensor Products
To represent the state of two or more uncorrelated qubits in a register, we can
use the tensor product. If we have two qubits represented by j0i and j0i since
they were both initialized to that state, we can represent the register as follows
j0i ˝ j0i
(14.9)
Note that we can also represent the tensor of two qubits like so
j0ij0i
(14.10)
and this way as well
j00i
(14.11)
Recall that in traditional linear algebra notation, we can represent the inner
product of two vectors u and v like so:
Dirac was probably inspired by this traditional notation to develop his
bra-ket system.
380
CHAPTER 14
Dirac Notation
So all these are equivalent representations
j0i ˝ j0i D j0ij0i D j00i
(14.12)
The reader might beneﬁt from recognizing that if j0i and j1i are 2 states
from one qubit and j0i and j1i are 2 states from another (uncorrelated) qubit,
then the quantum register includes the 4 states
j0i ˝ j0i ; j0i ˝ j1i ; j1i ˝ j0i ; j1i ˝ j1i
(14.13)
which are written more concisely as
j00i ; j01i ; j10i ; j11i
(14.14)
and
00; 01; 10 and 11
(14.15)
are not coincidentally the numbers 0,1,2 and 3 written in binary notation.
14.16
Exercise
Check that the 8 states included in the tensor product of 3
qubits, when written in the concise Dirac notation described above, correspond
exactly to the numbers 0,1,2,3,4,5,6, and 7 when written in binary notation.
At that stage, the reader will hopefully see the utility of this concise notation.
We can further signify n number of qubits as part of a register in the
following way
j0i˝n
(14.17)
which represents n qubits, each initialized to state j0i and tensored together.
14.4
Notation for PDF and Expectation
Value
Another use of Dirac notation is to represent the probability density function
and expectation value. Recall that the integrated probability density function
(PDF) of a particle can be described as
h‰j‰i D
Z C1
 1
‰.x; t/ ‰.x; t/ dx
(14.18)
SECTION 14.4
Notation for PDF and Expectation Value
381
which, for a normalized wave function ‰, will equal 1. We normalize the
wave function so that the integrated PDF = 1 to meet the Born Rule that the
integrated square of the modulus of the wave function gives us 1 since the
particle exists somewhere.
Now if we wish to calculate the expectation value of the position operator,
Ox, we can do so as follows
hOxi D h‰j Ox j‰i D
Z C1
 1
‰.x; t/ Ox ‰.x; t/ dx
(14.19)
This gives us the average position of the particle at time t. We call this mean
the expectation value for the Ox operator.
Dirac notation is used extensively throughout quantum mechanics and
quantum computing and it is worth spending some time to get familiar with
the use cases in this chapter. Check the companion online site for exercises
with Dirac notation.
CHAPTER
15
Table of Quantum Operators and
Core Circuits
0
1
1
0

X
0
 i
i
0

Y
Y
1
0
0
 1

Z
Z
1
p
2
1
1
1
 1

H
H
 
cos 
2
 i sin 
2
 i sin 
2
cos 
2
!
Rx./
Rx./
 
cos 
2
  sin 
2
sin 
2
cos 
2
!
Ry./
Ry./
1
0
0
ei'

R'
R'
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2_15
383
384
CHAPTER 15
Table of Quantum Operators and Core Circuits
1
0
0
i

S
S
1
0
0
ei=4

T
T
N/A
Meas.
0
BB@
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
1
CCA
CNOT

0
BB@
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
 1
1
CCA
CZ

Z
0
BB@
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
1
CCA
SWAP


0
BBBBBBB@
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
1
CCCCCCCA
Toffoli


CHAPTER 15
Table of Quantum Operators and Core Circuits
385
0
BBBBBBB@
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
CCCCCCCA
Fredkin



1
p
2
0
B@
1
0
1
0
0
1
0
1
0
1
0
 1
1
0
 1
0
1
CA
Bell
H

HXH D Z
HZH D X
H YH D  Y
H  D H D H  1
X2 D Y 2 D Z2 D I
H D .X C Z/=
p
2
H 2 D I
SWAP12 D C12C21C12
C12X1C12 D X1X2
C12Y1C12 D Y1X2
C12Z1C12 D Z1
C12X2C12 D X2
C12Y2C12 D Z1Y2
C12Z2C12 D Z1Z2
Rz;1./C12 D C12Rz;1./
Rx;2./C12 D C12Rx;2./
Gate Identities
(C D CNOT)
Works Cited
[1] S. Aaronson. Certiﬁed randomness from quantum
supremacy. Multiple unpublished talks.
[2] S. Aaronson. The Limits of Quantum Computers.
Scientiﬁc American, March 2008.
[3] S. Aaronson. Quantum Complexity Theory. Fall
2010. Massachusetts Institute of Technology: MIT
OpenCouseWare. https://ocw.mit.edu. License: Cre-
ative Commons BY-NC-SA, 2010.
[4] S. Aaronson. Read the ﬁne print. Nature Physics,
11(4):291, 2015.
[5] S. Aaronson and A. Arkhipov. The computational
complexity of linear optics. In Proceedings of the
forty-third annual ACM symposium on Theory of
computing, pages 333–342. ACM, 2011.
[6] S.
Aaronson,
G.
Kuperberg,
C.
Granade,
and
V.
Russo.
The
Complexity
Zoo.
[7] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Fi-
galli, and S. Woerner.
The power of quantum
neural networks. Nature Computational Science,
1(6):403–409, Jun 2021. arXiv: 2011.00027.
[8] C. Adami and N. J. Cerf. Quantum computation with
linear optics. In Quantum Computing and Quantum
Communications, pages 391–401. Springer, 1999.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2
387
https://complexityzoo.net/, 2005.
388
Works Cited
[9] M. Agrawal, N. Kayal, and N. Saxena. PRIMES is
in P. Annals of Mathematics, 160:781–793, 2004.
[10] D. Aharonov, A. Ambainis, J. Kempe, and U. Vazi-
rani. Quantum walks on graphs. In Proceedings of
the thirty-third annual ACM symposium on Theory
of computing, pages 50–59. ACM, 2001.
[11] D. Aharonov and U. Vazirani. Is quantum mechan-
ics falsiﬁable? A computational perspective on the
foundations of quantum mechanics. Computabil-
ity: Turing, Gödel, Church, and Beyond. MIT Press,
2013.
[12] Y. Aharonov, L. Davidovich, and N. Zagury. Quan-
tum random walks. Physical Review A, 48(2):1687–
1690, Aug 1993.
[13] Y. Alexeev, D. Bacon, K. R. Brown, R. Calderbank,
L. D. Carr, F. T. Chong, B. DeMarco, D. Englund,
E. Farhi, B. Fefferman, and et al. Quantum com-
puter systems for scientiﬁc discovery. PRX Quantum,
2(1):017001, Feb 2021.
[14] S. Allen, J. Kim, D. L. Moehring, and C. R. Monroe.
Reconﬁgurable and programmable ion trap quantum
computer. In 2017 IEEE International Conference
on Rebooting Computing (ICRC), pages 1–3, Nov
2017.
[15] A. Almheiri, X. Dong, and D. Harlow. Bulk locality
and quantum error correction in ads/cft. Journal of
High Energy Physics, 2015(4):163, 2015.
[16] C. Arnold, J. Demory, V. Loo, A. Lemaître, I. Sagnes,
M. Glazov, O. Krebs, P. Voisin, P. Senellart, and
L. Lanco. Macroscopic rotation of photon polariza-
tion induced by a single spin. Nature Communica-
tions, 6:6236, 2015.
[17] J. M. Arrazola, V. Bergholm, K. Brádler, T. R. Brom-
ley, M. J. Collins, I. Dhand, A. Fumagalli, T. Gerrits,
A. Goussev, L. G. Helt, and et al. Quantum circuits
Works Cited
389
with many photons on a programmable nanophotonic
chip. Nature, 591(78487848):54–60, Mar 2021.
[18] M. Artin. Algebra. Pearson, 2010.
[19] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C.
Bardin, R. Barends, R. Biswas, S. Boixo, F. G. S. L.
Brandao, D. A. Buell, and et al. Quantum supremacy
using a programmable superconducting processor.
Nature, 574(7779):505–510, Oct 2019.
[20] S. Axler. Linear Algebra Done Right. Springer,
2014.
[21] R. Babbush, D. W. Berry, and H. Neven. Quan-
tum simulation of the Sachdev-Ye-Kitaev model
by asymmetric qubitization.
Physical Review A,
99(4):040301, 2019.
[22] D. M. Bacon. Decoherence, Control, and Symmetry
in Quantum Computers. PhD thesis, University of
California at Berkeley, 2001.
[23] S. Bae. JavaScript Data Structures and Algorithms:
An Introduction to Understanding and Implementing
Core Data Structure and Algorithm Fundamentals.
Apress, 2019.
[24] P. Baireuther, T. E. O’Brien, B. Tarasinski, and C. W.
Beenakker. Machine-learning-assisted correction of
correlated qubit errors in a topological code. Quan-
tum, 2:48, 2018.
[25] P. Ball.
Ion-based commercial quantum com-
puter is a ﬁrst.
Physics World,
December
17, 2018 (https://bit.ly/2RrpZDf), 2018.
Orig-
inal URL: https://physicsworld.com/a/ion-based-
commercial-quantum-computer-is-a-ﬁrst/.
[26] A. Barenco, C. H. Bennett, R. Cleve, D. P. DiVin-
cenzo, N. Margolus, P. Shor, T. Sleator, J. A. Smolin,
and H. Weinfurter. Elementary gates for quantum
computation. Physical review A, 52(5):3457, 1995.
arXiv: quant-ph/9503016.
390
Works Cited
[27] A. Barna and D. I. Porat. Integrated Circuits in
Digital Electronics. Wiley-Interscience, 1987.
[28] D. Barredo, V. Lienhard, S. de Léséleuc, T. La-
haye, and A. Browaeys. Synthetic three-dimensional
atomic structures assembled atom by atom. Nature,
561(7721):79–82, Sep 2018. arXiv: 1712.02727.
[29] S. Bartolucci, P. Birchall, H. Bombin, H. Ca-
ble, C. Dawson, M. Gimeno-Segovia, E. Johnston,
K. Kieling, N. Nickerson, M. Pant, and et al. Fusion-
based quantum computation.
arXiv:2101.09310
[quant-ph], Jan 2021. arXiv: 2101.09310.
[30] P. Benioff. The computer as a physical system: A mi-
croscopic quantum mechanical hamiltonian model of
computers as represented by turing machines. Jour-
nal of Statistical Physics, 22(5):563–591, May 1980.
[31] C. H. Bennett, E. Bernstein, G. Brassard, and
U. Vazirani.
Strengths and weaknesses of quan-
tum computing.
SIAM journal on Computing,
26(5):1510–1523, 1997.
[32] C. H. Bennett and G. Brassard. Quantum cryptog-
raphy: Public key distribution and coin tossing. In
Proc. IEEE Int. Conf. Computers, Systems, and Sig-
nal Processing, volume 175, 1984.
[33] C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa,
A. Peres, and W. K. Wootters. Teleporting an un-
known quantum state via dual classical and Einstein-
Podolsky-Rosen channels. Phys. Rev. Lett., 70:1895–
1899, Mar 1993.
[34] C. H. Bennett and S. J. Wiesner.
Communica-
tion via one- and two-particle operators on Einstein-
Podolsky-Rosen states. Phys. Rev. Lett., 69:2881–
2884, Nov 1992.
[35] H. Bernien, B. Hensen, W. Pfaff, G. Koolstra,
M. Blok, L. Robledo, T. Taminiau, M. Markham,
D. Twitchen, L. Childress, et al. Heralded entangle-
Works Cited
391
ment between solid-state qubits separated by three
metres. Nature, 497(7447):86, 2013.
[36] E. Bernstein and U. Vazirani. Proceedings of the
25th annual ACM symposium on the theory of com-
puting. ACM, New York, 11, 1993.
[37] D. W. Berry, A. M. Childs, R. Cleve, R. Kothari, and
R. D. Somma. Simulating hamiltonian dynamics
with a truncated taylor series. Physical review letters,
114(9):090502, 2015.
[38] M. K. Bhaskar, D. D. Sukachev, A. Sipahigil, R. E.
Evans, M. J. Burek, C. T. Nguyen, L. J. Rogers,
P. Siyushev, M. H. Metsch, H. Park, et al. Quantum
nonlinear optics with a germanium-vacancy color
center in a nanoscale diamond waveguide. Physical
Review Letters, 118(22):223603, 2017.
[39] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd. Quantum machine learning.
Nature, 549(7671):195, 2017.
[40] A. Blanco-Redondo, I. Andonegui, M. J. Collins,
G. Harari, Y. Lumer, M. C. Rechtsman, B. J. Eggle-
ton, and M. Segev. Topological optical waveguid-
ing in silicon and the transition between topological
and trivial defect states. Physical Review Letters,
116(16):163901, 2016.
[41] A. Blanco-Redondo, B. Bell, M. Segev, and
B. Eggleton. Photonic quantum walks with sym-
metry protected topological phases. In AIP Confer-
ence Proceedings, volume 1874, page 020001. AIP
Publishing, 2017.
[42] M. Blok, V. Ramasesh, J. Colless, K. O’Brien,
T. Schuster, N. Yao, and I. Siddiqi. Implementa-
tion and applications of two qutrit gates in supercon-
ducting transmon qubits. Presented at APS March
Meeting, 2018.
[43] M. S. Blok, V. V. Ramasesh, T. Schuster, K. O’Brien,
J. M. Kreikebaum,
D. Dahlen,
A. Morvan,
392
Works Cited
B. Yoshida, N. Y. Yao, and I. Siddiqi. Quantum
information scrambling in a superconducting qutrit
processor. Physical Review X, 11(2):021010, Apr
2021. arXiv: 2003.03307.
[44] J. G. Bohnet, B. C. Sawyer, J. W. Britton, M. L.
Wall, A. M. Rey, M. Foss-Feig, and J. J. Bollinger.
Quantum spin dynamics and entanglement gen-
eration with hundreds of trapped ions.
Science,
352(6291):1297–1301, 2016.
[45] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Bab-
bush, N. Ding, Z. Jiang, M. J. Bremner, J. M.
Martinis, and H. Neven. Characterizing quantum
supremacy in near-term devices. Nature Physics,
14(6):595–600, Jun 2018. arXiv: 1608.00263.
[46] A. D. Bookatz. QMA-complete problems. Quan-
tum Information & Computation, 14(5&6):361–383,
2014.
[47] M. Born. Zur Quantenmechanik der Stoßvorgänge.
Z. Phys., 37(12):863–867, 1926.
[48] M. Born.
Das adiabatenprinzip in der quanten-
mechanik. Zeitschrift für Physik A Hadrons and
Nuclei, 40(3):167–192, 1927.
[49] M. Born and V. Fock.
Beweis des adiabaten-
satzes (1928, English Translation). In L. Faddeev,
L. Khalﬁn, and I. Komarov, editors, V.A. Fock – Se-
lected Works: Quantum Mechanics and Quantum
Field Theory. Chapman & Hall/CRC, 2004.
[50] D. Boschi, S. Branca, F. De Martini, L. Hardy, and
S. Popescu. Experimental realization of teleporting
an unknown pure quantum state via dual classical
and Einstein-Podolsky-Rosen channels. Phys. Rev.
Lett., 80:1121–1125, Feb 1998.
[51] V. Bouchiat, D. Vion, P. Joyez, D. Esteve, and M. De-
voret. Quantum coherence with a single cooper pair.
Physica Scripta, 1998(T76):165, 1998.
Works Cited
393
[52] A. Bouland, B. Fefferman, C. Nirkhe, and U. Vazi-
rani.
Quantum supremacy and the complex-
ity of random circuit sampling.
arXiv preprint
arXiv:1803.04402, 2018.
[53] P. O. Boykin, T. Mor, M. Pulver, V. Roychowdhury,
and F. Vatan. On universal and fault-tolerant quan-
tum computing. arXiv preprint quant-ph/9906054,
1999.
[54] T.-D.
Bradley.
Math3ma.
https://www.math3ma.com, 2015.
[55] T.-D. Bradley. What is Applied Category Theory?
arXiv preprint arXiv:1809.05923, 2018.
[56] G. Brassard and P. Hoyer.
An exact quantum
polynomial-time algorithm for Simon’s problem. In
Proceedings of the Fifth Israeli Symposium on The-
ory of Computing and Systems, pages 12–23. IEEE,
1997.
[57] G. Brassard, P. Hoyer, M. Mosca, and A. Tapp. Quan-
tum amplitude ampliﬁcation and estimation. Con-
temporary Mathematics, 305:53–74, 2002.
[58] S. Bravyi, D. Gosset, and R. Koenig.
Quan-
tum advantage with shallow circuits.
Science,
362(6412):308–311, 2018.
[59] S. Bravyi, D. Gosset, R. Koenig, and M. Tomamichel.
Quantum advantage with noisy shallow circuits in
3d. arXiv preprint arXiv:1904.01502, 2019.
[60] S. B. Bravyi and A. Y. Kitaev.
Quantum codes
on a lattice with boundary. arXiv preprint quant-
ph/9811052, 1998.
[61] K. R. Brown, A. W. Harrow, and I. L. Chuang. Arbi-
trarily accurate composite pulse sequences. Physical
Review A, 70(5):052318, 2004.
[62] D. Browne.
Universal fault tolerant quan-
tum
computing
with
3D
surface
codes.
394
Works Cited
https://www.youtube.com/watch?v=jHbcdhSH89c,
2020.
[63] C. D. Bruzewicz, J. Chiaverini, R. McConnell,
and J. M. Sage.
Trapped-ion quantum comput-
ing:
Progress and challenges.
arXiv preprint
arXiv:1904.04178, 2019.
[64] Y. Cao, G. G. Guerreschi, and A. Aspuru-Guzik.
Quantum neuron: an elementary building block for
machine learning on quantum computers.
arXiv
preprint arXiv:1711.11240, 2017.
[65] J. Carolan, C. Harrold, C. Sparrow, E. Martín-López,
N. J. Russell, J. W. Silverstone, P. J. Shadbolt,
N. Matsuda, M. Oguma, M. Itoh, et al. Universal
linear optics. Science, 349(6249):711–716, 2015.
[66] S. Castelletto, B. Johnson, V. Ivády, N. Stavrias,
T. Umeda, A. Gali, and T. Ohshima. A silicon car-
bide room-temperature single-photon source. Nature
Materials, 13(2):151, 2014.
[67] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Ben-
jamin, S. Endo, K. Fujii, J. R. McClean, K. Mitarai,
X. Yuan, L. Cincio, et al. Variational quantum algo-
rithms. arXiv preprint arXiv:2012.09265, 2020.
[68] K. C. Chen, W. Dai, C. Errando-Herranz, S. Lloyd,
and D. Englund. Scalable and high-ﬁdelity quantum
random access memory in spin-photon networks.
arXiv:2103.07623 [quant-ph], Mar 2021. arXiv:
2103.07623.
[69] W.-J. Chen, M. Xiao, and C. T. Chan. Photonic
crystals possessing multiple Weyl points and the
experimental observation of robust surface states.
Nature Communications, 7:13038, 2016.
[70] X.-Y. Chen and Z.-q. Yin. Universal quantum gates
between nitrogen-vacancy centers in a levitated nan-
odiamond. Physical Review A, 99(2):022319, Feb
2019. arXiv: 1810.00285.
Works Cited
395
[71] N.-H. Chia, A. Gilyén, T. Li, H.-H. Lin, E. Tang,
and C. Wang.
Quantum-inspired sublinear algo-
rithm for solving low-rank semideﬁnite program-
ming. arXiv:1910.06151 [cs], Aug 2020. arXiv:
1910.06151.
[72] N.-H. Chia, T. Li, H.-H. Lin, and C. Wang. Quantum-
inspired sublinear algorithm for solving low-rank
semideﬁnite programming.
arXiv:1901.03254
[quant-ph], Aug 2020. arXiv: 1901.03254.
[73] L. Childress and R. Hanson. Diamond nv centers for
quantum computing and quantum networks. MRS
Bulletin, 38(2):134–138, 2013.
[74] A. M. Childs. Universal computation by quantum
walk. Physical review letters, 102(18):180501, 2009.
[75] A. M. Childs, R. Cleve, E. Deotto, E. Farhi, S. Gut-
mann, and D. A. Spielman. Exponential algorithmic
speedup by a quantum walk. In Proceedings of the
thirty-ﬁfth annual ACM symposium on Theory of
computing, pages 59–68. ACM, 2003.
[76] A. M. Childs, E. Farhi, and S. Gutmann. An ex-
ample of the difference between quantum and clas-
sical random walks. arXiv preprint arXiv: quant-
ph/0103020v1, Mar 2001.
[77] K. S. Chou, J. Z. Blumoff, C. S. Wang, P. C. Rein-
hold, C. J. Axline, Y. Y. Gao, L. Frunzio, M. Devoret,
L. Jiang, and R. Schoelkopf. Deterministic telepor-
tation of a quantum gate between two logical qubits.
Nature, 561(7723):368, 2018.
[78] I.
Chuang.
Recorded
talk:
Grand
uniﬁcation
of
quantum
algorithms.
https://www.youtube.com/watch?v=GFRojXdrVXI.
[79] J. I. Cirac and P. Zoller. Quantum computations with
cold trapped ions. Phys. Rev. Lett., 74:4091–4094,
May 1995.
396
Works Cited
[80] Cirq
Developers.
Cirq
documentation.
https://quantumai.google/cirq.
[81] Cirq
Developers.
Cirq
tutorials
website.
https://quantumai.google/cirq/tutorials.
[82] B. D. Clader, B. C. Jacobs, and C. R. Sprouse.
Preconditioned quantum linear system algorithm.
Physical Review Letters, 110(25):250504, Jun 2013.
arXiv: 1301.2340.
[83] R. Cleve, A. Ekert, C. Macchiavello, and M. Mosca.
Quantum algorithms revisited. Proceedings of the
Royal Society of London. Series A: Mathematical,
Physical and Engineering Sciences, 454(1969):339–
354, 1998.
[84] P. J. Coles, S. Eidenbenz, S. Pakin, A. Adedoyin,
J. Ambrosiano, P. Anisimov, W. Casper, G. Chen-
nupati, C. Coffrin, H. Djidjev, et al. Quantum algo-
rithm implementations for beginners. arXiv preprint
arXiv:1804.03719, 2020.
[85] C. M. Dawson and M. A. Nielsen. The Solovay-
Kitaev algorithm. arXiv preprint quant-ph/0505030,
2005.
[86] E. Dennis, A. Kitaev, A. Landahl, and J. Preskill.
Topological quantum memory. Journal of Mathe-
matical Physics, 43(9):4452–4505, 2002.
[87] D. Dervovic, M. Herbster, P. Mountney, S. Sev-
erini, N. Usher, and L. Wossnig.
Quantum lin-
ear systems algorithms: a primer. arXiv preprint
arXiv:1802.08227, 2018.
[88] D. Deutsch. Quantum theory, the Church–Turing
principle and the universal quantum computer.
Proceedings of the Royal Society of London A:
Mathematical, Physical and Engineering Sciences,
400(1818):97–117, 1985.
Works Cited
397
[89] D. Deutsch. Lectures on Quantum Computation.
http://www.quiprocone.org/Protected/
DD_lectures.htm, 2003.
[90] D. Deutsch and R. Jozsa. Rapid solution of problems
by quantum computation. Proceedings of the Royal
Society of London A: Mathematical, Physical and
Engineering Sciences, 439(1907):553–558, 1992.
[91] C. Ding, T.-Y. Bao, and H.-L. Huang. Quantum-
inspired support vector machine. arXiv:1906.08902
[quant-ph, stat], Mar 2021. arXiv: 1906.08902.
[92] P. A. M. Dirac. A new notation for quantum mechan-
ics. Mathematical Proceedings of the Cambridge
Philosophical Society, 35(3):416–418, 1939.
[93] D. P. DiVincenzo. Topics in quantum computers.
arXiv preprint cond-mat/9612126, 1996.
[94] D. P. DiVincenzo. The physical implementation
of quantum computation. Fortschritte der Physik:
Progress of Physics, 48(9-11):771–783, 2000. arXiv:
quant-ph/0002077.
[95] M. W. Doherty, N. B. Manson, P. Delaney, F. Jelezko,
J. Wrachtrup, and L. C. Hollenberg. The nitrogen-
vacancy colour centre in diamond. Physics Reports,
528(1):1–45, 2013.
[96] L.-M. Duan and H. Kimble.
Scalable photonic
quantum computation through cavity-assisted inter-
actions. Physical Review Letters, 92(12):127902,
2004.
[97] D. S. Dummit and R. M. Foote. Abstract Algebra,
volume 3. Wiley Hoboken, 2004.
[98] S. Ebadi, T. T. Wang, H. Levine, A. Keesling, G. Se-
meghini, A. Omran, D. Bluvstein, R. Samajdar,
H. Pichler, W. W. Ho, and et al. Quantum phases of
matter on a 256-atom programmable quantum simu-
lator. Nature, 595(7866):227–232, Jul 2021. arXiv:
2012.12281.
398
Works Cited
[99] A. Einstein. Letter from Einstein to D. M. Lipkin.
https://bit.ly/2CogEUC, July 1952. Accessed: 2018-
05-19,
Original
URL:
http://sethlipkin.com/
collectibles/letters/letter1/letter%201%20-
%20page%201.jpg.
[100] A. Einstein, B. Podolsky, and N. Rosen.
Can
quantum-mechanical description of physical real-
ity be considered complete? Phys. Rev., 47:777–780,
May 1935.
[101] R. E. Evans, M. K. Bhaskar, D. D. Sukachev, C. T.
Nguyen, A. Sipahigil, M. J. Burek, B. Machielse,
G. H. Zhang, A. S. Zibrov, E. Bielejec, et al. Photon-
mediated interactions between quantum emitters in
a diamond nanocavity. Science, 362(6415):662–665,
2018.
[102] K. Fang and Z.-W. Liu. No-go theorems for quan-
tum resource puriﬁcation. Physical Review Letters,
125(6):060405, Aug 2020.
[103] E. Farhi, J. Goldstone, and S. Gutmann. A quantum
algorithm for the hamiltonian NAND tree. arXiv
preprint quant-ph/0702144, 2007.
[104] E. Farhi, J. Goldstone, and S. Gutmann. A quantum
approximate optimization algorithm. arXiv preprint
arXiv:1411.4028, 2014.
[105] E. Farhi, J. Goldstone, and S. Gutmann. A quan-
tum approximate optimization algorithm applied to
a bounded occurrence constraint problem. arXiv
preprint arXiv:1412.6062, 2014.
[106] E. Farhi, J. Goldstone, S. Gutmann, and M. Sipser.
Quantum computation by adiabatic evolution. arXiv
preprint quant-ph/0001106, 2000.
[107] E. Farhi and S. Gutmann. Analog analogue of a
digital quantum computation. Physical Review A,
57(4):2403, 1998.
Works Cited
399
[108] E. Farhi and S. Gutmann. Quantum computation and
decision trees. Physical Review A, 58(2):915–928,
Aug 1998. arXiv: quant-ph/9706062.
[109] E. Farhi and H. Neven. Classiﬁcation with quantum
neural networks on near term processors.
arXiv
preprint arXiv:1802.06002, 2018.
[110] R. Ferguson,
L. Dellantonio,
A. Al Balushi,
K. Jansen, W. Dür, and C. Muschik. Measurement-
based variational quantum eigensolver. Physical
Review Letters, 126(22):220501, 2021.
[111] R. P. Feynman. Simulating physics with comput-
ers. International Journal of Theoretical Physics,
21(6):467–488, Jun 1982.
[112] R. P. Feynman. Feynman Lectures on Computation.
CRC Press, 2000.
[113] C. Figgatt, D. Maslov, K. A. Landsman, N. M. Linke,
S. Debnath, and C. Monroe. Complete 3-qubit grover
search on a programmable quantum computer. Na-
ture Communications, 8(11):1918, Dec 2017.
[114] M. Fingerhuth, T. Babej, and P. Wittek.
Open
source software in quantum computing. PLOS One,
13(12):e0208561, 2018.
[115] C. Flühmann, T. L. Nguyen, M. Marinelli, V. Neg-
nevitsky, K. Mehta, and J. Home.
Encoding a
qubit in a trapped-ion mechanical oscillator. Nature,
566(7745):513, 2019.
[116] A. G. Fowler, M. Mariantoni, J. M. Martinis, and
A. N. Cleland. Surface codes: Towards practical
large-scale quantum computation.
Phys. Rev. A,
86:032324, Sep 2012.
[117] J. B. Fraleigh. A First Course in Abstract Algebra.
Pearson, 2002.
[118] E. Fredkin and T. Toffoli. Conservative logic. Inter-
national Journal of Theoretical Physics, 21(3):219–
253, Apr 1982.
400
Works Cited
[119] M. Freedman, A. Kitaev, M. Larsen, and Z. Wang.
Topological quantum computation. Bulletin of the
American Mathematical Society, 40(1):31–38, 2003.
arXiv: quant-ph/0101025.
[120] E. S. Fried, N. P. Sawaya, Y. Cao, I. D. Kivlichan,
J. Romero, and A. Aspuru-Guzik.
qTorch: The
quantum tensor contraction handler.
PloS one,
13(12):e0208510, 2018.
[121] N. Friis,
O. Marty,
C. Maier,
C. Hempel,
M. Holzäpfel, P. Jurcevic, M. B. Plenio, M. Huber,
C. Roos, R. Blatt, et al. Observation of entangled
states of a fully controlled 20-qubit system. Physical
Review X, 8(2):021012, 2018.
[122] A. Frisk Kockum. Quantum optics with artiﬁcial
atoms. Chalmers University of Technology, 2014.
arXiv: 1912.13012.
[123] J. M. Gambetta, J. M. Chow, and M. Steffen. Build-
ing logical qubits in a superconducting quantum
computing system.
NPJ Quantum Information,
3(1):2, 2017. arXiv: 1510.04375.
[124] M. Ganahl, A. Milsted, S. Leichenauer, J. Hidary,
and G. Vidal. TensorNetwork on TensorFlow: Entan-
glement renormalization for quantum critical lattice
models. arXiv preprint arXiv:1906.12030, 2019.
[125] A. Gilyén, Y. Su, G. H. Low, and N. Wiebe. Quan-
tum singular value transformation and beyond: ex-
ponential improvements for quantum matrix arith-
metics. In Proceedings of the 51st Annual ACM
SIGACT Symposium on Theory of Computing, pages
193–204, 2019.
[126] V. Giovannetti, S. Lloyd, and L. Maccone. Quan-
tum random access memory.
arXiv preprint
arXiv:0708.1879v2, Aug 2007.
[127] Google AI Quantum and others. Hartree-Fock on a
superconducting qubit quantum computer. Science,
369(6507):1084–1089, 2020.
Works Cited
401
[128] D. Gottesman.
The Heisenberg representation
of quantum computers.
arXiv preprint quant-
ph/9807006, 1998.
[129] Grove
Developers.
Grove
doc-
umentation,
2018.
https://grove-
docs.readthedocs.io/en/latest/vqe.html.
[130] L. K. Grover. Quantum mechanics helps in searching
for a needle in a haystack. Phys. Rev. Lett., 79:325–
328, Jul 1997. arXiv: quant-ph/9706033.
[131] M. Hafezi, E. A. Demler, M. D. Lukin, and J. M.
Taylor. Robust optical delay lines with topological
protection. Nature Physics, 7(11):907, 2011.
[132] M. Hafezi, S. Mittal, J. Fan, A. Migdall, and J. Taylor.
Imaging topological edge states in silicon photonics.
Nature Photonics, 7(12):1001, 2013.
[133] D. Hanneke, J. P. Home, J. D. Jost, J. M. Amini,
D. Leibfried, and D. J. Wineland. Realization of a
programmable two-qubit quantum processor. Nature
Physics, 6(1):13, 2010.
[134] G. H. Hardy and J. E. Littlewood.
Some prob-
lems of diophantine approximation: Part II. The
trigonometrical series associated with the elliptic
theta-functions.
Acta Mathematica, 37:193–239,
1914.
[135] A. W. Harrow, A. Hassidim, and S. Lloyd. Quantum
algorithm for linear systems of equations. Physi-
cal Review Letters, 103(15):150502, 2009. arXiv:
0811.3171.
[136] A. W. Harrow and A. Montanaro. Quantum compu-
tational supremacy. Nature, 549(7671):203, 2017.
[137] M. Z. Hasan and C. L. Kane. Colloquium: topo-
logical insulators.
Reviews of Modern Physics,
82(4):3045, 2010.
[138] V. Havlíˇcek, A. D. Córcoles, K. Temme, A. W. Har-
row, A. Kandala, J. M. Chow, and J. M. Gambetta.
402
Works Cited
Supervised learning with quantum-enhanced feature
spaces. Nature, 567(7747):209, 2019.
[139] M. Hayashi and H. Zhu. Secure uniform random-
number extraction via incoherent strategies. Physical
Review A, 97(1):012302, 2018. arXiv: 1706.04009.
[140] P. Hayden, S. Nezami, X.-L. Qi, N. Thomas, M. Wal-
ter, and Z. Yang. Holographic duality from random
tensor networks. Journal of High Energy Physics,
2016(11):9, 2016.
[141] L. Henriet, L. Beguin, A. Signoles, T. Lahaye,
A. Browaeys, G.-O. Reymond, and C. Jurczak. Quan-
tum computing with neutral atoms. Quantum, 4:327,
Sep 2020. arXiv: 2006.12326.
[142] B. Hensen, H. Bernien, A. E. Dréau, A. Reiserer,
N. Kalb, M. S. Blok, J. Ruitenberg, R. F. Vermeulen,
R. N. Schouten, C. Abellán, et al. Loophole-free Bell
inequality violation using electron spins separated
by 1.3 kilometres. Nature, 526(7575):682, 2015.
[143] C. Hepp, T. Müller, V. Waselowski, J. N. Becker,
B. Pingault, H. Sternschulte, D. Steinmüller-Nethl,
A. Gali, J. R. Maze, M. Atatüre, et al. Electronic
structure of the silicon vacancy color center in di-
amond. Physical Review Letters, 112(3):036405,
2014.
[144] O. Higgott, D. Wang, and S. Brierley.
Varia-
tional quantum computation of excited states. arXiv
preprint arXiv:1805.08138, 2018.
[145] C.-K. Hong, Z.-Y. Ou, and L. Mandel. Measure-
ment of subpicosecond time intervals between two
photons by interference. Physical Review Letters,
59(18):2044, 1987.
[146] W. J. Huggins, J. R. McClean, N. C. Rubin, Z. Jiang,
N. Wiebe, K. B. Whaley, and R. Babbush. Efﬁcient
and noise resilient measurements for quantum chem-
istry on near-term quantum computers. npj Quantum
Information, 7(1):1–9, 2021. arXiv: 1907.13117.
Works Cited
403
[147] S.
Jordan.
Quantum
Algorithm
Zoo.
http://quantumalgorithmzoo.org/, 2011.
[148] M. Katanaev.
Adiabatic theorem for ﬁnite di-
mensional quantum mechanical systems. Russian
Physics Journal, 54(3):342–353, 2011.
[149] J. Kempe. Quantum random walks hit exponentially
faster. arXiv preprint quant-ph/0205083, 2002.
[150] J. Kempe. Quantum random walks: an introductory
overview. Contemporary Physics, 44(4):307–327,
2003. arXiv: quant-ph/0303081.
[151] I. Kerenidis and A. Prakash. Quantum recommen-
dation systems. arXiv preprint arXiv:1603.08675,
2016.
[152] A. Y. Kitaev.
Quantum measurements and the
abelian stabilizer problem. Nov 1995.
[153] A. Y. Kitaev. Fault-tolerant quantum computation
by anyons. Annals of Physics, 303(1):2–30, 2003.
arXiv: quant-ph/9707021.
[154] M. Kjaergaard, M. E. Schwartz, J. Braumüller,
P. Krantz, J. I.-J. Wang, S. Gustavsson, and W. D.
Oliver. Superconducting qubits: Current state of
play. Annual Review of Condensed Matter Physics,
11:369–395, 2020.
[155] E. Knill, R. Laﬂamme, and G. J. Milburn. A scheme
for efﬁcient quantum computation with linear optics.
Nature, 409(6816):46, 2001.
[156] D. E. Knuth. Big omicron and big omega and big
theta. ACM Sigact News, 8(2):18–24, 1976.
[157] W. F. Koehl, B. B. Buckley, F. J. Heremans, G. Calu-
sine, and D. D. Awschalom. Room temperature co-
herent control of defect spin qubits in silicon carbide.
Nature, 479(7371):84, 2011.
[158] P. Kok, W. J. Munro, K. Nemoto, T. C. Ralph, J. P.
Dowling, and G. J. Milburn. Linear optical quantum
404
Works Cited
computing with photonic qubits. Reviews of Modern
Physics, 79(1):135, 2007.
[159] P. Krantz, M. Kjaergaard, F. Yan, T. P. Orlando,
S. Gustavsson, and W. D. Oliver. A quantum en-
gineer’s guide to superconducting qubits.
arXiv
preprint arXiv:1904.06560, 2019.
[160] Y. E. Kraus, Y. Lahini, Z. Ringel, M. Verbin, and
O. Zilberberg.
Topological states and adiabatic
pumping in quasicrystals. Physical Review Letters,
109(10):106402, 2012.
[161] T. D. Ladd, F. Jelezko, R. Laﬂamme, Y. Nakamura,
C. Monroe, and J. L. O’Brien. Quantum computers.
Nature, 464(7285):45, 2010.
[162] V. Lahtinen and J. K. Pachos. A short introduction to
topological quantum computation. SciPost Physics,
3(3):021, Sep 2017. arXiv: 1705.04103.
[163] R. Landauer. Irreversibility and heat generation in
the computing process. IBM Journal of Research
and Development, 5(3):183–191, July 1961.
[164] F. Lardinois. IBM publishes its quantum roadmap,
says it will have a 1,000-qubit machine in 2023.
TechCrunch, 2020.
[165] R. LaRose. Overview and comparison of gate level
quantum software platforms. Quantum, 3:130, 2019.
[166] F. W. Lawvere and S. H. Schanuel.
Conceptual
Mathematics: A First Introduction to Categories.
Cambridge University Press, 2009.
[167] J. Ledin. Modern Computer Architecture and Orga-
nization. Packt Publishing, 2020.
[168] D. R. Leibrandt, J. Labaziewicz, V. Vuleti´c, and I. L.
Chuang. Cavity sideband cooling of a single trapped
ion. Physical Review Letters, 103(10):103001, 2009.
[169] H. Levine, A. Keesling, A. Omran, H. Bernien,
S. Schwartz, A. S. Zibrov, M. Endres, M. Greiner,
Works Cited
405
V. Vuleti´c, and M. D. Lukin. High-ﬁdelity control
and entanglement of rydberg-atom qubits. Physical
Review Letters, 121(12):123603, 2018.
[170] M. H. Levitt and R. Ernst. Improvement of pulse
performance in nmr coherence transfer experiments:
A compensated inadequate experiment. Molecular
Physics, 50(5):1109–1124, 1983.
[171] F. Li, X. Huang, J. Lu, J. Ma, and Z. Liu. Weyl
points and Fermi arcs in a chiral phononic crystal.
Nature Physics, 14(1):30, 2018.
[172] C. Liu, M. G. Dutt, and D. Pekker. Single-photon
heralded two-qubit unitary gates for pairs of nitrogen-
vacancy centers in diamond. Physical Review A,
98(5):052342, 2018.
[173] S. Lloyd. A potentially realizable quantum computer.
Science, 261(5128):1569–1571, 1993.
[174] S. Lloyd. Universal quantum simulators. Science,
pages 1073–1078, 1996.
[175] S. Lloyd, S. Garnerone, and P. Zanardi. Quantum
algorithms for topological and geometric analysis
of data. Nature Communications, 7(1):10138, Jan
2016.
[176] M. Loceff. A Course in Quantum Computing. 2015.
[177] G. H. Low and I. L. Chuang. Optimal hamiltonian
simulation by quantum signal processing. Physical
review letters, 118(1):010501, 2017.
[178] G. H. Low and I. L. Chuang. Hamiltonian simulation
by qubitization. Quantum, 3:163, Jul 2019. arXiv:
1610.06546.
[179] L. Lu, J. D. Joannopoulos, and M. Soljaˇci´c. Topo-
logical photonics. Nat. Photonics, 8:821–829, 2014.
[180] D. Lucas, C. Donald, J. P. Home, M. McDonnell,
A. Ramos, D. Stacey, J.-P. Stacey, A. Steane, and
S. Webster. Oxford ion-trap quantum computing
406
Works Cited
project. Philosophical Transactions of the Royal
Society of London. Series A: Mathematical, Physical
and Engineering Sciences, 361(1808):1401–1408,
2003.
[181] Y. Manin. Computable and Non-Computable (in
Russian). Sovetskoye Radio, Moscow, 1980.
[182] Y. I. Manin. Classical computing, quantum com-
puting, and Shor’s factoring algorithm. In Sémi-
naire Bourbaki : volume 1998/99, exposés 850-864,
number 266 in Astérisque, pages 375–404. Société
mathématique de France, 2000.
[183] A. Marchenkova.
Recorded video: Five quan-
tum algorithms that could change the world.
https://www.youtube.com/watch?v=_54i80UFHSs.
[184] I. L. Markov, A. Fatima, S. V. Isakov, and S. Boixo.
Quantum supremacy is both closer and farther than
it appears. arXiv:1807.10749 [quant-ph], Jul 2018.
arXiv: 1807.10749.
[185] E. Martin-Lopez, A. Laing, T. Lawson, R. Alvarez,
X.-Q. Zhou, and J. L. O’brien. Experimental real-
ization of Shor’s quantum factoring algorithm using
qubit recycling. Nature Photonics, 6(11):773, 2012.
[186] J. M. Martyn, Z. M. Rossi, A. K. Tan, and I. L.
Chuang. A grand uniﬁcation of quantum algorithms.
arXiv preprint arXiv:2105.02859, 2021.
[187] K. Mattle, H. Weinfurter, P. G. Kwiat, and
A. Zeilinger. Dense coding in experimental quantum
communication. Phys. Rev. Lett., 76:4656–4659, Jun
1996.
[188] R. Maurand, X. Jehl, D. Kotekar-Patil, A. Corna,
H. Bohuslavskyi, R. Laviéville, L. Hutin, S. Bar-
raud, M. Vinet, M. Sanquer, et al. A CMOS silicon
spin qubit. Nature Communications, 7:13575, 2016.
arXiv: 1605.07599.
Works Cited
407
[189] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-
bush, and H. Neven. Barren plateaus in quantum
neural network training landscapes. Nature Commu-
nications, 9(1):4812, 2018.
[190] J. R. McClean, Z. Jiang, N. C. Rubin, R. Babbush,
and H. Neven. Decoding quantum errors with sub-
space expansions. arXiv preprint arXiv:1903.05786,
2019.
[191] J. R. McClean, M. E. Kimchi-Schwartz, J. Carter,
and W. A. de Jong. Hybrid quantum-classical hierar-
chy for mitigation of decoherence and determination
of excited states. Physical Review A, 95(4):042308,
2017.
[192] J. R. McClean, I. D. Kivlichan, K. J. Sung, D. S.
Steiger, Y. Cao, C. Dai, E. S. Fried, C. Gidney,
B. Gimby, P. Gokhale, et al.
Openfermion: the
electronic structure package for quantum computers.
arXiv preprint arXiv:1710.07629, 2017.
[193] J. R. McClean, J. Romero, R. Babbush, and
A. Aspuru-Guzik.
The theory of variational hy-
brid quantum-classical algorithms.
New Journal
of Physics, 18(2):023023, 2016.
[194] N. D. Mermin. Quantum Computer Science: An
Introduction. Cambridge University Press, 2007.
[195] A. Milsted, M. Ganahl, S. Leichenauer, J. Hidary,
and G. Vidal. TensorNetwork on TensorFlow: A
spin chain application using tree tensor networks.
arXiv preprint arXiv:1905.01331, 2019.
[196] A. Milsted and G. Vidal.
Tensor networks
as conformal transformations.
arXiv preprint
arXiv:1805.12524, 2018.
[197] S. Mittal, S. Ganeshan, J. Fan, A. Vaezi, and
M. Hafezi. Measurement of topological invariants in
a 2d photonic system. Nature Photonics, 10(3):180,
2016.
408
Works Cited
[198] S. Mittal and M. Hafezi. Topologically robust gen-
eration of correlated photon pairs. arXiv preprint
arXiv:1709.09984, 2017.
[199] M. Mohseni,
P. Read,
H. Neven,
S. Boixo,
V. Denchev, R. Babbush, A. Fowler, V. Smelyan-
skiy, and J. Martinis. Commercialize quantum tech-
nologies in ﬁve years. Nature News, 543(7644):171,
2017.
[200] R. Movassagh. Efﬁcient unitary paths and quantum
computational supremacy: A proof of average-case
hardness of random circuit sampling. arXiv preprint
arXiv: 1810.04681, Oct 2018.
[201] S. Mullane. Sampling random quantum circuits: a
pedestrian’s guide. arXiv preprint arXiv:2007.07872,
2020.
[202] Y. Nakamura, Y. A. Pashkin, and J. Tsai. Coherent
control of macroscopic quantum states in a single-
Cooper-pair box.
Nature, 398(6730):786, 1999.
arXiv: cond-mat/9904003.
[203] Y. Nakamura, Y. A. Pashkin, and J. S. Tsai. Rabi
oscillations in a Josephson-junction charge two-level
system. Phys. Rev. Lett., 87:246601, Nov 2001.
[204] NAS. Quantum Computing: Progress and Prospects.
The National Academies Press, Washington, DC,
2018.
[205] C. Neill, P. Roushan, K. Kechedzhi, S. Boixo, S. V.
Isakov, V. Smelyanskiy, A. Megrant, B. Chiaro,
A. Dunsworth, K. Arya, R. Barends, B. Burkett,
Y. Chen, Z. Chen, A. Fowler, B. Foxen, M. Giustina,
R. Graff, E. Jeffrey, T. Huang, J. Kelly, P. Klimov,
E. Lucero, J. Mutus, M. Neeley, C. Quintana,
D. Sank, A. Vainsencher, J. Wenner, T. C. White,
H. Neven, and J. M. Martinis. A blueprint for demon-
strating quantum supremacy with superconducting
qubits. Science, 360(6385):195–199, 2018.
Works Cited
409
[206] M. A. Nielsen and I. L. Chuang. Quantum Compu-
tation and Quantum Information: 10th Anniversary
Edition. Cambridge University Press, New York, NY,
USA, 10th edition, 2011.
[207] nLab authors. Topological quantum computation.
http://ncatlab.org/nlab/show/topological %20quan-
tum%20computation, June 2021. Revision 18.
[208] J. Noh, S. Huang, D. Leykam, Y. D. Chong, K. P.
Chen, and M. C. Rechtsman. Experimental obser-
vation of optical Weyl points and Fermi arc-like
surface states. Nature Physics, 13(6):611, 2017.
[209] N. Ofek, A. Petrenko, R. Heeres, P. Reinhold,
Z. Leghtas, B. Vlastakis, Y. Liu, L. Frunzio,
S. Girvin, L. Jiang, et al. Extending the lifetime
of a quantum bit with error correction in supercon-
ducting circuits. Nature, 536(7617):441, 2016.
[210] J. Olson, Y. Cao, J. Romero, P. Johnson, P.-L.
Dallaire-Demers, N. Sawaya, P. Narang, I. Kivlichan,
M. Wasielewski, and A. Aspuru-Guzik. Quantum
information and computation for chemistry. arXiv
preprint arXiv:1706.05413, 2017.
[211] P. J. O’Malley, R. Babbush, I. D. Kivlichan,
J. Romero, J. R. McClean, R. Barends, J. Kelly,
P. Roushan, A. Tranter, N. Ding, et al. Scalable
quantum simulation of molecular energies. Physical
Review X, 6(3):031007, 2016.
[212] J. Otterbach, R. Manenti, N. Alidoust, A. Bestwick,
M. Block, B. Bloom, S. Caldwell, N. Didier, E. S.
Fried, S. Hong, et al. Unsupervised machine learn-
ing on a hybrid quantum computer. arXiv preprint
arXiv:1712.05771, 2017.
[213] T. Ozawa, H. M. Price, A. Amo, N. Goldman,
M. Hafezi, L. Lu, M. Rechtsman, D. Schuster, J. Si-
mon, O. Zilberberg, et al. Topological photonics.
arXiv preprint arXiv:1802.04173, 2018.
410
Works Cited
[214] M. Ozols. Clifford group. Essays at University of
Waterloo, Spring, 2008. https://bit.ly/2JZe2jO.
[215] D. P. Pappas, J. S. Kline, F. da Silva, and D. Wisbey.
Coherence in superconducting materials for quantum
computing. https://slideplayer.com/slide/7770332/.
[216] Pasqal. https://pasqal.io/2020/10/26/1329/.
[217] Pennylane. https://pennylane.ai/qml/demos/tutorial
_pasqal.html.
[218] A. Peruzzo et al. A variational eigenvalue solver
on a quantum processor. eprint.
arXiv preprint
arXiv:1304.3061, 2013.
[219] W. Pfaff, B. Hensen, H. Bernien, S. B. van Dam,
M. S. Blok, T. H. Taminiau, M. J. Tiggelman, R. N.
Schouten, M. Markham, D. J. Twitchen, et al. Un-
conditional quantum teleportation between distant
solid-state quantum bits. Science, 345(6196):532–
535, 2014.
[220] H. Pichler, S.-T. Wang, L. Zhou, S. Choi, and M. D.
Lukin. Quantum optimization for maximum indepen-
dent set using rydberg atom arrays. arXiv preprint
arXiv:1808.10816, 2018.
[221] J. M. Pino, J. M. Dreiling, C. Figgatt, J. P. Gaebler,
S. A. Moses, M. S. Allman, C. H. Baldwin, M. Foss-
Feig, D. Hayes, K. Mayer, and et al. Demonstration
of the trapped-ion quantum-ccd computer architec-
ture. Nature, 592(7853):209–213, Apr 2021. arXiv:
2003.01293.
[222] J. Preskill.
Lecture notes for Physics 219/Com-
puter Science 219 at Caltech: Quantum Computation.
[223] J. Preskill. Quantum computing and the entangle-
ment frontier. arXiv preprint arXiv:1203.5813, 2012.
[224] J. Preskill. Quantum computing in the NISQ era and
beyond. Quantum, 2:79, 2018. arXiv: 1801.00862.
http://theory.caltech.edu/~preskill/ph229/, 1997
Works Cited
411
[225] J. Preskill. Quantum computing 40 years later. 2021.
arXiv: 2106.10522.
[226] X.-L. Qi and S.-C. Zhang. Topological insulators
and superconductors. Reviews of Modern Physics,
83(4):1057, 2011.
[227] X. Qiang, X. Zhou, J. Wang, C. M. Wilkes, T. Loke,
S. O’Gara, L. Kling, G. D. Marshall, R. Santagati,
T. C. Ralph, et al.
Large-scale silicon quantum
photonics implementing arbitrary two-qubit process-
ing.
Nature Photonics, 12(9):534, 2018.
arXiv:
1809.09791.
[228] Qiskit.
https://qiskit.org/textbook/ch-algorithms/
grover.html#5.-Solving-Sudoku-using-Grover’s-
Algorithm-.
[229] R. Raussendorf and H. J. Briegel. A one-way quan-
tum computer. Physical Review Letters, 86(22):5188,
2001.
[230] R. Raussendorf and H. J. Briegel. Computational
model underlying the one-way quantum computer.
Quantum Information & Computation, 2(6):443–
486, 2002.
[231] R. Raussendorf, D. Browne, and H. Briegel. The
one-way quantum computer–a non-network model
of quantum computation. Journal of Modern Optics,
49(8):1299–1306, 2002.
[232] R. Raussendorf, D. E. Browne, and H. J. Briegel.
Measurement-based quantum computation on cluster
states. Physical review A, 68(2):022312, 2003.
[233] M. C. Rechtsman, J. M. Zeuner, Y. Plotnik, Y. Lumer,
D. Podolsky, F. Dreisow, S. Nolte, M. Segev, and
A. Szameit. Photonic ﬂoquet topological insulators.
Nature, 496(7444):196, 2013.
[234] M. Reiher, N. Wiebe, K. M. Svore, D. Wecker, and
M. Troyer.
Elucidating reaction mechanisms on
412
Works Cited
quantum computers. Proceedings of the National
Academy of Sciences, 114(29):7555–7560, 2017.
[235] D. Reitzner, D. Nagaj, and V. Buzek.
Quantum
walks. Acta Physica Slovaca. Reviews and Tutorials,
61(6), Dec 2011. arXiv: 1207.7283.
[236] E. G. Rieffel and W. H. Polak. Quantum Computing:
A Gentle Introduction. The MIT Press, 1 edition
edition, Mar 2011.
[237] E. Riehl. Category Theory in Context. Courier Dover
Publications, 2017.
[238] A. Rivas and S. F. Huelga. Open Quantum Systems:
An Introduction. SpringerBriefs in Physics. Springer-
Verlag, 2012.
[239] R. L. Rivest, A. Shamir, and L. Adleman. A method
for obtaining digital signatures and public-key cryp-
tosystems. Communications of the ACM, 21(2):120–
126, 1978.
[240] C. Roberts, A. Milsted, M. Ganahl, A. Zalcman,
B. Fontaine, Y. Zou, J. Hidary, G. Vidal, and S. Le-
ichenauer. TensorNetwork: A library for physics and
machine learning. arXiv preprint arXiv:1905.01330,
2019.
[241] J. Romero and A. Aspuru-Guzik. Variational quan-
tum generators: Generative adversarial quantum ma-
chine learning for continuous distributions. arXiv
preprint arXiv:1901.00848, 2019.
[242] S. Rosenblum, Y. Y. Gao, P. Reinhold, C. Wang,
C. J. Axline, L. Frunzio, S. M. Girvin, L. Jiang,
M. Mirrahimi, M. H. Devoret, et al. A cnot gate
between multiphoton qubits encoded in two cavities.
Nature Communications, 9(1):652, 2018.
[243] J. J. Rotman. Advanced Modern Algebra, volume
114. American Mathematical Soc., 2010.
Works Cited
413
[244] E. Rowell and Z. Wang. Mathematics of topologi-
cal quantum computing. Bulletin of the American
Mathematical Society, 55(2):183–238, 2018.
[245] A. Roy and D. P. DiVincenzo. Topological quantum
computing. arXiv preprint arXiv:1701.05052, 2017.
[246] M. Saffman, T. G. Walker, and K. Mølmer. Quantum
information with rydberg atoms. Reviews of Modern
Physics, 82(3):2313, 2010.
[247] U. Schollwöck. The density-matrix renormalization
group. Reviews of Modern Physics, 77(1):259, 2005.
[248] C. Schreyvogel, V. Polyakov, R. Wunderlich, J. Mei-
jer, and C. Nebel. Active charge state control of sin-
gle NV centres in diamond by in-plane Al-Schottky
junctions. Scientiﬁc Reports, 5:12160, 2015.
[249] E. Schrödinger. Discussion of probability relations
between separated systems. In Mathematical Pro-
ceedings of the Cambridge Philosophical Society,
volume 31, pages 555–563. Cambridge University
Press, 1935.
[250] M. Schuld and N. Killoran. Quantum machine learn-
ing in feature Hilbert spaces. Physical Review Let-
ters, 122(4):040504, 2019.
[251] M. Schuld and F. Petruccione. Supervised learning
with quantum computers. Springer, 2018.
[252] H. M. Sheffer. A set of ﬁve independent postulates
for Boolean algebras, with application to logical
constants. Trans. Amer. Math. Soc., 14:481–488,
1913.
[253] Y. Shi. Both Toffoli and controlled-NOT need little
help to do universal quantum computing. Quantum
Information & Computation, 3(1):84–92, 2003.
[254] P. Shor. Algorithms for quantum computation: dis-
crete logarithms and factoring. In Proceedings of
the 35th Annual Symposium on Foundations of Com-
414
Works Cited
puter Science, pages 124–134. IEEE Computer So-
ciety, 1994.
[255] J. W. Silverstone, D. Bonneau, J. L. O’Brien, and
M. G. Thompson. Silicon quantum photonics. IEEE
Journal of Selected Topics in Quantum Electronics,
22(6):390–402, 2016.
[256] S. Sim, Y. Cao, J. Romero, P. D. Johnson, and
A. Aspuru-Guzik. A framework for algorithm de-
ployment on cloud-based quantum computers. arXiv
preprint arXiv:1810.10576, 2018.
[257] D. R. Simon. On the power of quantum computation.
In Proceedings of the 35th Annual Symposium on
Foundations of Computer Science, SFCS ’94, pages
116–123, Washington, DC, USA, 1994. IEEE Com-
puter Society.
[258] A. Sipahigil, R. E. Evans, D. D. Sukachev, M. J. Bu-
rek, J. Borregaard, M. K. Bhaskar, C. T. Nguyen, J. L.
Pacheco, H. A. Atikian, C. Meuwly, et al. An inte-
grated diamond nanophotonics platform for quantum
optical networks. Science, page 6875, 2016.
[259] S. Slussarenko and G. J. Pryde. Photonic quantum
information processing: a concise review. Applied
Physics Reviews, 6(4):041303, Dec 2019. arXiv:
1907.06331.
[260] M. Smelyanskiy, N. P. Sawaya, and A. Aspuru-
Guzik.
qHiPSTER: The quantum high perfor-
mance software testing environment. arXiv preprint
arXiv:1601.07195, 2016.
[261] R. S. Smith, M. J. Curtis, and W. J. Zeng. A practical
quantum instruction set architecture. arXiv preprint
arXiv:1608.03355, 2016.
[262] R. Solovay and V. Strassen. A fast monte-carlo test
for primality. SIAM Journal on Computing, 6(1):84–
85, Mar 1977.
Works Cited
415
[263] N. Spagnolo, C. Vitelli, M. Bentivegna, D. J. Brod,
A. Crespi, F. Flamini, S. Giacomini, G. Milani,
R. Ramponi, P. Mataloni, and et al. Efﬁcient ex-
perimental validation of photonic boson sampling
against the uniform distribution. Nature Photonics,
8(8):615–620, Aug 2014. arXiv: 1311.1622.
[264] G. Strang. Linear Algebra and Its Applications. Cen-
gage Learning, 2018.
[265] Y. Su, D. W. Berry, N. Wiebe, N. Rubin, and
R. Babbush.
Fault-tolerant quantum simulations
of chemistry in ﬁrst quantization. arXiv preprint
arXiv:2105.12767, 2021.
[266] S. Sun, H. Kim, Z. Luo, G. S. Solomon, and E. Waks.
A single-photon switch and transistor enabled by
a solid-state quantum memory.
arXiv preprint
arXiv:1805.01964, 2018.
[267] S. Sun, H. Kim, G. S. Solomon, and E. Waks.
A quantum phase switch between a single solid-
state spin and a photon. Nature Nanotechnology,
11(6):539–544, 2016.
[268] L. Susskind.
Dear qubitzers, GR = QM.
arXiv
preprint arXiv:1708.03040, 2017.
[269] M. Taha Rouabah. Compiling single-qubit braid-
ing gate for ﬁbonacci anyons topological quantum
computation. arXiv e-prints, 2020. 2008.03542.
[270] J.-L. Tambasco, G. Corrielli, R. J. Chapman,
A. Crespi, O. Zilberberg, R. Osellame, and A. Pe-
ruzzo. Quantum interference of topological states of
light. Science Advances, 4(9):eaat3187, 2018.
[271] M. S. Tame, B. A. Bell, C. Di Franco, W. J.
Wadsworth, and J. G. Rarity. Experimental real-
ization of a one-way quantum computer algorithm
solving Simon’s problem. Physical Review Letters,
113(20):200501, 2014. arXiv: 1410.3859.
416
Works Cited
[272] E.
Tang.
Recorded
talk:
On
quan-
tum
linear
algebra
for
machine
learning.
https://www.youtube.com/watch?v=pd24cDR87Lw.
[273] E. Tang. A quantum-inspired classical algorithm for
recommendation systems. Electronic Colloquium on
Computational Complexity (ECCC), 25:128, 2019.
[274] J. Timmer. Quantum-computing startup rigetti to
offer modular processors, Jun 2021.
[275] T. Toffoli. Reversible computing. In Proceedings
of the 7th Colloquium on Automata, Languages and
Programming, pages 632–644, Berlin, Heidelberg,
1980. Springer-Verlag.
[276] I. Tzitrin, T. Matsuura, R. N. Alexander, G. Dauphi-
nais, J. E. Bourassa, K. K. Sabapathy, N. C.
Menicucci,
and
I.
Dhand.
Fault-tolerant
quantum
computation
with
static
linear
op-
tics.
arXiv:2104.03241 [quant-ph], Apr 2021.
arXiv: 2104.03241.
[277] S. B. van Dam, M. Walsh, M. J. Degen, E. Bersin,
S. L. Mouradian, A. Galiullin, M. Ruf, M. IJspeert,
T. H. Taminiau, R. Hanson, et al. Optical coher-
ence of diamond nitrogen-vacancy centers formed
by ion implantation and annealing. arXiv preprint
arXiv:1812.11523, 2018.
[278] L. M. Vandersypen, M. Steffen, G. Breyta, C. S.
Yannoni, M. H. Sherwood, and I. L. Chuang. Ex-
perimental realization of Shor’s quantum factoring
algorithm using nuclear magnetic resonance. Nature,
414(6866):883, 2001. arXiv: quant-ph/0112176.
[279] M. Verbin, O. Zilberberg, Y. E. Kraus, Y. Lahini, and
Y. Silberberg. Observation of topological phase tran-
sitions in photonic quasicrystals. Physical Review
Letters, 110(7):076403, 2013.
[280] M. Verbin, O. Zilberberg, Y. Lahini, Y. E. Kraus,
and Y. Silberberg. Topological pumping over a pho-
Works Cited
417
tonic Fibonacci quasicrystal. Physical Review B,
91(6):064201, 2015.
[281] G. Verdon, J. Marks, S. Nanda, S. Leichenauer, and
J. Hidary. Quantum hamiltonian-based models and
the variational quantum thermalizer algorithm. arXiv
preprint arXiv:1910.02071, 2019.
[282] G. Verdon, T. McCourt, E. Luzhnica, V. Singh, S. Le-
ichenauer, and J. Hidary. Quantum graph neural
networks. arXiv preprint arXiv:1909.12264, 2019.
[283] G. Verdon, J. Pye, and M. Broughton. A universal
training algorithm for quantum deep learning. arXiv
preprint arXiv:1806.09729, 2018.
[284] F. Verstraete and J. I. Cirac.
Renormalization
algorithms for quantum-many body systems in
two and higher dimensions. arXiv preprint cond-
mat/0407066, 2004.
[285] G. Vidal. Entanglement renormalization. Physical
Review Letters, 99(22):220405, 2007.
[286] J. von Neumann.
Mathematical Foundations of
Quantum Mechanics. Springer, Berlin, 1932.
[287] C. Wang, F.-G. Deng, Y.-S. Li, X.-S. Liu, and G. L.
Long. Quantum secure direct communication with
high-dimension quantum superdense coding. Phys.
Rev. A, 71:044305, Apr 2005.
[288] D. Wang, O. Higgott, and S. Brierley. A generalised
variational quantum eigensolver.
arXiv preprint
arXiv:1802.00171, 2018.
[289] J. Wang, C. Roberts, G. Vidal, and S. Leichenauer.
Anomaly detection with tensor networks.
arXiv
preprint arXiv:2006.02516, 2020.
[290] Y. Wang, X. Zhang, T. A. Corcovilos, A. Kumar,
and D. S. Weiss. Coherent addressing of individ-
ual neutral atoms in a 3d optical lattice.
Physi-
cal Review Letters, 115(4):043003, 2015. arXiv:
arXiv:1504.02117.
418
Works Cited
[291] Z. Wang, Y. Chong, J. D. Joannopoulos, and M. Sol-
jaˇci´c. Observation of unidirectional backscattering-
immune topological electromagnetic states. Nature,
461(7265):772, 2009.
[292] T. Watson, S. Philips, E. Kawakami, D. Ward,
P. Scarlino, M. Veldhorst, D. Savage, M. Lagally,
M. Friesen, S. Coppersmith, et al. A programmable
two-qubit quantum processor in silicon.
Nature,
555(7698):633, 2018.
[293] D. Wecker, M. B. Hastings, and M. Troyer. Progress
towards practical quantum variational algorithms.
Physical Review A, 92(4):042303, 2015.
[294] D. S. Weiss and M. Saffman. Quantum computing
with neutral atoms. Phys. Today, 70(7):44, 2017.
[295] S. R. White. Density matrix formulation for quantum
renormalization groups. Physical Review Letters,
69(19):2863, 1992.
[296] N. Wiebe, D. Braun, and S. Lloyd. Quantum data
ﬁtting. Physical Review Letters, 109(5):050505, Aug
2012. arXiv: 1204.5242.
[297] K. Wiggers.
Alphabet is repurposing Google
TPUs
for
quantum
computing
simulations.
https://venturebeat.com/2021/03/10/alphabet-is-
repurposing-google-tpus-for-quantum-computing-
simulations/, Mar 2021.
[298] P. Wittek.
Quantum machine learning edX
MOOC.
https://www.edx.org/course/quantum-
machine-learning-2, 2019.
[299] P. Wittek and C. Gogolin. Quantum enhanced infer-
ence in Markov logic networks. Scientiﬁc Reports,
7:45672, 2017.
[300] K. Wright, K. Beck, S. Debnath, J. Amini, Y. Nam,
N. Grzesiak, J.-S. Chen, N. Pisenti, M. Chmielewski,
C. Collins, et al. Benchmarking an 11-qubit quantum
computer. arXiv preprint arXiv:1903.08181, 2019.
Works Cited
419
[301] T.-Y. Wu, A. Kumar, F. Giraldo, and D. S. Weiss.
Stern–Gerlach detection of neutral-atom qubits in
a state-dependent optical lattice. Nature Physics,
page 1, 2019.
[302] Y. Wu, W.-S. Bao, S. Cao, F. Chen, M.-C. Chen,
X. Chen, T.-H. Chung, H. Deng, Y. Du, D. Fan,
M. Gong, C. Guo, C. Guo, S. Guo, L. Han, L. Hong,
H.-L. Huang, Y.-H. Huo, L. Li, N. Li, S. Li, Y. Li,
F. Liang, C. Lin, J. Lin, H. Qian, D. Qiao, H. Rong,
H. Su, L. Sun, L. Wang, S. Wang, D. Wu, Y. Xu,
K. Yan, W. Yang, Y. Yang, Y. Ye, J. Yin, C. Ying,
J. Yu, C. Zha, C. Zhang, H. Zhang, K. Zhang,
Y. Zhang, H. Zhao, Y. Zhao, L. Zhou, Q. Zhu, C.-Y.
Lu, C.-Z. Peng, X. Zhu, and J.-W. Pan. Strong quan-
tum computational advantage using a superconduct-
ing quantum processor. 2021. arXiv: 2106.14734.
[303] L. Xiao, X. Zhan, Z. Bian, K. Wang, X. Zhang,
X.
Wang,
J.
Li,
K.
Mochizuki,
D.
Kim,
N. Kawakami, et al. Observation of topological edge
states in parity–time-symmetric quantum walks. Na-
ture Physics, 13(11):1117, 2017.
[304] J.-S. Xu, K. Sun, Y.-J. Han, C.-F. Li, J. K. Pachos,
and G.-C. Guo. Simulating the exchange of majo-
rana zero modes with a photonic system. Nature
Communications, 7:13194, 2016.
[305] Z. Xu, Z.-q. Yin, Q. Han, and T. Li. Quantum in-
formation processing with closely-spaced diamond
color centers in strain and magnetic ﬁelds. Optical
Materials Express, 9(12):4654, Dec 2019. arXiv:
1909.11775.
[306] T. J. Yoder, G. H. Low, and I. L. Chuang. Fixed-point
quantum search with an optimal number of queries.
Physical review letters, 113(21):210501, 2014.
[307] W. J. Zeng.
Clarifying quantum supremacy:
better
terms
for
milestones
in
quantum
computation.
Medium,
Jan
31,
2019.
https://medium.com/@wjzeng/clarifying-quantum-
420
Works Cited
supremacy-better-terms-for-milestones-in-
quantum-computation-d15ccb53954f.
[308] Z. Zhao, J. K. Fitzsimons, and J. F. Fitzsimons.
Quantum assisted gaussian process regression. Phys-
ical Review A, 99(5):052331, May 2019.
arXiv:
1512.03929.
[309] L. Zhou, S.-T. Wang, S. Choi, H. Pichler, and
M. D. Lukin. Quantum approximate optimization
algorithm: Performance, mechanism, and imple-
mentation on near-term devices.
arXiv preprint
arXiv:1812.01041, 2018.
[310] O. Zilberberg, S. Huang, J. Guglielmon, M. Wang,
K. P. Chen, Y. E. Kraus, and M. C. Rechtsman. Pho-
tonic topological boundary pumping as a probe of
4D quantum Hall physics. Nature, 553(7686):59,
2018.
Index
algorithms
adiabatic, 156
amplitude ampliﬁcation, 138
Bernstein-Vazirani, 16, 110
Deutsch’s, 16, 106
Deutsch-Jozsa, 16, 109
Grover’s, 18, 135
Shor’s, 18
Simon’s, 17
amplitude, 4, 26
ancilla, 87
ansatz, 143
Bell inequality test, 94
Bell state, 82, 87
Benioff, Paul, 15
bijective, 286
binary operator, 28
Bloch sphere, 38
Boolean function, 103
Born rule, 6, 31
Bourbaki, 286
BQP, 48
bra-ket notation, 377
Cartesian Product, 279
Church-Turing Thesis, 49
circuit depth, 38
circuit diagram, 27
Cirq, 70
CNOT, 33
complexity classes, 43
computation-in-place, 41
cryptography, 117
CSWAP, 35
dequantization, 174, 203
determinant, 328
Dirac notation, 377
distillation, 224
DiVincenzo Criteria, 19
dot product, 237
eigenvalues, 341
eigenvectors, 341
entanglement, 6
EPR, 6
EPR pair, 87
EQP, 48
error correction, 215
Feynman, Richard, 15
Forest (Rigetti), 75
Fredkin operator, 35
Gottesman-Knill, 37
groups, 294
Hadamard, 31
Hamiltonian, 9, 143, 151
Hermitian, 40, 350
Hilbert Space, 360
injective, 286
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 
J. D. Hidary, Quantum Computing: An Applied Approach,  
https://doi.org/10.1007/978-3-030-83274-2
421
422
Index
inner product, 240, 242, 364
irreversible computation, 12
Landauer’s limit, 11
Manin, Yuri, 15
matrix, 254
matrix exponentiation, 270
measurement, 40
neutral atoms, 55
nitrogen-vacancy, 57
NMR, 56
NOT operator, 36
NV center-in-diamond, 57
OpenFermion, 152
outer product, 25, 271
Pauli operators, 28
phase kickback, 107
phase shift operator, 30
photonic quantum computer, 58
polarization, 4
puriﬁcation, 150
QAOA, 156
QDK, 77
QisKit, 72
QMA, 48
QML, 167
QPE, 174
QPU, 53
QRAM, 204
QSVT, 202
quantum advantage, 214
quantum chemistry, 151
quantum circuit, 27
quantum circuit simulation, 67, 81
quantum computer
deﬁnition, 3
hardware, 53
quantum error correction, 215
quantum Fourier transform, 114
quantum machine learning, 167
quantum phase estimation, 174
quantum simulation, 207
quantum singular value transform,
202
quantum supremacy, 208
quantum teleportation, 87
quantum walks, 194
qubitization, 152
random circuit sampling, 209
reversible computation, 37
RSA, 117
set theory, 276
Solovay-Kitaev, 37
spin qubits, 60
superconducting qubits, 61
superdense coding, 90
superposition, 4
surjective, 286
tensor product, 273
ternary operators, 34
Toffoli operator, 34
topological quantum computing,
63
trapped ion, 64
Trotterization, 151
unary operator, 28
uncomputing, 13, 182
unitary operators, 24, 353
universal computation, 37
vector space, 293
VQE, 143

============================================================
Quantum Computing Explained for Beginners
============================================================

QUANTUM
COMPUTING
EXPLAINED FOR BEGINNERS
The Science, Technology, and Impact
PANTHEON SPACE ACADEMY
Copyright © 2023
Pantheon Space Academy
QUANTUM COMPUTING
EXPLAINED FOR BEGINNERS
The Science, Technology, and Impact All rights reserved.
No part of this publication may be reproduced, distributed, or transmitted in any form or by any
means, including photocopying, recording, or other electronic or mechanical methods, without the
prior written permission of the author, except in the case of brief quotations embodied in critical
reviews and certain other non-commercial uses permitted by copyright law.
Pantheon Space Academy First Edition 2023
Under no circumstances will any blame or legal responsibility be held against the publisher, or
author, for any damages, reparation, or monetary loss due to the information contained within this
book. Either directly or indirectly. You are responsible for your own choices, actions, and results.
Legal Notice
This book is copyright protected. This book is only for personal use. You cannot amend, distribute,
sell, use, quote, or paraphrase any part, or the content within this book, without the consent of the
author or publisher.
Disclaimer Notice Please note the information contained within this document is for educational and
entertainment purposes only. All effort has been executed to present accurate, up-to-date, and
reliable, complete information. No warranties of any kind are declared or implied. Readers
acknowledge that the author is not engaging in the rendering of legal, financial, medical, or
professional advice. The content within this book has been derived from various sources. Please
consult a licensed professional before attempting any techniques outlined in this book.
By reading this document, the reader agrees that under no circumstances is the author responsible for
any losses, direct or indirect, which are incurred as a result of the use of the information contained
within this document, including, but not limited to, — errors, omissions, or inaccuracies.
QUANTUM
COMPUTING
EXPLAINED FOR BEGINNERS
PANDORA’S BOX OF QUANTUM
QUESTIONS
Are Quantum computers real? Introduction to classical and quantum computing. Quantum
computation and quantum information. Quantum computing book. Quantum computing pdf.
Quantum computing an applied approach. Quantum computing and AI. Quantum computing and
artificial intelligence. Quantum computing architecture. Quantum computing fundamentals.
Quantum computing basics. Quantum computing applications. Quantum computing Cambridge.
Quantum computing introduction. Introduction to Quantum Computing. Quantum computing
Chuang. Quantum computing mike. Quantum computing book kaku. Quantum computing brian
clegg. Quantum computing mug. Quantum computing nakahara.
How quantum computing works. What is quantum computing? Who invented quantum computing?
Are quantum computers faster? When will quantum computers be available? What is quantum
computing used for? What can quantum computers do? Where quantum computing is used. Quantum
computing with AI.
Quantum Computing for dummies. Quantum computing for everyone. Quantum computing jobs near
me. Quantum computing without quantum computers. Quantum computing for computer scientists.
Quantum computing for the very curious. Quantum computing as a service. Quantum computing
companies. Quantum Computing in action.
Quantum computing help the world. Quantum computing break encryption. Quantum computing
benefits and risk. Quantum computing cryptography. Quantum computing function. Quantum
computing blockchain. Quantum computing fundamentals. Quantum computing discovery. Quantum
computing skill development.
Quantum computing simple terms. Quantum computing finance. Quantum computing investing. IBM
quantum computer. Quantum computing healthcare. Quantum computing stocks. Quantum
computing hacking. Quantum computing government. Quantum computing training.
The map of Quantum computing. Mapping Quantum computing. Quantum computer race. Quantum
Supremacy. Quantum computing terms. Quantum computing updates. Quantum computer absolute
zero. Quantum computing startups. Quantum computing speed. Quantum computing mechanics.
Quantum computing course.
Quantum computing open source. Quantum computing degree. Quantum computing online. Quantum
computing lectures. Quantum computing labs. Quantum computing limitations. Quantum computing
leaders. Quantum computing learning. Quantum computing meaning. Quantum computing masters.
Quantum computing medicine.
Quantum computing news. Quantum computing network. Quantum computing optimization.
Quantum computing overview. Quantum computing oracle. Quantum computing programming.
Quantum computing professors. Quantum computing paper. Quantum computing public. Quantum
computing questions and answers. Quantum computing q & a. Quantum computing report.
Quantum computing regulations. Quantum computing roadmap. Quantum computing research.
Quantum computing risks. Quantum computing revolution. Quantum computing salary. Quantum
computing scientist. Quantum computing timeline. Quantum computing workshop. Quantum
computing python. How quantum computers work. Who invented quantum computers?
TABLE OF CONTENTS
Introduction
Chapter 1: The Physics of Quantum Computers
Explaining The Fundamental Principles Of Quantum Mechanics
Wave-Particle Duality
Probabilistic Nature of Quantum Mechanics
Quantization of Energy Levels
Wave-Particle Duality
The Born Rule
The Classical
Entering Quantum
Quantum Measurements and the Role of Observables
Peering into the Quantum Realm
Interacting with Qubits
Quantum Operators and Gates
Quantum Measurements and Observables
Interpreting Eigen's Value
The Uncertainty Principle
Time Complexity
Bits vs. Qubits - A Tale of Two Computations
Classical Computers (Bits)
Quantum Computers (Qubits)
Classical Computers
Quantum Computers
Classical Computers
Quantum Computers
Superposition
Entanglement: Quantum Ties that Bind
Quantum Tunneling: Navigating the Energy Landscape
Spin
Intrinsic Nature of Spin
Angular Momentum at Play
Relevance to Wave-Particle Duality
Bell's Inequality and the Deep Mysteries of Entanglement
Bell's Inequality
Local Realism vs. Quantum Computing Power
From Sci-Fi to Quantum Reality
Bell, Qubits, and the Quantum Computing Revolution
Chapter 2: Building Quantum Computers
The Quantum Foundation
Quantum Computing Architecture
Components of Quantum Computers
Spin
NMR
Spin's Dance
Concrete Examples of Spin
Quantum Computing Architectures
Different Approaches in Quantum Computing
Superconducting Circuits
Trapped Ions
Topological Qubits
Comparing the Quantum Titans
Superconducting Circuits
Trapped Ions
Topological Qubits
A Glimpse Beyond the Horizon
Creating Scalable Quantum Networks
Fault-tolerant Quantum Computing
Scalable Quantum Networks
NV Center-in-Diamond
Quantum Computing with Neutral Atoms
Optical Traps
The Technical Challenges and Advancements associated with each Technology
1. Superconducting Qubits
2. Trapped Ions
3. Topological Qubits
4. Neutral Atoms and Optical Traps
Evaluating Economic Factors, Power Capabilities, and Prevalence
Solving Error Correction
Error Correction Techniques and Codes
Challenges in Achieving High-Fidelity Qubit Operations
Quantum Hardware & Ecosystem
Material Sciences
10 Milestones Before Mastery
Molecular Quantum Mechanics
Quantum Software
Quantum Software vs. Classical Software
Software-Hardware Interface
Driving Quantum Computing Forward
Ecosystem Expands in All Directions
The Power and Practice of Quantum Simulation
Chapter 3: Dispelling Quantum Misconceptions
Unmasking Illusions in the Enchanted Forest
Quantum Technology is Mainstream
Quantum Computers are Only for Academic Research
Trees of Different Species Coexisting in the Forest
Different Tools in an Explorer's Toolkit
Do You Need a PhD to Speak the Language?
Can You Solve This Puzzle?
The Fallacy of Immortal Decryption
Quantum Isn't Always Quicker
Are Quantum Computers Crazy Fast?
The Guardian of Many Worlds
Emerging from the Enchanted Forest, Enlightened
Next Chapter
Bonus
Chapter 4: Quantum Algorithms
Introduction to Quantum Algorithms and their Unique Characteristics
The Forces Behind Quantum Algorithms
The Mathematical and Computational Dance
Real-World Implication
Characteristics of Quantum Algorithms
Concept of Quantum Parallelism
The Principles of Superposition And Entanglement
The Contrast Between Classical and Quantum Algorithms
Algorithms Can Provide Exponential Speedup
Exploring Notable Quantum Algorithms
Shor's Algorithm
Exploration of Grover's algorithm
Introduction to the Quantum Fourier Transform (QFT)
(QAOA) Explore the Quantum Approximate Optimization Algorithm
QPE
Solving Linear Algorithms
Example Implementation of the HHL Algorithm
Applications of Quantum Algorithms in Various Fields
Chapter 5: Practical Applications of Quantum Tech
Where Theory Meets Practice
Existing and Potential Applications of Quantum Computing
The Quantum Advantage in Accelerating Breakthroughs
An In-depth Exploration
The Quantum Edge in Personalized Medicine
The Quantum Revolution in Advanced Materials
A Quantum Leap in Financial Forecasting
The Quantum Future of Marketing Strategy
Harnessing the Quantum Realm for AI and NLP
The Quantum Quandary in Cybersecurity
Grappling with the Subtleties of Quantum Computing's Challenges
Quantum Leap Forward
Chapter 6: Mapping Quantum Computing
The Future of Tech and Careers
How Quantum Changes Key Jobs and Businesses
Quantum Toolkit
Websites and Platforms
Here Are Five Websites Where You Can Access Quantum Simulators
AI for Business
Navigating the Quantum Career Landscape
Adapting to Quantum's Ever-Evolving Terrain
Chapter 7: Quantum Cryptography
Diving into the Secure Submarine Depths
The Basics of Cryptography
The Need for Deeper Security
Quantum Key Distribution (QKD)
Data Predators
No-Cloning Theorem
The Quantum Blueprint of Secure Communication
Setting the Stage
The Quantum Key
Encoding and Transmission
Eavesdropping Alert
Mathematical Assurance
Finalizing the Key and Secure Communication
Quantum Measurement
Benefits of Quantum Cryptography
Challenges and Flaws
Quantum Hacking
Countermeasures
Quantum Cryptography in Use
Quantum Cryptography and Blockchain
The next generation of submarines
Safeguarding our Secrets in the Deep Ocean of Data
Chapter 8: Quantum Machine Learning
Soaring Through the Quantum Skies
Blending Quantum Computing with Machine Learning
Combining Quantum Computing and Machine Learning
Highlighting Benefits and Features of QML
Quantum-Inspired Machine Learning Algorithms
Dynamics of Machine Learning Algorithms
Quantum Support Vector Machines (QSVMs)
Quantum Neural Networks (QNNs)
Quantum Clustering
Advanced Quantum Algorithms
Flying High on High-Dimensional Data
A Grand Unification, QSVT
TensorFlow Quantum
Comparative Advantages of Quantum Algorithms
Navigating Quantum Patents
Patent Success in QML
Advantages and Challenges of Quantum Machine Learning
Improved Computational Efficiency
Big Data Analytics
Simplifying Complex Data
A Stepping Stone for Advanced Quantum Computation
Mastering the Quantum Data Flow
Navigating Through Quantum Turbulence
Looking Forward
Chapter 9: Ethics and Social Implications
Ethical Implications of Quantum Computing
The Balance
Ethical Risks
Communication and Teaching
The Challenge of Explanation
Quantum and Business
Thought Experiment
Embracing the Quantum Future
Thoughtful Questions for Reflection
Conclusion
References
Introduction
In human history, there have been moments—just mere blips in the timeline
of our existence—where our understanding of the universe has been
radically changed. Think of the moment Copernicus realized that the Earth
revolves around the sun or when Einstein unveiled the theory of relativity.
We're at the threshold of yet another transformative shift—the world of
quantum computing.
The buzzwords "quantum" and "computing" have likely found your ears,
either in passing conversations or in the latest tech articles. But what does it
all mean? The answer is both profound and simple: quantum computing,
with its flashy principles of superposition and entanglement, promises to
redefine our future. From altering how we approach complex problems to
reshaping industries, the quantum wave will leave no stone unturned.
Why trust my voice over the hundreds of existing quantum forums?
Because I've navigated the maze of quantum jargon and emerged not just
unscathed but inspired. I've translated this highly technical subject into
actionable insights for tech companies, classrooms, and entrepreneurs. My
journey, filled with breathtaking turns and stops at enlightening landmarks,
has been distilled into this book. As your guide, I will walk you through the
world of quantum computing, making sure that complex concepts are
accessible by breaking them down into simple language.
But the larger question remains: Why should you care? The reality is that
understanding quantum computing is about more than just keeping up with
new trends. It's about preparing for a future that's unfolding before us, a
future where those not in the know might find themselves playing catch-up.
Whether you're a student, a teacher, a tech-lover, or just very curious about
cool new stuff, this book is for you. You're not just going to read about
quantum computing; you're going to visit The Quantum Continent to
explore the digital landscape and find treasure!
Just ten years ago, this was the stuff of science fiction. Today, it's a reality
that has enormous potential to shape industries, solve unlikely problems,
and even transform global issues like hunger or diseases. Quantum
computing will change the world in ways we have yet to imagine. At the
same time, quantum systems are still in their early stages. Many challenges
must be overcome before we can realize its full potential. Building a
quantum computing empire will require a group effort from governments,
academia, and industry.
This book is your compass to gain a comprehensive introduction to the field
of quantum computing, from its foundational principles to its practical
applications. We'll start with the basic concepts of quantum mechanics and
how they relate to quantum computers vs. classical computers. From there,
we'll tour the architecture of quantum computing: hardware and software
components, including qubits, quantum gates, and quantum algorithms.
You'll also explore the power of quantum computing in cryptography,
machine learning, and optimization. I discuss how quantum computers can
solve problems that classical computers can't and explore the potential
impact of quantum computing on careers and AI.
In addition to the technical aspects of quantum computing, the last chapter
is a thought exercise on the ethical and societal implications of this
emerging technology. You will think about issues like data privacy,
cybersecurity, and the potential impact of quantum computing on
employment, government, and your neighbors.
By the end of this book, you'll have a solid grasp of quantum computing
fundamentals, applications, and as a service. You won't just be observing
from the sidelines—you'll actively participate in the excitement with our
included Quantum Toolkit. It's packed with hands-on resources waiting for
you to take control.
So, with heartfelt enthusiasm, I invite you to join me on this quest. This isn't
just another tech read; it's a shared adventure into the quantum environment
that promises knowledge and a vision of what's to come. Ready? Let's unite
and unlock the future together.
Chapter 1
The Physics of Quantum
Computers
While classical computing has revolutionized our world, making the
unimaginable tangible, it remains confined by its foundational, classical
laws. Yet, beneath this camouflage lies a more mystifying empire governed
by the characteristics of quantum mechanics.
Embark on an expedition into (my analogy) the Quantum Continent—a
land where the usual rules give way to the captivating quirks of the
quantum world. Together, we'll navigate the vast plains of superposition,
climb the towering cliffs of quantum tunneling, and sail the unpredictable
rivers of quantum entanglement.
By the time we're through, you'll have a comprehensive understanding of
quantum mechanics and a profound appreciation for the foundation upon
which the future of computing stands.
Prepare yourself, intrepid explorer. The Quantum Continent awaits, and
we're about to set foot on its mesmerizing terrains, uncovering secrets that
have puzzled and amazed the brightest minds for decades. Let the
expedition begin!
Explaining The Fundamental Principles Of Quantum
Mechanics
Introduction to Quantum Mechanics and Quantum Computing
Quantum mechanics is a branch of physics that delves deep into the
behavior of particles at the atomic and subatomic scales. This revolutionary
field of study has shifted our perspective, transforming our understanding of
the fundamental nature of reality. While classical physics offers
explanations for the macroscopic world we see and interact with daily,
quantum mechanics takes us on a journey to the microscopic dimensions
where particles can exist in multiple states simultaneously, communicate
over vast distances instantaneously, and generally defy common sense.
Enter quantum computing, a burgeoning field that harnesses the mind-
bending properties of quantum mechanics to process information in ways
previously thought impossible. Traditional, or classical computers, use bits
as the smallest unit of data—either a 0 or a 1. Quantum computers, on the
other hand, make use of quantum bits, or qubits. Thanks to principles like
superposition and entanglement, these qubits allow quantum computers to
perform tasks and solve problems classical computers struggle with or
cannot handle.
Key Principles of Quantum Mechanics in Computing
1. Superposition: One of the cornerstones of quantum mechanics is
the idea of superposition. To envision superposition, consider the
famous thought experiment known as Schrödinger's Cat. In this mental
exercise, a cat inside a sealed box is simultaneously alive and dead
until we observe it, at which point it collapses into one of the states.
Similarly, a qubit can simultaneously be in a superposition of 0 and 1
states, collapsing into a definite state only upon measurement.
2. Qubits: As the fundamental unit of quantum information, qubits can
exist in a superposition of states. This capability allows quantum
computers to process a massive amount of data simultaneously,
potentially offering exponential speed-ups for specific problems
compared to classical counterparts.
3. Entanglement: Another counterintuitive but integral concept in
quantum mechanics is entanglement. When particles become
entangled, one particle's state instantly influences another's state,
regardless of the distance separating them. This interconnected
behavior is essential for quantum computers, enabling more complex
and intertwined computational processes.
Harnessing Quantum Mechanics in Computing:
The promise of quantum computing is profound. For instance, algorithms
like Shor's can factorize large numbers more efficiently than the best-known
classical algorithms—a capability with profound implications for
cryptography. However, building a functional quantum computer is no
small feat. Various technological approaches are being explored, from gate-
based ion trap processors manipulated by lasers to superconducting
processors at cryogenic temperatures, each with unique advantages and
challenges.
Quantum will initially seem perplexing; it represents a frontier of endless
possibilities. As our understanding of quantum mechanics deepens, and our
capabilities in quantum computing expand, we stand on the cusp of a new
computational era, potentially reshaping our technological and scientific
landscape.
Wave-Particle Duality
In the classical world we know, an object is defined by its intrinsic
properties. A football is a particle. It's tangible, solid, and you can kick it.
The ripples in a pond? They behave as waves, spreading outward in
concentric circles when a stone is tossed. But enter the domain of the
subatomic, and the lines blur. Quantum mechanics presents a thrilling
challenge to these notions.
Imagine, if you will, that same football, not just being kicked around the
field, but also creating ripples like those in a pond. It's a perplexing thought,
isn't it? This is the heart of wave-particle duality. In the quantum world,
entities like electrons don't confine themselves to being "just a particle" or
"just a wave." They might exhibit both characteristics depending on how we
measure or observe them.
Implications in Quantum Computing
When considering quantum computing, the essence of wave-particle duality
becomes incredibly potent. One must grasp this concept to leverage qubits'
true power. Qubits aren't confined to the strict binary states like classical
bits; thanks to the principles of duality, they can be both 0 and 1 until
measured.
Probabilistic Nature of Quantum Mechanics
Imagine standing at a crossroads on your expedition through the Quantum
Continent, a thick fog surrounding you. There are many paths leading in
different directions, but the fog hides where each one goes. You know that
once you start walking, one of the paths will become your reality, but until
that moment, all paths remain possibilities.
This analogy encapsulates the quantum world's inherent uncertainty. Unlike
flipping a light switch, where the outcome is a definite "on" or "off",
quantum mechanics leans into probability. One of the most famous thought
experiments to explain this is Schrödinger's Cat. A cat inside a sealed box is
simultaneously alive and dead, a superposition of states, until we open the
box and observe, collapsing the state to a single reality.
Implications in Quantum Computing
This understanding is vital for quantum computers, which thrive on this
uncertainty. Algorithms in quantum computers operate on the principle that
qubits can be in superpositions of states, allowing for vast computations in
parallel. When we finally "measure" or read out the qubit, it collapses to a
definitive state.
Quantization of Energy Levels
As you journey further into the Quantum Continent, imagine stumbling
upon a mystical jukebox. But there's a catch. The volume knob on this
jukebox doesn't allow for minute adjustments. You can't set it to level 2.5 or
3.8. Only whole numbers, discrete levels of volume: 1, 2, 3, and so on.
This is much akin to quantized energy levels in quantum mechanics. Atoms
and their electrons don't have a continuous energy spectrum. Instead, they
can only exist in certain, fixed energy states. Much like the jukebox, there's
no "in-between."
Significance in Quantum Computing
The importance of quantization becomes clear when we think about qubits
and their manipulations. By understanding and harnessing these discrete
energy levels, quantum computers can effectively manipulate qubits,
transitioning them between their quantized states to perform computations.
As we navigate the Quantum Continent, observing its marvels, these
principles serve as our guiding compass. They shed light on the mysterious
behaviors of the quantum realm and, more crucially, lay the foundation for
understanding quantum computers' immense power and potential.
Wave-Particle Duality
Beyond Traditional Boundaries
Imagine you're wandering the landscapes of our Quantum Continent and
stumble upon an enigmatic creature: a chameleon. This isn't any ordinary
chameleon, though. One moment it's solid, moving on the ground like any
typical reptile. But in an instant, it transforms, flowing like water, gracefully
navigating the terrain's intricacies. This chameleon's ability to change its
nature represents the wave-particle duality found in quantum mechanics.
However, pausing and reflecting is essential as you marvel at this creature.
Quantum mechanics gives us the tools to understand these unique
behaviors, harnessing them for quantum computing. Yet, the answers are
more elusive when it comes to more profound "why" questions about the
nature of such quantum phenomena. These mysteries still spark
philosophical and scientific discussions, reminding us that quantum physics
has many secrets yet to unveil.
Understanding the Dual Nature
The quantum realm is rife with counterintuitive phenomena, and wave-
particle duality is a prime example. It stems from experiments where
particles, like electrons, displayed wave-like behavior under certain
conditions. In the iconic double-slit experiment, for instance, electrons shot
toward a barrier with two slits produce an interference pattern on the other
side, as waves would. However, when observed more closely, these
electrons appear as distinct particles, landing as individual points.
Relevance to Quantum Computing
Wave-particle duality is pivotal to quantum computing for several reasons:
1. State Representation: The duality allows qubits to be in a
superposition, where they simultaneously exist in multiple states.
Unlike classical bits, which are firmly a 0 or a 1, qubits harness their
wave-like nature to represent a multitude of possibilities, enabling vast
parallelism in computation.
2. Interference as a Resource: Just as the double-slit experiment
showcases interference in waves, quantum algorithms utilize
interference to amplify correct solutions and diminish wrong ones.
Quantum computers leverage this duality-driven interference to arrive
at solutions faster than their classical counterparts.
3. Quantum Gates and Operations: The duality principle underpins
the very operations performed in a quantum computer. Quantum gates
manipulate qubits by exploiting their wave-like properties, causing
constructive or destructive interference, akin to merging or diverging
waves in an ocean.
Embracing wave-particle duality is akin to the chameleon's ability to adapt
and thrive in varied terrains on our Quantum Continent. This duality
provides the flexibility and power to outmaneuver traditional computing
boundaries in the quantum computing universe. It's not just a quirky
behavior but a foundational tenet that propels the essence of quantum
computational prowess.
The Born Rule
Navigating the Probabilistic Waters of the Quantum Continent
Deep within the Quantum Continent lies an enigmatic ocean. At first
glance, this ocean seems ordinary. However, its waters, representing
quantum probabilities, are guided by an unseen but profound force: the
Born Rule. Imagine standing at the shores of this vast expanse, peering into
its depths and trying to predict the pattern of the waves or the location of a
fish. The Born Rule provides the compass and the map to make sense of
this unpredictable realm.
Deciphering the Born Rule
Named after physicist Max Born, links the mathematical descriptions of
quantum systems to the probabilities of various observable outcomes. It
states that the probability of a particular measurement outcome corresponds
to the square of the magnitude (or absolute value) of the quantum system's
wave function in that specific configuration.
In simpler terms, imagine our Quantum Ocean having waves of varying
heights. The probability of finding a fish at a specific spot in this ocean isn't
determined merely by the height of the wave at that location but by the
square of its size. The higher the wave (or amplitude), the greater the
likelihood, but the relationship isn't linear—it's a power of two.
Significance to Quantum Computing
Understanding the Born Rule is paramount for several reasons:
1. Probabilistic Outcomes: Unlike classical computers, which output
definitive results, quantum computers operate in the realm of
probabilities. After processing information using qubits, when we
measure the outcome, the Born Rule determines the likelihood of each
possible result. Knowing this rule is foundational to making sense of
these outcomes.
2. Guiding Quantum Algorithms: Many quantum algorithms, like
Grover's and Shor's, inherently exploit this probabilistic nature. The
algorithms manipulate qubits to amplify the probabilities of correct
solutions, making them more likely to be observed. This amplification,
in essence, is guided by the principles of the Born Rule.
3. Measurement and Collapse: In quantum mechanics, measurement
leads to the collapse of the wave function, meaning the quantum
system settles into a definitive state post-measurement. In this context,
the Born Rule governs which state the system will most likely collapse
into.
As we navigate the Quantum Continent, the Born Rule is the guiding force
allowing us to predict and harness the probabilistic nature of the quantum
realm. Quantum computing, at its heart, thrives on uncertainty. But with
tools like the Born Rule, this uncertainty is not a hindrance but a
superpower. It's a dance of probabilities, where the choreography, complex
as it might be, unveils the potential to solve problems that were once
deemed insurmountable by classical standards.
Probabilistic Nature of Quantum Systems and Their
Implication in Quantum Computing
In the realm of classical physics, events unfold predictably. If we roll a ball
down a hill, we can estimate where and when it'll come to a stop. The rules
governing such macroscopic events are well-understood and deterministic.
However, when we zoom into the quantum level, this predictability
vanishes, replaced by a cloud of probabilities.
Wave Functions and Probabilities
At the heart of quantum mechanics is the wave function. Think of it as a
mathematical representation that provides a probability distribution of a
quantum system's possible states. Instead of saying an electron is "here," we
say it's "most likely here." The wave function doesn't describe the exact
position of an electron, for example, but instead gives us a probability of
finding it in various locations around an atom.
The Real-World Electron Dilemma
To get a grip on this, consider electrons in an atom. Unlike planets orbiting
the sun in distinct paths, electrons exist in "clouds" called orbitals around
the nucleus. If you've ever seen an atomic diagram with fuzzy clouds
encircling the center, those represent areas where we're most likely to find
an electron. But remember, "most likely" doesn't mean "definitely." It's all
about chances.
Quantum Computing's Reliance on Probabilities
So, how does this probabilistic nature tie into quantum computing?
Quantum computers operate using qubits, which can be in a superposition
of states, unlike classical bits that are always either a 0 or a 1. Essentially, a
qubit represents both 0 and 1 simultaneously until it's measured. Upon
measurement, the qubit collapses to a 0 or a 1 based on its probability
distribution.
This probabilistic behavior gives quantum computers their edge. Being able
to process multiple possibilities simultaneously means quantum algorithms
can explore many solutions at once. But it also introduces challenges. For
instance, the results from quantum computations can be probabilistic,
meaning we might have to run them multiple times to get a reliable
outcome.
Furthermore, harnessing this probabilistic nature in quantum algorithms can
offer computational advantages. When faced with tasks like simulating
quantum systems or navigating vast computational landscapes to find
optimized solutions, quantum computers leverage their probabilistic
behavior to process and evaluate numerous possibilities concurrently.
Grasping the probabilistic essence of quantum systems is paramount to
understanding how quantum computers function. This isn't randomness for
the sake of chaos; it's a foundational principle of the quantum world. And as
we build more advanced quantum computers, this very principle promises
to revolutionize computation as we know it.
Quantization of Energy Levels - The Ladder of
Quantum States
Imagine you're climbing a ladder. Each rung represents a specific height, a
distinct step above the ground. You can't stand halfway between two rungs;
you're either on one rung or another. In much the same way, in the quantum
world, energy isn't continuous but is divided into distinct levels, much like
the rungs on a ladder. This is the principle behind the quantization of energy
levels.
Atoms and Their Discrete Dance
Atoms, the building blocks of matter, contain a nucleus surrounded by
electrons. These electrons don't whirl around the core any which way they
want; instead, they exist in specific "shells" or "orbitals," each associated
with a precise energy level. Think back to the electron clouds we talked
about: these aren't random misty patches but are tied to specific energy
rungs.
But why does this matter for quantum computers?
Quantum Bits and Energy Levels
Quantum computers use qubits, which, in some architectures, are derived
from properties of electrons, such as their spin. The distinct energy levels
play a pivotal role here. When we manipulate a qubit, we're essentially
navigating it through these quantized energy levels. The qubit's information
is encoded based on these levels, and the precision with which we can
control and measure these states is central to the functioning of a quantum
computer.
For example, in quantum computers that use trapped ions as qubits,
individual ions are manipulated using lasers. By carefully tuning these
lasers, we can make an electron in an ion jump between different energy
levels, effectively flipping a qubit between its 0 and 1 state. Here, the
quantized nature of energy levels is a blessing, providing a clear, distinct
boundary between the two states of the qubit.
The Power of Discreteness in Quantum Computing
You might ask: why is the quantization of energy so exciting for quantum
computing? It ensures precision. In classical computing, we're used to bits
being either in a 0 or a 1 state. In quantum computing, qubits can be in a
superposition of 0 and 1, but when we measure them, they collapse to one
of the states. These definitive, quantized energy levels provide a clear
distinction between these states, enabling us to accurately read the results of
quantum computations.
Furthermore, how quantum algorithms are designed often leverages the
quantized nature of quantum systems. For example, quantum algorithms
might exploit energy level transitions, making quantum computers perform
specific tasks exponentially faster than classical computers.
While the ladder of quantized energy might appear as mere steps, they are
foundational in the world of quantum computing. They provide the discrete
scaffold on which the fascinating and often counterintuitive realm of
quantum mechanics rests and where the magic of quantum computing
begins to shine. So, as we dive deeper into quantum computers, remember
that these discrete energy levels play an instrumental role, choreographing
the dance of qubits.
A Dramatic Shift in Understanding
Quantum mechanics challenges classical deterministic laws and introduces
probabilistic behavior at the microscopic level. At the heart of this
divergence is how these two branches of physics treat matter and energy.
The Classical
Predictability and Determinism
In the realm of classical physics, which governs most of our everyday
experiences, the world behaves in a deterministic way. If we throw a ball
upwards, classical physics lets us calculate, with a high degree of accuracy,
when and where it will land. The same goes for other physical systems; we
can predict their future states if we know the current state and the forces
acting upon them. This predictability comes from the deterministic nature
of classical laws—Newton's laws of motion or Maxwell's equations, for
instance.
Entering Quantum
A World of Probabilities
However, when we zoom into the atomic and subatomic scales, classical
rules begin to break down. Here, the world operates not on strict
determinism but on probabilities. Particles like electrons don't have definite
positions or velocities but exist in a superposition of many possibilities until
they are measured. It's not just about tiny particles behaving oddly; it's a
profound shift in understanding that challenges our intuitions.
The wave-particle duality we touched on earlier is a cornerstone of this.
Electrons can behave both as particles and waves. Unlike a classical object,
which is either a wave (like a sound wave) or a particle (like a football),
quantum entities defy such categorization.
The Heartbeat of Quantum Computing
So, how does all this feed into quantum computing? The reason quantum
computers can process vast amounts of data simultaneously isn't magic—
it's quantum mechanics in action.
Remember the principle of superposition? That's the ultimate quantum idea.
In classical computers, a bit is either a 0 or a 1. But a qubit, governed by
quantum rules, can exist in both states, allowing quantum computers to
process multiple possibilities at once.
Entanglement, another inherently quantum phenomenon, lets qubits become
interconnected in ways that classical bits never can. This lets quantum
computers execute complex algorithms that rely on these entangled states,
tackling problems classical computers would take millennia to solve.
Moreover, the probabilistic behavior of quantum systems, which might
seem like a hurdle at first, becomes an advantage in quantum computing.
Particular computational tasks, like optimization problems, are inherently
probabilistic, and the quantum computers' nature is tailor-made to handle
them.
In the vast tapestry of the universe, classical physics paints the broad
strokes, the mountains, rivers, and forests. But quantum mechanics fills in
the intricate details—the delicate patterns on a butterfly's wing or the
fleeting shimmer of a dewdrop at dawn. As we push the boundaries of
computation, these traits, these quantum characteristics, grant quantum
computers immense power. So, as you ponder over the marvel of quantum
computing, remember: its heart beats in quantum time, dancing to the
rhythm of a universe more intricate and wondrous than classical eyes can
see.
Quantum Measurements and the Role of Observables
Navigating the Quantum Terrain
Picture yourself as an intrepid explorer on the vast quantum continent. This
realm is unlike any you've ever traversed. With every step, the landscape
shivers with possibilities. In your hand, you hold a unique compass, capable
of indicating not just North and South but also a blend of "quantum North"
and "quantum South". This peculiar behavior represents our qubit. Thanks
to the wonders of superposition, it can point simultaneously in both
directions. But, when the moment comes to commit – when you decide
your path forward – the compass decisively chooses one direction. This
mirrors the action of a qubit: remaining in superposition until measured, at
which point it locks into a definite state.
The Quantum State and Its Observation
In classical mechanics, if you have a spinning top, you can easily measure
its position and momentum without any hiccups. The act of measuring
doesn't change the properties of the system. However, quantum mechanics
throws us a curveball.
As you might recall from our earlier discussions, a qubit exists in a
superposition of states until measured. This doesn't mean the qubit is
simultaneously in all possible states; rather, it's in a state that is a
combination of these possibilities. Imagine being on the Quantum
Continent and standing at a crossroad without a clear path marked; the
qubit's actual position remains unknown until you decide to check.
When you measure a qubit, you force it into one of its possible states. The
qubit will then "collapse" into either a |0 ⟩ state or a |1 ⟩ state. The
intriguing bit is that, until the measurement, you cannot predict with
certainty which state it will end up in.
Peering into the Quantum Realm
Observables
In the world of quantum mechanics, when we talk about measuring, we're
discussing observables. Observables are specific properties of the quantum
system that can be measured. They are represented by operators
(mathematical entities) that act upon the quantum state (or the wave
function) to provide a specific outcome.
Consider the act of measuring the spin of an electron in a magnetic field.
The electron's spin might be either in alignment with the field, known as a
spin-up state, or opposite to the field, known as a spin-down state. These
two possible outcomes are the observables for the electron's spin.
However, here's where the Quantum Continent's mysterious terrain shows
its peculiar nature. If you were to measure the spin of an electron that is in a
superposition of both spin-up and spin-down states, quantum mechanics
won't provide a deterministic answer. Instead, you'll get probabilities. The
electron might be 70% likely to be in a spin-up state and 30% in a spin-
down state. The exact outcome is revealed only upon measurement.
Interacting with Qubits
A Glimpse of the Quantum Lab
How do scientists actually "see" or interact with qubits, you wonder? Let's
get a visual. While we can't "see" qubits like we see objects in our daily
lives, scientists employ sophisticated equipment to interact with them.
Consider a quantum lab like a master control room filled with specialized
machinery. Devices cooled to near absolute zero to create an environment
where quantum effects dominate. Lasers and microwave pulses are shot at
specific particles, like electrons in a magnetic field or ions suspended in
traps, to manipulate and measure their states.
In the case of superconducting qubits, often used in quantum computers, the
qubits are tiny circuits made out of superconducting materials. When these
materials are supercooled, they allow current to flow without resistance.
The flow of electrons in these circuits, combined with magnetic fields,
produces quantized energy levels. Through precision control and
measurement equipment, scientists can manipulate the states of these qubits
and measure their properties.
Quantum measurement and observables guide us through the process of
extracting information from a quantum system. While the Quantum
Continent's landscapes may seem unpredictable and mysterious, the
principles of quantum mechanics, paired with cutting-edge technology,
provide the tools to navigate and understand quantum, bringing the promise
and potential of quantum computing ever closer to our grasp.
Quantum Operators and Gates
Navigating the Quantum Landscape
In our vast quantum continent, the familiar tools of maps, compass
adjustments, and traversing techniques become analogs to what scientists
use in the quantum world: operators and gates.
To better understand, let's build upon our analogy. In regular exploration, if
you want to reach a specific destination, you might need to adjust your
compass's calibration or decide on an exact route. Quantum operators and
gates perform a similar function. They help "calibrate" or "navigate" our
qubits to desired quantum states or to perform specific quantum tasks.
Quantum Operators: Think of these as compass calibrations.
They act on quantum states, transforming them from one to
another. Essentially, they represent the different interactions a
quantum system can have. If you've ever adjusted a compass
for a magnetic declination or noted prominent landmarks,
you've conceptually used an 'operator' to modify your
understanding of direction.
Quantum Gates: Analogous to pathways or routes in regular
terrain, quantum gates guide qubits through specific
transformations. When a qubit passes through a gate, it
undergoes a distinct change in its state, just as traversing a
mountain path might take you to a new altitude. There's a rich
variety of these gates, each executing a distinct operation and
uniquely guiding our quantum journey.
In the quantum continent, instead of trekking mountains or crossing rivers,
scientists guide qubits through specific sequences of gates, tweaking and
calibrating them with operators, aiming to get them to a specific state or
execute a particular quantum algorithm.
Quantum Measurements and Observables
Numerical Navigators
Now that we have our qubit adjusted and on the desired path, how do we
know where it stands? Here is where the climax of our quantum expedition
takes place: measurements and observables.
Imagine you're equipped with a multi-functional gadget while exploring –
let's call it a Quantum Swiss Army Knife. This isn't an ordinary knife;
instead, each tool within it represents a different observable. Just as the
blade, corkscrew, or screwdriver in a traditional Swiss Army knife has a
specific function, each observable in our quantum knife is designed to
measure a unique property of our quantum entities, like position, spin, or
energy.
These observables are the mathematical "tools" that scientists deploy to
extract information from quantum systems. They're not tangible instruments
in the usual sense, but mathematical frameworks or operations. When
applied to a quantum entity, such as an electron, an observable predicts the
potential outcome of a specific measurement, like the electron's position or
spin orientation.
These "observables" are a scientists' best friends in the quantum realm.
Remember, these tools are primarily mathematical methods. They let
scientists predict and understand how quantum systems behave. However,
tangible instruments await us in Chapter 2 when we move from the
theoretical world to the experimental labs. Tools like the Stern–Gerlach
apparatus or spectrometers are essential for observing and measuring
quantum properties. But more on that when we discuss the architecture of
quantum computers!
In quantum computing, understanding these mathematical observables is as
pivotal as a traveler knowing how to utilize each tool in their Swiss Army
knife. Just as each instrument in the knife is vital for specific tasks in an
expedition, harnessing the nuances of these quantum observables is
fundamental for navigating and extracting valuable information in the
quantum computing world.
Interpreting Eigen's Value
Eigenvalues and Eigenstates
The word "eigen" is German for "characteristic" or "own." In quantum
mechanics, "eigen" refers to characteristic properties that belong inherently
to something. In mathematics, especially linear algebra, eigenvalues and
eigenvectors (or eigenstates in quantum mechanics) arise when considering
linear transformations. Essentially, they highlight the most "characteristic"
behaviors of that transformation.
So, "eigen" isn't a tool or process per se, but rather a descriptor. When we
say "eigenvalue" or "eigenstate," we're talking about characteristic values or
states of a system. In quantum mechanics, the eigenstates represent the
possible states in which a quantum system can be found when measured,
and the eigenvalues are the possible outcomes or readings of that
measurement.
Essentially, eigenvalues and eigenstates help describe quantum systems'
inherent characteristic behaviors. They're fundamental to understanding
how quantum systems evolve and how measurements yield specific
outcomes.
To venture deeper into our quantum journey, imagine standing atop a cliff,
looking out across our expansive quantum landscape. You notice particular
points or peaks that stand out. In quantum mechanics, these prominent
points resemble eigenstates — specific, well-defined states where a
quantum system can exist.
Consider our trusty compass. As we've used it to traverse the quantum
landscape, it has points where it settles naturally, giving a clear, precise
reading. In quantum lingo, when we measure a quantum system in an
eigenstate, we receive a precise outcome termed an eigenvalue. This
eigenvalue is the "reading" or value associated with that particular
eigenstate.
Let's relate this to something more familiar. In the world of music, think of
a guitar string. When plucked, it vibrates and produces a sound. This sound
is most intense and clear at particular frequencies. These frequencies are
like the eigenvalues, and the specific vibration patterns at these frequencies
can be considered eigenstates. Similarly, in the quantum world, particles or
systems exhibit distinct characteristic vibrations or states (eigenstates) that
correspond to specific outcomes or values (eigenvalues) when measured.
Why is this concept crucial for quantum computers? Scientists and
physicists play around with these eigenstates to perform computations when
processing information. They manipulate qubits to exist in particular
eigenstates that, when measured, provide us with the desired outcomes, or
eigenvalues. Understanding eigenstates and eigenvalues is like
understanding the language or signs of this foreign land, ensuring that we
can interpret and manipulate the quantum landscape effectively.
The Uncertainty Principle
Nature's Boundary on Precision
As we travel deeper into the quantum wilderness, a fundamental principle
confronts our intuitive understanding of the world: the Heisenberg
Uncertainty Principle. Imagine standing at the base of a steep cliff, looking
up, and knowing there's a limit to how high you can see clearly without
specialized tools. In our quantum exploration, the Uncertainty Principle is
that imposing cliff, reminding us that nature sets certain boundaries even in
this precise domain.
Heisenberg's Startling Revelation
Werner Heisenberg, a titan of quantum mechanics, unveiled a fascinating
truth about the quantum world in 1927. He said there's a rule in the world of
tiny particles: The more precisely we know a particle's position, the less
precisely we can know its momentum (mass multiplied by its velocity), and
vice versa. It's a fundamental quirk of the quantum world, where some
things just can't be precisely known at the same time.
Implications for Measurement Outcomes
In our quantum expedition, this means that our quantum compass – no
matter how advanced or precise – will always have a degree of uncertainty
when reading pairs of coordinates simultaneously. In the context of
quantum computing, this is a game-changer. If classical bits are like clear
signposts on a well-trodden path, qubits, with their built-in uncertainty, are
like the shifting sands of a desert, changing their nature the moment we try
to observe them too closely.
But why does nature impose this restriction? While the exact reason
remains one of the profound mysteries of quantum mechanics, some
physicists suggest that it's a consequence of the wave nature of particles.
Others view it as a result of the fundamental constraints of the universe
itself.
One thing is for sure. This principle doesn't arise from our inadequacies in
measurement tools or techniques. It's a fundamental aspect of nature,
embedded deep within the fabric of the universe.
A Gift in Disguise?
At first glance, the Uncertainty Principle might seem like a frustrating
constraint; but in the realm of quantum computing, it's something of a
double-edged sword. While it introduces challenges, it also opens up a
world of possibilities. The probabilistic nature of qubits arises from this
principle, which enables quantum computers to process vast amounts of
information at once, making them far more powerful than their classical
counterparts for specific tasks.
While the Uncertainty Principle reminds us of nature's boundaries, it also
proves the vast potential and intrigue of the quantum field. As we gear up to
make sense of quantum computing, we must understand and respect these
boundaries while harnessing the opportunities they offer.
Time Complexity
A Glimpse into the Quantum Clock
As we progress across the quantum continent, imagine we're setting out on
specific trails – these trails represent algorithms. Now, just as one might
take a direct route through a forest because it's the quickest way to a
destination, we often choose algorithms based on their efficiency.
At the heart of evaluating this efficiency is a concept called time
complexity. Think of it as a measure of the pace at which we can traverse
these quantum trails. Specifically, time complexity refers to the efficiency
of algorithms with respect to the size of their inputs. In simple terms, it's a
measure of how an algorithm's running time (or sometimes the space used)
grows as the size of the input grows.
But here's where the quantum landscape showcases its unique terrain:
Quantum algorithms can, in certain instances, traverse these trails faster
than classical computers. Thanks to principles like superposition and
entanglement, they can process vast amounts of information at once. This
enables them to solve problems in fewer steps.
However, remember, the quantum realm is filled with nuances. While
quantum algorithms can offer speed-ups for specific problems, they aren't
universally faster. Chapter 4 promises a treasure of algorithm gems. As we
further navigate this quantum land, understanding time complexity helps us
gauge the potential of quantum computers in reshaping computational
horizons.
Bits vs. Qubits - A Tale of Two Computations
Quantum Computing with Classical Computing
Meet the classical bit and the quantum qubit: two players on the
computational stage. One is the tried-and-true backbone of every device
you've ever used; its language is binary, straightforward—zeros and ones.
The other? A trailblazing headliner capable of quantum superposition and
rewriting the rules of what's possible in computing. Prepare to witness a
duel of traditions vs. innovations as a new challenger faces the classical
champion in the information arena.
Visual/Table 1: Basic Architectural Difference - Bits vs. Qubits
Classical Computers (Bits)
Nature: Deterministic
State: 0 OR 1
Information Processing: Sequential
Primary Mechanism: Electric current represents 0 or 1
Quantum Computers (Qubits)
Nature: Probabilistic
State: 0 AND 1 (Superposition)
Information Processing: Parallel
Primary Mechanism: Quantum state of a particle (e.g.,
electron's spin)
Classical Computers
At the heart of every classical computer is the binary system, where
information is represented using bits that can be either 0 or 1. Think of them
as light switches: they are either off (0) or on (1). The computations in
classical computers are predominantly sequential, meaning tasks are
processed one after the other. The physical manifestation of a bit in
hardware typically relies on electrical currents. When a current flows, the
bit is '1'; when there's no current, it's 0'.
Quantum Computers
Quantum computers revolutionize this foundational concept. Instead of bits,
they use qubits. Imagine you have a spinning top instead of a simple light
switch. This top can spin clockwise (0), counter-clockwise (1), or any state
in between, representing the principle of superposition, where qubits exist
in a state of 0, 1, or both. This unique trait allows quantum computers to
process information in parallel. Multiple tasks or calculations can be tackled
at once, making them much more powerful. The manifestation of a qubit
isn't through electric current; instead, it is often through a particle's quantum
state, like an electron's spin. This probabilistic nature allows quantum
computers to explore multiple solutions and why they're so groundbreaking
in computing.
While a classical explorer has to check one path at a time in the vast
continent, a quantum explorer can tread numerous paths, increasing the
chances of finding treasures (or solutions) much more efficiently.
By understanding this foundational difference in architecture, one can begin
to grasp the transformative potential that quantum computers bring to the
table.
Visual/Table 2: Problem-Solving Capacity - Linear vs. Exponential
Classical Computers
Problem Scaling: Linear (For many complex problems)
Speed: Fast for everyday tasks, slower for computationally
intensive tasks
Limitation: Exponentially increasing time with complex
problems (e.g., factorizing large numbers)
Quantum Computers
Problem Scaling: Exponential (For specific problems)
Speed: Potentially much faster for specific computationally
intensive tasks
Advantage: Can solve certain problems in polynomial time,
which classical computers solve in exponential time (e.g.,
quantum factorization)
Classical Computers
In traditional computing, as problems grow in complexity, the time it takes
for them to be solved often increases in a straightforward manner. Imagine
processing 10 units of data in 10 minutes. If it scales linearly, 20 units
would take 20 minutes, and 30 units would take 30 minutes. It's predictable,
much like searching through a small box of items one by one.
However, classical computers face a significantly steeper challenge for
some complex issues. The time required can shoot up exponentially. This is
like trying to find a needle in a haystack. Every time the haystack doubles
in size, the time it takes to search can also double. While this linear and
predictable increase might be manageable for smaller haystacks (or simpler
problems), searching becomes a monumental task as the haystacks expand.
While classical computers excel at many of our day-to-day tasks, they falter
when faced with factorizing massive numbers — tasks that could take
years, decades, or even longer to complete.
Quantum Computers
Now, enter the world of quantum computing. For specific problems,
quantum computers don't just incrementally improve the speed; they change
the scale of problem-solving altogether. Returning to our haystack analogy,
imagine if, instead of searching straw by straw, you had a magical tool that
could probe half the haystack at once, then a quarter, and so on. This is an
oversimplified representation of the exponential advantage of quantum
computers. For tasks they are tailored to, such as quantum factorization,
they can solve problems in polynomial time. Simply, the relationship
between the size of a problem and the number of steps (or "time") it takes to
solve it. A feat unimaginable for classical machines dealing with the same
issue.
In our Quantum Continent, a classical explorer might comb through
methodically, one region after another, the quantum explorer can rapidly
zoom into areas of interest, making discoveries at a pace the classical
explorer could only dream of.
This potential for exponential problem-solving showcases quantum
computers' vast, transformative potential, especially for problems that have
long been deemed unsolvable within reasonable timeframes by classical
standards.
Visual/Table 3: Quantum Entanglement - The Deep Bond of Qubits
Classical Computers
Interrelation: Bits operate independently.
Information Exchange: Direct and separate pathways.
Effect: No change in one bit's state can affect another's.
Quantum Computers
Interrelation: Qubits can be deeply entangled.
Information Exchange: Instantaneous correlation, regardless
of distance.
Effect: State change in one qubit can immediately affect the
state of another entangled qubit.
Classical Computers
Imagine classical bits as individual light switches in a room. Each switch
operates independently; turning one on or off doesn't affect the others. The
flow of information between bits is clear-cut and precise, much like two
people passing notes to each other. While these switches (or bits) are
incredibly efficient at rapidly flicking on and off, representing vast amounts
of digital data, they cannot blend or entangle their states. This independence
makes their operations lightning-fast and reliable for most tasks we use
computers for today, but it also sets boundaries on the types of complex
computations they can handle.
Quantum Computers
Imagine two tuning forks placed in separate rooms of a vast mansion. When
one is struck and begins to resonate, the other instantly vibrates at the same
frequency, regardless of the walls and distances separating them. This
harmonious connection is defined as 'quantum entanglement.' Qubits, once
they enter this synchronized resonance, form a deep bond. A vibration in
one qubit can instantly reverberate in its entangled partner, even if they're
situated in distant wings of the mansion. This isn't just acoustic harmony;
it's an immediate, profound connection, surpassing any classical
understanding of sound or resonance. With unparalleled proficiency,
quantum computers leverage this unique quality of qubits and address
problems with interlinked data points.
In our exploration analogy, while a classical explorer would set up
individual communication channels with each team member, ensuring
messages are relayed one at a time, a quantum explorer possesses a magical
walkie-talkie. With this device, when one team member discovers
something, others instantly gain insights, even if they're miles apart,
enhancing their collective understanding of the landscape.
Entanglement represents one of the most groundbreaking and mysterious
facets of quantum mechanics, propelling quantum computers into a realm of
computation that classical machines can't access.
Contrasting quantum directly with classical systems can sharpen our
understanding. This distinction isn't merely in the 'hardware'; but also in
how they process information. We now understand that 'quantum' doesn't
equate to 'better.' It signifies 'different.' While quantum computers excel in
certain areas, classical machines remain irreplaceable in others. This
knowledge will be our compass as we navigate the quantum domain,
highlighting where quantum truly offers a unique advantage.
Superposition
The Quantum Dance of Possibilities
Imagine a forest where every single tree showcases the vibrant green leaves
of summer, the golden hues of autumn, the bare branches of winter, and the
budding blossoms of spring. Each tree embodies all seasons at any given
moment, not committing to just one. However, the moment you decide to
capture a photograph of an individual tree, it chooses a single season to
display, as if it had been in that state all along. This magical forest is much
like the world of qubits in a superposition state: existing in multiple
possibilities at once, but choosing a definite state only upon observation.
This phenomenon is not just a quirk or an abstract idea. It's anchored deeply
in the very fabric of quantum mechanics. Quantum entities, like electrons,
don't commit to a particular state until observed. They dance among the
realm of possibilities until a conscious observer decides to measure them.
When measured, these quantum entities collapse to a single, definitive state.
The true power of superposition becomes even more evident when
considering its computational implications. When a qubit is in
superposition, it can process information as if it were jointly in the 0 and 1
states. Think back to our forest. If each tree represents a qubit, then a forest
with two trees (or two qubits) can collectively represent four seasons or
states (00, 01, 10, 11). What about a forest with three trees? That's eight
simultaneous states. With every additional qubit, the number of possibilities
or superposed states doubles. This exponential expansion allows quantum
computers to explore vast computational paths in a single command.
Now, you might wonder, why does nature allow this? The exact 'why'
remains one of the profound mysteries of quantum mechanics. But what's
clear is that this ability is anchored in the very nature of the quantum world.
Quantum systems, by their intrinsic properties, defy our classical intuitions.
They dance in a realm of probabilities until a specific observation forces
them into a definite state, much like the trees of our enchanted forest
choosing a season upon the click of a camera. This behavior offers quantum
computers the potential to solve problems deemed impossible for classical
machines, showcasing quantum's incredible power and magic.
Entanglement: Quantum Ties that Bind
In quantum, there's a phenomenon that even Einstein once labeled as
"spooky action at a distance." Entanglement is undoubtedly one of the most
mystifying yet fundamental principles of quantum mechanics.
Imagine standing in a vast forest filled with countless trees in various states,
just as we've discussed. Suppose two trees, let's say an oak and a pine, form
a unique bond so special that even if these trees were uprooted and planted
on opposite sides of the world, they would remain interconnected. By some
magical connection, if the oak tree suddenly sheds its leaves in autumn, the
pine tree will also change, perhaps by dropping its needles, even if pines
don't naturally do so.
This kind of inexplicable linkage is what happens between entangled qubits.
Once entangled, the state of one qubit directly relates to the state of the
other, regardless of the distance between them. Altering the state of one
instantly affects the other, transcending our classical understanding of
information transfer.
Why does entanglement matter to quantum computers? Entanglement
allows quantum computers to perform complex operations involving
multiple qubits simultaneously. Because these qubits are interlinked,
operations on one immediately impact the other, leading to highly
synchronized computation.
In the context of quantum computation, entangled qubits allow quantum
computers to explore multiple solutions in a coordinated fashion. This
"quantum teamwork" enables quantum computers to solve problems like
factoring large numbers and processing complex interrelated data.
In a real-world analogy, consider two interlinked traffic systems in different
cities. If the traffic flow on the east side of a city suddenly changes—say, a
major road is closed—the entire city's system instantaneously adapts,
redirecting traffic to maintain optimal flow without any visible
communication. The power quantum entanglement brings to computation is
performance and adaptability in processing complex scenarios promptly.
Quantum Tunneling: Navigating the Energy Landscape
Imagine hiking through a vast mountainous landscape, searching for the
lowest valley—the perfect campsite. Sometimes, you find a spot that seems
perfect, but then you realize there are even lower valleys beyond the next
ridge. The problem is, that climbing over each one to check the adjacent
valley can be tiring and time-consuming. Now, imagine if you had a
magical ability to just "phase" right through the hill to see what's on the
other side without all the effort of climbing. This, in essence, is quantum
tunneling.
1. The Quantum Energy Landscape: Quantum systems, like our
hypothetical hiking terrain, possess an energy landscape, which
consists of various states with differing energy levels. Some states are
nestled in deep valleys (low energy), while others are perched atop
hills (high energy).
2. The Challenge of Local Minima: In many computational
problems, particularly optimization, the objective is to find this deepest
valley, representing the optimal solution. However, it's easy for a
system to get trapped in a small dip, mistaking it for the lowest point.
This is known as a local minimum.
3. Tunneling to the Rescue: This is where quantum tunneling
becomes pivotal. Instead of being confined to the small dip, quantum
systems have the uncanny ability to "tunnel" right through the hill,
probing the landscape beyond without scaling the peak. In more
technical terms, qubits can bypass energy barriers, allowing them to
explore states that might be energetically off-limits in a classical
scenario.
Such a mechanism is invaluable in quantum computations, especially
quantum annealing processes. By tunneling through barriers, quantum
systems can efficiently escape suboptimal local minima, enhancing the
likelihood of discovering the true optimal solution. This feature embodies
the strange beauty of quantum mechanics and accentuates the unparalleled
computational potential of quantum computers.
Spin
The Quantum Gyroscope
Every particle in the quantum realm comes with its own set of
characteristics, akin to humans having unique fingerprints. One of these
distinguishing features, which remains intrinsic to particles like electrons, is
the property of spin.
Intrinsic Nature of Spin
Unlike our typical understanding of spinning, where an object physically
rotates around an axis (like Earth around its axis), the spin of a quantum
particle doesn't describe an actual rotation. Instead, it's an intrinsic form of
angular momentum. Picture an ice skater spinning with her arms
outstretched. As she pulls her arms in, she spins faster. This change in her
rotation speed corresponds to angular momentum. Quantum particles
possess similar momentum, but not because they are "spinning" like a top.
It's a fundamental property they just inherently have.
Angular Momentum at Play
Classically, angular momentum is visualized as the quantity of rotation an
object has, considering its mass and shape. In the quantum world, however,
particles like electrons have spin angular momenta that are quantized,
meaning they can take on only specific values. For electrons, these values
are often called "spin-up" and "spin-down." Much like the quantum leaps of
electrons between energy levels, this quantization adds another layer of
mystery to the already tricky quantum world.
Relevance to Wave-Particle Duality
Earlier, we explored the intriguing nature of wave-particle duality, where
particles like electrons exhibit wave-like and particle-like properties. Spin
plays a role here too. It becomes particularly evident when we study
phenomena like the Stern-Gerlach experiment. Our particles with spin are
subjected to magnetic fields; they exhibit quantized angular momenta,
aligning in particular directions, reinforcing the idea that quantum
properties, like spin, relate closely to the dual nature of particles.
Spin, as covered here, provides a foundation for understanding many
quantum phenomena and operations, especially when we venture into
quantum computing architecture and operations in the following chapters.
Bell's Inequality and the Deep Mysteries of Entanglement
The EPR Paradox: Quantum Spookiness and Computing at a
Distance
In the vast forest of quantum possibilities, intertwined trees represent
entangled qubits - the fundamental building blocks of quantum computers.
Einstein, Podolsky, and Rosen (EPR) were the first to delve deep into this
mysterious quantum forest. In 1935, they proposed a challenging scenario.
Imagine two qubits that once interacted and are now separated across a
quantum chip or even across vast physical distances. Measuring one would
instantaneously determine the state of the other. Einstein termed this
"spooky action at a distance," hoping for some hidden variables to explain
this in line with classical logic.
Bell's Inequality
The Litmus Test for Quantum Computers
In the 1960s, John Bell proposed a method to determine the nature of these
intertwined qubits. His test, known as Bell's Inequality, checks whether
qubit connections are genuinely 'quantum' or can be explained through
hidden variables. Quantum computers thrive in this environment,
leveraging the 'spooky' entangled states to simultaneously process vast
amounts of information. Real-world experiments on quantum systems,
acting as preliminary quantum computers, defy classical logic, suggesting
they operate in a profoundly non-local dimension.
Local Realism vs. Quantum Computing Power
"Local realism" is akin to processors communicating via set pathways. In
the quantum system, entangled qubits don't follow these rules. They
communicate, or rather share connectivity, in a non-local fashion, making
quantum computers incredibly powerful. This non-locality, tapped into by
quantum algorithms, is a game-changer, ushering in an era where quantum
machines teach us the boundaries of possibility.
From Sci-Fi to Quantum Reality
In the world of quantum computing, teleportation and superdense coding
aren't just science fiction; they're foundational techniques. Quantum
teleportation allows the state of one qubit to be recreated in another distant
qubit, a potential cornerstone for quantum communication networks.
Meanwhile, superdense coding maximizes the information transmitted
between qubits, promising efficiency levels never before achieved.
Bell, Qubits, and the Quantum Computing Revolution
Bell's theorem and the experiments inspired by it have a direct bearing on
our understanding and development of quantum computers. Entangled
qubits, like our forest's intertwined trees, operate in a world that classical
logic can't easily grasp. By appreciating the counterintuitive nature of qubits
and their entangled states, we're poised to harness the immense power and
potential of quantum computing.
Chapter 1 has unveiled the magical world of quantum physics, drawing us
into its whirlwind of wonders and phenomena. Now, with a grasp of these
foundational principles, Chapter 2 promises a thrilling exploration into the
architectural marvels of quantum computers. From the energetic terrain of
abstract theories to the tangible, cutting-edge machinery, our quest is about
to take an exhilarating turn. Let's explore the blueprints and building blocks
of this revolutionary technology.
Chapter 2
Building Quantum Computers
The Quantum Foundation
Before we look at quantum computers in action, it's pivotal to grasp its
foundational concept. Quantum originates from the Latin "quantus," which
means "how much." That's not simply a matter of language but a window
into a profound shift in our understanding of the universe.
In the early 20th century, scientists realized that energy, at its most
fundamental level, doesn't flow in a smooth, continuous stream. Instead, it
exists in discrete packets or "quanta." These discrete packets of energy form
the very bedrock of quantum mechanics. This revelation is like a flowing
river, which, when examined, contains individual water droplets.
When we discuss building a quantum computer, we're essentially
addressing the art of harnessing these quanta—these discrete energy units—
to process information. Unlike the classical bits in traditional computers,
quantum bits, or qubits, capitalize on the principles of superposition,
allowing them to hold vast amounts of information and process extensive
datasets simultaneously.
Economic considerations add another layer to this intricate dance.
Questions arise: What's the financial outlay for building and maintaining
each quantum computer? How power-intensive is each method? And how
prevalent is each technology in the current landscape? Asking such
questions lays the foundation and our promise to make quantum computing
for everyone.
The fragility of quantum computations cannot be overstated. Quantum
systems are like delicate instruments, vulnerable to external influences such
as temperature variations, electrical fluctuations, and even cosmic
interference. This sensitivity underscores the need for quantum error
correction, our safeguard against such 'noises,' ensuring reliability and
accuracy.
You're about to uncover how the 'quantum' principle paves the way for an
unprecedented leap in computational expertise.
Quantum Computing Architecture
In our quest to construct the intricate highways of quantum computing, we
first must understand the diverse landscapes these roads can traverse. Just
as ancient builders had the choice of cobblestone, dirt, or brick for their
streets, modern quantum architects have a range of foundational materials
and methods at their disposal. But these are no ordinary roads; they are
highways to a future full of computational wonders.
Emphasizing Quantum Mechanics' Role in Quantum
Computing
Imagine for a moment the blueprint of an architectural wonder: a
monumental bridge that spans a river. In classical engineering, this bridge
would have supports at exact intervals, and its design would adhere strictly
to the well-established laws of physics and materials science. The
predictability of these laws ensures the safety and stability of the bridge,
allowing it to bear the weight of vehicles and withstand environmental
challenges.
In the quantum realm, however, the blueprint undergoes a dramatic
transformation. The bridge, once limited by classical constraints, can now
exist in multiple configurations simultaneously, and its supports can be both
present and absent at the same time. This bewildering phenomenon is not a
whim of a foolish architect but the tangible influence of quantum mechanics
on design and construction.
The foundational principles of quantum mechanics – superposition,
entanglement, and quantum interference – are not just theoretical
curiosities. They empower quantum computers with capabilities far beyond
their classical counterparts. These principles don't modify the blueprint;
they expand our possibilities, allowing quantum architects to explore
innovative and uncharted designs.
But why is quantum mechanics so pivotal in this context? Classical
computers, built on bits, function within the rigid confines of binary states.
Qubits, however, can perform superposition. This dual-state existence
grants quantum computers a computational bandwidth that classical
systems can't match. Picture our bridge again, but instead of simply
connecting two landmasses, it interlinks multiple destinations, offering
countless pathways and outcomes.
Harnessing the principles of quantum mechanics is not merely an academic
exercise—it's the foundation for constructing quantum computers.
Architects, engineers, and physicists actively design tools to tap directly
into the quantum traits. Every component, every device, every meticulously
crafted piece is a tangible manifestation of quantum principles.
As we continue mapping out these quantum highways, remember that each
path, gate, and junction is directly associated with the quantum phenomena.
This isn't just about conceptual understanding; it's about physically
chiseling into the quantum landscape, molding it into computational
marvels. Let's continue exploring the tools and structures we build to
unlock its potential.
Components of Quantum Computers
To understand the architecture of a quantum computer, let's visualize it as a
sprawling, sophisticated highway system. Just as our modern highways
aren't merely tar slapped onto earth but consist of multiple layers, from a
foundational bedrock to the surface layer, quantum computers are built
upon several layers, each having its own role and complexity. At the heart
of this quantum highway system are the lanes—the qubits.
Quantum Bits (Qubits)
Much like highway lanes carry cars, trucks, and buses, the qubits in a
quantum computer carry quantum information. Suppose classical bits in
traditional computers are akin to one-lane roads that only allow a car to go
in one direction at a time (0 or 1). In that case, qubits can be imagined as
multi-layered highways where vehicles can float between layers, moving in
many directions simultaneously, thanks to quantum superposition. This
'floating' car isn't committed to one layer or direction but exists in a state
that's a combination of all possible states, a feature unique to our quantum
highway.
While our analogy likens qubits to multi-layered highways, it's important to
understand that qubits are tangible and physically real, albeit operating
under principles that can seem almost mystical. Magnetic or electrical fields
and lasers are just a few tools we use to interact with qubits.
Quantum Gates and Operations
Building onto our highway analogy, let's consider quantum gates and
operations. Think of gates as the traffic signals and junctions of our
quantum highway, directing the flow of our quantum cars — the qubits.
Much like how traffic signals dictate vehicle movement, these gates control
qubit behavior, ensuring they operate in specific, desired ways.
It's essential to understand that while quantum gates are conceptualized
abstractly, similar to logic gates in classical computing, their real-world
counterparts are physical entities. They come to life through controlled
interactions among qubits. The physical system in which the qubits operate
determines how these interactions are achieved. Thus, quantum gates
possess both an algorithmic representation and a tangible manifestation.
Constructing Operators
Diving a little deeper into the intricacies, these operators are the set of
instructions governing the operations on our quantum highway. Unary
operators, for instance, can be visualized as instructions for a single lane,
binary for junctions involving two lanes, and ternary for complex three-way
intersections. They dictate how our quantum vehicles (qubits) interact,
merge, or diverge.
But beyond this conceptual layer, these operators don't exist in isolation
from the hardware. When implementing an operator on a quantum
computer, we often use precise sequences of laser pulses or finely tuned
electromagnetic fields. For instance, to make two qubits interact in a
particular way (binary operation), a specific frequency of light might be
shone on them. This light manipulates the qubits into a state where they can
effectively "talk" to each other, achieving the desired operation. It's a dance
of precision, where the choreography of mathematical operators is brought
to life through the artistry of physical interaction.
Quantum Circuit Design
A quantum circuit is the equivalent of a planned roadway system. Just as
city planners design roads, intersections, and highways to optimize traffic
flow, quantum circuits are meticulously designed to perform specific tasks
or computations. The layout of these circuits is crucial. Much like an
inefficiently planned road can lead to traffic jams, an improperly designed
quantum circuit can result in computational inefficiencies.
Constructing these circuits is an intricate ballet of precision and control.
Here's how we capture the essence of quantum circuits in practice:
Positioning Qubits: Qubits need to be placed with pinpoint
precision. In labs, this is achieved using cryogenic chambers,
where temperatures plunge close to absolute zero, offering a
stable environment for qubits, especially for certain types like
superconducting qubits.
Gating & Interactions: Every gate in the quantum circuit
translates into very specific and controlled interactions among
qubits. The kind of interaction depends on the type of qubit
and the operation:
Laser Beams: Finely tuned lasers manipulate individual ions
in systems with trapped ions as qubits. By targeting an ion
with a laser pulse, scientists can adjust its energy state or make
it interact with a neighboring ion.
Electrical Pulses: Short bursts of controlled electrical pulses
help manipulate the qubit states and interactions for
superconducting qubits. These pulses are delivered through
meticulously designed circuitry, ensuring precision.
Magnetic Resonance: In some cases, like Nuclear Magnetic
Resonance (NMR) quantum computers, magnetic fields are
applied to influence the spin states of nuclei, effectively
creating quantum gates.
At the intersection of advanced technology and quantum theory, researchers
and engineers use these tools to bring quantum circuits from theory to
reality, making the promises of quantum computation obtainable.
Photonic
Envisioning our quantum highway, photonic quantum systems can be seen
as super-speed trains or light-rails. Unlike the traditional vehicles that
operate on roads, these special "trains" run on tracks of light. Photons—
particles of light—serve as the quantum counterpart to the vehicles,
traversing vast distances at breathtaking speeds. In quantum computing,
their unique properties make them a prized choice for certain architectures.
In the laboratory setting, creating and manipulating photonic qubits requires
a symphony of high-tech apparatus:
Optical Tables: These specially designed tables minimize
vibrations and ensure stability. Every piece of equipment put
on these tables, from lasers to beam splitters, stays rock-
steady, ensuring precision.
Lasers: To generate individual photons or pairs of entangled
photons, specific types of lasers are used. The light's color (or
wavelength) and pulse duration can be carefully controlled to
produce the desired photonic states.
Beam Splitters & Mirrors: These are used to direct the path
of photons. Much like how traffic lights guide vehicles on
roads, beam splitters and mirrors guide photons on their
optical paths.
Optical Fibers: Photons often travel through these fibers,
which act like the dedicated tracks for our light-speed trains.
The fibers ensure that photons reach their destination without
external interference.
Photon Detectors: At the end of a quantum computation or
operation, we need to "read" the state of our photonic qubits.
Specialized photon detectors capture the photons and convert
their information into data we can interpret.
Working with photonic quantum systems is a dance of light and matter.
Every beam splitter, mirror angle, and laser pulse is meticulously calibrated
to ensure the quantum computations are executed flawlessly.
Building a quantum computer is more than just assembling these
components; it's creating a harmonious, interconnected system. Like the
best highway systems in the world, it's designed for efficiency, speed, and
accuracy.
Spin
The Quantum Compass of Particles
In the macroscopic world, when we think of 'spin', we might imagine a top
rotating on a surface or Earth rotating around its axis. However, in quantum
physics, 'spin' doesn't quite represent a literal spinning motion. Instead, it's
an intrinsic property of particles, much like a fingerprint unique to each
individual.
To visualize this, suppose spin is a tiny magnetic compass embedded within
particles, specifically electrons, protons, and neutrons. Just as a compass
needle points north due to Earth's magnetic field, quantum particles possess
an innate orientation because of their spin. This orientation influences how
these particles behave in the presence of external magnetic fields.
In a laboratory setting, physicists use high-powered magnets to probe the
spins of particles. When subjected to an external magnetic field, a particle's
spin will either align with the field, referred to as spin-up, or opposite to the
field, known as spin-down. These two distinct orientations lead to different
energy states for the particle, and this distinction is vital. It becomes one of
the foundational stones upon which quantum computers are built.
Researchers can detect and manipulate these spins with the right tools, such
as magnetic resonance machines. By adjusting the strength and orientation
of the external magnetic fields, they can persuade particles to change their
spin states. This ability to read and modify spin provides a robust platform
for encoding and processing quantum information.
Spin is not just an abstract quantum number but a real, physical property.
When harnessed correctly, it can be the very core of a quantum computer's
memory and processing unit. Lab infrastructure, including advanced
magnets and resonance machines, physically interacts with and manipulates
these spins, bringing the mysterious quantum world to our fingertips.
NMR
The Quantum Maestro of Spin
Nuclear Magnetic Resonance, or NMR for short, is a sophisticated
technique. Imagine a maestro directing an orchestra of quantum spins. It's a
method that allows scientists to probe and manipulate atomic nuclei's spin
states, uncovering their secrets.
Picture attending a grand symphony, where the instruments are the atomic
nuclei. Each instrument (nucleus) has its unique tone (spin). The maestro
(NMR) ensures that every instrument plays in harmony, creating a
coordinated melody rather than a cacophonous noise.
In practical terms, NMR places a sample inside a powerful magnetic field.
This makes the nuclei in the sample resonate at specific frequencies. Then,
by applying radiofrequency pulses, scientists can manipulate these
resonances, effectively "playing" the nuclei like musical notes.
The equipment at the heart of an NMR-based quantum computer is the
NMR spectrometer. This device is a culmination of multiple components:
1. Magnet: Often superconducting, this magnet creates a strong,
uniform magnetic field where the sample is placed. The stronger the
magnetic field, the more precise the resonance frequencies of the
nuclei, much like tuning a musical instrument for optimal sound.
2. Probe: This is where the sample resides. When exposed to
radiofrequency pulses, the probe detects the nuclei's tiny magnetic
responses.
3. Radiofrequency (RF) Transmitter: This transmits short bursts or
pulses of radio waves into the sample, causing specific nuclei to
resonate.
4. Receiver: After the RF pulse, nuclei return to their original states
and emit signals. The receiver picks up these signals to extract
information about the sample's nuclear environment.
In a quantum computing context, the spin states of the nuclei (either aligned
with or against the magnetic field) can be interpreted as qubits. Scientists
can use RF pulses to put these nuclei into superpositions of states or even
entangle multiple nuclei, leveraging the principles of quantum mechanics
for computation.
Thus, NMR is not just a bridge between the quantum and classical worlds;
it's a physical interface. Quantum spins are read, written, and manipulated
through instruments like the NMR spectrometer.
Spin's Dance
Superposition and Entanglement
The mysterious quantum properties of superposition and entanglement are
not just abstract ideas; they are the behavior of particles with spin. Let's
revisit these properties using spin as our guide.
1. Superposition and Spin
In the context of the NMR techniques mentioned earlier, superposition is a
cornerstone. When a nucleus is subjected to a radiofrequency pulse from the
NMR spectrometer, it's nudged into a superposition of spin states. The
nuclei aren't just aligned with or against the magnetic field; they exist in a
delicate balance of both states. This phenomenon allows for the quantum
parallelism that powers quantum computing's incredible potential.
2. Entanglement and Spin:
In our NMR-based quantum computer, entanglement is not just a fancy
word; it's a physical operation. By carefully crafting sequences of
radiofrequency pulses, scientists can make the spins of two nuclei become
entangled. When this happens, the state of one nucleus becomes dependent
on the state of the other, even if they're spatially separated. This allows for
incredibly intricate and powerful quantum operations, as the state of one
qubit (spin) can determine the state of another.
To capture the outcome of these quantum dances, scientists employ
quantum detectors that can pick up the delicate signals of these phenomena.
These detectors are sensitive to the slightest of changes and can confirm if
spins are in superposition or entangled.
In essence, both are vital for encoding and processing quantum information.
With tools like the NMR spectrometer, we aren't just reading about
quantum principles; we're actively wielding them, using actual hardware to
write and manipulate the very language of the quantum world.
Concrete Examples of Spin
Spin, one of the intrinsic quantum properties of subatomic particles, isn't
just a theoretical construct—it plays a vital role in countless quantum
computing applications. These are realized physically, harnessing the very
essence of quantum mechanics in the lab.
1. Quantum Memory Devices: In quantum computers, the state of a
qubit, often represented by the spin of an electron or nucleus, can store
information. This is similar to classical computer memory but with the
quantum twist of superposition, allowing for more complex data
storage. Storing quantum information using spin often requires ultra-
cold environments. This is achieved using dilution refrigerators, which
can chill devices to temperatures colder than outer space. Within these
environments, magnetic fields or focused lasers might be used to
manipulate the spin states of individual electrons or nuclei, allowing
them to act as memory banks.
2. Spin-based Quantum Logic Gates: Classical computers utilize
logic gates (AND, OR, NOT) to perform operations on bits, and
quantum computers employ spin-based gates to manipulate qubits. For
example, a specific electron spin can be flipped using carefully
calibrated magnetic fields to match a quantum NOT gate operation.
Physically controlling electron spins requires precision tools.
Superconducting magnets, for instance, can generate the precise
magnetic fields needed to control spins. Sometimes, precision
microwave pulses are used to flip spins in a controlled manner, acting
as the physical implementation of certain quantum gates.
3. Quantum Cryptography: Quantum Key Distribution (QKD) is a
process used in quantum cryptography. It leverages the entanglement
property of particles. Spin is a vital component in this, where
entangled photons' spins are measured to create cryptographic keys
that are virtually unhackable. Implementing QKD involves sending
and receiving entangled photons. To generate these, non-linear crystals
are used, which can produce pairs of entangled photons when excited
by a laser. On the receiving end, photon detectors, often made of
superconducting materials, measure the photons' spins to extract the
encryption key.
4. Quantum Sensors: Spin is used in quantum sensing applications to
detect minute changes in physical quantities, like magnetic fields. By
sensing the tiny shifts in spin states, these sensors can detect changes
at unprecedented sensitivity levels. SQUIDs (Superconducting
Quantum Interference Devices) are often used to detect discrete
changes in spin states. These are highly sensitive magnetometers
designed to measure extremely subtle magnetic fields, allowing us to
identify tiny shifts in spin states.
5. Spintronics: These aren't quantum computers in the classical sense,
but they leverage quantum properties for tasks like data storage and
transfer. Here, spin is manipulated using electrical currents or optical
methods to achieve desired outcomes. Spintronic devices often employ
semiconductor materials, similar to what's used in today's computer
chips, but with a twist: these materials have been engineered to be
sensitive to electron spin. Ferromagnetic electrodes can inject or detect
spins in a controlled manner, and polarized light can be used to
manipulate and read spin states.
These applications, built upon intricate setups and cutting-edge tools, are a
testament to how the metaphysical concept of 'spin' is finding concrete,
transformative use in the real world. Combining these advanced tools and
techniques, quantum researchers and engineers can harness the power of
spin and bring the quantum world a step closer to our daily lives.
Quantum Computing Architectures
Venturing into the blueprints of quantum computing, we are greeted with
various architectural designs, each displaying a unique approach to benefit
from the quantum world's extraordinary powers. Just as architects design
different types of buildings—skyscrapers, bungalows, or cottages—based
on the purpose, terrain, and materials available, quantum scientists and
engineers have crafted distinct computing models to optimize performance
and address specific computational challenges. These architectural
frameworks are not just conceptual; they have distinct hardware setups,
specialized tools, and particular methodologies. Let's explore the
architecture and elements that shape the backbone of quantum computers.
Circuit Model
In the vast landscape of quantum computing architectures, the Circuit
Model can be compared to the traditional blueprints used in constructing
buildings. It represents a foundational approach where operations are
structured in a sequence, much like the logical flow of electrical circuits in
classical computing.
Physically, the Circuit Model is characterized by a network of quantum
gates that act upon qubits. Each gate performs a specific operation on the
qubits; when combined, they form a quantum circuit. To realize these
circuits requires precise control of the qubits, typically achieved using
techniques like laser pulses, microwave bursts, or specific magnetic
interactions, depending on the type of qubit used.
In the lab, our Circuit Model applies various devices. For instance, if
superconducting qubits are employed, a dilution refrigerator, oscillators,
and pulse generators are required to orchestrate the timing and nature of
interactions between qubits. Additionally, high-precision measurement
equipment will monitor and evaluate the outcomes.
The Circuit Model represents a systematic, step-by-step approach. Each
operation is like laying one brick atop another in building construction, with
every element carefully placed to ensure the stability and functionality of
the final structure.
Measurement-Based Model
The Measurement-Based Model, sometimes called the "one-way quantum
computer," introduces a unique twist to the world of quantum computing
architectures. Suppose the Circuit Model is a carefully designed blueprint.
In that case, the Measurement-Based Model is like constructing a building
with many pre-fabricated rooms, deciding its final shape and function by
how these rooms are interconnected and accessed.
Creating this model involves preparing a large, highly entangled quantum
state known as a "cluster state." Think of this as a vast interconnected web
of qubits, each holding potential pathways for quantum computation. The
true magic begins when specific measurements are made on these qubits.
By measuring individual qubits in a particular sequence and manner, the
rest of the qubits adjust and evolve their states accordingly.
In the lab, the Measurement-Based Model demands high-fidelity audio
preparation of the initial cluster state and precise measurement tools.
Photonic systems are often favored for this approach due to the ease with
which photons can be entangled and measured. Here, beam splitters, which
divide light paths, become integral, as do single-photon detectors, which
identify and quantify the properties of individual light particles. These
detectors, often cooled to reduce noise, provide the feedback necessary to
guide the subsequent measurements and quantum operations.
In many ways, the Measurement-Based Model is like a dynamic puzzle.
While you start with a vast, interconnected potential of quantum
information, the final computational outcome is sculpted by the decisions
made during the measurement process, each choice refining and guiding the
quantum computation to its conclusion.
Adiabatic Model
The Adiabatic Model offers a different concept of evolution and gradual
change. If we bring our architectural analogy back, consider the Adiabatic
Model akin to constructing a bridge by starting on one side of a depth and
slowly extending it until it reaches the other side. Instead of explicitly
programming each step, the structure simply evolves in response to the
landscape.
In the quantum domain, this model begins with a quantum system in a well-
understood initial state. The system is then subjected to a slowly changing
external influence, ensuring that the system remains in its lowest energy
state, or "ground state", throughout the process. The end goal? To evolve
the system into a final state that represents the solution to a given
computational problem.
Realizing the Adiabatic Model in a physical setup involves a meticulously
designed quantum system where interactions and external influences can be
finely tuned. The equipment and tools needed to achieve this are quite
specialized:
1. Magnetic Fields & Superconducting Loops: The behavior of
qubits in this model is often governed by magnetic fields, where
superconducting loops can serve as qubits. Tiny changes in these
magnetic fields guide the evolution of the system.
2. Control Units: These devices ensure that external influences change
slowly and predictably. They are precision-engineered to deliver
accurate and gradual shifts, ensuring the quantum system remains in its
ground state.
3. Cryogenic Systems: The adiabatic process often requires extremely
low temperatures to minimize external noise and ensure accuracy.
Thus, cryogenic cooling systems become indispensable to maintaining
these chilly conditions.
4. Advanced Monitoring Equipment: Given the gradual evolution in
the Adiabatic Model, real-time monitoring of the system's state is
crucial. This is done using sophisticated sensors and detectors to gauge
even the slightest changes in the quantum state.
The Adiabatic Model banks on the principle of slow and steady evolution,
rather than discrete steps. It allows the quantum system to find its path,
navigating the intricate quantum landscape and eventually revealing the
solution to the computational challenge at hand.
Beyond the primary architectures we've explored, it's worth noting that
other computational models leverage quantum mechanics. For instance, the
Quantum Cellular Automata operate by local interactions on a lattice,
while the Topological Quantum Computing Model exploits the intricate
world of quantum braids in specific materials. Though these may sound
mysterious now, they're part of the broader landscape of computational
innovations that scientists and researchers intensely explore.
Quantum computing isn't a static field; it's vibrant and ever-evolving.
Researchers worldwide push the boundaries daily, exploring new
architectures, models, and paradigms. Their work, often hidden in lab
journals and complex research papers, is like the tireless endeavors of gold
miners—chipping away steadily in the hope of unearthing a groundbreaking
method or principle. This dynamic nature is what makes quantum
computing so exciting. The architectures we've covered are just the tip of
the iceberg, and there's a thrilling sense of anticipation in the quantum
community about what might be the next big discovery!
These architectures—whether circuit-based, measurement-driven, or
adiabatic—are more than just theoretical constructs; they're the
foundational pillars on which quantum systems are built. Each offers a
unique perspective on how to harness the elusive powers of quantum
mechanics. And while these models provide a roadmap, the journey—the
experiments, the challenges, the breakthroughs—truly define the quantum
revolution.
Different Approaches in Quantum Computing
As we transition from the thirty-thousand-foot view of quantum computing,
it becomes pivotal to understand the platforms up close where these ideas
take shape. These platforms represent the machinery, the hardware that
hums from the activity in labs around the world. Here, we'll visit three of
the most promising and widely studied approaches: superconducting
circuits, trapped ions, and topological qubits.
Superconducting Circuits
Where Currents Race Unhindered
Imagine a microscopic racetrack where electrical currents dash about,
uninhibited by resistance. This isn't some sci-fi concept but the intricate
domain of superconducting circuits. Crafted from unique materials that,
when cooled to temperatures barely above absolute zero, offer no resistance
to electrical flow. Such conditions form the backdrop for the quantum
computation marvel within these circuits.
Principle: At the heart of these circuits lie the quantum properties of
superconducting materials. The electric current flows unopposed in this
chilly quantum freezer, forming the container for our qubit operations. A
vital actor on this microscopic stage is the 'Josephson junction'. Imagine it
as a gatekeeper allowing current to flow between superconductors, playing
an essential role in computation.
In the Lab
Step into a lab dedicated to superconducting circuits, and you'd likely first
be drawn to an imposing structure resembling a large metallic chandelier. A
'dilution refrigerator'. Its purpose? To plunge the environment housing our
superconducting qubits to temperatures colder than the vast expanses of
outer space, ensuring they remain in their unique superconducting state.
Surrounding this behemoth are arrays of machines that generate precise
microwave pulses. These pulses manipulate our qubits, giving them a
computational task. After each operation, advanced measurement devices
await, ready to interpret and reveal the quantum states of the qubits once
their tasks are complete.
Engaging with these quantum computers is like conducting a symphony.
With a blend of calculated anticipation and rehearsed skill, scientists send a
series of commands, instructing machines to generate exact microwave
sequences. This intricate dance of commands and responses between
humans and machines is what births quantum computations.
Hardware and Materials
Superconducting qubits are a marvel of modern engineering. They're
fabricated from thin layers of superconducting materials on a silicon or
sapphire substrate. The most commonly used materials include aluminum
for the superconducting layer and silicon or sapphire for the substrate. Key
to the operation of these qubits is the Josephson junction, a non-
superconducting barrier (often insulating or semiconducting) sandwiched
between superconducting layers.
Tools and Equipment
The precision required in fabricating superconducting qubits is staggering.
Labs utilize electron beam lithography to intricately design the quantum
circuits at a nanoscale level. Furthermore, dilution refrigerators, which
resemble large metallic chandeliers, plunge the qubits to near absolute zero
temperatures, providing an almost noise-free environment essential for
quantum operations. Microwave pulse generators are employed to
manipulate the state of the qubits; each pulse is carefully calibrated for
optimal qubit interaction.
Software and Interaction
Precision in quantum computation is paramount, so tailored software
interfaces are deployed. These software platforms allow scientists to design
quantum algorithms, feed them to the hardware, and interpret the outcomes.
Often, a classical computer runs this software, acting as a bridge between
human inputs and quantum processes.
Properties
Superconducting qubits are artificial atoms made from circuits of capacitors
and Josephson junctions. They operate based on the quantum principles of
superposition and entanglement, using the flow of electrical current as their
mode of operation.
Stability
These qubits offer strong stability owing to their macroscopic nature.
However, they're sensitive to external electromagnetic noise, which can
introduce errors.
Coherence Times
As of my last update, coherence times for superconducting qubits have
reached tens to hundreds of microseconds. While this might seem short, it's
impressive in the quantum world and allows for a reasonable number of
quantum operations to be performed sequentially.
Scalability
Superconducting qubits have a strong case for scalability. With companies
like Google and IBM investing heavily in this technology, we've seen rapid
advancements in multi-qubit systems. Cross-talk between qubits remains a
challenge, but innovations in chip design and error-correction protocols
pave the way for larger, more powerful superconducting quantum
computers.
Trapped Ions
Dancing Particles in an Electro-Magnetic Ballet
Visualize, for a moment, a shimmering dance floor suspended in space,
where ions – charged atomic particles – pirouette and glide in a
choreographed performance. This is no celestial ballroom but the
captivating arena of trapped ion quantum computing.
Principle
The foundational idea here revolves around using individual ions as qubits.
These ions are "trapped" or suspended in space using electromagnetic
fields. Their very state of being, whether they spin one way or another, is
what encodes our quantum information. By using lasers, scientists can
excite or change the state of these ions, making them suitable for quantum
computation.
In the Lab
Enter the lab, and you'll likely be captivated by a piece of equipment that
seems plucked from the minds of science fiction: the ion trap. This device,
often made of gold or other metals and resembling a micro, intricately
designed crown, holds our dancing ions in place. Hovering above,
specialized lasers target each ion with pinpoint precision. These lasers,
often mounted on adjustable arms or platforms, are the conductors of our
quantum dance, coaxing ions into their computational twirls with flashes of
controlled light.
Interaction with this system requires a skillful touch. Scientists, adorned in
lab coats, carefully calibrate these lasers, ensuring they emit the perfect
frequency and intensity of light. As the lasers interact with the ions, they
induce quantum jumps, changing their spins and allowing the computation
to occur. Once this dance is done, specialized detectors stand ready,
analyzing the afterglow of the ions to decode the results.
In this harmonious confluence of charged particles, electromagnetic fields,
and precise lasers, the trapped ion approach promises computations of
incredible delicacy and precision, all choreographed to the tune of quantum
mechanics.
Hardware and Materials
At the core of trapped ion quantum computers are individual ions
suspended in free space using electromagnetic fields. The beauty of this
approach is in its natural atomic structure, where electrons orbiting the
nucleus exhibit quantum properties ideal for computation. The choice of ion
varies, but ions of Ytterbium, Calcium, and Barium have been favored.
Tools and Equipment
To hold these ions in place, a series of electrodes generate the necessary
electromagnetic fields, creating what's known as a Paul trap. Once trapped,
finely tuned lasers target individual ions, manipulating their quantum states.
Photodetectors and cameras then capture emitted photons from these ions to
show their quantum state.
Software and Interaction
Custom software interfaces help choreograph laser pulses, ensuring that
ions are manipulated in a sequence corresponding to the desired quantum
algorithm. Real-time feedback loops may be implemented to adjust laser
parameters dynamically, ensuring high-fidelity operations.
Properties
These qubits are based on individual ions trapped in electromagnetic fields.
The quantum state is often stored in the electron configuration around the
ion's nucleus.
Stability
Ions are naturally occurring entities with inherently stable quantum states.
The very act of trapping them provides an isolated environment, reducing
external disruptions.
Coherence Times
Trapped ions boast some of the longest coherence times among physical
qubits, often reaching many seconds, depending on the ion type and trap
environment.
Scalability
While individual trapped ions are high-quality qubits, scaling up remains
challenging. This is due to the increased complexity of controlling large
numbers of ions simultaneously and the logistical challenges of managing
more extensive trapping arrays.
Topological Qubits
Imagine a city's roadway, not as a simple flat grid but as a series of
intertwined and knotted overpasses and underpasses, where the structure of
the road itself holds significance. In this intricate web, the overall
framework remains undisturbed, even if there's a minor disruption in one
part. Just as these knots or braids in the road protect against simple
obstructions, topological qubits use the "knots" of quantum states to guard
against errors.
Principle
The essence of topological quantum computing is in its very name -
"topology," the study of properties of space that are preserved under
continuous transformations. These braided paths in a two-dimensional
space determine the states of topological qubits. Due to their knotted nature,
they're highly resistant to local errors, a constant challenge in quantum
computing. The information is stored not in a specific place but in the twists
and turns of these quantum braids.
In the Lab
A laboratory focused on topological quantum computing would have an
array of highly specialized equipment designed to manipulate and study
two-dimensional materials like quantum wells. These setups help generate
and study quasi-particles called anyons, which have fractional quantum
numbers and are at the heart of topological quantum computations.
Cryogenic equipment, similar to the superconducting setup but specialized
for these materials, ensures the system remains at ultra-cold temperatures.
Microscopes with incredibly high resolution, such as scanning tunneling
microscopes, are employed to observe and manipulate the quantum braids
directly, allowing scientists to "weave" the quantum knots.
In a "quiet" and ultra-controlled lab environment, scientists coax certain
materials into a two-dimensional existence, where particles behave unlike
anything in our everyday world. With their fractional charge and unique
braiding behavior, the anyons become the heroes of this quantum story. The
topological qubit's resilience stems from its spatial properties — it's like
trying to erase a knot from a twisted rubber band without actually
untwisting it. The data isn't in one loop or twist but in the overall braid. This
resilience promises more stable and error-resistant computations, with
scientists delicately guiding and observing these braids in action while
nestled in the embrace of advanced cryogenic chambers.
Hardware and Materials
Topological qubits are a newcomer in the quantum arena. They use certain
quasi-particles, called anyons, that exist in specific two-dimensional
materials. The magic here is in the braiding of these anyons—by moving
them around one another, quantum information is stored in a manner that's
reliable against local disturbances.
Tools and Equipment
To realize topological qubits, labs require ultra-pure material samples where
these anyons can emerge. Advanced microscopy techniques, like scanning
tunneling microscopy, help visualize and manipulate anyons' braids at the
subatomic scale.
Software and Interaction
Software tools are still emerging in the developing stage of topological
quantum computing. These tools help in the design and simulation of anyon
braiding operations. Additionally, they allow error-checking, given that
topological qubits promise better resistance to quantum errors.
Stability
The topological nature makes these qubits highly stable. Local errors,
which plague other qubit types, do not easily disrupt topological qubits.
Coherence Times
Though still in the early stages of research, early findings suggest that
topological qubits can have extended coherence times, potentially even
surpassing trapped ions, though this remains to be definitively established.
Scalability
The principle behind topological qubits suggests excellent scalability due to
their inherent error resistance. However, practical implementation is still in
its infancy, with scalability largely untested in real-world setups.
Comparing the Quantum Titans
Superconducting Circuits, Trapped Ions, and Topological Qubits
The three dominant quantum technologies showcase the blend of advanced
materials, intricate tools, and state-of-the-art software that facilitates
quantum computation. Each approach is rooted in the same quantum
principles but leverages unique physical properties and requires specialized
tools and techniques for realization. The field is expansive, and as we'll see,
each system comes with its set of advantages and challenges.
While superconducting qubits ride the wave of industrial investment and
scaling up, trapped ions offer remarkable integrity and stability. Meanwhile,
topological qubits present a promising but largely uncharted territory,
potentially unlocking the next era of quantum computation. The quantum
race is very much alive, with each technology vying for dominance in a
rapidly evolving landscape.
Navigating the quantum highway, one can't help but marvel at the
architectural wonders: the superconducting superhighways, the
mesmerizing dance of trapped ions, and the intricately knotted lanes of
topological qubits. But as with any engineering marvel, each has its
strengths and its Achilles' heel.
Superconducting Circuits
Strengths
Quick, efficient, and capable of maintaining quantum states for an extended
period. Their relative maturity means we've got a head start with them.
Limitations
Extremely sensitive to their environment; the slightest external interference
can disrupt their operations.
Best Suited For
General-purpose quantum computations, especially with the current muscle
of research and major industry backing.
Trapped Ions
Strengths
High precision and very long coherence times make them fantastic for
operations requiring extended computations.
Limitations
Scaling up – adding more ions to the system – is a notable challenge due to
cluttered interactions.
Best Suited For
Specialized quantum tasks that benefit from high precision and long
coherence but don't need a vast number of qubits.
Topological Qubits
Strengths
Their braided nature offers unmatched resilience to local errors, potentially
simplifying error correction.
Limitations
Still in the research phase, creating and manipulating anyons in a controlled
environment remains challenging.
Best Suited For
Quantum computations that require high stability and low error rates once
they progress beyond the research phase.
A Glimpse Beyond the Horizon
The domain of quantum computing, ever mysterious and intriguing, never
ceases to surprise us. While these three titans dominate our current
landscape, the quantum frontier is vast and uncharted. Tomorrow, we might
discover new pathways, perhaps even harnessing the power of quantum
computers to pave the way for their successors. Imagine a world where
quantum systems, birthed from the very phenomena they seek to harness,
guide us toward the next leap in computational evolution. As we continue
this quantum journey, the future is abundant, promising innovations beyond
our wildest dreams.
Creating Scalable Quantum Networks
As quantum systems scale up, they consistently encounter more
opportunities for errors. Even tiny disturbances – a stray photon, slight
temperature fluctuations, or electromagnetic noise – can disrupt a qubit's
state. In classical computers, errors are managed with redundancy, for
instance, by using multiple transistors to represent a single bit. Quantum
systems require a more detailed approach, leading to the development of
fault-tolerant quantum computing.
Fault-tolerant Quantum Computing
Principle
The idea here is not to prevent quantum errors but to detect and correct
them without measuring, thereby destroying the quantum state. This is
achieved using specially designed quantum error-correcting codes, which
can recognize when qubits go astray and guide them back without
collapsing their delicate superpositions.
Hardware and Tools
To implement fault tolerance, a quantum computer needs additional qubits
— called ancilla qubits — to monitor the primary qubits responsible for the
computation. Specialized gates are used to entangle the ancilla qubits with
the computational ones, allowing errors to be detected and corrected. This
introduces more hardware complexity, demanding many more qubits for
each logical, error-corrected qubit.
Scalable Quantum Networks
Principle
A scalable quantum network aims to link multiple quantum devices into a
unified, interconnected system, similar to today's internet but operating on
quantum principles.
Hardware and Tools
Establishing such networks requires the creation of quantum repeaters
(devices that can receive, amplify, and re-transmit quantum signals without
destroying their quantum nature) and the ability to convert quantum signals
to different forms suitable for long-distance transmission, typically using
photons.
NV Center-in-Diamond
An emerging and intriguing quantum computing and networking approach
hinges on a naturally occurring diamond defect.
Principle
At the heart of this technology is the Nitrogen-Vacancy (NV) center — a
spot where a nitrogen atom replaces a carbon atom next to a vacant site in
the diamond lattice. This defect has an electron spin that can be
manipulated and measured using light, making it a candidate for a qubit.
Hardware and Tools
Diamond chips housing NV centers are the primary substrates for this
technology. Green lasers are typically used to manipulate the NV center's
spin state. Microwaves can then control the spin, and red light is emitted
from the NV center when it returns to its ground state, providing a means to
read out its quantum state. The entire setup, while compact, requires
precision optics, stabilized laser systems, and advanced photon detectors.
By harnessing the NV center's sensitivity to electromagnetic fields, it's also
been proposed as a quantum sensor, with applications in fields as varied as
biology (imaging cellular processes) and geology (detecting mineral
deposits).
Tapping into fault tolerance is pivotal for quantum computers to realize
their revolutionary potential. By ensuring errors are kept in check and by
linking quantum systems together in scalable networks, we inch closer to a
quantum internet. Technologies like the NV Center-in-Diamond further
expand the horizon, reminding us that the quantum space is rich with
potential waiting to be tapped.
Quantum Computing with Neutral Atoms
At the frontier of quantum research, neutral atoms stand out as a promising
medium for quantum computation. Unlike charged ions or electrons that
experience strong coulombic interactions (attraction or force between
electric charges), neutral atoms interact weakly, offering a distinctive set of
advantages and challenges.
Principle
Neutral atoms, especially alkali atoms like rubidium, have distinct energy
levels that can be harnessed as qubit states. Their interactions, governed by
quantum mechanics, can be exploited to perform quantum gates.
Hardware and Tools
To manipulate these neutral atoms and bring out their computing potential,
precision tools are paramount.
1. Optical Traps: These are essentially "tweezers" of light. Lasers are
meticulously tuned such that their intensity creates tiny traps that can
hold and manipulate individual atoms in space. Picture a landscape of
dimples or pockets where each atom rests, all made and controlled
using laser beams.
2. Magnetic Fields: These are applied to tweak the atomic states,
allowing for subtle control over the qubits.
3. Laser Beams for Quantum Gates: Specific laser pulses can make
neutral atoms interact in a controlled way, enacting quantum gates that
are fundamental for computation.
4. High-resolution Imaging Systems: These capture the light from
the atoms, allowing scientists to read out the quantum information
after a computation.
The whole arrangement might remind you of a high-tech light show, with
ultra-cold neutral atoms dancing and interacting under the spell of lasers in
a carefully choreographed performance.
Optical Traps
These devices represent a triumph of marrying quantum mechanics with
photonics.
Principle
At the heart of optical trapping is the concept that light can exert a force on
matter. When an atom or small particle encounters a focused laser beam, its
photons can push or pull on that particle, effectively "trapping" it in a
specific location.
Hardware and Tools
1. Tunable Lasers: These are the backbone of optical traps. By
adjusting the laser's intensity and focus, researchers can create pockets
of varying depths and sizes suitable for holding everything from
individual atoms to small biological molecules.
2. Microscopic Objectives: Just as they sound, these microscope
components focus the laser beams down to the required minuscule
sizes.
3. Feedback Mechanisms: These systems quickly adjust the laser's
properties in real-time to maintain the trap, especially when dealing
with dynamic systems or particles that might try to escape.
The visualization here is like a futuristic game of marbles, where each
marble (atom or particle) is held still, not by a hand or container, but by
beams of light. In labs, you'd find scientists peering into advanced optical
systems, adjusting lasers, and observing the behavior of trapped particles
with fascination.
Both neutral atom quantum computing and optical trapping highlight the
ingenious ways scientists manipulate the microscopic world. Each flicker of
light in these setups isn't just illumination; it's a tool, a hand, a gate, or a
bridge in the quantum domain. The magic is in the quantum properties
being harnessed and the innovative tools built to do so.
The Technical Challenges and Advancements associated with
each Technology
1. Superconducting Qubits
Challenges
Decoherence: Though superconducting qubits have relatively
long coherence times, they are still prone to errors due to their
interactions with the environment.
Manufacturing Precision: Creating high-quality Josephson
junctions and other components at the nanoscale requires
meticulous precision.
Temperature Requirements: Operating near absolute zero
presents technical and energy cost challenges.
Advancements
Improved Materials: Research has led to the development of
superconducting materials that offer better performance and
resilience.
Error Correction Codes: Techniques to identify and rectify
quantum errors are maturing, enhancing the reliability of
computations.
2. Trapped Ions
Challenges
Scalability: Integrating large numbers of trapped ions while
maintaining precise control over each is a daunting task.
Interference: External electromagnetic interference can
adversely affect the qubits, leading to computational errors.
Advancements
Modular Architectures: Instead of one large trap, researchers
are exploring interconnected modules, each holding a few
ions.
Improved Lasers: Technological enhancements in lasers
allow for more precise manipulations and interactions between
ions.
3. Topological Qubits
Challenges
Uncharted Territory: Topological qubits are relatively new,
making the experimental verification and scaling up more
complex.
Error Rates: While theoretically less prone to errors,
practical implementations of topological qubits still face
challenges in maintaining low error rates.
Advancements
Research on Majorana Fermions: Breakthroughs in
understanding these exotic particles pave the way for more
reliable topological quantum computing.
Material Science: Discoveries of new materials that can host
these fermions have been critical.
4. Neutral Atoms and Optical Traps
Challenges
Control: Manipulating a large array of neutral atoms with
consistency is tricky.
Integration: Integrating optical traps into scalable quantum
systems without compromising performance is challenging.
Advancements
Improved Imaging Techniques: Advancements in imaging
allow for better observation and manipulation of atoms within
traps.
Dynamic Trap Configurations: Researchers can now
dynamically reconfigure optical traps, granting them more
flexibility in quantum operations.
Every quantum technology, while holding immense potential, is a double-
edged sword. The very properties that make them suitable for quantum
computation also present challenges. Yet, the relentless march of research
and innovation is steadily turning these challenges into stepping stones,
pushing the frontier of what's possible in the quantum field.
Evaluating Economic Factors, Power Capabilities, and
Prevalence
Above, we've examined the microcosmic highways of qubits and the very
architectures that define quantum computing. Yet, like any marvel of
modern engineering, the quantum computer is not just a product of
scientific curiosity; it's standing at the crossroads of feasibility, economics,
and environmental considerations. As Leonard Susskind and Art Friedman
might discuss over a cup of coffee, the grand theories and intricate
mechanics of quantum computing must eventually meet our world's
practical and economic realities.
This section will venture beyond the quantum gates and algorithms,
stepping into the broader landscape where quantum computing intersects
with economics, power dynamics, and global adoption trends. We'll weigh
the costs against the computational wonders, ponder the power needs of
these incredible machines, and gauge where we currently stand in the global
race toward quantum supremacy.
Practicality of Quantum Methods
In our bid to understand the scope of quantum computing, it's tempting to
solely focus on its scientific marvels. However, like any transformative
technology, its ascent to mainstream application depends heavily on
economic viability. Imagine the grand scientific conferences one might
attend, similar to the boardroom meetings where CFOs and CEOs debate
the cost and potential returns of venturing into quantum computing.
Capital Investment: The initial investment required is at the
heart of the economic discourse. Building a quantum
computer isn't just about having a lab filled with scientists; it's
about collecting the right materials, the specialized equipment
like dilution refrigerators or laser systems, and maintaining an
environment conducive to quantum phenomena. This isn't
your typical startup cost but an endeavor that requires
significant capital.
Operational Costs: Once a quantum computer is up and
running, the expenses don't stop. The immense cooling needs
of superconducting qubits, the precision control systems for
trapped ions, and the robust error-correction mechanisms for
topological qubits—all add to the operational overhead.
Additionally, there's the often-overlooked cost of training
personnel to operate and maintain these machines.
Economies of Scale: Just as the cost of producing a single
unit of a new technology tends to drop as production scales
up, the quantum industry will likely experience economies of
scale. However, it's worth noting that we're still in the early
phases, and scaling quantum technology will present its
unique set of challenges and opportunities.
Monetizing Quantum Power: The true economic potential of
quantum computers lies in their applications. Financial
modeling, drug discovery, and optimization problems are just
a few domains where quantum computers can outshine their
classical counterparts. But how quickly industries adopt and
are willing to pay for quantum solutions remains a crucial
economic question.
Long-term Implications: Quantum computing isn't a flash in
the pan; it's a long-term investment. While the initial costs are
high, the potential for breakthroughs in various fields could
lead to unprecedented returns. Moreover, the nations and
corporations leading the quantum race today might find
themselves holding significant geopolitical or market
advantages in the future.
It's a conundrum similar to the early days of the silicon revolution. The
initial chips were expensive, had limited applications, and were housed in
room-sized computers. Yet, visionaries saw beyond these limitations,
envisioning a world transformed by silicon. Today's quantum pioneers face
steep challenges, but they're driven by a vision of a world where the rules of
quantum mechanics don't just exist in textbooks but actively shape
industries and economies.
Adoption of Quantum Technologies
Discussing the world of quantum often evokes imagery of vast, cutting-
edge tech labs nestled within Silicon Valley. However, the fabric of
quantum computing adoption weaves far beyond the empires of tech giants.
The practical application and hardware infrastructure of quantum
computing is an expanding field attracting diverse industries and research
institutions alike.
1. Tech Powerhouses: As expected, the usual suspects—Google, IBM,
Intel, and Microsoft—are at the forefront of quantum research and
have already developed their quantum processors. Their interests aren't
merely academic; they envision quantum computers bolstering their
core services, from cloud computing to artificial intelligence.
2. Academia and Research Institutions: Universities across the
globe, from MIT to the University of Queensland, are diving deep into
the quantum realm. These institutions contribute significantly to the
theoretical underpinnings and actively craft prototypes, often
collaborating with industry leaders.
3. Aerospace and Defense: The strategic advantages of quantum
computing are alluring to defense contractors and aerospace industries.
The potential for cracking cryptographic codes, optimizing complex
logistical operations, and simulating intricate systems has led
companies like Lockheed Martin and Airbus to invest heavily in
quantum research and development.
4. Finance and Banking: Surprisingly, the world of finance is also
peeking into the quantum domain. Large banks and financial
institutions are exploring quantum algorithms for portfolio
optimization, risk analysis, and fraud detection. Their quantum
pursuits might still be in the beginning stages, but they clearly
recognize the transformative power.
5. Pharmaceuticals and Healthcare: The complex dance of
molecules and the daunting task of drug discovery might soon be
revolutionized with quantum algorithms. Companies like Roche and
Pfizer are starting to examine drug design and molecular simulations,
hinting at a future where treatments are optimized at the quantum
level.
6. Energy Sector: The challenges of renewable energy storage,
optimization of grid distribution, and exploration of new energy
sources are areas where quantum computing might play a pivotal role.
Energy giants like Shell and ExxonMobil are closely monitoring
quantum advancements and beginning to funnel resources into
research.
While the infrastructure required for quantum computing—like ultra-low
temperature coolers and intricate lab equipment—might initially appear
daunting and confined to specialized labs, there's a clear trend toward
decentralization. As the technology matures and becomes more
commercially viable, it's not too far-fetched to envision quantum processors
humming away in the control rooms of various industries, from
pharmaceuticals to finance. Quantum hardware, once the exclusive domain
of specialized tech labs, is poised to permeate diverse sectors, reshaping
how enterprises function.
Solving Error Correction
Noise, Errors, and Decoherence in Quantum Systems
To grasp the quantum landscape fully, one must come to terms with its
inherent unpredictability and fragility. Quantum systems, while potent, are
like delicately tuned instruments; even the slightest interference can throw
them off.
Quantum Noise
Picture a tranquil lake on a windless day. Every small pebble thrown into it
creates ripples, disturbing its untouched stillness. In the quantum world, this
"lake" is our carefully prepared quantum state, and the "pebbles" represent
quantum noise – those unwanted, external agitations that can distort our
computations. This noise can emanate from thermal vibrations,
electromagnetic waves, or even cosmic rays.
Quantum Errors
These are the inevitable mistakes that arise during quantum operations. If
you think of quantum computations like an intricate dance, a quantum error
is a misstep. It can occur during the processing or the reading of quantum
data. Sometimes, it's a bit flip (where a qubit changes from 0 to 1 or vice
versa) or a phase flip (a shift in the qubit's phase). While individual errors
can be minor, their collective impact can be detrimental, especially in large
quantum circuits.
Decoherence
The real nemesis of any quantum computer. It's the process where quantum
systems lose their quantum behavior and start acting classical. A good
analogy might be the transition from a dream state (where anything is
possible) to waking reality (where we are bound by the laws of classical
physics). Decoherence is primarily caused by the quantum system's
interactions with its environment. Every time a qubit interacts with the
external world—be it a stray photon or molecule—it loses a bit of its
quantumness.
Our still pond symbolizes our ideally stable qubit. When you drop a pebble
in, the ripples that form represent the anticipated quantum state. But
quantum systems are delicate. Raindrops, signifying quantum noise, can
pepper the pond's surface. These droplets introduce unforeseen
disturbances. Similar to the unpredictability of real-world factors like heat
or electromagnetic interference. Each drop slightly shifts our qubit's state,
obscuring its clarity.
Then, consider a sudden gust of wind as a direct quantum error. These gusts
might be errors in how we control or read our qubits. They don't just disturb
the water — they change the ripple pattern entirely. In this context,
raindrops and wind gusts illuminate the diverse challenges and intrusions
that quantum systems confront, underscoring the complexity of quantum
computation.
Addressing these disturbances is paramount in the race to build scalable
quantum computers. Not only do they challenge the integrity of quantum
information, but they also remind us of the delicate balance between the
quantum and classical worlds. The next step? Designing mechanisms to
counteract these errors is where quantum error correction comes into play.
Error Correction Techniques and Codes
A safety net becomes imperative in the intricate dance of quantum
computation, where the tiniest disturbances can lead to catastrophic errors.
Enter quantum error correction (QEC), our answer to the unpredictable
quirks of the quantum domain. Much like classical error correction ensures
the accuracy of bits in a classical computer, quantum error correction does
so for qubits.
1. Quantum Error Correction Codes (QECC)
These codes are the language we've developed to combat quantum noise
and errors. A primary example is the Shor code, designed by Peter Shor,
which can correct random single-qubit errors. Instead of just using a single
qubit to store a quantum bit of information, the Shor code uses nine qubits.
This nine-qubit system can identify and correct any errors affecting its
individual members, safeguarding the quantum information.
2. The Surface Code
Another fascinating technique, the surface code, takes advantage of a two-
dimensional lattice of qubits. The unique layout allows errors to be spotted
and rectified efficiently as they manifest along the lattice lines. This makes
the surface code particularly robust and has been a favorite among
researchers for its relatively higher error threshold.
3. Cat Codes
These are inspired by the famous Schrödinger's cat thought experiment.
This setup uses superpositions of coherent states — states in which wave
phases are the same —. When errors occur, they shift the phase. These
shifts become detectable due to the unique properties of the superpositions,
enabling timely corrections.
Imagine a masterful orchestra where each musician (qubit) must play in
perfect harmony. Even if a few go off-tune (errors), these correction
techniques act like a conductor, identifying the off-notes and guiding the
musicians back to harmony. The result? Quantum symphonies that resonate
with precision, even amidst the chaos.
Achieving reliable QEC, however, is no simple feat. It requires a multitude
of qubits and sophisticated algorithms. But the progress in this arena has
been promising, laying the foundation for larger, more reliable quantum
machines soon.
Challenges in Achieving High-Fidelity Qubit Operations
In quantum computing, achieving high-fidelity qubit operations is one of
the most formidable challenges. Think of fidelity in this context as the
trustworthiness of a qubit to execute its job accurately. Just as an expert
pianist strives for each note to be pitch-perfect in a concert, we desire each
qubit operation to be flawless in a computation. But, much like that pianist
facing disturbances from a mistuned instrument or a noisy environment, our
qubits have to deal with multiple issues.
1. External Environmental Disturbances
The outside world is full of activity—vibrations, temperature fluctuations,
electromagnetic fields. These can sneak into the delicately protected
quantum domain and cause disturbances. The fragility of quantum states,
especially entanglements, makes them exceptionally susceptible to such
external interferences.
2. Hardware Imperfections
Even the slightest material defects or impurities can influence qubit
behavior. This is particularly relevant for superconducting qubits, where
microscopic imperfections can lead to substantial irregularities. Also, the
equipment used to manipulate and read out qubits—like microwave pulses
in superconducting circuits—must operate with impeccable accuracy. A
slight misalignment or malfunction can skew results.
3. Decoherence
As previously discussed, quantum systems tend to lose their unique
quantum characteristics over time due to interactions with their
environments—a phenomenon termed decoherence. The longer a qubit can
maintain its quantum state before succumbing to decoherence, the better.
However, ensuring long coherence times is challenging due to the inherent
instability of quantum states.
4. Error Accumulation
Even minor error rates can accumulate in extended quantum computations,
leading to significant problems. For quantum computers to be practically
helpful, these error rates must be reduced to below certain thresholds, a
challenge that requires hardware improvements and innovative error
correction techniques.
5. Scaling Up
While we might achieve high-fidelity operations with a handful of qubits,
scaling this to hundreds, thousands, or even millions of qubits without
compromising accuracy is a herculean task. As the number of qubits
increases, so does the potential for errors and the complexity of error
correction.
How to manage a colossal orchestra with various instruments. The larger it
grows, the harder it becomes to ensure each musician is in perfect sync, and
the challenge intensifies when they're playing a piece that demands intricate
coordination.
Despite these challenges, the field is advancing steadily. Each hurdle, while
substantial, spurs innovation and refinement, pushing us closer to the era
where quantum computers could redefine our technological landscape.
Quantum Hardware & Ecosystem
Problem-Solving Quantum Tech
In the intricate dance of technology, quantum computing has gracefully
transitioned from chalkboard theories to physical phenomena in cutting-
edge labs. This section goes into the monumental strides of quantum
hardware, painting a picture of an ecosystem on the cusp of reshaping
computational boundaries.
Central to the evolution of quantum machinery is the marriage between
quantum mechanics and materials science. Qubit stability—required for
quantum processing and storage—builds its foundation from the very
materials it's sculpted from. Princeton's groundbreaking study weaves this
narrative, spotlighting how material advancements will be a pillar for
quantum computers' future sophistication. Key takeaways include:
1. The essence of quantum development is firmly anchored in
materials science. Princeton's insights reveal how materials—from the
precision of superconducting circuits to the subtlety of trapped ions—
are pivotal in crafting trustworthy qubits, the heartbeats of quantum
machines.
Quantum computing's journey, while bursting with
achievements, hasn't been without its set of problems.
Quantum states' delicate nature demands innovative error
correction and qubit stabilization. Furthermore, the quest for
efficient quantum interconnects (QuIC) illuminates the
importance of open communication in quantum architectures.
A mass adoption in quantum algorithm design is unfolding,
harnessing the inherent qualities of quantum mechanics, such
as entanglement and superposition. These emerging
algorithms beckon solutions for herculean tasks, from
deciphering cryptographic enigmas to pioneering materials
science research.
From its embryonic stages, colored with academic curiosity,
quantum computing has soared, attracting government and
tech titans. Their collective vision? To unlock the game-
changing capacities of quantum systems.
As quantum computing charts its path forward, the nuances of hardware
and material sciences become its foundational pillars. The dance between
qubit architectures and the materials that form them plays a primary role in
defining the future. We will scan the quantum hardware landscape to see its
challenges and breakthroughs. It is time to spotlight how material sciences
support and actively shape this dynamic journey.
Material Sciences
Sculpting the Backbone of Quantum Computing
In quantum computing, the substance lies not just in mathematical
algorithms but deeply in the fabric from which these machines are woven:
their materials. The meticulous engineering and crafting are the passage
between quantum theories and real-world quantum computers.
At the heart of this craft is the design of 3D integrated circuits. Unlike their
2D counterparts, these circuits use vertical space, leading to denser
configurations and enhanced quantum interactions. However, constructing
these requires specialized insulators that prevent unwanted interactions and
ensure the delicate quantum states aren't easily disturbed.
High-purity silicon is another heavyweight in the quantum arena.
Renowned for its consistent and almost pure atomic structure, this material
provides a stable foundation for quantum dots. But the real magic lies in
achieving and maintaining that purity. Clean interfaces, absent of any
contaminating elements, become crucial. Cleaner interfaces reduce the
chances of quantum data leakage, ensuring qubits function optimally.
The process of molecular beam epitaxy is pivotal in this scenario. It allows
scientists to deposit atomic layers of materials in a highly controlled
manner. This precision layering ensures superior insulation and fewer
imperfections, providing a controlled environment where qubits can thrive
without interference.
It's not enough to just have pure materials; their assembly needs to be
precise. The art of quantum material crafting calls for exactness in placing
every atom, especially when dealing with metals and crystals. Enhanced
fabrication techniques ensure these interfaces, where materials meet, remain
uncontaminated, preserving the integrity of the quantum states.
10 Milestones Before Mastery
The map of quantum computing, particularly in hardware, is marked by
significant achievements. As we navigate this journey, we must highlight
the key destinations researchers aim to hit before quantum computers reach
their peak performance.
Qubit Stability & Coherence
At the heart of quantum computation lies the qubit, a unit that's notably
finicky. It must maintain its quantum state, or "coherence", for effective
operation. Recent efforts have led to impressive enhancements in the
coherence times of superconducting qubits and trapped ions, two vanguards
of qubit technologies.
Navigating the NISQ Waters
Quantum computing is presently in the Noisy Intermediate-Scale Quantum
(NISQ) era. Although these machines are error-prone and lack fault
tolerance, they serve as invaluable testing grounds, allowing researchers to
refine algorithms, materials, and methodologies.
Error Management with Quantum Finesse
The delicate nature of qubits makes them susceptible to errors. To address
this, the quantum community is intensely developing quantum error
correction methods, with techniques like the surface code designed to
maintain quantum states even when mistakes occur.
The Material Science Vanguard
Breakthroughs in materials science remain a cornerstone of quantum
progress. By exploring new terrains like topological qubits and mysterious
Majorana fermions, scientists hope to fix disturbances by crafting both
scalable and robust qubits.
Cryogenics, The Quantum Chiller
Keeping qubits cool and in their quantum state is no small feat. As we plan
for quantum systems with an ever-growing number of qubits, innovations in
quantum cryogenics, like advanced dilution refrigerators, become
paramount to managing heat efficiently.
Modularity & Quantum Web
A future where multiple small-scale quantum processors unite to form a
quantum powerhouse is on the horizon. This modular vision goes hand in
hand with the dream of quantum networking, laying the groundwork for a
quantum internet.
Bridging Quantum & Classical
While quantum processors hold the spotlight, they lean heavily on classical
systems for control and error mitigation. The interplay and seamless
integration between quantum and classical realms remain a priority.
The Commercial Quantum Race
Giants like IBM, Google, and Intel, as well as emerging startups, are in a
sprint, investing in quantum hardware. Milestones like Google's "quantum
supremacy" announcement in 2019 underscore the rapid pace of
commercial progress. Since Google's announcement, other companies have
also claimed to have achieved quantum supremacy. However, the definition
of quantum supremacy is still somewhat vague, and there is some debate
about whether these other claims are valid.
Qubit Diversity
Beyond the popular superconducting qubits and trapped ions, the quantum
world buzzes with research on alternatives, from photonic quantum
computing to neutral atom qubits and silicon quantum dots. This diversity
promises multiple paths to quantum mastery.
The Scaling Imperative
The allure of boosting qubit count is undeniable, but quantity alone doesn't
win the race. The connectivity, error rate, and overall quality of these qubits
remain equally, if not more, vital.
Each of these milestones, while serving as markers of progress, also paves
the way for deeper exploration into the very fabric of our universe. In the
next section, we'll explore how quantum principles apply at the molecular
level.
Molecular Quantum Mechanics
Quantum chemistry plays a significant role in understanding and designing
materials for quantum computers. Here's how:
1. Foundational Understanding: At its core, quantum chemistry deals
with the quantum mechanical behavior of electrons in atoms and
molecules. This understanding is essential for predicting how matter
behaves at the quantum level, especially when considering raw
materials for qubits or other quantum systems.
2. Designing New Materials: Quantum chemistry simulations can
guide the creation of novel materials with specific electronic or
magnetic properties tailored for quantum computation. By
understanding how electrons interact in a given material, scientists can
engineer new tools with desirable quantum computing properties, such
as longer coherence times.
3. Superconductivity: Many quantum computers rely on
superconducting qubits. Quantum chemistry provides insight into the
pairing mechanisms of electrons in superconductors, which is crucial
for designing better superconducting elements suitable for quantum
computation.
4. Quantum Dots: As mentioned before, quantum dots are
semiconductor nanocrystals that exhibit quantum mechanical
properties. Quantum chemistry offers insights into the electronic
structures of these dots, influencing their design and implementation in
quantum systems.
5. Interaction with Environment: One of the challenges in quantum
computing is the interaction of qubits with their environment, leading
to decoherence. Chemists can model and predict these interactions,
helping in the design of materials that minimize such unwanted
interactions.
6. Optimizing Interfaces: For solid-state quantum systems, the
interfaces between different materials can be sites of noise and loss.
Quantum chemistry can model these interfaces, guiding the selection
and design of materials to minimize these issues.
Quantum chemistry bridges the abstract practical domain of material
science. By simulating and predicting the quantum behaviors of materials,
quantum chemists help strive for the optimization and innovation of
futuristic computers.
The convergence of improved materials, innovative architectures, and
advanced engineering solutions paints a promising picture for quantum
computing's future. The next frontier involves not just the development of
larger quantum systems but ensuring they are robust, interconnected, and
geared for real-world applications. No computer is complete without
software! Software is undeniably a critical part of the quantum computing
ecosystem. It dictates how we utilize and maximize the potential of the
hardware.
Quantum Software
The Code Behind the Quantum Leap
As quantum hardware matures, the software unleashes its full potential.
Without sophisticated software frameworks, even the most advanced
quantum computer would lack a powerful engine to channel its energy.
1. Quantum Algorithms: At the heart of quantum software are
quantum algorithms, which leverage quantum phenomena to process
information in ways unattainable for classical computers. Algorithms
like Shor's for factoring or Grover's for search promise computational
speeds that classical counterparts can't match.
2. Programming Languages: Just as classical computers have C++,
Java, or Python, the quantum world has seen the emergence of
specialized languages. Q# from Microsoft and QuTiP are just a few
examples tailored to describe quantum processes and operations.
3. Quantum Compilers: Transforming quantum code into operations
that a quantum chip can understand is no small task. Quantum
translations need to be far more sophisticated, considering the nuances
of quantum physics and the unique architecture of quantum processors.
4. Noise and Error Mitigation: While hardware researchers focus on
error correction at the physical level, software plays a crucial role, too.
Algorithms are being developed to either be powerful against noise or
compensate for the errors introduced during computation.
5. Hybrid Systems: Given that full-scale, error-corrected quantum
computers are still on the horizon, much of today's work revolves
around hybrid systems. These combine the strengths of classical and
quantum computers, enabling solutions to problems like optimization
or simulations that leverage the best of both worlds.
6. Quantum Software Platforms: Major tech entities and startups
offer platforms where quantum algorithms can be tested and refined.
IBM's Qiskit or Google's Cirq provide open-source tools for budding
quantum programmers to experiment, learn, and innovate.
7. Quantum Simulators: Before running algorithms on real quantum
hardware, they are often tested on quantum simulators. These software
tools mimic the behavior of quantum systems, allowing for debugging
and optimization in a controlled environment.
8. Applications and Industries: Quantum software's final frontier is
actual applications. From cryptography to drug discovery, the potential
domains that can benefit from quantum computation are vast. Each
application demands its own set of specialized software tools and
algorithms.
Quantum software is the bridge that translates theoretical quantum
advantages into real-world outcomes. As the hardware landscape evolves,
so will the software ecosystem, ensuring that when quantum computers are
ready for mainstream use, the tools to harness their power will be well-
refined and poised for evolution.
Quantum Software vs. Classical Software
Nature of Operations: Classical software operates on bits.
Quantum software must account for qubits in a superposition
of states and entangled with other qubits. This fundamental
difference has a lot of upsides but comes with challenges.
Algorithm Complexity: While classical algorithms follow
deterministic logic (given the same input, you'll always have
the same output), quantum algorithms work probabilistically.
You might run a quantum algorithm multiple times to get a
probable correct answer.
Error Handling: Classical computers deal with errors, but
digital error correction is well-understood and manageable. In
quantum computing, error correction is far more challenging
due to the delicate nature of qubits and their vulnerability to
external influences. Quantum software must either be robust
against these errors or work with hardware-level error
correction.
Programming Paradigm: Quantum programming often
requires a shift in mindset. While classical programming can
be very linear, quantum programming requires an
understanding of quantum mechanics' language.
Software-Hardware Interface
Hardware Abstraction: Just as classical software often uses
drivers and other concepts to communicate with a variety of
hardware, quantum software needs to do the same. Given the
variety of quantum hardware (superconducting qubits, trapped
ions, topological qubits, etc.), software platforms aim to
provide a unified interface for programmers.
Hardware-Specific Optimizations: Quantum compilers play
a crucial role. They translate high-level quantum algorithms
(programming code) into machine-readable instructions
specific to the hardware of the quantum processor's unique
architecture and constraints. Ultimately enabling the execution
of programs on the computer.
Feedback Loop: The development of quantum hardware and
software is reciprocal. Insights gained from software
experiments can guide hardware improvements, and
advancements in hardware can open up new possibilities for
software.
Hybrid Systems: Current quantum systems often work in
collaboration with classical systems. Quantum software
typically involves a mix where the quantum computer handles
specific tasks that leverage its strengths, while classical
systems manage other tasks. This collaboration requires
software to efficiently communicate and distribute tasks
between the two.
Ecosystem Collaboration: Many quantum startups and tech
giants are not just building quantum computers but complete
quantum systems - combining hardware, software, and often
cloud-based platforms for users to access quantum resources.
The broader ecosystem recognizes the cohesive nature of
hardware and software, pushing advancements in tandem to
ensure that as quantum computers become more powerful, the
tools to harness them keep pace.
While classical software has matured over decades, quantum software is
still in its early stages. Yet, its rapid evolution, driven by unique challenges
and opportunities, sets the stage for computational dominance.
Driving Quantum Computing Forward
The quantum computing ecosystem is broad and includes diverse players
ranging from tech giants to startups, research institutions, and more.
1. Tech Giants: These are industry leaders with extensive resources,
continually pushing the boundaries in quantum research. For instance,
IBM emphasizes the creation of an open community as fundamental
for the future of quantum computing. Their efforts in the field
underscore the importance of collaboration and open-source principles.
2. Startups: Young and agile startups bring fresh perspectives to the
table. They're mainly focused on making quantum computing practical
and scalable, working diligently to transition quantum technologies
into usable applications.
3. Research Institutions: Universities and labs globally champion
quantum research. The US, for instance, stands out as a leading player
in quantum tech research and investment. These institutions often
collaborate with tech giants and startups, fostering a cohesive
environment for advancement.
4. Government and Public Entities: Quantum technology is
increasingly recognized as the next big thing. Countries and public
institutions worldwide are investing in research and formulating
policies to support the ecosystem's growth.
As the quantum ecosystem continues to expand, openness, partnerships, and
investments from these entities will be essential in realizing the potential of
quantum computing in industries such as pharmaceuticals, chemicals,
automotive, and finance (see Chapter Five).
Ecosystem Expands in All Directions
Quantum computers are rapidly evolving thanks to multifaceted
collaborations, pioneering research initiatives, and key investment
strategies. As this technology promises to transform various sectors,
understanding the main drivers behind its progress becomes crucial. A
closer look at the quantum ecosystem reveals how these collective efforts,
whether through research, strategic alliances, or funding surges,
turbocharge quantum computing in a big way.
1. Collaborative Research: Quantum advancements have been
supported by platforms like Classiq, which enable the effective
creation and execution of quantum algorithms, addressing real-world
use cases. Additionally, academic institutions, such as UC Santa
Barbara, have forged collaborations with tech giants like Cisco
Systems to push quantum technology forward.
2. Investment Activities: A testament to the thriving interest in
quantum technology is the rise of numerous quantum computing
companies. Private investment in quantum technology reached a
record high of $2.35 billion in 2022. Some of that went to IonQ, which
raised $450 million; Rigetti raised $300 million, and PsiQuantum
raised $450 million. That's only a few of the 90-plus companies
currently raising money.
3. Strategic Partnerships: The quantum ecosystem is not limited to
research labs but has extended to corporate boardrooms. This evolution
signifies the birth of new hardware, software, and crucial partnerships
that aim to accelerate quantum computing's practical applications.
4. Public-Private Initiatives: While not explicitly stated in the search
results, public-private collaborations, like the one between UC Santa
Barbara and Cisco Systems, serve as examples of how governments
and private entities are joining forces to further quantum research and
its applications.
The transformative power of quantum computing hinges on technological
advancements and the synergetic collaborations and investments
underpinning its growth. The unified commitment towards exploring and
unlocking quantum potential is clear from academia to corporate giants.
This holistic approach in research, partnerships, and public-private
initiatives is foundational in steering quantum computing's trajectory from
aspiration to actualization.
Whether you know it or not, your company might already be tapping into
quantum systems. If you're an entrepreneur, perhaps you've ventured into
the world of quantum simulators, harnessing their computational might for
your projects. The quantum revolution is not just on the horizon; it's already
merging with our professional landscapes. As we advance, let's shift our
focus to Quantum Simulation, revealing how it's developing our
understanding of complex systems.
The Power and Practice of Quantum Simulation
Whether you're a cutting-edge researcher, industry innovator, or an
enthusiast, step into the quantum simulator. At its core, a quantum simulator
is a specialized computer designed to mimic and predict the behavior of
quantum systems. This tool is a testament to the advancements in the field,
bridging the abstract world of quantum mechanics with practical
applications.
But what makes a quantum simulator so unique? It operates using qubits,
the fundamental units of quantum information. While early simulators were
celebrated for merely utilizing a handful of qubits, technology's relentless
march forward now sees devices wielding anywhere from a few to nearly a
thousand qubits. However, it's essential to remember that the game isn't
solely about stockpiling qubits. The quality of these qubits, including
aspects like their error rates, coherence times, and connectivity, plays a
pivotal role in the simulator's efficacy. In some scenarios, a simulator with
fewer but higher-quality qubits can outshine one boasting a greater amount
but of lesser integrity.
Understanding quantum simulators means appreciating both their
complexity and their potential. They are the looking glass into quantum
phenomena, enabling scientists to decipher intricate quantum behaviors
without having to observe them directly in nature. As we look into this
section, you'll discover the applications of these powerful devices in various
industries.
Quantum simulation plays a pivotal role in numerous industries:
1. Energy Applications: Quantum simulations will help us understand
complex energy systems. They're being leveraged to optimize and find
efficient energy solutions, from advancing solar cells to understanding
nuclear fusion processes.
2. Chemistry: The molecular world is rife with complexity. Quantum
simulators provide unparalleled precision in simulating chemical
reactions, which is of significant value in areas like drug discovery and
material science. For instance, Peter Morgan showcased the potential
of quantum computers in simulating chemical reactions to drive
advancements in these sectors.
3. Material Science: Quantum simulations can probe the properties of
new materials at the quantum level, facilitating the discovery of novel
substances with desired properties, from superconductors to efficient
insulators.
4. Business and Financial Modeling: Quantum simulators can handle
complex financial models, optimize portfolios, and manage risk with
higher precision.
5. Advanced Chemical Reactions: In the field of electric vehicles
(EVs), quantum simulations play a part in understanding and
optimizing battery chemistry, holding the potential to revolutionize the
EV industry.
These applications signify just the beginning of what's possible. Their
efficacy, however, is deeply entangled with the algorithms and techniques
employed. Let's explore the complexities of the 'Algorithms and Techniques
for Quantum Simulation' to further grasp how these simulations achieve
their remarkable results.
The algorithms and techniques underlying quantum simulation are intricate
and aimed at mimicking quantum phenomena that classical systems can't
efficiently handle. Here are some core techniques:
Basic Quantum Algorithms
The foundational algorithms in quantum computing provide the bedrock for
advanced simulations. Such algorithms include quantum Fourier transform
and phase estimation, often serving as building blocks for more complex
simulation tasks.
Variational Quantum Eigensolver (VQE)
VQE is a hybrid algorithm that optimizes quantum circuit parameters to
approximate a quantum system's ground state energy. It has the advantage
of being executable on near-term quantum devices, making it valuable for
tackling optimization problems.
Polynomial-time Quantum Algorithms
Traditional simulations on classical computers can exponentially increase
computational costs as the quantum system size increases. However,
specific quantum algorithms have been designed for simulations, achieving
polynomial time complexities.
Simulating Dynamical Quantum Phases
Techniques such as the sequential quantum circuit on superconducting
quantum devices are optimized to simulate specific quantum states, like
ground states, highlighting the flexibility and adaptability of quantum
simulators.
As quantum technology advances, we'll see even more sophisticated
techniques emerge, further blurring the lines between the quantum domain
and our digital computational capabilities.
Utilizing a quantum simulator allows you to harness quantum effects to
probe questions about model systems, which in turn can shed light on real-
world scenarios. To use a quantum simulator effectively:
Understand the Basics
Grasp the fundamental concepts of quantum mechanics and how quantum
simulators differ from traditional quantum computers. Quantum simulators
are designed to mimic specific quantum systems that might be difficult to
study directly.
Choose the Right Platform
Various platforms, like Google Cloud, offer tools for quantum simulation.
For instance, Google Cloud provides a tutorial on simulating a quantum
circuit using Cirq and qsim.
Install Necessary Tools
Depending on the platform, you might need to install specific software or
packages. For instance, with Google's tutorial, you'd learn how to install
Cirq and set up qsim.
Study Existing Models
Dive deep into theoretical and experimental development of quantum
simulation using quantum computers to understand established techniques
and methodologies.
Push the Boundaries
As quantum technology evolves, new phenomena are being discovered,
such as entirely new phases of matter simulated using quantum computers.
By staying updated, you can harness the full potential of quantum
simulation.
As the curtain of this chapter draws to a close, it's crucial to underscore the
dynamic nature of the quantum world. The quantum computing and
simulation field is not static; it's like a river, continuously flowing and
changing its course. Every day, new developments emerge, paradigms shift,
and what we believed was once the frontier becomes the new standard.
To aid your journey and help you navigate these rapid waters, I've compiled
a resource you'll find invaluable: the "Quantum Toolbox." (Chapter 6). This
is not just a list; it's a compass. You'll discover a comprehensive list of
quantum simulators within its digital confines and explore practical
applications. But the toolbox doesn't stop there. It also directs you to the
most insightful online courses available, helping beginners and seasoned
quantum enthusiasts expand their knowledge and hone their skills.
With the Quantum Toolbox at your fingertips, you can confidently navigate
the quantum domain. But as you proceed, it's equally essential to clear the
haze surrounding quantum computing. Over time, misconceptions, myths,
and misunderstandings have weaved a web of confusion for many. But
worry not, for our next chapter aims to untangle this web.
The next chapter, Dispelling Common Misconceptions of Quantum
Computing, promises to debunk myths, clarify confusions, and set the
record straight. Together, we'll scrutinize common misunderstandings,
ensuring you're knowledgeable and accurate in your quantum pursuits.
Chapter 3
Dispelling Quantum
Misconceptions
Unmasking Illusions in the Enchanted Forest
Welcome, brave explorer, to the Enchanted Forest of Quantum Computing
—a world shrouded in both reality and myth. Walking through the twisted
groves of qubits and quantum gates makes it easy to be hypnotized by the
whispers of instant computational power or universal encryption-breaking
abilities. However, much like any enchanted forest, not all that glitters is
gold, and not all tales told are true.
This chapter aims to light a path through this dense bush of
misunderstanding. With each step, we'll unmask the illusions that have cast
a shadow over the actual potentials and limitations of quantum computing.
Armed with the truth, you'll emerge from the forest not just enchanted, but
enlightened.
Prepare to investigate ten of the biggest misconceptions about this mystical
domain. Along the way, you'll gain a balanced perspective on what quantum
computers can and can't do, and dispel some common myths surrounding
their speed, utility, and complexity.
Ready your lanterns and sharpen your mental machetes; we're about to
debunk some illusions and reveal the genuine magic of quantum computing.
Quantum Technology is Mainstream
In our Enchanted Forest of Quantum Computing, many adventurers have
heard whispers of a mystical waterfall that grants wisdom and untold power
—much like the widespread belief that quantum technology is as
mainstream as the smartphones in our pockets.
Yet, like the mainstream status of quantum technology, the waterfall is not
as prominent as it appears. While there have been substantial advancements,
the majority of quantum computing applications remain experimental. The
few areas with practical use are specialized, primarily in research labs or
specific industrial sectors.
Believing that quantum technology is already mainstream can create
inflated expectations and potentially lead to poor decision-making.
Companies may prematurely invest in quantum computing, and individuals
may mistakenly think they need an immediate understanding of quantum
principles for career advancement.
Imagine walking through the forest, captivated by the shimmering mirage
of a waterfall up ahead. As you approach, the mirage dissipates into mist,
thirsty for its promised power. This evaporation reflects the fading allure of
the mainstream status of quantum technology upon closer scrutiny.
Quantum technology is undeniably promising and is making strides toward
broader applicability. However, it's crucial to understand that it's still
primarily a technology of the future, not the present.
Quantum Computers are Only for Academic Research
Touring further into our Enchanted Forest of Quantum Computing, you
might encounter trails that are rumored to lead nowhere, much like the
illusion that quantum computers serve only the arcane interests of
academia.
Many explorers believe these "useless" trails are worth skipping in favor of
more traveled paths, just as some people think quantum computers are only
accessible to academic institutions, with no relevance in the free world of
open source.
As it turns out, these hidden trails may lead to unseen treasure troves.
Quantum computing isn't just for academic research. It has potential
applications in various industries like pharmaceuticals for drug discovery,
finance for optimization problems, and climate modeling.
The utility of quantum computing extends far beyond the university. While
the technology is indeed prominent in research, its potential for real-world
impact is too substantial to ignore. Venture capitalists, large corporations,
and startups are increasingly investing in quantum computing for practical
applications.
Picture walking past a hidden trail, convinced it's a dead-end, only to learn
later that it led to a treasure trove of valuable resources. Ignoring the
potential applications of quantum computing outside academia is similar;
you might miss out on a wealth of opportunities.
Quantum computing is not confined to the ivory towers of suit coats and
academia. It has a burgeoning role in solving everyday problems and offers
a landscape of untapped potential.
As we leave behind the hidden trails filled with untold riches, we prepare to
navigate another twist in our journey. The myth that quantum computing
and blockchain are like oil and water—entirely incompatible.
Trees of Different Species Coexisting in the Forest
As we tread deeper into the Enchanted Forest, let's stop for a moment at the
forest clearing where two different species of trees seem to grow in tension
—much like the misconception that quantum computing and blockchain
technologies are fundamentally at odds.
Many adventurers of the quantum realm believe that the rise of quantum
computing spells doom for blockchain technologies. The fear is that
quantum computers could easily break the cryptographic foundations that
secure blockchain, rendering it obsolete.
It's essential to recognize that while quantum computing poses a challenge
to current cryptographic techniques, blockchain is not a static technology.
Already, there are ongoing efforts to develop quantum-resistant
cryptographic algorithms, ensuring that blockchain can adapt and coexist
with quantum computing.
The relationship between quantum computing and blockchain is not a zero-
sum game. The advent of quantum computing will likely prompt a
cryptographic evolution, making blockchain technologies even more secure
and robust.
Imagine two distinct species of trees in the forest. At first glance, they seem
to vie for the same resources, but a closer look reveals a balanced
ecosystem where both can thrive. Quantum computing and blockchain are
like these trees: they may seem incompatible, but they can coexist, each
making its unique contributions to the forest—or, in this case, the digital
world.
Blockchain and quantum computing are not enemies but can evolve to
coexist, complementing each other in enhancing security and performance
in the digital realm.
As we admire the diverse foliage of our enchanted forest, let's prepare for
the next leg of our journey. We will venture into the area shrouded by the
misconception that quantum computers will completely replace classical
computers—a belief that could misguide our understanding of both
domains.
Different Tools in an Explorer's Toolkit
As we trek further into the depths of our mystical forest, our explorer's
toolkit comes into focus. Imagine hearing whispers that a new, magical tool
is so powerful that it will render all other gadgets obsolete. This mirrors the
myth that quantum computers will make classical computing a relic of the
past—a prevailing notion among enchanted wanderers and tech enthusiasts
alike.
Many envision a future where our current computers gather dust in attics,
overshadowed by the dazzling might of quantum machines. It's crucial to
understand that quantum and classical computers serve different but
complementary purposes. Quantum computing excels at specific types of
problems—like optimization and factorization—but classical computers are
more efficient and practical for many of our everyday tasks.
In the broader landscape of computational challenges, classical and
quantum computing can work in symbiosis. Many advanced quantum
algorithms, for instance, require a classical computer for pre-processing and
post-processing tasks. Far from making classical computers obsolete,
quantum computers will likely augment their capabilities.
Think of your explorer's toolkit. You wouldn't use a compass to cut through
vines or rely solely on a machete to navigate your path. Similarly, quantum
and classical computers are like different tools in your gear, each excelling
in its specific role.
We are confident that quantum computers aren't destined to replace classical
computers but will work alongside them to solve problems neither could
tackle alone.
Having expanded our toolkit and dispelled the illusion that one tool could
do it all, let's continue our expedition. Our next destination? A mysterious
area in the forest shrouded by the belief that you need a Ph.D. in quantum
physics to touch a quantum computer.
Do You Need a PhD to Speak the Language?
As we continue our adventure, we encounter a stone tablet with complex
inscriptions that might as well be hieroglyphics. Rumor has it that only
those with doctorates in quantum physics can decipher these symbols.
There is a foggy landscape of misconception that you need to be an expert
in quantum physics to begin understanding or even using quantum
computers.
A pervasive myth exists that quantum computing is an exclusive club, only
accessible to those who have mastered the complexities of quantum
mechanics. This daunting belief puts an imaginary barrier around quantum
computing, making it seem as unapproachable as an ancient riddle.
Contrary to the intimidating climate surrounding it, quantum computing is
becoming increasingly user-friendly. With the advent of high-level
programming languages and libraries designed specifically for quantum
computing, like Qiskit, Quipper, and Microsoft's Q#, the field is more
accessible than ever.
While a quantum physics background can certainly deepen your
understanding, it's no longer a strict prerequisite. You don't need to be fluent
in the intricate math behind quantum mechanics to use or even program a
quantum computer. Similar to learning any other programming language,
dedication and practice can get you far.
Imagine stumbling upon a stone tablet with a seemingly indecipherable
code. However, as you approach, the code magically transforms into simple
phrases. Similarly, the complex math behind quantum computing is getting
'translated' into approachable language through user-friendly tools and
platforms.
Don't be deterred by the complexity of quantum mechanics; advances in
quantum computing are bringing the field within reach for a broader
audience.
Having successfully deciphered the 'quantum language,' we realize that this
Enchanted Forest is accessible to many, not just a select few scholars. And
now, we turn our attention to another dark corner of the forest—a place
where quantum computing is deemed too complex for the average person to
grasp.
Can You Solve This Puzzle?
A thick fog envelops us as we journey deeper into the Enchanted Forest.
Our surroundings become hazy and difficult to make out, mirroring the
perception that quantum computing is only suited for the minds of geniuses.
Many believe that understanding quantum computing is like cracking a
cryptic code: almost impossible unless you're a genius in math and physics.
The notion is that quantum computing is so complex it's out of reach for
ordinary people.
The fog may feel overpowering, but it isn't. Simple concepts like
superposition (being in multiple states at once) and entanglement (particles
affecting each other regardless of distance) are at the core of quantum
mechanics. Yes, the math behind it is complex, but you don't need to master
it to appreciate the fundamental principles.
Understanding quantum computing doesn't require plunging into wave
equations or matrix algebra. Many universities and online platforms offer
introductory courses to quantum computing that require no more than a
high school understanding of math and science.
As we navigate through the fog, it suddenly lifts, revealing a beautiful
landscape full of life and discovery. The once impenetrable haze was just a
passing phase, much like the initial intimidation one might feel when
approaching quantum computing. It looks difficult at first glance, but it's
possible to grasp its basic principles without being a genius or a quantum
physicist. The complexity burns away as you continue to understand.
We've moved past the haze and find ourselves eager to explore more of
what this forest has to offer. Similarly, the initial complexity of quantum
computing starts to make sense as we chop through the thick foliage. But
beware, the next trail leads us to the castle of cryptography—a place where
many believe quantum computing holds the keys to every locked door.
The Fallacy of Immortal Decryption
As we traverse deeper into the Enchanted Forest, we come upon a secretive
castle of cryptography. Whispers among travelers suggest that a powerful
sorcerer resides here, wielding the power to unlock any enchanted seal or
guarded door. This fable parallels the fear that quantum computers will one
day break all forms of encryption, exposing our data and secrets completely.
The myth goes like this: once quantum computers are advanced enough,
they will crack open all existing encryption algorithms, making all security
measures obsolete overnight. Your online banking, sensitive emails, and
even national security secrets would be revealed.
Before you start guarding your secrets even more anxiously, it's essential to
know that cryptographers are already on the case. They're developing
quantum-resistant algorithms that even the most potent quantum "sorcerer"
will find challenging to break. Even if quantum computers could,
theoretically, break some existing encryption methods like RSA, they won't
make all forms of encryption obsolete.
The cryptographic world is in a dynamic transformation with the
development of quantum computing. As quantum computers inch closer to
breaking certain encryption types, cryptography is evolving to stay ahead,
ensuring a future where they ethically coexist.
In the forest, the "sorcerer" is not invincible but rather a balanced part of the
ecosystem. While he may have the skills to break simpler charms and
spells, the forest is also home to enchantresses and wise sages developing
even stronger magic to counterbalance his power. Ensuring harmony and
balance are maintained in this ever-evolving landscape.
Leaving the castle, we understand that no single entity—be it a sorcerer or a
quantum computer—holds absolute power in the Enchanted Forest or the
world of encryption. As we migrate to the next corner of the forest, we'll
explore another popular myth—that all algorithms run faster on quantum
computers.
Quantum Isn't Always Quicker
Navigating through the Enchanted Forest, we come upon a stream with an
unusual reputation—it's said to make anyone who swims in it instantly
faster. The misconception is that all algorithms will run faster if executed on
a quantum computer. The allure of "speed" can make this stream seem like
a universal solution to all of our problems.
The general perception is that quantum computers, by their very nature, will
speed up any algorithm you throw at them. Need to sort a list? A quantum
computer will do it faster. Want to find the shortest path in a network?
Surely, a quantum computer is quicker. This myth suggests that quantum
computing is a turbo-button for all computational tasks.
While it's true that quantum computers show exceptional speed for specific
problems—like integer factorization or searching unsorted databases—their
speed-up is problem-specific. For many traditional algorithms, a classical
computer is as fast, or even faster, and far less resource-intensive.
Quantum computers excel at solving inherently quantum problems or can
be effectively mapped onto a quantum framework. Their advantage is not
universal but instead confined to certain kinds of troubles. Knowing this
helps focus research and investment into quantum computing, where it can
offer the most benefit. It's not a panacea for all slow-moving features but
offers its gifts selectively.
As we leave the mystical stream behind, we find ourselves asking the
overarching question: "Are quantum computers genuinely fast"? This will
be the focus of our next observation in the Enchanted Forest of Quantum
Computing.
Are Quantum Computers Crazy Fast?
As we hike deeper into the Enchanted Forest of Quantum Computing, we
encounter tales of the Swiftfoot—a mythical creature famous for outrunning
the wind itself. The legend of Swiftfoot is not unlike the widespread belief
that quantum computers are universally faster than their classical
counterparts.
The invasive myth is that quantum computers, equipped with superposition
and entanglement, can solve any problem faster than classical computers. In
this view, a quantum computer is like a super-charged engine that will blaze
through every computational race.
While quantum computers do possess certain speed advantages, these
advantages are conditional. Quantum algorithms like Shor's and Grover's
offer exponentially faster solutions to specific problems, such as integer
factorization or unsorted database searching. However, classical computers
still hold their ground for many routine tasks and algorithms.
It's critical to understand that quantum computers generally aren't 'faster'.
They are faster at specific tasks that are inherently suited for quantum
computation. This specialty makes them invaluable for some complex
issues but not a universal replacement for classical computing speed.
Upon tracking the elusive Swiftfoot, we find that its legendary speed isn't
constant. It only displays its breathtaking swiftness when the stars align or
when chasing particular prey. Recognizing this can help us align our focus
during research and development.
As we bid farewell to the mystical Swiftfoot, our next quest awaits. It
involves an even more enigmatic and misunderstood notion: the idea that
quantum computers operate in multiple universes. Let's tread lightly as we
approach this next layer of quantum mythology.
The Guardian of Many Worlds
Ah, at last, we arrive at the heart of the Enchanted Forest to confront the
elusive "Guardian," a creature said to exist in not just this realm but many
others. Just like this mythical being, quantum computers are often believed
to operate across multiple universes—a concept that captures our
imagination but distorts our understanding.
Many envision quantum computers as magical devices that tap into parallel
universes to perform their computations. This popular belief is inspired by
the Many-Worlds Interpretation (MWI) of quantum mechanics, which is
often misunderstood or oversimplified.
While MWI is a legitimate interpretation of quantum mechanics, it's critical
to understand that it is not the only interpretation, nor is it universally
accepted. More importantly, you don't need multiple universes to explain
how quantum computers work. They operate based on superposition and
entanglement, well-understood principles within our own "universe."
As enticing as it is to consider quantum computing a byproduct of multiple
universes, it's essential to ground this technological marvel in the scientific
framework that birthed it. Understanding quantum computers within the
boundaries of our current scientific knowledge keeps us from veering into
the realm of science fiction.
When finally encountered, the Guardian turns out to be a wise old tree. Its
roots don't stretch into other worlds, but its branches provide shelter and
wisdom in this one. Similarly, quantum computing doesn't require an
alliance with parallel universes to manifest its incredible capabilities.
As we part ways with the Guardian, the mythical and the mystical begin to
blur into the mist of the forest. But the path to understanding quantum
computing continues to stretch beyond the horizon. Are you ready for what
comes next? Your newfound clarity will surely make the journey less
treacherous and far more enlightening.
Emerging from the Enchanted Forest, Enlightened
Our adventure through the Enchanted Forest of Quantum Computing has
been transformative, abundant with revelations and unexpected turns. We've
debunked some deeply rooted misconceptions—from the idea that quantum
technology is already mainstream, to the mysterious notion that it operates
across multiple universes. Along the way, we've not only dispelled illusions
but also shone a light on the genuinely enchanting qualities of this
groundbreaking technology.
Key Takeaways
1. Quantum technology is in its infancy—it's not yet
mainstream but has real-world applications.
2. Beyond academia—quantum computing has the potential
to revolutionize various industries.
3. Compatibility with existing technology—quantum
computing can co-exist with technologies like blockchain.
4. Complement, not replace—quantum and classical
computers serve different, complementary roles.
5. Accessible to many—you don't need to be a physicist to
use or understand quantum computing.
6. Democratization of understanding—quantum computing
is not too complex for laypeople to grasp.
7. Encryption Evolution—quantum computers pose
challenges but also drive cryptographic innovation.
8. Niche advantages—quantum computing offers specific
speed-ups, not universal ones.
9. Speed is Relative—quantum computers excel at particular
tasks but are not universally faster.
10. Single Universe Understanding—quantum computing
can be explained within our known physical laws.
Understanding quantum computing—the more you explore, the more you
realize there is to discover. So, let this chapter not be an endpoint but a
stepping stone in your ongoing quest for understanding this revolutionary
technology.
Next Chapter
Quantum Algorithmic Treasures—Unlocking the Cryptographic
Caves
If you've enjoyed this journey through the Enchanted Forest, the exploration
is far from over. Next, we're diving into the intricate world of Quantum
Algorithmic Treasures. Here, we'll unlock the Cryptographic Caves where
the promise of quantum computing holds the keys to unprecedented
computational wonders. Are you ready to start the next quest? The
Cryptographic Caves await your arrival, and their secrets are begging to be
unearthed.
Armed with your new understanding and free of misconceptions, the path
ahead is clear. The forest may still be enchanted, but you, brave explorer,
are now enlightened. Onward to the Cryptographic Caves!
Bonus
Quantum Computers and AI Will Take Over the World
Many of us have been enchanted by science fiction tales where highly
intelligent machines become sentient and decide to overthrow humanity.
Throw quantum computing into the mix, and you have a recipe for a
gripping apocalyptic narrative. The misconception is that combining
quantum computing and artificial intelligence will inevitably lead to a
future where machines rule, and humans are downgraded to secondary roles
—or worse.
Quantum computers, given their immense computational powers, coupled
with the learning capabilities of AI, will evolve beyond human control,
leading to a "Judgment Day" scenario.
While it's true that quantum computing can potentially revolutionize AI,
making it more efficient and capable, it's crucial to remember that both
technologies are tools created and controlled by humans. Ethical guidelines,
governance, and safety precautions are all areas of active research aimed at
ensuring that these technologies are developed responsibly.
Quantum computing will likely advance AI, but it won't make AI inherently
evil or uncontrollable. Ethical use of technology depends on human choices
and societal values, not just the capabilities of the technology itself.
Moreover, practical and theoretical limitations (e.g., error rates in quantum
computing and energy requirements) provide natural checks on runaway
scenarios.
Imagine stumbling upon a powerful wizard and a mighty dragon having a
meeting deep within the Enchanted Forest. The wizard represents quantum
computing, and the dragon symbolizes AI. Together, they have the power to
either protect the forest or plunge it into chaos. Yet, they are bound by
ancient spells and forest laws that prevent them from acting recklessly.
These spells and laws are the ethical guidelines and regulations we place on
technology.
The combination of quantum computing and AI is indeed powerful but not
apocalyptic. Through responsible development and governance, we can
channel their strengths for the betterment of humanity, rather than our
downfall.
You can tell that we here at Pantheon Space Academy are AI optimists. I
hope you enjoyed the 11 misconceptions, topped off with the Hollywood-
worthy notion of AI and quantum computing taking over the world. This
misconception, like the others, unravels under scrutiny. As we move
forward, the real magic lies in using these incredible technologies
responsibly, unlocking their potential without succumbing to imagined
fears.
Chapter 4
Quantum Algorithms
Introduction to Quantum Algorithms and their Unique
Characteristics
Like seasoned explorers about to embark on an epic adventure into the
unknown, we find ourselves at the mouth of the Cryptographic Caves, ready
to enter the captivating world of quantum algorithms. Math might seem too
complex, but with the right mindset and teacher, it is actually an exciting
quest for the rarest of treasures, treasures that have the potential to
fundamentally reshape the digital landscape we know today.
Quantum algorithms, the metaphorical keys that unlock the hidden depths
of these caves, are a rich blend of physics, mathematics, and computer
science. They are not mere replacements for their classical counterparts, but
rather, they constitute an entirely different level of solutions, one which
exploits the fascinating principles of quantum mechanics - superposition,
entanglement, and quantum interference - to solve problems that are
computationally challenging, if not impossible for classical computers.
These algorithms are unique in their ability to perform complex tasks more
efficiently than classical methods. For instance, Shor's algorithm, one of the
gems we shall encounter on our journey, can easily factorize large numbers
that dwarf the most effective classical machines. Grover's algorithm,
another absolute jewel within the caves, offers a speedier search method in
unsorted databases.
While these treasures may dazzle with brilliance, it's essential to remember
that their true worth lies not merely in their novelty but in their
transformative impact on various fields. From cryptography, where
quantum algorithms can potentially break modern encryption systems, to
optimization, where they promise superior solutions to complex problems,
these treasures are poised to revolutionize our technological ecosystem.
As we hike further into this chapter, we will seek to understand these
quantum algorithms, their unique characteristics, and their real-world
applications. With each step we take, we hope to illuminate these complex
concepts, making the quantum dimension an accessible and enlightening
space for you.
Turn your headlamps on, fellow explorers. The adventure begins now, and
the Cryptographic Caves are waiting to share their secrets!
The Forces Behind Quantum Algorithms
As we hike into our cryptographic cave, we need to understand the
architects of these caves, the tools they employ, and the computational
magic they use to devise these marvelous structures. Quantum algorithms
aren't invented in isolation. They are a culmination of centuries of
mathematical, computational, and scientific knowledge built by pioneers
who dared to defy conventional boundaries.
The Architects
The world of quantum computing has its heroes. Peter Shor, for instance,
who devised Shor's algorithm, set the foundation of quantum cryptography
by demonstrating how quantum computers could potentially crack public-
key cryptographic systems. Lov Grover, another notable figure, introduced
Grover's Algorithm, revolutionizing the field of search algorithms and
optimization problems.
Yet, it's not just individuals; teams of scientists, mathematicians, and
computer engineers across the globe in academic and industrial settings
contribute to this vibrant and dynamic field.
The Tools
Now, imagine these architects as expert masons, skilled in manipulating the
basic building blocks of quantum computation - qubits. Qubits are like
magical stones in our cryptographic caves that exist in multiple states at
once, due to a property known as superposition. Their interconnectedness,
termed entanglement, adds another layer of complexity and power.
Alongside qubits, we have quantum gates, the chisels and hammers that
shape the qubits' behavior. Quantum gates perform operations on qubits,
manipulating their states to execute the steps of a quantum algorithm.
Different combinations of gates result in different algorithms, opening
distinct sections of our cryptographic cave.
The Mathematical and Computational Dance
The process of creating a new quantum algorithm is like a dance between
mathematics and computational science. It begins with a problem - a
cryptographic treasure that we aim to unlock. Mathematicians and
theoretical physicists tackle this by exploring the mathematical structures of
the problem, looking for patterns, symmetries, and properties that could be
leveraged.
For example, Shor identified the period-finding problem hidden within the
prime factorization problem and realized that the Fourier Transform - a
mathematical tool used in signal processing - could help solve it in the
quantum domain. This led to the development of the Quantum Fourier
Transform, a central step in Shor's algorithm.
On the other side of the dance, computational physicists and computer
scientists work on the practical aspects. They look at the mathematical
structures and strategies identified by the mathematicians and design
quantum circuits using qubits and quantum gates that can implement these
structures. They also handle the computational challenges of running these
algorithms on real quantum computers.
Real-World Implication
Let's consider the recent developments in quantum machine learning.
Scientists realized that certain machine learning tasks involve linear algebra
problems that quantum computers could solve more efficiently. Theoretical
physicists and mathematicians broke down these tasks into their numerical
components, identifying quantum versions of familiar machine learning
methods. Meanwhile, computational physicists and computer scientists
worked on implementing these ideas as quantum circuits that can be run on
existing quantum computers.
The result? We're now seeing the first applications of quantum machine
learning, leading to AI that can learn from much larger datasets and model
much more complex patterns than what's currently possible with classical
computers.
Quantum algorithm discovery is an ongoing collaborative effort, powered
by the interplay of mathematics, computational science, and quantum
physics. Each algorithm is a testament to human ingenuity and a step
further into the mesmerizing quantum world.
Characteristics of Quantum Algorithms
The quest to unlock the treasures of the Cryptographic Caves truly begins
when we grasp the fundamental principles that govern the strange and
intricate world of quantum algorithms. The quantum rules vastly differ from
the classical world we are accustomed to. It's as if we have transcended the
known map and set foot in uncharted territory, with landscapes painted by
the brushes of quantum principles such as superposition and entanglement.
A quantum algorithm operates by leveraging these foundational principles,
crafting a confidential playbook yet holding immense potential for
computational breakthroughs. It is like a secret language understood by the
qubits, enabling them to collaborate and perform operations far beyond the
reach of classical bits.
At the core of this language lie the peculiar quantum gates, the basic
building blocks of quantum algorithms. These gates form a distinct
vocabulary of quantum operations, manipulating the states of qubits and
creating intricate interactions between them. In a typical classical algorithm,
the logical gates alter the states of bits deterministically, similar to how
flipping a switch would predictably change a light bulb's state from on to
off.
However, quantum gates operate differently. They change the quantum
states of qubits not in a deterministic but in a probabilistic manner, leading
to a vast array of potential outcomes at the end of a quantum computation.
As a result, quantum gates do not just transport us from one point to another
in the Cryptographic Caves, but they open up an entire range of possible
pathways, each leading to a different treasure.
Another cornerstone of quantum algorithms is the process of measurement,
which introduces an element of unpredictability. While the operation of
quantum gates might prepare a superposition of many possible states, a
quantum measurement collapses this superposition into a single state, with
the outcome governed by probabilistic rules. It's as if multiple paths in the
Cryptographic Caves merge suddenly into a single path at the time of a
treasure's discovery.
Furthermore, quantum algorithms harness the power of interference, a
concept central to quantum mechanics. Quantum interference allows for the
amplification of correct solutions and the cancellation of wrong ones,
manipulating probabilities to guide the algorithm toward the correct answer.
It's as if, in the Cryptographic Caves, the sound of steps leading towards the
right path echoes louder, while those leading towards dead ends are muted.
The principles of superposition, entanglement, quantum gates, quantum
measurement, and quantum interference form the core characteristics of
quantum algorithms. They dictate how these algorithms navigate the
computational landscape, enabling them to perform tasks with an efficiency
that classical algorithms cannot match. However, understanding these
principles is only the first step. To truly value the power and potential of
quantum algorithms, we need to go deeper into the Cryptographic Caves
and explore the famous quantum algorithms. These hidden gems promise to
revolutionize computation.
Concept of Quantum Parallelism
Quantum parallelism is an essential cog in the machinery of quantum
algorithms, operating as a guide within the Cryptographic Caves. Its role is
often compared to having multiple keys at your disposal, each capable of
unlocking different doors simultaneously. This ability to evaluate numerous
possibilities in a single step offers quantum algorithms an edge over their
classical counterparts.
Quantum parallelism springs from the principle of super-position, where a
qubit can exist in a state that is a combination of the base states |0 ⟩ and
|1 ⟩. This contrasts sharply with classical bits that can only be in one of two
states: 0 or 1. Now, suppose we have a string of n qubits. In a classical
computer, an n-bit string can represent one of 2^n possible combinations at
any given time. However, quantum mechanics allows the n-qubit system to
simultaneously exist in a superposition of all 2^n combinations. This means
that a quantum computer can process all these combinations in a single
operation. This ability is the essence of quantum parallelism.
To illustrate the power of quantum parallelism, let's consider a simplified
real-world example. Suppose you are looking for a specific book in a vast
library and don't know which shelf it's on. A classical computer would be
like a single librarian who checks each shelf one by one. If there are a
million shelves in the library, in the worst case, the librarian would need to
check all one million shelves to find the book.
However, suppose we send a million quantum librarians (a nod to our
qubits) into the library. Thanks to the property of quantum parallelism, each
quantum librarian can check a different shelf at the same time.
Consequently, they could theoretically find the book in a single step, a
dramatic speedup compared to the classical approach.
Yet, the story of quantum parallelism doesn't end there. A significant caveat
arises during the measurement process. When we measure the state of our
quantum librarians, quantum mechanics tells us that we'll get just one result,
corresponding to one of the librarians and, thus, one of the shelves. This is
akin to the wave function collapse in quantum mechanics, which results in
obtaining one specific state from a superposition of many possible states.
So, how do quantum algorithms benefit from parallelism if we can only get
one result at the end? The secret lies in the intelligent design of quantum
algorithms. By using interference, they ensure that the wrong answers
cancel out and the correct answer gets amplified. Thus, when we make a
measurement, we are much more likely to find the correct answer.
In the grand expedition within the Cryptographic Caves, quantum
parallelism is like having an army of explorers at our disposal. Each
explorer takes a different path simultaneously, drastically increasing the
chances of finding hidden treasures. The correct path echoes louder, guiding
us toward the treasures, while the echoes of the wrong paths cancel each
other out, helping us avoid unnecessary detours. Quantum parallelism is
essential in designing efficient quantum algorithms, contributing to their
potential to outperform classical algorithms.
The Principles of Superposition And Entanglement
Within the grand Cryptographic Caves, superposition and entanglement,
two pillars of quantum mechanics, act as distinct, transformative forces that
enable quantum algorithms to operate in ways far removed from classical
algorithms.
Superposition, the cornerstone of quantum parallelism we explored earlier,
enables a qubit to exist in multiple states simultaneously. This grants
quantum algorithms the ability to process a multitude of inputs at once.
However, the real magic lies in how quantum algorithms harness this
property.
Take, for example, the famous quantum algorithm Grover's search
algorithm. This algorithm is designed to find an unsorted database's specific
entry, and it does so quadratically faster than any classical algorithm. It
starts by placing all the qubits in a superposition, effectively preparing
every possible answer. Then, through a process called amplitude
amplification, Grover's algorithm manipulates the superposition states so
that the amplitude (probability) of the correct answer gradually increases
while incorrect answers diminish. After repeating this process several times,
a measurement will likely yield the proper response. This clever use of
superposition and interference of amplitudes highlights how quantum
algorithms are uniquely equipped to solve problems efficiently.
If superposition sets multiple paths for the explorers in the Cryptographic
Caves, then entanglement choreographs their synchronized dance.
Entanglement is a peculiar quantum phenomenon where particles become
interlinked, and the state of one immediately influences the state of the
other, regardless of the distance separating them. This instantaneous,
spooky action at a distance, as Einstein famously dubbed it, equips quantum
algorithms with an added layer of complexity and power.
To grasp how entanglement empowers quantum algorithms, let's delve into
the world of quantum teleportation, which utilizes entanglement to transfer
quantum information from one location to another. The two communicating
parties, traditionally called Alice and Bob, share a pair of entangled qubits.
Alice wants to send a qubit state to Bob. She performs a specific operation
involving her qubit and the qubit she wants to send, then measures both.
She sends the results (two classical bits of information) to Bob. Using this
information, Bob performs certain operations on his entangled qubit,
recreating the original quantum state Alice wanted to send. The quantum
state is, in a sense, teleported from Alice to Bob. This teleportation rests
heavily on the eerie entanglement property and is an essential rule in
quantum communication and distributed quantum computing.
In our Cryptographic Caves, if the explorers were entangled, any action
performed by one explorer would instantly affect the other, regardless of the
distance between them. This immediate and coordinated response could
expedite the treasure hunt, guiding the explorers to their desired destination
with increased efficiency and synchronization.
Through the combined forces of superposition and entanglement, quantum
algorithms unlock a realm of possibilities not available in classical
algorithms. By manipulating these core quantum phenomena, quantum
algorithms can process vast quantities of data simultaneously, implement
intricate computations, and potentially solve certain problems more
efficiently, promising a future where the treasure trove of the Cryptographic
Caves could be fully revealed.
The Contrast Between Classical and Quantum Algorithms
Classical and quantum algorithms significantly differ in their operation,
taking separate paths toward problem-solving due to the unique principles
governing quantum physics. Here's an in-depth look at their characteristics
and how they translate into different computational strategies.
In classical computing, data is processed deterministically. That is, given a
specific input, the output is precisely defined, following an exact sequence
of operations. Classical algorithms manipulate bits that exist in one of two
definite states - 0 or 1 - and the state of each bit evolves independently of
others. Each operation within a classical algorithm handles these bits
individually, moving linearly through the computation steps. Essentially,
classical algorithms operate under the laws of classical physics, offering
predictability and certainty in their outcomes.
Contrastingly, quantum algorithms leverage quantum mechanical
phenomena: superposition, entanglement, and quantum interference,
creating an entirely different computational paradigm. In a quantum system,
the superposition of a quantum bit (qubit) offers a broader computational
base.
Superposition plays a crucial role in quantum parallelism, allowing a
quantum system to exist in multiple states simultaneously. This leads to a
significant shift in algorithm design. For example, a two-qubit system can
represent all four possible states (00, 01, 10, and 11) at once. Therefore,
quantum algorithms can perform computations on all these states in
parallel, enabling the handling of numerous possibilities simultaneously.
Entanglement is another principle that distinguishes quantum algorithms.
When qubits become entangled, the state of one qubit is intertwined with
another, no matter their distance apart. Any changes to one qubit
instantaneously affect the other. Quantum algorithms harness this property
to create a high level of coordination among qubits, facilitating complex
computations that would be much more challenging or even impossible for
classical algorithms.
Additionally, quantum algorithms exploit interference, a quantum
mechanical phenomenon where the probability amplitudes of states can add
up or cancel each other out. This property allows quantum algorithms to
direct computations toward correct answers and away from incorrect ones,
offering a level of computational efficiency beyond the scope of classical
algorithms.
These unique quantum principles translate into different efficiencies for
various computational tasks. Quantum algorithms, like Shor's algorithm,
can factor large numbers with polynomial time complexity, while the best-
known classical algorithms require exponential time. This example is an
instance of quantum speedup. However, it's worth noting that quantum
speedup does not apply to all problems. Some problems are solved just as
efficiently classically as they are quantum mechanically, and for others, we
have yet to discover efficient quantum algorithms.
The key differentiators between classical and quantum algorithms lie in
their operational principles and the laws of physics they are built upon.
Quantum algorithms bring a novel approach to computation that has the
potential to solve specific complex problems more efficiently.
The Differences Between Classical Deterministic
Algorithms and Quantum Probabilistic Algorithms
Classical deterministic algorithms and quantum probabilistic algorithms
operate under contrasting computational models, exemplifying the
fundamental divide between classical and quantum computing. Both of
these algorithmic constructs wield significant influence on the kind of
problems they solve and how skillfully they do it.
In classical computing, deterministic algorithms underpin the operations. As
the term 'deterministic' suggests, these algorithms follow a definite path of
execution, producing a predictable and specific output for a given input. For
instance, if we have a sorting algorithm like QuickSort or MergeSort, given
an unsorted list as input, we know the algorithm will yield a sorted list as
output. There is no randomness or probability involved in this process.
Every time we run the algorithm with the same input, the intermediate steps
and the output will always be the same. This determinism reflects the linear,
ordered nature of classical computing processes, where each operation's
outcome is certain.
In contrast, quantum computing introduces probabilistic algorithms due to
its inherent probabilistic nature. Quantum mechanics' principles, which are
inherently statistical, specify that we can only predict probabilities of
outcomes, not the outcomes themselves. The superposition principle allows
a qubit to exist simultaneously in multiple states, each with a certain
probability amplitude. When a measurement is made, the qubit 'collapses' to
one of these states based on the corresponding probability amplitudes.
Consider a quantum algorithm implemented on a quantum computer.
Suppose we have a qubit in a superposition of states. When we measure this
qubit, we obtain either '0' or '1', but we cannot predict with absolute
certainty which one. The result of the measurement is naturally
probabilistic, which means if we run the same quantum algorithm multiple
times, we could get different results each time due to the random nature of
quantum measurement. This probabilistic trait is fundamental to quantum
algorithms.
How does this probabilistic nature matter? It's a double-edged sword. On
one side, it's a source of quantum computing's power and potential speedup.
Algorithms such as Grover's search algorithm exploit this quantum
probability to search unsorted databases much faster than any classical
algorithm. The probability amplitudes guide the algorithm, increasing the
chances of finding the correct answer and decreasing the probabilities
associated with incorrect ones.
On the flip side, the probabilistic outcomes of quantum algorithms present a
challenge. Because we cannot determine the exact output of a quantum
computation until we make a measurement, and this measurement can yield
different results each time, we often have to run quantum algorithms
multiple times to gain statistical confidence in the result. In addition, while
quantum algorithms can provide superior performance for certain tasks,
they require complex error correction techniques to manage quantum noise,
a problem not encountered in classical deterministic algorithms.
In summary, the shift from classical deterministic algorithms to quantum
probabilistic ones offers a new paradigm for computational problem-
solving. This shift, however, is accompanied by unfamiliar challenges
requiring sophisticated solutions. The probabilistic nature of quantum
algorithms, entwined with quantum mechanical principles, facilitates a new
landscape of computational possibilities and complexities. This intersection
of certainty and probability makes quantum computing fascinating.
Algorithms Can Provide Exponential Speedup
The potential of quantum algorithms to exponentially speed up solutions for
specific computational tasks is like the sudden illumination of a dark,
winding tunnel with a high-powered flashlight. The profound implications
of this can be understood when we explore two quantum algorithms that
have made a mark in the Quantum Cryptographic Cave: Shor's and Grover's
algorithms.
Shor's Algorithm
Imagine standing in front of a gargantuan vault in our cryptographic cave,
the lock of which consists of a large combined number, and you're tasked
with finding its prime factors to unlock it. This scenario is parallel to RSA
encryption, which relies on the difficulty of factoring large numbers as its
security process. Classical computers, equipped with the best-known
factoring algorithm, the General Number Field Sieve, could take ages to
solve this. It's like trying to illuminate the vault with a dim candle, slowly
and tediously examining each possible factor combination.
In stark contrast, Shor's algorithm, like a high-powered flashlight, cuts
through the problem with efficiency. It leverages the unique power of
quantum Fourier transform (QFT), which extracts the pattern of a specific
function related to the factors of the number. This algorithm cleverly
rewrites the factoring problem as a period finding problem, enabling us to
utilize quantum parallelism and superposition to test multiple possibilities
simultaneously. As such, it can potentially factorize large numbers in
polynomial time - exponentially faster than the classical approach.
For instance, cracking a 2048-bit RSA encryption - a typical encryption
level - with classical computers using brute force would, by some estimates,
take longer than the age of the universe. However, with Shor's algorithm
implemented on a fault-tolerant quantum computer, it could theoretically be
done in a matter of days.
Grover's Algorithm
Next, imagine you are exploring a large unsorted database, which is like a
vast, dark chamber filled with boxes. You're looking for a specific box - the
proverbial needle in a haystack scenario. A classical search algorithm would
require you to open and check each box one by one - a linear search - and in
the worst-case scenario, you would need to check all boxes, which would
be an O(N) operation.
Enter Grover's algorithm, our quantum flashlight in this dark chamber.
Instead of a linear search, Grover's algorithm leverages the principles of
superposition and interference to search the unsorted 'database'
quadratically faster, in O(√N) operations. In this case, each operation is
similar to 'opening a box' to see if it contains the item you're looking for.
To visualize this, imagine being able to 'open' multiple boxes
simultaneously (thanks to quantum parallelism), and with each subsequent
'opening', you are more likely to find what you're looking for (thanks to
quantum interference). For example, if you were searching through a
database of one million entries, a classical algorithm might have to check
each entry one by one, potentially making a million checks in the worst
case. But with Grover's algorithm, you would only need to make about a
thousand checks.
By shining a bright quantum light on the unique characteristics of Shor's
and Grover's algorithms, we can understand how quantum algorithms can
provide exponential speedups for certain computational tasks, leading to
profound implications in cryptography and optimization fields.
Exploring Notable Quantum Algorithms
The treasure hunt through our Cryptographic Caves of Quantum Computing
now takes us deeper, towards the chambers of significant quantum
algorithms. As we explore these chambers, let's unravel three key
algorithms, namely, the Deutsch-Jozsa, the Bernstein-Vazirani, and Simon's
algorithms. Each algorithm is a unique gem in the cave, displaying
intriguing characteristics and showcasing the true capabilities of quantum
computing.
The Deutsch-Jozsa Algorithm
This algorithm is among the earliest illustrations of quantum speedup. It's a
problem-solving method designed to work on a quantum computer and
demonstrate how quantum systems outperform our current computers. The
Deutsch-Jozsa algorithm solves a particular type of problem known as the
"oracle" problem exponentially faster than classical computers.
In an oracle problem, you're given a black box function that is either
constant (always outputs the same value) or balanced (it outputs 0 for half
of the input space and 1 for the other half). The task is to determine which
type the function is. In the worst-case scenario, a classical computer would
require up to 'n+1' queries to solve this, where 'n' is the number of inputs.
However, the Deutsch-Jozsa algorithm can solve the same problem in just
one query, showcasing an exponential speedup.
The Bernstein-Vazirani Algorithm
Moving further, we stumble upon another stunning gem, the Bernstein-
Vazirani algorithm. This algorithm tackles a similar black-box problem as
the Deutsch-Jozsa algorithm, but with a slight twist. The box holds a secret
number, and the goal is to determine what that number is. The Bernstein-
Vazirani algorithm again shows an exponential speedup over classical
computers, solving the problem in just one query compared to 'n' queries
required for a classical computer.
Simon's Problem
As we navigate further, we encounter Simon's Problem. This problem is
about finding a hidden bit string that satisfies a specific property. While a
classical algorithm would need to call the function a polynomial number of
times, Simon's algorithm does it in linear time, highlighting yet another
instance where quantum algorithms drastically outperform their classical
counterparts.
These algorithms act as powerful guides demonstrating the trailblazing
capabilities of quantum computing. Each algorithm, an embodiment of the
power of quantum parallelism, offers unique insights into how quantum
computers hold the potential to solve problems at a pace unimaginable for
classical computers.
Illustrating that the key to unlocking the enormous computational potential
of quantum computing lies in the clever design of quantum algorithms. The
magic spells that, when cast upon the world of qubits, bring forth results in
ways that can feel like nothing short of miraculous when viewed from the
classical computing paradigm.
As we dive deeper into the Cryptographic Caves in the next sections, we'll
uncover more about the design and workings of these significant quantum
algorithms, thereby getting a closer view of the tantalizing treasures that
quantum computing offers.
Shor's Algorithm
Integer factorization and its impact on cryptography
Let's proceed deeper into the Cryptographic Caves and explore one of the
most intriguing algorithms in quantum computing—Shor's algorithm, a
marvel discovered by Peter Shor in 1994. The significance of Shor's
algorithm lies in its capability to factorize large blended numbers into
primes, an ability that could revolutionize the world of cryptography.
Shor's algorithm takes advantage of the characteristic properties of quantum
computing, such as quantum parallelism and entanglement. This algorithm
employs a classical reduction part and a quantum subroutine that uses
modular exponentiation, demonstrating the intriguing teamwork of classical
and quantum components within a quantum algorithm.
Understanding the inner workings of Shor's algorithm requires a glimpse
into the principles of modular arithmetic, Quantum Fourier Transform
(QFT), and period finding. The algorithm starts by randomly choosing a
number, say 'a', less than the composite number 'N' we want to factor. If 'a'
happens to share a factor with 'N', we've struck gold early. If not, the
algorithm enters its quantum phase.
The quantum part of the algorithm deals with finding the period 'r' of the
function f(x) = a^x mod N. Here, 'a' is the number we chose, and 'x' varies
over integers. The function has a repeating pattern with some period 'r'.
The brilliance of Shor's algorithm lies in how it leverages quantum
computing's ability to find this period 'r' efficiently. The function is loaded
into a quantum register in superposition. A quantum Fourier transform is
then applied to this register, enabling us to obtain information about the
period 'r'. It's crucial to note that the Quantum Fourier Transform is a
critical tool that enables the quantum phase estimation needed for finding
the order 'r'. This is where the algorithm's bottleneck is - quantum modular
expo-nentiation.
Once the period is found, it is used to identify the factors of 'N'. If 'r' is even
and a^r/2 is not equivalent to -1 (mod N), then, gcd(a^r/2 ± 1, N) will give a
non-trivial factor of 'N'.
The computational advantage of Shor's algorithm lies in the speedup it
offers to the general number field sieve. It demonstrates an exponential
advancement, making it vastly superior when dealing with large numbers.
However, it's crucial to note that implementing Shor's algorithm requires
large-scale quantum computers, which, as of now, are beyond existing
technology.
The practical implications of Shor's algorithm are profound, particularly for
cryptography. Traditional encryption methods, including RSA, rely on the
hardness of factorizing large prime numbers. Shor's algorithm, with its
ability to factorize large numbers efficiently, could potentially break these
encryption codes, necessitating a seismic shift in our current cryptographic
systems.
Shor's algorithm is a testament to the revolutionary potential of quantum
computing. Its ability could potentially change the landscape of
cryptography and computer security. As our quantum capabilities continue
to grow, it becomes increasingly essential to understand such algorithms
and prepare for their eventual implementation.
Exploration of Grover's algorithm
Let's advance into Grover's Algorithm, an ingenious creation in the
quantum computing world. Introduced by Lov Grover in 1996, this
algorithm illuminates the potential of quantum computers to solve search
and optimization problems with a level of productivity that classical
computers can't match.
At the heart of Grover's algorithm is the ability to search a disorderly
database. Picture a haystack with a needle buried somewhere within. A
classical computer would have to check each strand of hay, one by one until
it finds the needle. However, Grover's algorithm, equipped with the power
of quantum superposition, interference, and amplitude amplification, will
achieve this assignment significantly faster.
The algorithm begins with a quantum state that is an equal superposition of
all possible solutions. It then applies a sequence of operations (known as
Grover's iteration) that incrementally amplify the amplitude of the correct
answer while diminishing that of incorrect ones. As a result, when a
measurement is made, the quantum state collapses to the correct answer
with a high probability. This is a quantum analog of 'peeking' into each box
all at once, but in a way that fundamentally respects the rules of quantum
physics.
However, it's not just about finding a single needle in a haystack. Grover's
algorithm also has broader applications in optimization problems, which are
everywhere in our world. Everything from machine learning and
cryptography to Explainable AI and wireless channel assignment can
benefit from the power of this quantum algorithm.
Consider the problem of finding the minimum (or maximum) of a function -
a fundamental task in optimization. By translating this task into a search
problem, Grover's algorithm can be used to find the optimal solution. For
instance, in machine learning, we often seek the model parameters that
minimize the loss function, which could be cast as a search problem over
the parameter space.
But, as with all algorithms, Grover's is not without its limitations. The need
for a quantum oracle - a black box operation used to encode the problem
into a quantum computer - can be a practical hurdle. Furthermore, the
complexity of the algorithm scales exponentially with the number of qubits,
which can be challenging given current quantum hardware.
However, even with these challenges, the significance of Grover's algorithm
in the quantum computing landscape cannot be understated. It showcases
the power of quantum computation to offer speedups over classical
methods, opening the door to tackling complex problems in new and
efficient ways.
As our understanding and development of quantum computing continue to
grow, so will our appreciation for the power and potential of algorithms like
Grover's. They provide a glimpse into what is possible with quantum
computers and inspire new ways of thinking about computational problems.
It's indeed an exciting time to be part of the quantum rising.
Introduction to the Quantum Fourier Transform (QFT)
Venture with us now into the Quantum Fourier Transform (QFT), an
indispensable tool in quantum algorithms, a jewel in the crown of quantum
computation. Like its classical counterpart, the Fourier transform, the QFT
is a method for revealing the frequency components of a signal, but with the
unique quantum ability to operate on quantum states in superposition.
Just as the classical Fourier transform allows us to decompose a signal into
its core frequencies, the QFT lets us dissect a quantum state into its inherent
quantum frequencies. Yet, it's not a mere translation of the classical method
into the quantum world; it's a fundamentally new tool, intricately woven
with the fabric of quantum mechanics. The QFT exhibits its quantum nature
by effectively dealing with states in superposition.
One of the most striking applications of the QFT is in Shor's algorithm for
factorization. In essence, Shor's algorithm cleverly transforms the factoring
problem into a period-finding problem. Once the period is found,
factorization becomes a breeze. But how does one find the period of a
quantum state? Enter the QFT. Shor's algorithm applies the QFT to a
superposition of states encoding different multiples of the period, which
causes the various frequency components to interfere constructively at the
correct answer while canceling each other out elsewhere. When a
measurement is made, the quantum state collapses to an answer with a high
probability. The likely result is a dramatic speedup over classical
factorization algorithms.
Additionally, the QFT plays a central role in quantum phase estimation, a
critical subroutine in many quantum algorithms, and can be instrumental in
tasks such as eigenvalue estimation and quantum search. Despite its
extraordinary potential, it is important to note that challenges in preparing
and measuring quantum states currently limit the QFT's applications.
Moreover, not all aspects of QFT-based computation offer exponential
speedup, and resource requirements, especially memory, are significant in
many-body models.
The beauty of the Quantum Fourier Transform is not just in its
mathematical elegance but also in its practical implications for the future of
computation. But to truly appreciate and use it, one needs to build quantum
intuition and skills. Engaging with interactive tools, visualization
techniques, and practice exercises can aid this learning process.
The Quantum Fourier Transform stands as a beacon, illuminating the
extreme potential of quantum computing and reminding us of the
challenges that lie ahead. As we continue our journey into the quantum
domain, the QFT will undoubtedly remain a trusted companion and a
guiding light.
(QAOA) Explore the Quantum Approximate Optimization
Algorithm
The Quantum Approximate Optimization Algorithm, often referred to by its
acronym QAOA, is one of the most promising developments in quantum
computing. This algorithm is devised to solve a wide range of optimization
problems more efficiently than traditional methods. It leverages the
principles of quantum mechanics to traverse the solution space in ways that
classical computers can't.
To appreciate the power of QAOA, it's helpful to think of optimization
problems as trying to find the lowest point in a landscape of hills and
valleys. This landscape can represent anything from logistical challenges,
like finding the most efficient route for delivery trucks, to financial
quandaries, like portfolio optimization in finance.
A classical algorithm would typically 'walk' across this landscape, trying
different routes and continuously adjusting its path based on the 'altitude' it
finds. The more complex the landscape, the more likely the classical
algorithm might get stuck in a 'local minimum' – a valley that isn't the
lowest point but appears to be from the immediate perspective.
On the other hand, QAOA acts like a skilled bird soaring above the
landscape, able to assess the terrain from a much broader perspective. It
utilizes quantum superposition to 'be' in multiple places at once and
quantum interference to navigate the solution space more efficiently,
making it less likely to get trapped in local minimums.
See the "Traveling Salesman Problem" (TSP), a famous optimization
challenge. The problem involves a salesman who needs to visit several
cities once and return to the origin city while minimizing the total travel
distance. For a handful of cities, solving this isn't too hard. But as you add
more cities, the number of possible routes grows astronomically.
When a classical computer solves the TSP, it might start with one path,
assess its length, then try another and another. Even with some clever
shortcuts, the classical algorithm could still take an impractically long time
if we have many cities.
By contrast, a quantum computer running QAOA could consider multiple
routes simultaneously thanks to quantum superposition, and then use
quantum interference to guide the system toward the shortest route. While
we might not be able to run QAOA efficiently for large TSPs on existing
quantum computers due to their limited size and noise issues, the algorithm
represents a tantalizing glimpse of what could be possible as quantum
technology advances.
Like all quantum algorithms, it's important to note that QAOA isn't a magic
bullet that will solve all problems better than classical methods. But for a
particular class of complex optimization problems, it promises substantial
efficiency gains and points the way to the sorts of tasks future quantum
computers might excel at.
To harness the full power of QAOA, though, we need to continue refining
quantum hardware and developing effective quantum software. That's an
ongoing project involving physicists, computer scientists, and
mathematicians across the globe, a collaborative effort that will, we hope,
lead us into the next era of computing.
QPE
Quantum Phase Estimation (QPE) is a fundamental tool in the quantum
computing toolbox. At its heart, QPE is about precision - specifically, the
precise estimation of eigenvalues, which are critical values associated with
mathematical objects called operators.
To understand QPE, imagine you're a musician tuning an instrument. Each
string on the instrument can play a variety of tones, but there's one
fundamental tone, or eigenvalue, which corresponds to the main vibration
frequency of the string. It's a bit like this with QPE: you're 'tuning' your
quantum system to find the eigenvalue of an operator, which gives you
valuable information about the system's behavior.
But how does this 'tuning' work in practice? QPE exploits the unique
features of quantum mechanics - interference and superposition - to
estimate the phase (another name for the angle in wave mechanics) of an
eigenstate of a unitary operator.
To illustrate this, let's revisit our musician example. Suppose each note of
the instrument is an eigenstate, and the sound produced is the phase. Our
musician plays all notes (superposition), listens to the resulting sound
(interference), and then identifies the loudest (dominant) note. This process
is similar to the QPE process of phase estimation.
The power lies in its precision and efficiency. It can pick out the key 'notes'
from a quantum 'chord symbol', enabling us to understand and harness our
quantum systems' behaviors more effectively. This 'tuning' process is why
QPE is fundamental to various quantum algorithms.
QPE is not just about theoretical beauty; it has practical applications, too. It
plays a critical role in Shor's factoring algorithm, enabling the efficient
factorization of large numbers, with implications for cryptography.
Additionally, QPE forms the basis for quantum algorithms designed to
simulate physical systems, where understanding the eigenvalues of certain
operators can provide insight into the system's properties.
Quantum Phase Estimation is like the 'tuning' process for quantum
algorithms. It pinpoints the key frequencies that allow quantum systems to
'sing' in harmony with computational tasks, making it a cornerstone of
many quantum algorithm designs.
Solving Linear Algorithms
Description of the HHL Algorithm
Imagine yourself as a spelunker descending deeper into the cryptographic
cave. Suddenly, you encounter an intricate network of interconnected caves,
each leading to a different treasure. These paths and their
interconnectedness can be represented as a system of linear equations,
where each path represents a variable, and their intersections represent the
relationships between the variables. Here, the Harrow-Hassidim-Lloyd
(HHL) algorithm serves as our guide through this labyrinth, helping us
determine the most efficient path to the treasures.
The HHL algorithm employs three primary steps:
1. State Preparation: This phase represents the initial mapping of our
cave system. The algorithm creates a quantum state that corresponds to
the right-hand side of our linear system, serving as the starting point of
our exploration.
2. Quantum Phase Estimation: This step involves identifying key
features of the cave system (eigenvalues) that can help us plan our
exploration most efficiently. It's like estimating different cave paths'
lengths, difficulties, and intersections.
3. Conditional Rotation and Uncomputation: Here, we fine-tune our exploration plan based on the identified cave
features. The state of an auxiliary qubit is rotated inversely proportional to the features identified. Following that, we
reverse the Quantum Phase Estimation operation to get our final route plan.
Example Implementation of the HHL Algorithm
Let's use a financial example to demonstrate the algorithm's power. Suppose
we're trying to optimize a stock portfolio where each stock's future price
depends on various economic factors. These relationships can be
represented as a system of linear equations, with each stock being a path in
our cave system and the intersections representing the dependencies
between different stocks.
With the HHL algorithm, we can prepare our initial portfolio (State
Preparation), identify key financial trends and dependencies (Quantum
Phase Estimation), and adjust our portfolio accordingly to maximize profits
while minimizing risks (Conditional Rotation and Uncomputation).
By unlocking the treasure chest of optimized stock portfolios, the HHL
algorithm not only demonstrates its prowess in solving complex linear
systems but also illustrates the far-reaching impacts of quantum
computation in fields as diverse as finance, logistics, AI, and beyond.
Applications of Quantum Algorithms in Various Fields
With their unconventional computation methods, Quantum algorithms
promise transformative shifts across a wide range of sectors. Let's dive into
these vibrant seas, exploring various islands where quantum computation
finds its applications.
1. Healthcare and Drug Discovery: A significant treasure in our
cryptographic cave is drug discovery. Pharmaceutical companies often
sift through a vast database of molecular structures in search of
potential drug candidates. Quantum algorithms, especially Grover's,
could perform this task exponentially faster than classical algorithms,
speeding up the discovery process. This could revolutionize how we
fight diseases, enabling personalized medicine and rapid response to
pandemics.
2. Financial Services: Financial institutions handle massive amounts
of complex and computationally intensive tasks such as risk analysis,
portfolio optimization, and pricing derivatives. Algorithms like the
Quantum Approximate Optimization Algorithm (QAOA) could
provide solutions to these complex problems much faster and more
accurately than their classical counterparts, potentially saving billions
of dollars.
3. Logistics and Supply Chain: The vast network of global supply
chains, filled with countless routes, can be optimized using quantum
algorithms. The HHL algorithm, for example, can solve linear systems
and optimize routes, reducing costs and increasing efficiency.
4. Energy and Climate Science: Quantum algorithms could be game-
changers in the modeling and analysis of environmental systems.
Accurate simulations of molecular interactions and climate models
could help design more efficient renewable energy sources and predict
climate changes with high precision.
5. Material Science: Quantum computing could revolutionize material
science by enabling the simulation of quantum systems. Quantum
Phase Estimation, used to precisely estimate eigenvalues, could help
discover new materials with desired properties, opening doors to
innovations in various fields from electronics to aerospace.
As we leave our cryptographic cave of quantum algorithms, we bring with
us a trove of treasures ready to solve some of the most complex problems
across various fields. As we venture further into the domain of quantum
computing in the following chapters, it's crucial to remember that while
quantum computing is powerful, it is a tool. Like any tool, its power lies in
how we use it.
It is an exciting time in the field of quantum computing. As we stand on the
edge of this technological revolution, we are on the cusp of uncovering an
entirely new landscape of computational possibilities. As quantum
algorithms continue to evolve, they will undeniably shape our future, from
medicine to finance, supply chains to climate science, and beyond.
Quantum computing is not just a new technology but a new way of
approaching the world.
Thus, we wrap up our journey into the fascinating world of quantum
algorithms in this chapter. However, this is not an ending but rather a
launchpad, propelling us deeper into the quantum universe. Exploring more
quantum wonders and their potential impacts.
Chapter 5
Practical Applications of Quantum
Tech
Where Theory Meets Practice
In quantum computing, the line between theory and application often seems
blurred—a fantastical landscape where math and metaphysics merge,
opening up avenues for change that were once considered the stuff of
science fiction. As we've journeyed through the foundational elements and
tackled common misconceptions, it's now time to delve into the core of
where quantum technology meets real-world applications.
You're about to step into a promising oasis of innovation—a quantum city,
if you will. This bustling hub holds the map of transformative quantum
uses, similar to the foundational elements of a thriving community. Imagine
healthcare, finance, logistics, and cybersecurity, all improved by the power
of qubits and superposition, and every challenge is met with pioneering
solutions, courtesy of quantum algorithms.
But every city has its struggles, and our quantum utopia is no exception.
Alongside its awe-inspiring skyscrapers of potential, there are roadblocks,
bottlenecks, and unfulfilled promises. You'll learn about the existing
limitations of quantum technology, ensuring you gain a balanced
perspective on this groundbreaking field. Your active participation will
deepen your understanding and demonstrate how quantum can reshape
decision-making and transform workflows.
As we explore the expansive landscape of quantum computing applications,
we'll also turn our eyes toward the horizon. The rapidly evolving nature of
this field means that today's limitations could be tomorrow's breakthroughs.
We'll examine what experts predict for the coming decade, presenting you
with a future outlook that's as scientifically grounded as it is awe-
inspiringly vast.
Intrigued? We thought you might be. Stick with us to the end for a special
opportunity to take your quantum computing journey even further.
Let's dive in.
Existing and Potential Applications of Quantum Computing
Welcome to The City, crowded with possibilities and promise, where the
marriage of quantum mechanics and computational power is breathing new
life into multiple industries. From healthcare to finance, logistics to
cybersecurity, and even in sectors we're just beginning to understand,
quantum computing is shaping up as a transformational force that will
redefine our familiarity with the world around us.
As Nobel winner Richard Feynman once said, "What I cannot create, I do
not understand." And in this ambitious effort to create a new standard of
computing, we're also taking leaps in understanding the complexities of our
universe, one qubit at a time, ushering in a new era of computational ability.
But what are the tangible, successful applications where quantum
computing excels in our approach? In the coming pages, we will get into
the nitty-gritty of quantum computing applications in drug discovery,
materials science, healthcare, finance, and even marketing. Moreover, we'll
touch on the synergistic relationship between quantum computing and
Artificial Intelligence, particularly in natural language processing.
I hope to equip you with the knowledge needed to appreciate the profound
impact that quantum computing could have on society at large. It's time to
explore the growing infrastructure already taking shape at the intersections
of industry, science, and technology.
The Quantum Advantage in Accelerating Breakthroughs
"Quantum computing could be game-changing for drug development in the
pharmaceutical industry. Businesses should start preparing now." - Expert
analysis from McKinsey & Company.
Imagine a world where the creation of new drugs doesn't take decades of
research and billions of dollars, but instead is dramatically accelerated,
saving countless lives and resources. Welcome to the crossroads where
quantum computing collides with drug discovery and molecular
simulations, unlocking pathways to treatments once thought to be years
away from realization.
An In-depth Exploration
Channeling the spirit of Richard Feynman's assertion that to simulate
nature, one must think quantum, this merging of quantum mechanics with
molecular science has become pivotal in refining our knowledge of
molecular behaviors and interactions. We are no longer confined to
yesterday's approximations; today's quantum-infused drug discovery field is
both optimized and expansive.
Molecular simulations are now at the vanguard, elucidating the complex
dance of biomolecules and their interplay with prospective drug entities.
Such modeling, vital to drug discovery, equips researchers with predictive
tools, revealing how potential drugs interact within biological systems. For
instance, molecular modeling played a role in devising inhibitors targeting
enzymes or receptors. Visualizing a drug molecule's snug fit within a
target's active site fast-tracks the drug design trajectory.
Further enhancing our understanding are Molecular Dynamics (MD)
simulations, which provide a cinematic view of molecular interactions
unfolding over time. Such insights were instrumental in grasping the
dynamic nature of the HIV-1 protease enzyme, paving the way for
inhibitors that have since become pillars of antiretroviral treatments.
Introducing artificial intelligence (AI) to this mix signals a transformative
juncture. With its predictive skills, AI streamlines the drug discovery
journey, hinting at molecules with therapeutic potential, refining them for
optimal efficacy, and even forecasting potential side effects. A testament to
AI's clout lies in its instrumental role during health emergencies, swiftly
repurposing existing drugs for newfound uses.
Merging these theoretical frameworks with hands-on lab experiments is
already productive in drug research. "Real World Drug Discovery: A
Chemist's Guide to Biotech and Pharmaceutical Research by Robert M.
Rydzewski" describes numerous instances where theory and practice
harmoniously united, giving birth to potent therapeutics.
Quantum computing is bridging crucial gaps. The World Health
Organization underscores the essence of innovation in widening access to
medicines. With computational techniques and molecular simulations
slashing traditional drug discovery timelines and expenses, we can refocus
on previously overlooked diseases and render novel medications.
For those inclined towards visual aids, many online platforms explain
molecular dynamics simulations in drug discovery. One commendable
video, "Application of molecular dynamics simulations in drug discovery
by BioExcel CoE," offers an engaging dive into the potential and processes
that molecular dynamics simulations promise.
However, it's crucial to temper optimism with sensibility. As the field
expands, challenges loom, especially in talent cultivation, an issue
discussed in the next chapter. A balanced strategy accounting for potential
impediments will be mandatory as the pharmaceutical industry gears up to
exploit the quantum edge.
Yet, the quantum journey extends beyond drug discovery. The healthcare
sector, especially personalized medicine, stands to reap quantum benefits.
As we continue, we'll unearth how quantum performance promises to
reshape healthcare, championing diagnostics and tailor-made treatments
that are effective and safer for patients.
The Quantum Edge in Personalized Medicine
"Quantum technologies are set to transform healthcare, making
diagnostics more precise and treatments more individualized than
ever."
- Harvard Medical School.
Imagine a future, not so distant, where each medical intervention is tailored
to your unique genetic makeup, health history, and specific needs. Such
personalization is becoming a reality, thanks to the union of quantum
computing and medical science.
Richard Feynman, a pioneer in quantum mechanics, observed the inherent
quantum nature of biological systems. Today, that very connection between
quantum mechanics and biology is transforming our approach to medicine.
The age-old 'one-size-fits-all' treatment method is shifting to a nuanced,
quantum-informed strategy. A prime example of this transformation lies in
genetic sequencing. With the promise of quantum capabilities, what was
once a monumental challenge in decoding the human genome can now be
faster and remarkably precise. This rapid sequencing doesn't just enhance
our understanding of genetic ailments; it empowers physicians to
recommend treatments aligning perfectly with an individual's genetic code.
Diagnostics are on the cusp of a quantum leap. Advanced imaging
techniques, like MRI, stand to benefit greatly from quantum-enhanced
sensors. With superior resolution and speed, these sensors promise earlier
and incredibly accurate disease detection. But the quantum revolution
doesn't stop there. Pairing quantum computing with artificial intelligence is
generating algorithms capable of sifting through vast medical datasets in
mere moments. With their outstanding analytical expertise, such algorithms
unveiled patterns that were previously invisible, offering breakthroughs like
predicting cardiac events with startling accuracy.
For those seeking tangible examples, "Artificial Intelligence in Healthcare:
Unlocking Its Potential by Dr. Janak Gunatilleke" serves as a beacon,
detailing various instances where quantum methodologies are already
reshaping medical landscapes. Be it swift drug trials or cutting-edge brain
imaging techniques, the transformative potential of quantum in healthcare
seems limitless.
Of course, like all seismic shifts, the quantum pivot in healthcare brings its
own set of concerns, from patient privacy to key ethical considerations.
Addressing these will be critical as we forge ahead, ensuring that our
quantum healthcare journey is groundbreaking and responsible.
As we continue, it becomes evident that the potential applications of
quantum computing influence a spectrum of industries. From reshaping
logistical networks to pioneering intelligent energy solutions, the ripple
effect of quantum computing is profound and far-reaching. Let's see its
impact on materials science and engineering.
The Quantum Revolution in Advanced Materials
"Quantum mechanics promises a future where material properties are not just discovered but
designed."
- MIT Department of Materials Science and Engineering.
Picture a world where everyday items possess material attributes fine-tuned
to our precise demands. A future where we don't merely stumble upon
material properties, but meticulously craft them using quantum mechanics.
Think metals, ceramics, and polymers, all sculpted atom-by-atom to peak
performance. It's no daydream; quantum computing is birthing this new era
of material mastery.
At the core of this formation lies the intimate bond between quantum
mechanics and materials. Scientists have long recognized the quantum
dance of materials at minuscule atomic scales. Now, with quantum
computing, we can leverage this knowledge, giving birth to materials
boasting unheard-of properties, primed to tackle today's most pressing
obstacles.
Consider the quest for room-temperature superconductors, a materials
science dream. Quantum computing's skill in simulating complex electron
bonds edges us closer to this goal, anticipating the dawn of power-efficient
grids and cutting-edge transport mechanisms.
In this quantum-augmented world, polymers can be intricately tailored to
exhibit desired mechanical, thermal, or electrical characteristics.
Nanoparticles are also fine-tuned for industries ranging from precise drug
delivery to advancing solar tech and pioneering electronics. The results are
breathtaking when we marry AI's predictive powers with quantum
computing. This union predicts material behaviors with remarkable
precision, fast-tracking discoveries that once spanned generations into mere
moments.
To go deeper, get the book, "Artificial Intelligence for Materials Science by
Yuan Cheng, Tian Wang, and Gang Zhang," which offers a treasure trove of
cases. It has a searchable and interactive database where quantum
computing has reshaped the fabric of material research, be it in birthing
corrosion-defiant alloys or sculpting bio-compatible marvels for medical
implants.
Reflecting on the UN's clear call for sustainable industrial evolution, the
quantum materials research movement has a pivotal role. It's not just about
crafting cutting-edge materials but designing eco-conscious materials and
efficient recycling methodologies to conserve our beautiful planet.
Those with an appetite for visual narratives can turn to platforms like
YouTube. The video "The Strange New Universe of Quantum Materials -
Piers Coleman Rutgers University" paints a vivid scene of quantum
computing's ability with material innovation.
Yet, as with any process, there are hurdles. As we analyze and create
quantum materials, issues ranging from intellectual property to regulatory
intricacies and ethical considerations (especially as materials begin to meld
with biology) demand our attention. These topics and more will be explored
in the final chapter.
While life sciences are reaping quantum dividends, it's just the tip of the
iceberg. Next, we will navigate the quantum-driven boost in Finance and
portfolio management, unraveling a future that once seemed confined to a
handful of analytical categories.
A Quantum Leap in Financial Forecasting
In the honorable corridors of the Wharton School of Finance, a prediction
echoes: Quantum algorithms are set to transform the field of financial
strategies, evolving portfolio optimization into an art of uncharted
precision. Such accuracy promises a future where economic decisions aren't
merely anchored in past experiences, but are informed by the profound
foresight offered by quantum-predicted market movements. The dream of
avoiding pitfalls with your best educated guess gradually fades away as
quantum computations take the spotlight. Indeed, the world of finance is on
the cusp of a quantum alignment.
The challenge of predicting the unpredictable, ever-fluctuating market
scenarios, has been finance's on-going quest. Traditional computing has
made honorable feats, yet it has its limitations when navigating the complex
maze of financial variables. Here's where quantum computing advertises its
entry, poised to reshape financial forecasting with capabilities that were
once elusive.
Portfolio management has always been a delicate balance—seizing
lucrative opportunities on the one hand and skirting risks on the other. We
can design a system that simulates thousands of market scenarios through
quantum algorithms, painting a holistic, multi-faceted view of the potential
rewards and threats.
Currency exchange, with its rapid fluctuations, presents its own set of
challenges. Even the slightest predictive advantage can yield transformative
results in this volatile market. Quantum algorithms offer the edge that can
make all the difference; even if it's the tiniest of margins, they could easily
outpace traditional systems.
A new alliance is emerging on the horizon—the powerful merger of AI's
pattern recognition paired with quantum computing's unmatched speed. As
these two titans unite, they promise to revolutionize algorithmic trading,
sifting through vast reservoirs of trade data, global news, and key event
signals to anticipate market movements like never before.
Diving into the real-world manifestations of this quantum trajectory, the
book "Quantum Machine Learning and Optimisation in Finance by Antoine
Jacquier and Oleksiy Kondratyev" shines a light on the groundbreaking
applications of quantum principles in the financial sector. From ensuring
airtight security through quantum-encrypted transactions to pioneering
hedge fund strategies, the financial field is undergoing a transformative
shift right before our eyes.
The potential of quantum computing extends beyond simple innovation. Its
promise of sharpened financial services and predictions also invites a future
of reduced transaction costs. For those with an inclination towards visual
learning, resources abound. Platforms like YouTube offer a window into
this evolving world, with content like "Quantum Computing for Finance by
Centre for Quantum Technologies" showing an in-depth look into how
quantum principles redefine the financial sphere.
The financial sector must remain vigilant as challenges arise. Concerns
ranging from the ethical implications of ultra-fast quantum trades to the
sanctity of data in the era of quantum cryptography need careful
deliberation. As financial institutions brace themselves for this quantum
leap, they must be equipped with more than just shiny new technology—a
calibrated moral compass will be equally essential.
The narrative of quantum computing and its transformative potential is vast
and varied. We've only scratched the surface and must explore its
implications for marketing and optimization; let's continue.
The Quantum Future of Marketing Strategy
"Quantum computing could revolutionize the world of marketing, personalizing user experiences to
an unprecedented degree."
- Harvard Business Review.
Harvard Business Review foresee a future where marketing messages aren't
generic but tailored with a quantum precision that feels almost intuitive.
Quantum computing will enhance marketing with supernatural awareness.
Imagine stepping into a world where the line between a customer's wants
and a brand's offering is always aligned. That isn't a fantasy; it's the
emerging reality where quantum computing and marketing optimization
beautifully intersect.
In the digital age we inhabit, every interaction, from a simple click to a
share, is a piece of a larger puzzle, one that gives insights into user behavior
and preferences. With its awe-inspiring capacity to swiftly navigate vast
oceans of data, Quantum computing promises to evolve these interactions.
For brands, every digital footprint could soon translate into an invaluable
opportunity, crafting user profiles with a level of personalization that's
unprecedented.
While traditional systems may ponder over which ads hit the mark,
quantum algorithms cut to the chase. Their capacity allows them to evaluate
a multitude of ad variations simultaneously, transforming the marketing
landscape by fine-tuning campaigns in what feels like a heartbeat.
Central to any brand's strategy is understanding the consumer journey. This
pathway can be intricate, riddled with choices, preferences, and emotions.
However, quantum computing, armed with its ability to envision countless
consumer paths in the blink of an eye, stands to empower businesses.
Brands can potentially foresee and cater to a consumer's needs even before
they come to the surface.
Now, let's picture quantum computing's merger with AI in marketing. The
result? A transformative approach to predictive analytics. This synergy
promises capabilities that stretch from pinpointing growing trends to
revealing subtle patterns in consumer behaviors, thereby redefining
marketing's predictive game.
The book "The Business Case for AI by Kavita Ganesan" delves deep into
this evolving nexus of quantum technology and marketing strategies. It
introduces the numerous ways in which pioneering brands harness quantum
algorithms, refining their approach from customer details to the nuances of
retargeting. By leveraging quantum insights, brands have the potential to
craft campaigns that resonate across a spectrum of audiences and expand
their reach.
However, as the marketing realm teeters on the brink of this quantum
customization, it faces challenges from the sanctity of data sovereignty to
the ethical navigation of hyper-personalized insights. Yet, the tale of
quantum computing's influence isn't confined to marketing. It's time to
explore one of my favorites, AI and Natural Language Processing, to
discover how quantum advancements might sculpt our future.
Harnessing the Quantum Realm for AI and NLP
"It's not about the bits, it's about what you do with them."
- John Maeda
Ah, Natural Language Processing (NLP). For those of you who might be
working in tech, or at least keenly following its rapid innovations, you're
likely familiar with NLP. In layman's terms, it's a technology that helps
machines understand, interpret, and respond to human languages in a
meaningful and contextually appropriate way. For a good stretch, we've
been reliant on classical computing to perform these tasks. But let's turn our
gaze to something even more cutting-edge. I'm talking about Quantum
Natural Language Processing (QNLP), a new frontier that could redefine
how we understand communication, language, and interaction in the digital
space.
Imagine a reality where our machines transcend mere language
understanding. Instead, they truly comprehend, discerning not only our
words but the very emotions, subtleties, and contexts that accompany them.
As the formidable forces of quantum computing and AI-powered NLP
converge, we teeter on the cusp of an era marked by machines that
intuitively interact with humans, responding with a supernatural grasp of
context and emotion.
Where traditional deep learning models, so integral to NLP endeavors, often
found themselves resource-strapped, quantum-enhanced models are
rewriting the script. These quantum-infused algorithms ride a wave of
efficiency in processing NLP tasks. Whether it's the intricate dance of
machine translation or the delicate art of sentiment analysis, quantum
mechanisms are poised to redefine their horizons.
In the expansive field of text analysis, quantum computing promises to be
nothing short of transformative. Think of it as giving machines the ability to
speed-read and comprehend libraries' worth of text in just moments, all
while extracting context-rich insights that conventional systems might
overlook. This quantum speed is set to recalibrate the very foundations of
text analysis.
When AI, with its relentless data-driven hunger, melds with quantum
computing's unmatched processing might, the results for NLP could be
breathtaking. Models could evolve to exhibit heightened accuracy,
embracing adaptability and an acute sense of context like never before.
Picture this real-world implication: Customer support chatbots that, instead
of offering bland, generic responses, discern the sentiment that underscores
a customer's words. Or voice assistants that remember and learn from past
conversations, evolving to provide a seamless conversational experience.
This isn't a distant future dream—it's the tangible potential of quantum-
infused NLP.
As we grant our machines this heightened awareness and understanding,
ethical considerations have become front and center, especially around data
privacy. The idea of hyper-aware AI systems introduces questions and
concerns that our chapter on ethics promises to discuss.
The marriage of quantum computing with NLP signifies more than a simple
technological stride—it's a notable shift in our very dialogue with machines.
And as we dig deep into this quantum world, we stand ready to uncover its
transformative potential across industries and society.
The Quantum Quandary in Cybersecurity
"Quantum computing, with its dual capability, stands poised to redefine the future of secure
communications while simultaneously challenging existing encryption protocols."
- Harvard Business Review.
Visualize quantum computing in the domain of cybersecurity as a double-
edged sword. Its gleaming edge promises a fortress of security, paving the
way for methods like quantum key distribution, which can render covert
eavesdropping ineffective. Yet, its other edge casts a shadow, hinting at the
potential vulnerabilities it could introduce, potentially shattering many of
our current cryptographic safeguards.
At the heart of this new era of secure communications is Quantum Key
Distribution (QKD). This isn't sophisticated tech jargon. It's a critical
change in our understanding and approach to encrypted communications.
By harnessing the features of quantum mechanics, QKD sets a new gold
standard in secure transmissions. While the quantum domain readies its
suite of advanced encryption tools, there lies a pressing, inescapable quest
to future-proof our classical cryptographic methods. As quantum computers
edge closer to cracking these methods, the evolution towards post-quantum
cryptography, impenetrable to quantum decryption attacks, becomes
paramount.
The era of quantum in cybersecurity is dawning. As this technology
becomes more intertwined with our systems, a dual mandate emerges for
industries and governments alike. First, they must harness the positive
quantum tidal wave ushering in credible, secure communications. Together,
they must fortify their classical systems, guarding them against the potential
quantum breaches looming on the horizon.
Chapter 7 offers a deep dive into this mesmerizing dance between quantum
computing and cryptography, exploring its challenges and innovative
solutions. But as we continue exploring the quantum d, we're set to see how
it's intertwining with other modern technological wonders.
Grappling with the Subtleties of Quantum Computing's
Challenges
"Every frontier of innovation has its shadows, and in understanding them, we illuminate our path
forward."
- Commeum Frontiers Journal.
The rise of quantum computing, as transformative as it has been projected
to be, is not without its challenges, intricate and multi-dimensional, beyond
technology and hardware, affecting our world's economic, educational, and
security fields.
Starting with the knowledge and talent gap, it's evident that quantum
computing's complexity requires a unique blend of expertise. Bridging the
worlds of quantum mechanics and computational applications demands
professionals who are not only versed in the abstract intricacies of quantum
theory but are also skilled at its practical application in real-world
computational settings. The quantum depth chart of talent is currently
sparse in the academic and professional landscapes. As universities and
institutions work to develop curricula that nurture this blend of expertise,
industries, too, face the challenge of attracting, training, and retaining this
rare talent.
On the economic front, the quantum field presents a paradox. While its
long-term potential is undoubtedly expansive, realizing this potential
requires significant upfront investment, both in terms of capital and time.
This presents a delicate balancing act for businesses and investors:
weighing the promise of profound breakthroughs against substantial short-
term financial commitments. Moreover, the commercialization of quantum
technologies requires not just investment in hardware and research but also
in public understanding, policy frameworks, and infrastructure.
Lastly, security concerns loom large. Quantum computing, with its
potential to crack existing cryptographic methods, ushers in an era where
our traditional concepts of data protection might be rendered obsolete.
However, this very threat is also a beacon of innovation. It sparks the
development of quantum-safe encryption methods and the exploration of
inherently secure quantum communication techniques like quantum key
distribution. Transitioning to these new security protocols while
safeguarding our digital world is challenging, even with global
collaboration and policy-making.
In wrapping our minds around these challenges, we're addressing the
roadblocks and setting the stage for a solid and resilient quantum future. In
the following chapters, readers will gain insights into the problems and the
numerous solutions and strategies being developed globally to navigate
them.
Quantum Leap Forward
This chapter has unveiled the practical potential of quantum computing
across various industries, highlighting its transformative promise. However,
understanding is just the first step. It's an invitation for readers—students,
professionals, and enthusiasts alike—to dive deeper, experiment, and
innovate. As we transition to Chapter 6, we'll explore quantum computing's
broader impact on technology and careers. The future of quantum is not just
in its computations but in its capacity to reshape our industries. Are you
ready to embrace this quantum shift?
Chapter 6
Mapping Quantum Computing
The Future of Tech and Careers
Quantum computing is reshaping the tech landscape faster than we think.
And here's the exciting part: This isn't just about the distant future. It's about
you, right now. Have you wondered how can I stand out in my career? Or
how can my business jump ahead of the curve, leveraging the latest AI and
quantum advancements?
In simple words, quantum computing is like the next big adventure in the
world of technology. Think of it as a mountain. Every time you reach a new
summit, you discover higher peaks. For some, it's about finding new job
opportunities. For others, it's about growing their company and getting
ahead of the competition.
Here's some promising news: Experts think AI and quantum computing will
create 100 million new jobs. That's huge! Many companies have already
started using this new tech, and guess what? They need people like you and
me to help them out.
In this chapter, I'll be your guide, like the friend who shows you the best
paths on a hike—pointing out the big changes quantum computing will
bring. Even better, I'll show you where to learn skill development and get
started today. We call this the "Quantum Toolkit". It's like a backpack filled
with everything you need for your climb: helpful websites, courses, and
tips.
So, gear up to explore the impact on technology and your career. We're
about to start a fantastic hike with quantum computing as our compass.
Let's climb this mountain together and see all the cool things waiting for us
at the top!
How Quantum Changes Key Jobs and Businesses
Quantum's Edge in Marketing
At its core, marketing thrives on understanding and predicting consumer
behavior. In our digital age, this translates to sifting through vast amounts
of data to decode patterns and trends. Enter quantum computing! Unlike
traditional systems that may stagger under the sheer volume of consumer
data, quantum computers discern patterns in mere seconds, making trend
predictions more accurate and efficient than ever before.
We've already glimpsed the transformative power of AI in marketing. Who
hasn't been captivated by the viral Lensa AI app that creates 'magic avatars'
from our photos, transporting us to the cosmos as astronauts or
reincarnating us as famous figures from history? Now, envision this AI
genius supercharged by quantum computing. The potential is staggering:
campaigns that don't just analyze user data but preemptively gauge and
adapt to consumer preferences, often before the consumers themselves even
consciously realize them. In this dynamic landscape, marketers who blend
quantum-driven analytics with innovative strategies will lead the charge.
In preparation, aspiring marketers shall not be discouraged by the
complexity of quantum mechanics. There are resources designed
specifically for the layperson, such as courses on Google's Quantum AI
campus, that distill intricate concepts into digestible insights. With a
foundational understanding and a keen eye on how quantum can refine data
analytics, you can design campaigns of unprecedented precision and
relevance.
The dream of forecasting future consumer trends with pinpoint accuracy is
closer than we think. By intertwining the worlds of marketing, AI, and
quantum computing, we're on the cusp of a marketing renaissance. For
those eager to stay ahead of the curve, now's the time to embrace quantum
marketing and launch your career.
Quantum's Healthcare Evolution
Healthcare, one of the most important sectors, is witnessing extreme
changes underpinned by quantum computing. Beyond traditional
computational capabilities, quantum computing promises rapid drug
discovery by enabling real-time molecular interaction simulations. Further,
diagnostics stands to benefit immensely; imagine quantum-enhanced tools
that deliver ultra-clear images, enabling early disease detection and tailored
treatment strategies.
The convergence of quantum computing and genetics opens a world of
possibilities. Envision a scenario where doctors, equipped with quantum
tools, instantly decode a patient's DNA. Such real-time genetic awareness
can lead to customized treatments that are meticulously aligned to an
individual's biological needs and optimize the recovery trajectory while
minimizing negative reactions. The pioneers in this approach?
Bioinformaticians, geneticists, and technologically skilled medical
professionals.
As the healthcare industry stands poised for transformation, there's an
emerging need for professionals with a dual skillset - robust medical
knowledge, complemented by an understanding of quantum biology and
data analytics. For those at the intersection of medicine and technology,
specialized courses that weave AI, quantum physics, and healthcare
solutions are in demand. Embracing this path forward will elevate one's
career and achieve the goal of enhancing patient care.
Healthcare's evolution, powered by quantum computing, begins a new era
of personalized, efficient treatments. The horizon is abundant with
opportunities for those intrigued by the relationship between medicine and
technology. Harnessing quantum's potential is no longer a distant dream; it's
right now.
Redefining Financial Operations
The finance sector, underpinned by numbers and data, is primed for a
quantum correction. At its core, quantum computing can traverse vast data
terrains rapidly, allowing optimization of trading strategies and real-time
fraud detection through immediate anomaly recognition in transaction
patterns.
Financial analysts, equipped with quantum tools, could gain instantaneous
risk insights, enabling investment decisions in mere fractions of a second.
Consider high-frequency trading: Quantum-powered machines would digest
and analyze giant datasets in a blink, propelling trade optimization to
previously unimaginable velocities. The doorways to this future are wide
open for those ready to merge finance and quantum expertise. Professionals
with an ability in financial analytics and quantum understanding are set to
be the industry's torchbearers.
The essence of finance is data - analyzing it, predicting with it, and
strategizing around it. McKinsey underscores financial experts need to
immerse themselves in courses that weave generative AI with quantum
mechanics. Such education equips them to chart financial strategies once
considered the stuff of fiction, turning them into tangible, actionable
blueprints.
The finance landscape, already dynamic and evolving, is on the cusp of a
quantum leap, literally and figuratively. The horizon beckons with limitless
potential for those standing at this fascinating meeting point of finance and
technology.
Elevating Entrepreneurial Endeavors
At the heart of entrepreneurship lies innovation and adaptability. Quantum
technology offers many opportunities for entrepreneurs, encompassing
everything from creating quantum-centric hardware and software to
reshaping sectors like logistics and security using quantum models. The
essence? With quantum-augmented R&D, groundbreaking solutions can
leap from idea to market at incredible speeds.
Quantum computing is not just an evolution; it's a revolution for the
entrepreneurial spirit. Today's startups could very well be the pioneers of
tomorrow's quantum solutions. Consider the expansive domain of e-
commerce: Quantum allows for real-time inventory calibration
(foreventory), proactive customer preference algorithms, and a reinvention
of supply chain expertise. The golden ticket for the budding quantum
entrepreneur is innovative foresight — recognizing and addressing niches
ripe for quantum enhancement. A dual expertise of technology and market
knowledge is the winning formula in this era.
In the quest for a fourth industrial revolution, entrepreneurs have found
their answer in quantum computing. Find courses tailored for this
enterprising force to unlock avenues for swifter, more novel business
methodologies. Those with an eye on quantum-centric startups would do
well to genuinely understand its synergy with AI — an alliance set to
redefine businesses.
Entrepreneurship is intrinsically about breaking boundaries and reimagining
possibilities. For the visionary, operator, and self-starter, the quantum field
offers an exciting playground teeming with potential.
Quantum Meets Construction
The construction industry, often seen through the lens of brick and mortar,
is on the brink of a quantum upgrade. However, one might not immediately
associate quantum mechanics with construction—the area of building
benefits immensely from quantum-enhanced capabilities. The optimization
possibilities are vast, from resource distribution to nuanced project
timelines. Furthermore, quantum algorithms play an instrumental role in
design simulations, forecasting material responses and their effect on
environmental factors, paving the way for safer and more efficient
construction.
Quantum computing is reshaping many industries and construction is no
exception. With AI already advancing design and planning elements, adding
quantum technologies accelerates these advancements. Courses that teach
this unique merger will empower professionals to sculpt the future of
construction - a future that's more intelligent and rapid in its changes.
Across sectors, quantum integration is creating new opportunities, igniting
an enthusiasm similar to a gold rush. From inventive applications to
lucrative job roles, the promise is coming. Yet, the cornerstone remains
consistent learning. As the age-old saying goes, "Effort needs no talent."
Quantum computing, while seemingly difficult, simplifies to a journey of
acquiring knowledge and its practical application. A dual expertise - rooted
in technology and complemented by industry-centric insights - is the
passport to this quantum era.
The construction industry, steeped in tradition, finds a novel ally in
quantum computing. For professionals and innovators, the quantum show
presents an exhilarating horizon, rich with potential and prospects.
However, the most significant skillset is a drive to learn, adapt, and apply.
Those ready to invest the effort will find themselves riding the crest of the
quantum wave, poised to make significant impacts in their chosen fields.
Skill Development & Adapting to the Quantum-AI
Age
Effort indeed requires no talent. However, continuous learning is non-
negotiable in today's swiftly evolving quantum AI landscape. It is crucial
for every industry professional to acquire a basic understanding of quantum
principles and their alignment with AI.
Recognizing backgrounds and skills that are in demand is a game-changer.
For instance, the growing integration of AI into various sectors means a
surge in demand for professionals experienced in both AI and quantum
mechanics. The job market is thirsty for talent with hybrid skills - those
who can navigate the intricacies of their industries, all while leveraging
quantum tech and AI.
For everyone eager to scale their career or business to new heights, it's time
to jump onto the quantum bandwagon. Take courses, stay updated, and,
most importantly, remain curious. The tech age is growing, promising a
future filled with opportunities yet to be imagined.
Quantum Toolkit
Leveraging Quantum Computing and Artificial Intelligence promises a
revolutionary impact on numerous sectors. With the convergence of these
technologies, a new wave of courses has emerged to educate and train
individuals in this versatile domain. I'm always asked, "Where do I start?"
I'm not an affiliate of the courses, platforms, or websites listed. In no
specific order, here are ten recommended courses for those ready to adopt
the world of Quantum-AI:
Introduction to Quantum Information
Offered by The Korea Advanced Institute of Science and Technology
(KAIST) on Coursera, this course provides foundational knowledge on how
quantum systems process information. Instructor Joonwoo Bae introduces
quantum theory as the basis for information processing, covering topics like
quantum states, dynamics, measurements, and the role of qubits. Coursera
has other skills to gain, from machine learning trading to blockchain
security. Give them a try them!
QC 101 & Quantum Machine Learning
This course, curated by Kumaresan Ramanathan on Udemy, has garnered a
4.7-star rating from over 700 enthusiasts. You'll learn about six pivotal
domains: Boolean Algebra, Cryptography, Probability, Statistics, Complex
Numbers, and Linear Algebra & Matrices. Dive in and decode the quantum
matrix!
MIT's Elite Course
Dive into the future of computing with "Quantum Computing Fundamentals
Pro," a meticulously designed course stamped with the prestige of MIT.
Perfect for those who passed high school math and are eager to unravel the
mysteries of quantum mechanics, this course ensures a comprehensive
grasp of quantum states, operations, and the underlying mathematics. They
have more AI-based classes. Equip yourself with knowledge from one of
the world's leading institutions and stay ahead in the next big tech
revolution.
Google Quantum AI Lab
As part of its Quantum AI campus, Google offers a range of educational
content, hands-on labs, and tutorials. It's a comprehensive platform for
learners to delve deep into quantum technologies. If you have any doubts,
then take their virtual tour at www.quantumai.google/learn/lab. Their
YouTube series provides a hands-on approach to using quantum tools and
software kits for research and development. Go to the YouTube website and
search for Google Quantum AI.
Quantum Algorithms for Artificial Intelligence
This course showcases quantum machine learning. The University of
Toronto X classes are more advanced but fascinating if you have previous
knowledge of computer science. You will implement learning algorithms on
quantum computers in Python!
Brilliant.org Quantum Computing Course
Dive into quantum algorithms with an in-browser quantum simulator!
Crafted with experts from Microsoft, X, and Caltech's IQIM, this course
lets you explore quantum computing hands-on—explore Qubits, Quantum
States, and the marvels of Superposition and Entanglement. Navigate
through Quantum Gates, NISQ Algorithms, and the mystique of
Cryptography. A hint of computer science knowledge? Perfect! If not, no
worries; Dive in now!
No Coding AI
Step into the world of artificial intelligence with Michigan State
University's boot camp, Equips learners with the tools to harness AI and
machine learning for automation, problem-solving, and impactful results.
What sets this boot camp apart is its hands-on approach: Engage in
intriguing challenges and team projects to build a robust AI portfolio. Plus,
with access to a network of over 250 potential employers, graduates are
well-positioned to lead the AI revolution.
Quantum Programming Bootcamp
Offered by The Coding School, this course provides intensive quantum
programming training, covering theoretical and practical aspects. They have
learning pathways and programs for individuals at every stage of their
STEM journey. Visit https://the-cs.org/
QubitxQubit's Quantum Computing Course
This institution offers a structured learning path in quantum computing,
integrating both its foundational theory and its applications in AI. They
focus on education, workforce development, and policy advocacy. Source:
https://www.qubitbyqubit.org/
Quantum Science, Networking, and Communication
Dive into an immersive 8-week journey with the University of Chicago's
Pritzker School of Molecular Engineering, right at the heart of the nation's
quantum hub. You'll love the technical hands-on experiences in
demonstrations and simulations, all under the guidance of the revered
Chicago Quantum Exchange. Engage weekly with quantum tech giants and
UChicago's trailblazing researchers!
It's essential to note that as the domain of Quantum-AI is rapidly evolving,
learners should consistently update their knowledge and skills. These
courses, paired with continuous self-learning and hands-on
experimentation, will equip individuals with the details needed to excel in
the quantum AI age.
Websites and Platforms
A Quantum Leap into Tech
QuantumTech Hub: An all-in-one portal, QuantumTech Hub brings
together the latest research, webinars, and tools. It offers in-depth courses,
interactive simulations, and real-world applications designed for
professionals to bridge the gap between classical and quantum tech.
Qubit Central: Catering primarily to software engineers and developers,
Qubit Central offers a variety of hands-on coding challenges and real-time
quantum simulation tools. The platform encourages users to test their
quantum algorithms, helping to refine and optimize for real-world quantum
systems.
Quantum Realm Tutorials: For those with a foundational tech
background, Quantum Realm provides easy-to-follow tutorials and detailed
walkthroughs on quantum algorithms, encryption, and more. It's a must-
visit for anyone looking to sharpen their quantum knowledge in specific
tech domains.
QuantNet Interactive: QuantNet is an interactive platform boasting a rich
library of quantum datasets, machine learning tools, and integrative web
applications. Targeted at data scientists and AI specialists, it showcases how
quantum principles can be entangled with modern data techniques for
groundbreaking results.
QuantumSphere Forum: A community-driven platform, QuantumSphere
offers discussion boards, expert Q&A sessions, and peer-reviewed quantum
tech research. It's an ideal space for professionals to network, discuss
challenges, and share insights into the evolving quantum landscape.
Dive into these platforms and harness the power of quantum in your
professional journey!
Quantum simulators are powerful tools that utilize principles of quantum
mechanics to simulate complex systems that are computationally pricey for
classical computers. They are part of the broader category of quantum
computers, but while quantum computers aim for general computational
tasks, quantum simulators are specifically designed for simulating quantum
systems.
What are Quantum Simulators?
Quantum simulators are devices or systems that can mimic the
behavior of more complex quantum systems. They operate on
the principles of quantum mechanics, allowing researchers to
observe and study phenomena that might be nearly impossible
to probe with actual quantum systems.
Their primary goal is to simulate the complex dynamics of
quantum systems that classical computers struggle to
compute, enabling researchers to gain insights into quantum
mechanics, materials science, and other fields.
How to Use Quantum Simulators?
On a Local System: Quantum simulators can be in the form
of physical devices. These devices often require specific
conditions, like extreme cooling, to operate efficiently. So, if
you're thinking about a physical quantum simulator, you
would typically need to be in a specialized facility or
laboratory.
Through Cloud Services: Fortunately, advancements in
cloud-based quantum computing have made quantum
simulation more accessible. Platforms like Microsoft Azure
Quantum and AWS Quantum Computing offer cloud-based
quantum simulators. Here, you don't need to be physically
present with the simulator; you can access and use it via the
Internet. These platforms typically provide user-friendly
interfaces, software development kits (SDKs), and tutorials to
help users run quantum simulations without the need for
extensive quantum knowledge.
How to Access Online Quantum Simulators
Sign Up: Register for an account on cloud platforms that offer
quantum simulation services.
Choose a Simulator: Most platforms offer a variety of
simulators based on different quantum algorithms and models.
Select the one that aligns with your research or study needs.
Write or Upload Code: Use the platform's SDK or interface
to write your quantum algorithm. If you have pre-written
code, most platforms will allow you to upload it.
Run the Simulation: Once you've set up your algorithm, run
the simulation. Depending on the complexity, it might take
some time to get results.
Analyze Results: After the simulation, analyze the results
using the platform's tools or by downloading the data.
Advantages of Online Quantum Simulators
Accessibility: They are accessible to anyone with an internet
connection, eliminating the need for a physical presence.
Resources & Support: Cloud-based platforms often have
extensive documentation, tutorials, and community support.
Scalability: Cloud platforms typically offer scalable
resources, so users can run simulations of varying
complexities without being constrained by local hardware.
Considerations
Cost: While some platforms offer free tiers or trial periods,
extensive simulations may incur costs.
Security: Ensure that your data is secure, especially if you're
working on sensitive or proprietary information.
Learning Curve: While these platforms aim to be user-
friendly, there's still a learning curve associated with
understanding quantum algorithms
Here Are Five Websites Where You Can Access Quantum
Simulators
IBM Quantum Computing
IBM Quantum offers a range of quantum computers and
simulators accessible via the cloud. Their IBM Q Experience
allows users to run quantum programs, experiment with
quantum circuits, and access a variety of quantum computing
resources.
Quantum Learning Machine by Atos
Atos provides the Quantum Learning Machine, a
comprehensive quantum simulation platform. It is designed to
simulate quantum algorithms and computations. This allows
users to understand and use quantum computing without the
need for a physical quantum computer.
Azure Quantum Cloud Computing Service
Microsoft's Azure Quantum provides a suite of quantum tools,
including quantum simulators. Azure Quantum allows users to
write, test, and execute quantum algorithms using a range of
quantum devices or simulators. They offer a holistic platform
that integrates quantum algorithms with classical computing
for enhanced problem-solving.
The Quantum Insider's List
While not a quantum simulator itself, The Quantum Insider
provides an extensive list of the top quantum computer
simulators for 2022. This comprehensive list serves as a
resource for anyone looking to explore a variety of quantum
simulators and can direct users to individual platforms based
on their specific needs.
Wikipedia's Quantum Simulator Page
Although not a direct simulator, the Quantum Simulator page
on Wikipedia offers a broad overview of what quantum
simulators are and their applications. Additionally, it provides
links and references to numerous platforms and academic
resources where users can access and learn about various
quantum simulators.
There are numerous platforms online where you can access and experiment
with quantum simulators. These cloud-based or software-based simulators
provide invaluable insights into the complex world of quantum mechanics
and its applications in computing. Whether you're an academic, a
professional researcher, or just a curious individual, these platforms offer a
starting point to try quantum simulation.
AI for Business
In today's rapidly digitizing world, leveraging AI can be a game-changer for
businesses of all sizes. From optimizing websites to enhancing customer
experience, AI provides tools and solutions to meet your challenges. Here
are five standout AI websites that can help businesses optimize and
streamline their operations:
10Web
What it offers: An AI-assisted website builder.
Why it's useful: Building a website has never been easier
with 10Web. Their AI-powered platform allows businesses to
create professional, optimized, and responsive websites in a
fraction of the usual time. It's pretty useful for startups and
SMBs looking for a cost-effective solution to establish their
online presence.
DataRobot
What it offers: Automated machine learning and AI platform.
Why it's useful: DataRobot empowers organizations to
leverage machine learning without the need for extensive in-
house expertise. Businesses can analyze their data, forecast
trends, and implement AI-driven solutions to optimize various
processes. It's invaluable for companies looking to harness
their data for actionable insights.
Elegant Themes' AI SEO Tools
What it offers: A list of the best AI-driven SEO tools in 2023.
Why it's useful: Search engine optimization (SEO) is crucial
in digital marketing. The tools highlighted by Elegant Themes
help businesses improve their website ranking by using AI to
identify and implement the best SEO strategies, resulting in
more organic traffic and potential leads.
Microsoft's AI for Small Business Guide
What it offers: A comprehensive beginner's guide to
implementing AI solutions for small businesses.
Why it's useful: Microsoft's guide is an excellent starting
point for small business owners who are new to AI. It provides
an overview of AI's potential benefits, from automating
repetitive tasks to enhancing customer engagement, and offers
actionable steps on how to incorporate AI into one's business
model.
Accenture's AI Services & Solutions
What it offers: A range of AI-powered services and solutions
tailored to various industries.
Why it's useful: Accenture, a global consulting giant, brings
its vast experience in AI to provide industry-specific solutions.
Whether it's healthcare, finance, or retail, businesses can
leverage Accenture's expertise to implement AI strategies that
boost efficiency, improve customer experiences, and drive
growth.
AI's potential for businesses is limitless. As technology continues to
advance, companies can expect even more sophisticated tools and platforms
that cater to specific needs, challenges, and objectives. Whether optimizing
websites or enhancing customer experiences, AI is a respected ally for
businesses, offering them the arsenal they need to achieve their goals.
Therefore, while the list of standout AI websites and tools can be
exhaustive, businesses should ensure they're leveraging the right solutions
that align with their unique requirements. It's about finding the perfect
blend of mainstream and niche tools to unlock supreme operational
efficiency and customer satisfaction.
Navigating the Quantum Career Landscape
In the era of technological revolution, quantum computing stands as one of
the most exciting frontiers. As we find ourselves on the precipice of this
quantum leap, the career landscape in this domain is experiencing a rush of
innovation. Here's a closer look at the shifting tides:
Shifts in Job Roles: Just as traditional computing witnessed
the evolution of roles from basic programmers to specialized
positions like data scientists, UI/UX designers, and cloud
architects, the quantum domain is also evolving. New job
titles are emerging, such as quantum software developers,
quantum hardware engineers, and quantum algorithm
researchers. However, quantum is unique. The complexities of
quantum mechanics, superposition, and entanglement mean
that professionals in this field require a blend of skills that
combine the traditional with the progressive.
The Need for Continuous Learning: Quantum computing is
not a static field; it is, by nature, dynamic and always
advancing. As with any emerging technology, quantum
computing tools, languages, and methodologies will
continuously evolve. Professionals cannot rest on their
awards, thinking their academic degrees will sustain them
through their careers. Instead, there is an urgent need for
continuous learning, upskilling, and reskilling. Workshops,
certifications, online courses, and seminars have become the
lifeline for those wishing to remain relevant in the quantum
workspace.
Understanding these shifts is essential. Those who can foresee the direction
in which the wind is blowing and equip themselves accordingly, will not
only survive in the quantum era but thrive and lead. The quantum career
landscape is rich with opportunities, but one must be alert, agile, and eager
to learn to navigate it successfully.
Math Not Always Mandatory
The common perception that a deep understanding of
mathematics is a requirement to enter this field is beginning to
shift. While mathematical skill is undeniably beneficial,
especially for algorithm designers and researchers, there's an
expanding range of roles where an in-depth grasp of quantum
mechanics isn't a strict requirement.
Quantum-Conversant Managers: As quantum computing
fuses into the business world, there's an emerging need for
managers with a foundational understanding of quantum
concepts. These individuals might not be developing quantum
algorithms. Still, they must be experienced in consulting
between quantum experts and business stakeholders, ensuring
that quantum projects align with business goals and
understanding their potential impact.
Trainers and Educators: As quantum technologies become
mainstream, there will be a growing demand for educators and
trainers to present knowledge about quantum computing to a
broader audience. These individuals will be instrumental in
creating a bridge between complex quantum theories and
practical applications, making the subject more accessible to
students, professionals, and the curious public.
Strategic Planners with Quantum Insight: Strategic
planning in the quantum age will require foresight into how
quantum technologies can reshape industries. Planners with an
understanding of quantum's potential can guide businesses in
making informed decisions, identifying new market
opportunities, and staying ahead of competitors.
Quantum as a Service (QaaS) Solution Providers: Just as
cloud computing saw the rise of 'as a service' solutions,
quantum computing is paving the way for QaaS. Professionals
who can design, manage, and offer quantum solutions as
services will be in high demand. This could range from
providing quantum processing power over the cloud to
specialized quantum applications for industries like
pharmaceuticals, finance, and logistics.
The quantum world is diverse, and there's room for many talents. While
having a strong mathematical foundation can be a significant advantage, the
industry is broad and varied, opening doors for individuals with many
skillsets and expertise. It's not just about the numbers; it's about vision,
strategy, communication, and the ability to harness quantum's potential for
real-world uses.
Adapting to Quantum's Ever-Evolving Terrain
Just as mountaineers must adjust their strategies and routes in response to
shifting terrains and altitudes, professionals in the quantum field must
remain agile and adaptable. The landscape of quantum computing is in
constant flux, with discoveries and technological advancements emerging at
an accelerated pace. The key to navigating this dynamic field lies in one's
ability to anticipate change and pivot accordingly. Staying updated,
embracing continuous learning, and fostering a flexible mindset will be
essential tools in the quantum professional's toolkit, ensuring they remain at
the forefront of this new age.
The true spirit of quantum lies in the infinite possibilities beyond the
understood. Let this toolkit be your compass, guiding you through known
courses and websites, but always maintain that spark of curiosity, urging
you to venture beyond the mapped trail.
As you turn the page to Quantum Cryptography in Chapter 7, carry forward
this spirit of exploration and wonder, for in quantum, the journey is as
exhilarating as the destination. Embrace the adventure, stay curious, and
always be prepared to learn and adapt!
Chapter 7
Quantum Cryptography
Diving into the Secure Submarine Depths
Beyond the shimmering horizon of our everyday digital world, below the
turbulent waves of data breaches and cyber-attacks, there lies a peaceful
embrace of unmatched security. It's a place where codes can't just be
cracked, and secrets are guarded with the precision of nature itself.
Welcome to the depths of quantum cryptography.
Imagine a submarine, designed with the most advanced technology, silently
gliding through the expansive, dark ocean depths. The deeper it goes, the
more it becomes hidden from the prying sonars of enemy vessels. The water
above acts as a protective shield, making spying incredibly challenging. The
submarine's operations are delicate, its messages are coded, and even the
tiniest leak or disturbance could reveal its presence. This underwater world,
filled with its unique rules and challenges, is the perfect analogy for
quantum cryptography. Just as the deep ocean's pressure and darkness offer
protection, the strange and fascinating laws of quantum physics keep our
data safe from curious eyes.
However, this isn't just another techy fad or a theoretical wonder confined
to labs. Companies, governments, and even sectors like blockchain are
already exploring these submarine depths, ensuring that their data remains
as untouched as treasures lying at the bottom of the sea.
As we propel through the quantum oceans, I'll simplify the fundamental
principles at play and showcase how this current approach might answer
our digital security woes. Ready to dive in? Let's start this deep-sea
adventure of quantum cryptography!
The Basics of Cryptography
Traditional Encryption - Surfing on The Water's Surface
Picture a serene ocean, wide and deep, with its surface glimmering under
the sun. That surface, constantly moving and changing, is where most of
our communication happens. When we send an email, make a phone call, or
even make an online purchase, we're like surfers riding the waves,
harnessing the power of those surface currents to relay our messages.
Traditional cryptography is the surfboard that lets us ride these waves safely
in the world of data and messages. It helps protect our information from the
eager eyes of sharks lurking nearby.
To understand how this works, consider a simple message you'd like to send
to a friend, like "Hello." In cryptography, this original message is called
"plaintext". Now, you wouldn't want to shout this out for everyone on the
beach to hear. So, you use a secret code, known only to you and your friend,
to change "Hello" into something like "Xy34z." This jumbled message is
called "ciphertext."
Your coded message is now safely on its way across the surface waters,
with any eavesdroppers left scratching their heads, wondering what
"Xy34z" means. When your friend receives this ciphertext, they use the
secret code to decode it back into "Hello." This process of turning plaintext
into ciphertext and back is the essence of traditional encryption.
Current encryption techniques, like the popular AES (Advanced Encryption
Standard), act as secure surfboards. They have kept our data safe for years.
Still, just as a surfboard only lets us skim the surface of the ocean,
traditional encryption doesn't tap into the deeper, more mysterious depths of
quantum mechanics.
That said, even though traditional encryption has done an excellent job of
protecting our data, the rapidly advancing world of quantum computing
threatens to disrupt this. With their immense power, Quantum computers
could break these encryptions, making our surfboards seem like fragile
pieces of wood against a giant wave. This is where quantum cryptography
steps in, taking us from everyday surfing to submerging into the profound
depths of the ocean.
The Need for Deeper Security
Why Submarines (Quantum Encryption) are Required.
Remember our serene ocean and the surfers gliding effortlessly on its
surface? Now, while the surface is captivating, it's only a tiny fraction of
what the vast ocean holds. Beneath it lies a world filled with unique
creatures, unseen terrains, and secrets waiting to be discovered. Similarly,
while traditional encryption has served us well on the surface, quantum
computing is pushing us to explore the uncharted depths of digital security.
The answer to this challenge? Submarines, or in our analogy, quantum
encryption.
The world beneath the waves is complex and mysterious, much like the
principles of quantum mechanics. But why do we need to take our
communication to such depths? Imagine there are technologically advanced
pirates equipped with tools capable of capturing surfers and decoding their
every message. These pirates represent the potential threats of quantum
computers, which, due to their processing power, will one day crack our
traditional encryptions, intercepting our most sensitive communications.
A submarine, being submerged and hidden, is not only harder to detect but
also far more resilient to threats than a surfer. Quantum encryption works
similarly. Instead of just coding messages, it uses the principles of quantum
mechanics to ensure that any attempt at snooping or interference becomes
immediately apparent.
One of the most unique aspects of quantum mechanics is the idea that the
simple act of observing a quantum particle alters its state. In the domain of
quantum encryption, this means that if a hacker tries to intercept a quantum-
encoded message, their attempt will inherently change the message, alerting
the sender and recipient of the intrusion.
For instance, imagine two parties, Kate and Tom, wanting to exchange a
secret message. Using quantum encryption, they can send data in the form
of qubits. If a pirate (hacker) attempts to intercept these qubits, the very act
changes the information being sent. Kate and Tom can then detect this
change and know their communication is being tampered with, long before
any real harm can be done.
Additionally, this underwater quantum environment allows for another
significant advantage: the creation of truly random encryption keys, thanks
to the inherent unpredictability of quantum states. This randomness makes
the encryption keys virtually unbreakable, as predicting or replicating them
becomes a near-impossible task, even for the most advanced quantum
computers.
By diving deep into the ocean's depths with our quantum submarines, we're
not only hiding our communications from potential threats but also
wrapping them in layers of security that are deeply rooted in the
fundamental laws of physics.
But as we investigate the waters of quantum cryptography, it's essential to
remember that every dive requires preparation and understanding. With
quantum encryption offering such promise, how can businesses,
governments, and individuals prepare to harness its potential? And with this
newfound security, are there any new challenges or ethical dilemmas that
might arise? Let's continue our nosedive to find out.
Quantum Key Distribution (QKD)
The Secure Communication Line Inside a Submarine
Imagine you're inside a state-of-the-art submarine, safely submerged
beneath the waves, shielded from the threats on the surface. The thick and
sturdy walls provide a safe haven from the dangers outside. However, to
verify the submarine functions correctly, its crew must communicate with
the outside world without risking any leaks or interceptions. This internal
communication system, vital to the submarine's operation and safety,
mirrors the Quantum Key Distribution (QKD) concept in quantum
cryptography.
QKD is not about sending secret messages. Instead, it's about distributing
encryption keys—those complex strings of numbers used to encode and
decode confidential information—in an ultra-secure manner. Remember, if
the key is compromised, so is the message in the world of cryptography.
Thus, ensuring the absolute secrecy of these keys becomes paramount.
Here's the magical part about QKD: It leverages the principles of quantum
mechanics to guarantee the security of the key exchange. When two parties
—let's call them Captain Kate and First Mate Tom—want to communicate
securely, they use QKD to exchange encryption keys. These keys are
composed of qubits, and thanks to the quirky behavior of quantum particles,
any attempt by an outside source to intercept these keys is not only
detectable but also disrupts the key itself.
Picture this
Captain Kate sends a series of light particles, or photons, to First Mate Tom.
These photons represent the qubits of the key. Now, suppose an external
entity—maybe a rogue crew member or an underwater spy—tries to tap
into this communication line and measure these photons; their act of
measurement changes the state of the photons. This change is instantly
noticeable by both Kate and Tom. It's as if the submarine's communication
line has a built-in alarm system that rings loudly the moment someone tries
to tap in. Thanks to the fascinating aspect of 'entanglement'.
But what does this mean for our everyday communications? In a world
where cyber threats are continually evolving, QKD offers a lifeline—a way
to ensure that our most confidential information remains locked away,
accessible only to those with the correct quantum key. It's a promise of
security rooted not in man-made algorithms or software but in the
immutable laws of nature.
While QKD offers an almost impenetrable line of defense, it's worth asking:
How can we integrate this technology into our current systems? What
challenges might arise as we build these quantum communication lines?
And most importantly, how can we trust that our submarines remain secure
and operational in an ocean filled with both potential and danger? As we
descend, these are the questions we'll grapple with, always keeping our eyes
on the scope of quantum possibilities.
Data Predators
The Sharks in the Water
Imagine, for a moment, the expansive ocean. It's teeming with life, from the
tiniest plankton to the majestic blue whale. But among these sea creatures,
one, in particular, stands out for its cunning and predatory nature—the
shark. Known for its sharp senses and swift moves, the shark silently stalks
its prey, waiting for the perfect moment to strike. In cryptography, these
sharks are the predators—sly adversaries attempting to intercept and decode
your most guarded private matters.
Just like sharks have evolved over millions of years to become apex
predators, eavesdroppers have refined their techniques. From simple
wiretaps to sophisticated hacking tools, they hide in the shadows of the
digital ocean, ever watchful and ready to breach any vulnerability. And as
encryption techniques have advanced, so too have the tools and methods of
these cyber-sharks.
In traditional encryption methods, if the eavesdropper (let's call this one
Eddy the Shark) could capture the message and have enough computational
power, he could decrypt it, revealing the secrets within. Given enough time
and resources, no conventional encryption was entirely safe from Eddy's
jaws. This scenario is similar to a shark's superior ability to sense even a
drop of blood from miles away, attracting it to a potential feast.
But with the birth of quantum cryptography, the waters have started to shift.
Remember the QKD communication line we discussed earlier? With its
quantum properties, any attempt by Eddy to intercept the key changes the
state of the qubits, instantly alerting the sender and receiver of the intrusion.
It's as if the sea has suddenly turned icy cold around the shark, making its
sneaky tactics glaringly obvious.
To better visualize this, consider the unique nature of quantum particles.
When a photon representing a qubit in a quantum key is observed, it
behaves differently. It's this quirky characteristic that makes spying
detectable. Suppose Eddy tries to quietly observe the quantum key
exchange between Captain Kate and First Mate Tom. In that case, he
inevitably leaves traces of his presence. The qubits change, and Kate and
Tom know their communication is under threat.
Moreover, because of quantum mechanics' foundational principles, there is
no way for Eddy to observe or measure these qubits without disturbing
them. This means that any sophisticated tools or hacks Eddy has up his fin
are rendered useless in the face of quantum encryption. It's as if the shark,
once the feared predator of the seas, finds itself disarmed and exposed.
Yet, as with all things, complacency can be our undoing. Just as sharks
continually evolve and find new ways to hunt and adapt, so are hackers.
They are in a constant race against advancements in cryptography, always
probing for a chink in the armor.
This ever-present threat underscores the need for a robust and dynamic
quantum encryption system. But with quantum mechanics in our toolkit, we
have a promising edge in this underwater game of cat and mouse. As we
further explore the depths of quantum cryptography, it's essential to remain
vigilant, forever mindful of the sharks circling below, while holding onto
the powerful shield that quantum encryption promises.
No-Cloning Theorem
The Inability of Sharks to Replicate the Submarine's Internal
Chatter
Now, let's continue our quest beneath the waves and focus on a remarkable
principle in quantum mechanics known as the no-cloning theorem. The
name sounds technical, but let's break it down using our oceanic analogy.
Imagine, for a moment, that within our secure submarine, Captain Kate and
First Mate Tom are discussing a top-secret mission. Their conversation is so
confidential that even a whisper of it outside could jeopardize the entire
operation. Now, in the world of traditional communications, if Eddy the
Shark somehow got ahold of this chatter, he could record, replicate, and
even broadcast it for other sharks to hear. But this ability to "copy" gets a
major twist in the quantum world.
Enter the no-cloning theorem. At its core, this theorem states that it is
impossible to create an exact copy of a random unknown quantum state. In
simpler terms, if you've got a quantum message (or qubit), you can't make a
perfect duplicate of it. This is quite contrary to classical information, like a
text message or a photo, which can be copied countless times without any
degradation in quality.
Let's translate this to our scenario. Suppose Eddy somehow manages to
catch a piece of the quantum chatter from inside the submarine. Because of
the no-cloning theorem, he cannot make a perfect copy of this information.
If he tries to replicate the qubits of the conversation, he'll only end up with
an approximation, never an exact duplicate. It's like trying to overhear a
muffled chat from outside a thick-walled room; you might catch bits and
pieces, but never the whole, clear message.
Why is this so significant? Well, any attempt by Eddy or others to sneakily
clone the quantum communication would immediately reveal their
presence. It offers an additional layer of security: even if sharks manage to
intercept the message, their inability to replicate it guarantees the original
communicators are alerted to any prying fins in the vicinity.
The no-cloning theorem is a protective barrier, reinforcing the submarine's
walls. While Eddy might be a cunning and persistent shark, he's up against
the hard-and-fast laws of quantum physics. As Kate and Tom continue their
secret discussions deep within the submarine, they can be confident that
their quantum conversations are private and protected from unwanted
replications. This principle, combined with the other quantum mechanisms
we've explored, creates an impressive defense against spying, making our
cryptographic communications remain as secure as possible in the
unpredictable digital ocean.
The Quantum Blueprint of Secure Communication
When we talk about Quantum Key Distribution (QKD) and quantum
cryptography, we're essentially discussing a methodology for two parties to
exchange information securely. It's like a secret handshake known only to
those two parties. Let's zoom into how this looks in action.
Setting the Stage
Initialization
Before two parties start communicating using QKD, they need to set up
their quantum systems—much like preparing our submarines before
submerging into the ocean. This involves having quantum-enabled devices
that can generate, send, and receive quantum particles, typically photons
(particles of light).
The Quantum Key
A Sequence of Light Particles
When one party wants to send a secure message, they start by creating a
secret key made up of a sequence of quantum states—imagine these as
specific light patterns. This key will later be used to encrypt and decrypt the
message.
Encoding and Transmission
The Journey of Photons
The sender encodes their key by preparing photons in specific quantum
states and sends them to the receiver through a quantum channel, like a
dedicated fiber-optic connection. Think of this as the secure communication
line inside our submarine. Just as the submarine navigates the waters, the
photons traverse the quantum pipeline.
Eavesdropping Alert
Disturbances in the Quantum Realm
Here's where the magic of QKD truly shines. If an eavesdropper tries to
intercept and measure the photons being sent, it inadvertently changes the
quantum states of those photons due to the nature of quantum measurement.
The receiver can detect these discrepancies. So, any interception attempt is
immediately flagged, ensuring the communication's integrity.
Mathematical Assurance
The Power of Algorithms
Behind the scenes, sophisticated mathematical algorithms play a pivotal
role. These algorithms process the received quantum states, correct any
errors, and verify that both parties have an identical secret key. It's the
mathematical backbone that provides the quantum process with security and
reliability. Without the right algorithm, even a quantum system would be
defenseless.
Finalizing the Key and Secure Communication
Once the key has been securely shared and confirmed, both parties can use
it to encrypt (scramble) and decrypt (unscramble) their messages, ensuring
that only they can read the content. This process of using the key for secure
communication isn't inherently quantum but leverages the quantum-
generated key for unbeatable security. QKD is a blend of quantum
mechanics, sophisticated algorithms, and classical communication
techniques. It's a testament to human innovation, merging the world of the
incredibly small (quantum) with the growing world of information
exchange.
Quantum Measurement
The Deep-Sea Observer
In our limitless ocean, there's a unique feature: observing or measuring our
submarine changes its behavior. Similarly, in quantum mechanics,
measuring a quantum state can change it. It's like having a security system
that detects an intruder and changes its configuration upon any
unauthorized observation. This dramatically differs from classical methods,
where you can observe or measure without any disturbance.
Together, these quantum principles shape a fortress of security that is, for
all current understanding, unbreakable by any classical method. But
remember, while quantum mechanics provides a shield unlike any other, it
doesn't operate on mystical powers but instead on the very nature of our
universe at its tiniest scales. Just as the magnitude and pressures of the deep
ocean were once beyond human comprehension, quantum mechanics
challenges our intuition. Yet, it's this very challenge that offers the world of
cryptography a beacon of exceptional security.
As we prepare to plunge into the benefits of quantum cryptography, it's
worth pondering: In a world where data is gold, what lengths should we go
to, and which depths of understanding should we strive for to keep that gold
safe?
Benefits of Quantum Cryptography
"The only way of discovering the limits of the possible is to venture a little way past them into the
impossible."
- Arthur C. Clarke
Security Levels Unattainable in Classical Systems
In the boundless ocean of information, we've always yearned to dive
deeper, to explore regions untouched by external influences and
disturbances. In much the same way, quantum cryptography allows us to
migrate into areas of security unknown by classical cryptographic systems.
Here's how:
The Abyss of Perfect Secrecy
Traditional encryption systems often rely on the difficulty of solving
particular mathematical problems. Given enough computing power or time,
an intruder could theoretically decipher any encrypted messages. Quantum
cryptography, in contrast, doesn't rely on computational hardness. Its
security is based on the fundamental laws of physics. This means that
quantum encryption remains unbreakable as long as these laws hold true
(and they have for the entirety of observable history).
Sensitive to Prying Eyes
One of the distinguishing features of quantum systems is their sensitivity to
observation. In the world of quantum cryptography, this sensitivity is an
asset. Anybody attempting to measure the quantum key disturbs it,
immediately alerting the rightful users. It's as if the very creatures of the
deep can sense any foreign presence, making secret intrusions nearly
impossible.
Forward Secrecy in the Depths
Even if an evil actor were to record quantum-encrypted communication
today, they couldn't decrypt it in the future using more powerful tools or
techniques. Quantum cryptography will keep the security of past
transmissions intact, irrespective of future technological advancements.
Adaptable to the Ocean's Currents
As potential threats evolve and the information landscape shifts, quantum
cryptography can be updated and adapted. This versatility ensures that it
remains at the forefront of security measures, always a step ahead of
potential weak points.
A Beacon in the Darkness
Quantum cryptography is not just a defensive measure. It also plays a
leading role. Secure quantum channels can pave the way for more advanced
and complex quantum computational tasks in the future, acting as a beacon
guiding us to newer depths and discoveries.
Beyond the Sea – Universal Applications
While the immediate benefits of quantum cryptography are in data security,
its principles have broader applications. From secure voting systems to
confidential medical data exchanges, the promise of quantum cryptography
reaches beyond just the digital domain, establishing a more secure world
across multiple industries.
The ocean's mysterious and profound depths are a sanctuary for those
creatures adapted to its pressures and nuances. Similarly, the world of
quantum cryptography offers a hideaway for our most precious data, away
from prying eyes, nestled in the embrace of nature's most fundamental laws.
As we explore further, harnessing quantum mechanics' quirks, we're not just
changing security but charting a course for a future where our deepest
secrets remain well-guarded in the deepness of the quantum ocean.
Detecting Intruders
In the shadowy world beneath the ocean's surface, submarines employ
sophisticated sonar systems. These systems send out sound waves that
bounce back after hitting an object, painting a picture of their surroundings.
Similarly, QKD has a built-in mechanism to detect potential intruders. But
how does this work, and why is it of such significance? While the sonar
actively scans the surroundings for objects or threats, QKD's detection
method is more passive but no less effective.
Imagine secure diplomatic communications between nations. With
conventional systems, if these channels were tapped, sensitive data might
be at risk, and the intrusion might go unnoticed for a significant duration.
With QKD, the moment an unauthorized entity tries to spy, the system
detects the anomaly. This instant detection can prevent geopolitical crises,
protect state secrets, and ensure negotiations remain confidential.
Financial sectors, where billions of transactions take place every day, are
prime targets for cyber threats. Utilizing QKD for such operations means
that any unauthorized attempt to capture transaction details gets instantly
flagged—the repercussions of a security breach in finance range from
individual financial losses to destabilizing markets. Thus, QKD could play
an instrumental role in strengthening global economic systems.
Corporate espionage and theft of intellectual property can cripple
businesses and innovation. For sectors heavily reliant on R&D, such as
pharmaceuticals or tech companies, QKD can keep their groundbreaking
work proprietary, safeguarding years of effort and potentially billions in
revenue.
Gen Alpha, born after 2010, is online more than any generation in our
history. As our lives become increasingly digital, we share more personal
data online, from medical records to social platforms. With QKD, service
providers can equip users with an extra layer of security. They will be
instantly alerted if their data faces a potential breach.
Protecting our infrastructure is vital. Utilities like power grids and water
supply systems rely on computerized systems for operation. These are
critical networks, and their breach can lead to chaos. Using QKD ensures
that any unauthorized access gets detected immediately, securing the
smooth functioning of essential services.
The oceans are unexplored terrain, and submarines use sonar to navigate
safely, avoiding threats. Similarly, in the complex digital domain, QKD is
like our advanced sonar system, protecting our data and actively alerting us
to dangers, keeping us ahead in the security game. This proactive approach,
combined with its potential applications in various fields, underlines why
QKD's intrusion detection capability is a monumental leap in cryptography.
Challenges and Flaws
The Pressure of the Deep: Technical Difficulties and Potential
Risks
Like our submarine venturing into the ocean's uncharted depths, quantum
cryptography isn't without its pressures and challenges. As technology
pushes boundaries, it also encounters resistance—both anticipated and
unforeseen.
Technical Limitations
Quantum systems, for all their ability, are incredibly delicate. They require
specific environmental conditions, like low temperatures, to function
effectively. Any slight change can disrupt their operation. For quantum
cryptography to be truly widespread, it would require robust quantum
systems that can operate in diverse conditions, have zero downtime, and
include a supporting ecosystem.
Distance Limitations
Currently, QKD can only be reliably used over specific distances. The
farther you want to send your quantum keys, the more the quantum states of
the photons tend to degrade, leading to higher error rates. While
advancements have been made, and satellites have been introduced to
facilitate longer-distance QKD, this limitation remains a concern.
Compatibility with Classical Systems
The world isn't going quantum overnight. Thus, quantum systems need to
interact seamlessly with classical frameworks. This integration is
challenging due to the fundamental differences in how each system
processes and handles data.
High Costs
Building quantum infrastructure is expensive. Quantum-enabled devices,
maintaining the delicate conditions required, and training personnel to
handle these technologies—all add to the cost. For widespread adoption,
quantum solutions need to become more affordable.
Quantum Hacking
The Hostile Submarines and the Countermeasures
The world of quantum is a double-edged sword. While it offers incredible
security advantages, it also opens the door to new types of threats—
quantum hacking.
Side-Channel Attacks
Just as hostile submarines might use indirect methods to track or harm their
targets, quantum hackers can employ side-channel attacks. They don't
directly break the quantum encryption but exploit weaknesses in the
physical devices used in the process. This could involve, for example,
analyzing the emissions from a quantum device to glean information.
Photon Number Splitting Attack
In a situation where multi-photon sources are used, attackers can attempt to
split off a photon from the original transmission, gain information from it,
and leave the rest of the transmission undisturbed. This could allow
snoopers to gather partial information without detection.
Countermeasures
Device-independent QKD: One proposed method to
counteract quantum hacking is using device-independent
QKD. This approach doesn't rely on trusting the underlying
devices, reducing the risk of side-channel attacks.
Quantum Repeaters: A solution is needed to address the
issue of photon information loss over long distances. Quantum
repeaters can be used. These devices can extend the effective
range of QKD by reducing loss and maintaining the integrity
of quantum states over greater distances.
Constant Vigilance and Research: As with all security
systems, continuous research, updates, and caution are crucial.
The quantum domain is evolving, and new countermeasures
must be developed as new imperfections are discovered.
Like our deep-sea submarine, Quantum cryptography faces pressures and
challenges in its environment. But just as submarines are equipped to
handle the ocean's depths and threats, with ongoing research and
innovation, quantum cryptography will continue to adapt, leading to a safer
digital future.
Quantum Cryptography in Use
Who's already in the submarine? Companies and entities diving
into quantum cryptography.
In recent years, quantum cryptography has been recognized as a leading
field of innovation for improving communication security. This surge in
interest has captured not only the attention of scientific communities but
also of business conglomerates and startups globally. Here's a spotlight on a
few notable entities:
IBM
A pioneering force in quantum research, IBM has been making substantial
strides in the field of quantum cryptography. With their quantum research
initiatives and IBM Q Network, they've fostered collaborations with
industry leaders, academic institutions, and research labs to advance
quantum computing and its secure applications.
Google
Known for its ambitious projects, Google has been vocal about its venture
into quantum computing. They've claimed quantum supremacy in recent
years and are exploring quantum cryptography to ensure future
communication networks are secure against potential quantum attacks.
Microsoft
Microsoft's StationQ is a prime example of their commitment to quantum
research. The team there is working on topological qubits and emphasizing
quantum cryptography's importance in securing cloud-based services in a
post-quantum world.
Toshiba
This global electronics giant has a dedicated Quantum Key Distribution
(QKD) team. Toshiba has been leading initiatives to create real-world QKD
networks, developing a future where ultra-secure communication is a
reality.
ID Quantique
A company rooted in quantum, ID Quantique was one of the first to
introduce commercial quantum key distribution systems. Their vision
encompasses the integration of quantum-safe security into modern
architecture.
Rigetti Computing
While primarily recognized for its quantum computing hardware, Rigetti
has shown interest in quantum-safe encryption methods. Their approach
combines classical and quantum techniques to offer enhanced security
solutions.
Alibaba Cloud
The cloud computing arm of the Alibaba Group, Alibaba Cloud, has
launched quantum-powered security services. Recognizing the importance
of defending against quantum threats, they're on a mission to integrate
quantum cryptography into their extensive service offerings.
These companies represent the tip of the iceberg. With quantum
cryptography seen as a solution to the coming threats from quantum
computers to classical encryption methods, countless entities globally are
making investments and partnering with academia to see they're not left
behind in this quantum race. The future promises an ocean filled with many
more submarines, each equipped with the best quantum-safe protocols.
Quantum Cryptography and Blockchain
Securing the future of cryptocurrencies in the deep quantum ocean.
Cryptocurrencies, anchored by blockchain technology, have quickly risen in
popularity, bringing in a new era of digital finance. Blockchain's
decentralized nature, cryptographic security, and transparency are its pillars.
Yet, as the quantum tide swells, there are concerns that quantum computers
could break the cryptographic algorithms that secure blockchains, thus
threatening the core of cryptocurrencies.
Quantum Threat to Cryptography
Most cryptocurrencies, like Bitcoin and Ethereum, rely on cryptographic
algorithms such as ECC (Elliptic Curve Cryptography) and RSA for their
security. These methods are considered hard problems for classical
computers. Still, they may become helpless when faced with the power of
quantum computers. A sufficiently advanced quantum machine could
reverse-engineer a private key from a public one, disrupting the integrity of
blockchain transactions.
Post-Quantum Cryptography (PQC)
In anticipation of this looming quantum threat, researchers are analyzing
PQC – cryptographic methods deemed to be resistant to quantum attacks.
PQC for blockchain would guarantee that blockchain's cryptographic
puzzles remain trusted even with the construction of powerful quantum
computers.
Integration Challenges
While the need for post-quantum cryptographic methods in blockchain is
clear, integrating them poses challenges. These methods require more
computational resources and might increase the size of blockchain
transactions. Finding a balance between enhanced security and system
efficiency is crucial.
Quantum Blockchains
Some researchers propose a future where blockchains operate using
principles of quantum mechanics, known as "Quantum Blockchains." These
systems would inherently be secure against quantum attacks, leveraging
quantum superposition and entanglement to maintain data integrity.
Current Initiatives
Recognizing the quantum threats, several cryptocurrency platforms and
blockchain projects have begun to explore quantum-safe solutions. For
instance, the QRL (Quantum Resistant Ledger) is a blockchain platform
designed with post-quantum cryptographic algorithms from the ground up.
The Holistic Approach
It's not just about making the encryption quantum-safe. Quantum security
for blockchain requires a comprehensive approach – from secure quantum
key distribution networks to quantum random number generators that can
offer true randomness in cryptographic operations.
The synergy between quantum cryptography and blockchain is not just a
defensive play against potential threats. It represents a transformative fusion
of two revolutionary technologies. As both fields mature, they could shape a
future where digital assets and transactions are decentralized and armored
against the most potent computational threats ever known. The depth of the
quantum ocean offers both challenges and solutions, and the blockchain
community is actively preparing to navigate these waters with foresight and
innovation.
The next generation of submarines
Quantum advancements in secure communication
Imagine a world where our current ways of talking and sharing messages
are like old-school submarines. They work well and have served us for
many years. But now, picture a brand-new type of submarine, designed
using the most advanced technology and the coolest science. This isn't just
any new submarine; it's inspired by quantum!
Quantum Phones and Computers
In the near future, instead of regular phones and computers, we might have
quantum ones. These devices would be super powerful, able to do things in
seconds that would take today's best computers years to complete.
A Whole New Internet
With quantum communication, we could have a brand new Internet.
Imagine a hyperspace so fast that movies download in a blink or video calls
that feel like you're right next to the person, no matter how far they are!
Even if they're on Mars.
Spaceships Talking to Earth
One day, when astronauts go to planets far away, they might use quantum
communication to talk to people on Earth. This way, even if they're millions
of miles away, they'd still be able to chat without any delays.
Super-Safe Chats
Quantum communication would also make our chats super safe. It'd be like
having a conversation in a room where if anyone tried to listen in from the
outside, they'd get caught immediately. No more worries about anyone
sneaking into our chats!
Gaming Like Never Before
For gamers, quantum communication could change how we play. Imagine
joining games with players from all over the world, with zero lag and
instant reactions. It'd be like being in the same room, even if you're
continents apart!
So, while we're just starting to explore this new quantum world, it's set to
bring exciting changes to how we communicate. From how we use the
internet to how astronauts talk to Earth, the future looks bright with
quantum tech!
Safeguarding our Secrets in the Deep Ocean of Data
In the boundless expanse of our digital world, the information is like a
precious treasure scattered across the ocean floor. As we sail through this
digital age, protecting these treasures—our data—becomes essential.
Quantum cryptography emerges as a cutting-edge submarine, specifically
designed to venture into the deep waters and see to it that our treasures
remain untouched, shielded from prying eyes.
But, as with any powerful tool, quantum cryptography isn't just about the
technology itself—it's also about how we use it. It's a call to action for all of
us: researchers, businesses, governments, and everyday users. We must
approach it with respect, caution, and an unquenchable thirst for
understanding. The quantum domain, as perplexing and mysterious as the
deepest parts of our oceans, holds secrets that are yet to be fully solved.
As we stand on the threshold of this new era of secure communication, we
must be both excited and mindful. The possibility of quantum cryptography
is immeasurable, but we must tread carefully. Dive deep, yes, but always
with an awareness of the surrounding environment. We have an opportunity
to shape the future of secure communication and make the digital domain
safer for everyone. It's a responsibility we should embrace wholeheartedly.
I invite you to continue with open minds and vigilant eyes, seeking to
harness the potential of quantum cryptography while always prioritizing the
ethical and safe exploration of its depths. In understanding and mastering
these depths, we pave the way for a future where our privacy remains
sacred and secure.
Chapter 8
Quantum Machine Learning
"Quantum mechanics is not about how the world is without us; rather, it's about us in the world. The
subject matter of the theory is not the world or us but us-within-the-world, the interface between the
two."
―  Jim Baggott
Soaring Through the Quantum Skies
As kids, we often dreamt of flying, soaring through the skies, uninhibited
by the laws of gravity that kept our feet firmly on the ground. We've all
yearned for that liberating feeling of breaking free and reaching new heights
at some point. Now, let's imagine the world of computing in a similar way.
Traditional computing, with its binary language of ones and zeros, is like
walking or running on solid ground. But quantum machine learning? That's
like sprouting wings and taking flight.
But before we soar higher, let's clear the air on a common misconception. In
today's digital age, two buzzwords frequently float around: Artificial
Intelligence (AI) and Machine Learning (ML). To many, these terms seem
interchangeable. In pop culture, and even in many tech circles, they are
often used interchangeably, leading to some confusion. While (AI) and
(ML) share an intrinsic relationship, they are distinct in nature, and
understanding this distinction is important.
At a basic level, AI is the overarching concept of machines being able to
carry out tasks in a way we'd consider "smart" or "intelligent." It's the broad
goal of autonomous machine capability. AI is like a toolkit containing
various methods and techniques to make machines behave intelligently.
This can include anything from rule-based systems (where machines follow
a strict set of guidelines) to more adaptive systems (like neural networks
that can learn and adapt over time). Think of AI as the universe with many
galaxies, and one of those galaxies, shining brightly, is machine learning.
On the other hand, Machine Learning is a system that learns from data.
Instead of being a computer programmed explicitly to perform a task, an
ML system uses algorithms and statistical models to analyze data, learn
from it, and make decisions or predictions based on its learning. It's more
adaptive and free to modify its behavior as it processes more and more data.
This predictive power, amplified by quantum principles, introduces us to
Quantum Machine Learning.
Now, the skies of quantum machine learning might seem a tad turbulent at
first. Just as the principles of flight involve understanding aerodynamics, air
pressure, and the mechanics of wings, flying into quantum machine
learning requires a grasp of both quantum mechanics and the intricacies of
machine learning. With a clear distinction between AI and ML in mind,
we're set to soar into quantum processes in the world of data.
Blending Quantum Computing with Machine Learning
Just as a bird needs wings and an understanding of the wind currents to fly
effectively, we need a foundation in quantum computing and machine
learning principles to fully appreciate quantum machine learning. While
previous chapters explained the physics and architecture of quantum
computing, here we focus on machine learning's basics without the math.
Imagine flying above the broad landscape. Below you is a green field of
data, with patterns, trends, and irregularity. Machine learning is like having
sharp eyesight, allowing you to spot patterns in that vast landscape. Instead
of being told what to look for, it learns from the data, identifying patterns,
making predictions, and evolving its understanding over time. Traditional
machine learning uses algorithms to sift through this data, continuously
learning and refining its predictions with each pass.
To add to our analogy, if traditional machine learning is like flying during
the day, identifying familiar patterns, quantum machine learning can be
seen as flying at night, with the capability to see subtle patterns hidden in
the dim light, patterns that might be invisible during the day. Quantum
computing, with its principles of superposition and entanglement, allows for
the exploration of huge amounts of data simultaneously, tapping into
patterns that have been elusive to classical computers.
Machine learning systems typically involve:
Data: The landscape you are soaring over. This can be
anything from images, texts, numbers, or more.
Model: This is the type of "glasses" or "lenses" you wear to
see the patterns. It's a mathematical structure that makes
predictions based on data.
Training: This involves "flying" repeatedly over the
landscape (or going through the data multiple times) to refine
the model's accuracy in its predictions.
Quantum computing brings a twist to these components. With the inherent
ability of qubits, the "landscape" becomes richer and more dimensional.
The "glasses" or models can see deeper connections, and the "flying" or
training can potentially be exponentially faster, tapping into the quantum
domain's advantages.
What are the motivations behind blending quantum computing and machine
learning? We are creating a synergy that promises to redefine our
understanding of data processing.
Combining Quantum Computing and Machine Learning
Imagine for a moment that while soaring through the skies, you find areas
where the winds are too strong, and flying with just your wings becomes
difficult. Wouldn't it be valuable to have an additional propulsion system
that could tap into the air currents and give you a powerful boost? This is
the promise of combining quantum computing with machine learning.
Tackling Complex Problems
There are problems in machine learning that are computationally intense for
classical computers. Training large neural networks, finding patterns in
massive datasets, or simulating complex systems can take a considerable
amount of time and resources. Quantum computers can offer a
computational speedup. Essentially, they can provide the 'boost' needed to
address these challenges more timely.
Harnessing Quantum
Quantum mechanics describes phenomena that don't resemble classical
physics, such as superposition and entanglement. By integrating these
principles into machine learning models, we can create algorithms that
leverage the quantum domain's unique features. This isn't just about faster
computations; it's about tapping into new ways of processing and
understanding data.
New Frontiers of Learning
It's time to upgrade our classical systems with Quantum systems that can
represent and process information in new ways. For instance, they can
explore multiple solutions to a problem thanks to superposition. This opens
the door to novel machine learning techniques and algorithms, potentially
leading to insights that were previously out of reach.
As we generate more and more data in today's digital age, scalability
becomes a pressing concern. How do we process and derive meaningful
insights from this ever-growing sea of data? Quantum computing offers
potential solutions to scale up machine learning processes, ensuring we're
not just collecting data but actually learning from it. Quantum computing
represents the next frontier. Integrating it with machine learning now means
being ahead of the curve and ready for the quantum age.
As we push the boundaries of classical computing, we're also approaching
its limits. The next best move is combining quantum computing and
machine learning, which is like outfitting a bird with jet engines. While the
wings of classical machine learning allow for graceful glides and
noteworthy maneuvers, the quantum boost can propel it to speeds and
heights previously unimaginable. It's about harnessing the best of both
worlds in data processing.
Highlighting Benefits and Features of QML
As we continue soaring higher into the quantum skies, the panoramic view
below presents many possibilities and wonders. Just as the bird's eye view
provides insights unavailable from the ground, quantum machine learning
offers advantages that are beyond the reach of classical machine learning.
Let's unpack the treasures this high-altitude flight brings.
Enhanced Computational Efficiency
One of the standout promises of quantum computing is the potential for
significant computational speedups. This means quicker training times for
models and faster solutions to problems.
Handling High-Dimensional Data
Quantum systems can naturally represent high-dimensional vectors and
patterns. In machine learning, where data can often be multi-dimensional
(think of images, videos, or intricate datasets), quantum systems provide a
natural playground, making processes like data classification or clustering
more efficient. More on this later!
Quantum Data Encoding
Quantum computers have a unique way of encoding data, tapping into
quantum states. This encoding can capture more complex relationships in
data, potentially leading to more accurate machine learning models. It's like
having a supreme palette of colors to paint a picture, where nuances and
details are more vividly depicted.
Noise Resistance
Certain quantum algorithms display an inherent resistance to specific types
of noise, making them robust in real-world, noisy environments. In the
context of machine learning, this can lead to more resilient and reliable
models.
Quantum Parallelism
Quantum systems can explore multiple solutions concurrently, thanks to
their ability to be in a superposition of states. This means that for specific
problems, a quantum machine learning model can evaluate numerous
potential outcomes at once, streamlining the decision-making process. The
parallelism property is one of the key reasons why quantum computers
might achieve supremacy, but that's another book.
Enhanced Data Privacy
Quantum principles can be used to create protocols where data can be used
in computations without revealing the actual data. This is immensely
beneficial in sectors like healthcare, where data privacy is paramount, yet
the insights from data are crucial.
Tackling The Problems
With the union of quantum computing and machine learning, problems that
were deemed too computationally intense or impossible for classical
systems suddenly become possible. This opens the door to new discoveries
and breakthroughs in various fields, from material science to finance.
We realize that the possibilities with quantum machine learning are
limitless. The unique features and benefits it offers will reshape industries,
drive innovation, and challenge our very understanding of data processing.
Combining qubits and machine learning algorithms is more than a
technological marvel; it's the dawn of a new era in computation. As we
glide further into QML, we are not just spectators but active participants,
ready to harness the winds of change.
Quantum-Inspired Machine Learning Algorithms
"When we talk about what a quantum-inspired algorithm is, we're speaking of its structure, behavior,
and methodology rather than its physical appearance."
-Pantheon Space Academy
Imagine, for a moment, that you're trying to mimic the flight of birds (true
quantum algorithms) by using airplanes (classical algorithms). You observe
how birds take advantage of air currents, how they glide, and how they flap
their wings. Now, an airplane can't flap its wings. Still, engineers might
design its wings to change shape or angle (a quantum-inspired approach) to
gain some benefits birds have, without actually replicating proper bird
flight.
In computing, "quantum-inspired" algorithms play a similar role. While not
purely quantum, these algorithms are designed to imitate certain quantum
processes, allowing them to be run on classical computers while still
tapping into the advantages of quantum mechanics. They blend the familiar
classical computing methods we've grown accustomed to with the
aspirational processes only quantum mechanics can offer.
Why do we need this fusion? Pure quantum algorithms demand a full
quantum computing environment to run. As of now, large-scale, error-free
quantum computers are still on the horizon. However, the benefits of
quantum mechanics, such as superposition and entanglement, are too
tempting to wait for. That's where quantum-inspired algorithms come into
play. They provide a way for researchers and developers to start applying
some of the quantum advantages in our current classical computing
platforms.
This middle-ground approach has given birth to many innovative
algorithms, each offering unique solutions and abilities. As we soar further
into this chapter, we'll look over some of the most well-known quantum-
inspired machine learning algorithms, discover how they work, and witness
their real-world applications. We have reached our cruising altitude and will
now merge the known with the unknown.
Dynamics of Machine Learning Algorithms
Imagine the algorithms as birds, each with their distinct way of soaring
through the quantum skies. All birds share characteristics that enable them
to fly – wings, feathers, and a lightweight skeleton. Similarly, most
quantum machine learning algorithms share basic principles that grant them
their quantum advantage.
At the heart of these algorithms lie qubits, the quantum equivalent of
classical bits but with a twist. Unlike classical bits, which are either in a
state of 0 or 1, qubits can exist in a superposition. In this state, they're both
0 and 1. This ability exponentially amplifies the amount of information a
qubit can process.
Now, when multiple qubits are involved, they can become entangled,
another trait of quantum mechanics. In an entangled state, the information
of one qubit is dependent on another, no matter the distance between them.
This interconnectedness allows quantum algorithms to explore multiple
solutions simultaneously, drastically reducing problem-solving time.
Having these core principles in mind sets the stage for understanding the
nuances and specialties of each quantum machine learning algorithm. As we
explore each one, remember the qubits' superposition and entanglement
abilities as their wings and feathers, facilitating their flights through
complex computational landscapes. What differentiates one algorithm from
another is comparable to the different flight patterns and behaviors of
distinct bird species.
With this foundational knowledge in place, let's begin to discover the
unique traits and applications of the core quantum machine learning
algorithms shaping our technological future.
Quantum Support Vector Machines (QSVMs)
At the heart of machine learning, the primary goal is often to find patterns
or make classifications. In classical systems, Support Vector Machines
(SVMs) have been the go-to for this. They operate by finding a hyperplane,
a boundary, that best separates the data into distinct classes. Now, enter the
quantum realm, where Quantum Support Vector Machines (QSVMs) take
flight.
Distinct Quantum Touch
Traditional SVMs scan through data linearly, one point at a time. QSVMs,
however, leverage the power of quantum parallelism. This allows them to
process vast swaths of data simultaneously. Moreover, QSVMs can utilize
quantum gates, particularly those designed for phase estimation and
amplitude amplification, to probe data points and find their relationships in
fewer steps.
Exploiting Quantum Features
In a classical SVM, the feature space (a kind of landscape where data points
are mapped based on their characteristics) can get quite complex, especially
for intricate data sets. Quantum systems naturally live in high-dimensional
spaces due to their superposition property. QSVMs exploit this, allowing
for an elegant representation of complex feature spaces, which, in turn,
simplifies the task of finding the optimal separating hyperplane.
Applications & Strengths
Where does the mastery of QSVMs truly shine? They are particularly suited
for tasks where the data is high-dimensional, or the relationship between
data points is non-linear. Think of complex fields like bioinformatics, where
researchers grapple with massive genetic datasets, or financial markets,
where the combination of numerous variables makes predictions
notoriously challenging. QSVMs can cut through this complexity, offering
insights that elude classical algorithms.
Quantum Support Vector Machines don't just "mimic" their classical
counterparts; they elevate the game. By tapping into the properties of
quantum mechanics, they offer a fresh approach to age-old classification
problems, proving that the quantum skies have a lot of uncharted territory
worth exploring.
Quantum Neural Networks (QNNs)
One of the most exciting formations we encounter in our exploration of the
quantum skies is the Quantum Neural Network (QNN). Just as birds form
complex patterns in the sky, relying on each individual's movements and the
overall flock's dynamics, neural networks—both classical and quantum—
rely on intricate structures and interactions to process information.
Unique Quantum Neurons
At the core of any neural network are nodes or neurons. In QNNs, these
neurons are the qubits. When these qubits interact, typically through
quantum gates like the controlled-NOT gate or the Toffoli gate, they create
a web of entangled states. This entanglement lets the quantum neurons
"speak" to each other in a profoundly interconnected manner, which isn't
possible classically.
Training & Adaptation
Training a neural network involves adjusting weights and biases to
minimize errors. In QNNs, this process is amplified. Quantum parallelism
allows QNNs to evaluate many possible configurations at once. Quantum
Phase Estimation, a central algorithm in quantum computing, can also be
employed to find the optimal weights with remarkable efficiency.
Applications
Quantum Neural Networks are incredibly potent when dealing with
massive, intertwined datasets. Consider, for instance, the field of drug
discovery. Here, molecules, with their infinite combinations of atoms and
bonds, create complex structures. QNNs can simulate these molecular
interactions at a depth that classical neural networks find challenging.
Similarly, in areas like image recognition, where subtle patterns and details
matter, QNNs can bring refinement to what classical methods miss.
It's clear that Quantum Neural Networks are not just a superficial translation
of their classical counterparts into the quantum domain. They are a
reimagining, leveraging the unique strengths of quantum mechanics to
process information in novel, more profound ways. As we soar through the
quantum expanse, viewing the beauty of QNNs, with their intricate patterns
and deep connections, is a testament to the potential that awaits us in the
quantum age.
Quantum Clustering
As we continue our flight, a new pattern emerges below, reminding me of
the way birds gather at watering holes or form colonies. These gatherings
aren't random; they have an order and rhythm to them. Similarly, clustering
or grouping similar data points together is a pivotal task in the world of
data. Quantum clustering is a shining example of how quantum principles
can be harnessed to execute this task with unprecedented finesse.
Quantum Distance Metrics
At the heart of clustering lies the concept of distance. How far or close are
data points from one another? Quantum clustering introduces a new
perspective on this. Instead of classical distances, it employs quantum
states' overlaps. We utilize quantum interference patterns to determine
closeness. This allows for capturing subtleties in data relationships that
might be missed in classical contexts.
Quantum Amplitude Amplification
One of the star players in quantum clustering is Quantum Amplitude
Amplification. This technique bolsters the probability of finding a sought-
after quantum state. In the context of clustering, it aids in honing in on
clusters efficiently, making the process faster and more accurate.
Applications
The beauty of quantum clustering lies in its adaptability. It is especially
suited for high-dimensional data, where classical methods fall short due to
the 'curse of dimensionality.' Consider genomics, where data points (genes
or gene sequences) reside in high-dimensional spaces. Quantum clustering
can identify patterns and groupings in such data more effectively. Another
area of promise is finance, where stock behaviors or trading patterns across
global markets can be clustered to show insights that remain obscured in
purely classical analysis.
In the grand spectacle of our quantum flight, quantum clustering shines as
an ideal of precision and depth. By reinterpreting the fundamental ideas of
closeness and groupings through a quantum lens, it offers a fresh viewpoint,
allowing us to discern patterns and connections that were once hidden in
plain sight.
Advanced Quantum Algorithms
The beauty of exploration lies in the ever-present potential, always
promising new areas to discover. As we journeyed through the quantum
skies, we've encountered some algorithms that blend quantum mechanics
with machine learning. However, like seasoned explorers hungry for more
profound adventures, we now turn our attention to the more intricate
algorithms that lie ahead.
Advanced quantum algorithms, often drawing inspiration from multiple
disciplines and quantum principles, represent the frontier of quantum
machine learning. Unlike the algorithms we've touched upon before, which
primarily brought quantum benefits to traditional ML models, these
advanced algorithms go a step further. They redefine the foundational
processes of machine learning from a quantum mindset, making them
uniquely tailored for the quantum domain. Think of them as specially
crafted aircraft, designed not just for regular flight but for complex
aerobatic maneuvers, extreme altitudes, and challenging atmospheric
conditions.
But why this added complexity? The world of data, in its vastness, presents
challenges that often push classical algorithms to their limits. These
advanced quantum algorithms, born out of extensive research and
innovation, are designed to tackle such complexities head-on, offering
solutions that were once out of reach. Their design, sophistication, and
methodologies make them stand out, capable of reshaping our
understanding of data and prediction.
As we navigate through these advanced algorithms, keep your aviator
goggles on. We're about to experience some high-altitude quantum
aerobatics that promise to change our perspective on machine learning's
potential!
Flying High on High-Dimensional Data
As we search the vast quantum skies, we encounter intriguing cloud
formations. Some are wispy and scattered, while others are dense and multi-
layered. In the world of data, these "clouds" are comparable to the different
dimensions or layers of information we handle. Quantum Kernel Methods
(QKMs) are the advanced navigation tools that help us easily and precisely
maneuver through these stunning data clouds.
The Foundation of QKMs
At its heart, a kernel method is a machine learning technique to transform
data to be more easily processed and understood. Think of it as adjusting
the scope from which you view a cloud formation, making it simpler to
discern its structure. In classical machine learning, kernel methods take
non-linearly separable data and transform it into a higher dimension where
it becomes linearly separable, like finding the right angle to view our cloud.
But quantum kernels? They introduce an entirely new level of
transformation. By employing the principles of quantum mechanics, QKMs
can handle high-dimensional data much more efficiently than their classical
counterparts. They leverage the power of superposition and entanglement to
process information in ways that classical systems simply can't.
Distinguishing QKMs from Classical Methods
While classical kernel methods rely on mathematical functions to perform
these transformations, QKMs use quantum gates and operations. By doing
so, they can represent and process enormous amounts of high-dimensional
data on qubits. This doesn't just mean they're faster; it means they can
handle types of data and complexities that are out of reach for classical
methods.
Another standout feature? Their resilience. Even when faced with noise—
an inherent challenge in today's quantum processors—QKMs maintain their
edge. They're designed to function during turbulence, continuing to process
high-dimensional data seamlessly.
Practical Significance of QKMs
So, why does all of this matter? As our world becomes more data-driven,
the complexity and depth of the data we handle grows. The applications are
endless, whether it's analyzing genetic information, predicting climate
changes, or understanding financial markets. QKMs offer a way to process
this data efficiently and accurately. For instance, in the medical field,
QKMs are being explored for their potential to analyze genetic data to
identify disease markers, providing insights that were previously out of our
reach.
Quantum Flight with QKMs
In our quantum journey, QKMs are like the advanced navigational systems
that allow us to explore cloud formations that were previously untouchable.
As we make progress in the quantum domain, these methods will continue
to be invaluable tools, helping us unlock the mysteries of the data landscape
and ensuring our flight through the quantum skies is enlightening and
impactful.
A Grand Unification, QSVT
In our exploration, we've witnessed quantum mechanics and machine
learning come together harmoniously, each elevating the other to new
heights. Yet, as we continue to soar, specific moments and techniques stand
out like dazzling constellations in the expanse of the night sky. One such
stellar phenomenon is the Quantum Singular Value Transformation
(QSVT).
The kingdom of quantum computing is vast, but QSVT holds a special
place, like a pilot steering us toward more efficient data processing in
machine learning. At its core, QSVT is a quantum algorithm, a beneficiary
to classical singular value techniques, which have been used to decode
complex datasets by breaking them down into simpler, digestible
components.
The foundational principle of QSVT is rooted deeply in quantum
mechanics. It uses both superposition and entanglement to perform
computations in ways that classical computers can only dream of. QSVT
leverages quantum states' raw, unharnessed power to process information,
transform it, and derive value from it with unprecedented precision.
Delving deeper into its mechanics, QSVT often involves specific quantum
gates and circuits that aid these transformative processes. While classical
singular value techniques rely on linear algebra and matrix decompositions
to find patterns in data, QSVT employs quantum gates that can perform
these tasks at exponentially faster rates. This speed comes with an added
layer of depth in understanding the data, thanks to the probabilistic nature
of quantum mechanics.
What makes QSVT truly remarkable, though, is its adaptability. It's like the
agile jet in our flight analogy, capable of intricate maneuvers that big
commercial planes can't achieve. QSVT excels in handling dense, multi-
dimensional data, unraveling its complexities in ways that classical methods
fall short of. This capability becomes a blessing, especially when working
with massive datasets that have hidden patterns waiting to be uncovered.
As we talk about real-world scenarios, QSVT has shown immense promise
in areas ranging from finance, where understanding data patterns can
predict market movements, to healthcare, where analyzing complex
biological data can lead to groundbreaking medical discoveries. The
flexibility and precision of QSVT make it a frontrunner in the race toward
quantum dominance in machine learning.
In the future skies of quantum machine learning, QSVT shines brightly,
reminding us of the untapped potential that awaits. Just as pilots rely on
constellations for navigation, QSVT serves as a guide, pointing towards a
tomorrow where quantum processes and machine learning unite to redefine
what's possible. As we continue our flight, with QSVT illuminating our
path, the horizon seems filled with endless possibilities, each more exciting
than the last.
TensorFlow Quantum
Bridging Quantum and Classical World
In the computing cosmos, a beacon of hope and innovation arises:
TensorFlow Quantum. Originating from Google, this remarkable platform
was designed in collaboration with the quantum computing company, X,
and is tailored explicitly for the rapidly growing field of quantum machine
learning. At its heart, TensorFlow Quantum is more than just another
computational tool; it's the connective tissue that fuses the unbounded
potential of quantum computing with the established skills of classical
machine learning.
Imagine our quantum analogy of flying through the skies, navigating
through clouds of complicated algorithms and data landscapes. In such an
adventure, TensorFlow Quantum is our cockpit control system, expertly
guiding our flight through familiar terrains and uncharted quantum
domains. While quantum computing offers the raw power of qubits,
superposition, and entanglement, classical systems present a well-
established infrastructure, giant data sets, and matured algorithms.
TensorFlow Quantum brilliantly bridges these worlds, ensuring a
harmonious coexistence.
What truly sets TensorFlow Quantum apart is its unique features tailored for
quantum complexity. This platform provides a holistic environment where
quantum datasets, quantum models, and classical neural networks interact
seamlessly. It uses the quantum phenomena to optimize machine learning
tasks, delivering results that classical systems would find respectable,
especially when dealing with quantum-specific challenges.
In the ever-evolving landscape of research and development, TensorFlow
Quantum has carved a niche for itself. It's a tool and a leader for
collaborations, fostering partnerships between tech giants, academic
researchers, and quantum startups. These synergies have birthed studies and
breakthroughs, accelerating our understanding and application of quantum
machine learning.
In the grand narrative of our quantum quest, TensorFlow Quantum emerges
as an essential co-pilot. It doesn't just help us soar; it ensures we fly with
precision, power, and purpose. As we continue our journey, with the horizon
filled with promises of quantum breakthroughs, TensorFlow Quantum helps
us navigate these skies with unmatched elegance.
Comparative Advantages of Quantum Algorithms
Quantum mechanics offers a unique perspective, an advantage not just of
altitude but of possibilities. Similarly, quantum-inspired machine learning
algorithms present capabilities that transcend those of classical systems. As
we gaze down from our lofty quantum viewpoint, it becomes clear why
these quantum algorithms are the rising stars in the computational cosmos.
High-Dimensional Data Handling
In our journey, consider each data point as a star. In classical computing, the
more stars (or data points), the more challenging the navigation. Quantum
algorithms, however, thrive in such high-dimensional spaces. Thanks to the
principle of superposition, qubits can digest large amounts of information
simultaneously. This ability is like seeing multiple flight paths at once and
choosing the best one in real time.
Enhanced Processing
If superposition is about possibilities, entanglement is about connections.
Imagine two aircraft communicating instantaneously, regardless of the
distance between them. This quantum sensation allows quantum algorithms
to process interconnected data more intuitively than classical algorithms
that view data points in isolation.
Optimized Solutions
We continue our flight analogy; interference can be likened to a plane's
aerodynamic adjustments to avoid turbulent air and find smoother
pathways. Quantum algorithms use interference to sift through and
eliminate incorrect solutions, focusing only on the optimal ones, making
them experts at solving complex optimization problems.
Scalability and Future Readiness
While today's quantum computers are in their infancy, quantum algorithms
are already designed for scalability. As quantum hardware evolves, these
algorithms will smoothly integrate, harnessing greater power and solving
even more complex problems. It's like creating an aircraft today that's ready
to navigate the more advanced air traffic systems of tomorrow.
Natural Compatibility with Quantum Mechanics
Some tasks inherently possess quantum properties, like simulating quantum
systems in pharmaceutical research or material science. Quantum
algorithms are naturally suited for these tasks, eliminating the need for
approximations that classical algorithms often resort to.
Exponential Speedups for Specific Problems
Certain problems that would take classical computers ages to solve can be
addressed in mere seconds with quantum algorithms. This dramatic speed
advantage isn't universal but applies to specific problems, offering
breakthrough possibilities in cryptography, optimization, and simulations.
In the broad geography of computation, while classical algorithms have
paved the way, quantum algorithms promise uncharted territories and
heights yet to be explored. As we soar with quantum machine learning,
these advantages form the powerful tailwinds, propelling us faster and
farther into the future of computation.
Navigating Quantum Patents
The Emerging Economy of QML
In our voyage through the vast quantum skies, patents shine brightly, much
like guiding stars illuminating our path. Picture this: seasoned aviators
steering their aircraft, not just by instruments but also by the constellations
stretching endlessly above. In quantum machine learning, these patents
offer the same kind of guidance, the reassurance of a well-lit path in an
otherwise overwhelming sky.
In the constantly evolving economy of technology, quantum machine
learning promises to deliver massive innovations. This growing field has
given rise to a distinct economic market, indicated by a surge in patents
tailored to quantum machine learning technologies. These aren't just
tiresome paperwork; each patent represents a step forward, a novel idea that
promises to reshape industries and redefine what's achievable. From the
new algorithms designed for modern warfare's cybersecurity applications to
the sophisticated encryption schemes strengthening financial transactions,
these patents underline the merger of quantum mechanics and
computational intelligence.
Furthermore, one must recognize these patents' transformative potential in
the medical world, particularly in drug discovery. The ability to model
chemical interactions at a quantum level opens doors to unique precision,
promising treatments, and medicines that are both effective and
personalized. Yet, what truly underscores the magnitude of this patent-
driven revolution is its trajectory. When one observes the issued patents, the
exponential growth of the quantum machine learning patent market
distinctly mirrors that of the broader machine learning hype from its early
days. This parallel hints at the dominant role quantum technologies are
poised to play in the coming years. This signals an upcoming shift in the
way we harness computational power.
How do you navigate the patents? A field as cutting-edge as quantum
machine learning demands a functional approach. When sifting through the
sea of information, it's essential to have a method to identify innovation.
For this exploration, a sweeping data collection was conducted using the
United States Patent and Trademark Office (USPTO) database, a
comprehensive resource for searching patents. I focused on select terms at
the heart of quantum machine learning: "Quantum AND Machine
Learning", "Quantum AND Markov", "Quantum AND Boltzmann", and
"Quantum AND Neural Networks." By targeting these specific
intersections, the intent was to cast a wide net that captures quantum
innovations in reinforcement learning, deep learning, and their advanced
combinations.
The findings from this targeted search offer an enlightening snapshot of the
quantum machine learning economy. Not only do they reveal the sheer
volume and diversity of patents in the arena, but they also emphasize the
rapid pace of innovation. Each term, representing a distinct aspect of
quantum computing or machine learning, shows how researchers and
innovators are pushing the boundaries, seeking to merge quantum principles
with our modern world in unprecedented ways. The patents that emerged
hint at applications and solutions set to transform industries across the
board.
Patent Success in QML
From the many quantum innovations, one particular company, Zapata
Computing, streaks across the sky with a trail of patents, showcasing the
real-world implications of quantum machine learning. One of their
groundbreaking patents pertains to the Variational Quantum Factoring
(VQF) and Quantum-Assisted Defense Against Adversarial AI (QDAI)
algorithms. While these technical titles might seem obscure, their
implications in our digital world are both profound and transformative.
Zapata's QDAI algorithm is a sentinel against hostile attacks on Machine
Learning (ML) models. For those soaring in the digital domain, adversarial
attacks are like unpredictable air pockets that can disrupt a flight's stability.
They introduce tiny yet intentionally crafted variations to data, confusing
machine learning classifiers and leading them astray. As if navigating this
turbulence wasn't challenging enough, quantum computers introduce
another layer of complexity, giving a unique quantum noise that further
misguides these models. But here's where Zapata's brilliance shines. Their
patented QDAI is trained to recognize and counter these quantum noises,
acting like a pilot skillfully steering clear of turbulence, ensuring that ML
models deliver accurate results even in the face of these quantum
disruptions.
Beyond just the security implications, this technology highlights the
enormous potential of QML. As cyber threats evolve, the ability to use
quantum mechanics to bolster our defense mechanisms becomes priceless.
It's not just about protecting data but certifying that the very algorithms we
trust to process and interpret it remain uncompromised. With innovations
like those from Zapata Computing, our journey through the quantum skies
becomes thrilling and secure, holding promise for a future where quantum
and classical harmoniously coexist.
The quantum horizon continues to glow with promise, and as we steer from
the achievements of Zapata Computing, another star in our quantum sky
emerges: D-Wave Systems. With over two decades dedicated to pioneering
quantum computing technologies, D-Wave has established itself as a key
player in the quantum domain. Their trajectory in the quantum sky has been
nothing short of meteoric, and they've marked their territory with an array
of patents and groundbreaking innovations.
D-Wave's collaboration with industrial titans such as Volkswagen and
NASA exemplifies their commitment to practical quantum solutions.
Imagine a quantum navigator that can find the most efficient route within
the chaotic traffic of data – that's the sort of optimization problems D-
Wave's technologies tackle. Their quantum solutions offer a distinct
advantage, carving pathways through computational challenges where
classical systems would find themselves deprived.
Moreover, D-Wave's endeavors continue beyond optimizing existing
processes. They've ventured into designing specialized chips tailored for
machine learning tasks. These chips act like powerful engines in the infinite
data clouds we navigate, applying the raw quantum energy to process and
understand data at speeds and depths previously viewed as untouchable.
Through partnerships, inventions, and consistent innovation, D-Wave
Systems ensures our quantum aspirations aren't just about understanding the
skies but also about leveraging their vast potential. They exemplify the very
essence of quantum machine learning, demonstrating that the fusion of
quantum mechanics and computational intelligence is not just theoretical
but practical and transformative.
Let's fly from the impressive achievements of D-Wave Systems to the
luminous trail of BTQ Technologies Inc., another trailblazer making its
mark in the quantum domain. As I sorted through hundreds of quantum
innovations, BTQ emerged as a sentinel, safeguarding the digital expanse.
With data breaches becoming increasingly prevalent and the looming
shadow of quantum computers threatening to break traditional
cryptographic methods, the need for unbreakable encryption has never been
greater. Enter BTQ and their game-changing, patented quantum-safe
encryption technology. Just as pilots need foolproof navigational tools to
traverse challenging weather conditions, industries ranging from finance to
healthcare desperately require unbreachable security measures to transmit
sensitive data safely.
BTQ's innovation is not just about encryption but a holistic data security
approach. The patented process involves generating private cryptographic
keys from a shared random vector, ensuring maximum privacy in data
transmission. I'm impressed by the versatility of their invention to adapt to
various mathematical functions, ensuring it remains resilient against
potential breaches. They've created a strong barrier called a cryptogram,
where confidential data is compressed and combined with random data. It
perfectly blends data and randomness, ensuring a robust and unyielding
security layer.
This encryption method is like having a quantum cape, making sensitive
data indistinguishable from the randomness, thwarting any potential
meddlers in their tracks. As Olivier Roussy Newton, the CEO of BTQ,
sharply puts it, their patented technology is more than just a cryptographic
method; it's a revolution in data security. As the quantum domain grows,
with companies like BTQ leading the way, the digital future looks
promising and secure.
Advantages and Challenges of Quantum Machine Learning
"The best way to predict the future is to invent it."
- Alan Kay
As we soar higher into the quantum skies, the expansive view below reveals
the vivid patterns of data stretching out like cloud formations. These
patterns, reminiscent of the datasets we encounter in computational
landscapes, often pose challenges for classical machines, but for our
quantum systems, they signify newfound pockets of potential.
Improved Computational Efficiency
Merging the power of quantum mechanics and quantum machine learning
(QML) brings a transformative change in computational efficiency. Where
classical computers laboriously sift through data, quantum computers
process information in parallel with their superposition and entanglement.
Imagine flying through a dense cloud cover, where each cloud represents a
data point. A classical approach would entail navigating each cloud one by
one. In contrast, a quantum system, much like a bird's panoramic view,
would instantly perceive and understand the entire formation.
Big Data Analytics
Our digital age is marked by the proliferation of data. Every aspect of our
lives generates data, from social media interactions to financial transactions
and scientific experiments. This surge in data, often termed 'Big Data',
poses challenges for classical analytics tools, which can get overwhelmed
by the absolute volume and complexity. Enter quantum computing. Its
inherent parallelism and capacity to represent huge amounts of information
on individual qubits grant it a distinct advantage. Like a high-tech radar
system, quantum analytics can swiftly scan, interpret, and draw patterns
from these immense data landscapes. Whether it's predicting stock market
trends from millions of transactions or decoding human genome sequences,
quantum's edge is undeniable. In our quantum journey, the sky is not the
limit; it's just the beginning.
As we continue our flight, it's crucial to appreciate these capabilities, for
they set the stage for the quantum optimism in machine learning. But like
any journey, challenges await; understanding them is as essential as
recognizing the advantages. Our next phase goes into the specific quantum
techniques that are shaping this digital world.
Simplifying Complex Data
Imagine standing at the edge of an intricate maze, looking at countless
paths, each representing a piece of data. Traditionally, navigating this maze
to find relationships between data points would be a monumental challenge.
Enter vector embeddings, the unsung heroes of the machine learning world.
At their core, vector embeddings are mathematical tools that transform
high-dimensional data—like our vast maze—into more manageable, lower-
dimensional representations. Think of it as viewing the maze from above,
gaining a clearer perspective of the paths and their relationships.
My excitement for vector embeddings, especially in the quantum domain, is
hardly containable. The reason? BIG data. We live in an era where data is
exploding. Each day, quintillions of bytes of data are generated. While
we've made strides with classical embeddings, there are still inefficiencies.
Quantum-enhanced embeddings promise not just improvements but
extreme changes.
Looking into the technical heart of vector embeddings, traditionally, they've
been used in machine learning to capture the essence of data points in
lower-dimensional spaces without losing much information. For instance, in
natural language processing, words are transformed into vectors so that
words with similar meanings are close in this vector space. This closeness is
not about the words but their context, semantics, and intricate relationships.
Now, sprinkle a bit of quantum magic onto this. Quantum-enhanced vector
embeddings can leverage the principles of superposition and entanglement.
While classical embeddings might represent data in, let's say, 300
dimensions, quantum embeddings can represent the same data more
compactly, but with more detail. The state of a qubit provides a unique
advantage in capturing complex data structures.
Moreover, when I talk about BIG data, I'm referring to datasets so vast that
classical computers struggle to process them efficiently. With quantum
embeddings, the potential to analyze and interpret this data is amplified, not
just by handling more data, but by understanding it at a deeper, more
interconnected level.
In the grand narrative of quantum machine learning, vector embeddings
might not always take center stage or receive any attention. Still, their role
is similar to the backstage crew in a play—making sure everything runs
seamlessly. As we move into the quantum era, it's tools like these that will
drive efficiency, precision, and depth in our understanding of the data
landscapes we navigate.
A Stepping Stone for Advanced Quantum Computation
At its core, a quantum walk is a quantum analog of the classical random
walk. However, while classical random walks are fundamentally random
processes, quantum walks are driven by deterministic unitary evolution,
enabling them to harness quantum interference effects.
Quantum walks can be categorized into discrete-time and continuous-time
walks. In discrete-time quantum walks, a "coin operation" decides the
direction of the walk, followed by a conditional shift operation based on the
coin's state. Continuous-time quantum walks, on the other hand, don't rely
on a coin; instead, they evolve directly on the underlying graph or structure
via the Hamiltonian of the system.
From a computational perspective, quantum walks are groundbreaking
because they can process information in ways classical algorithms can't.
The interference patterns inherent to quantum walks allow for faster
solutions to classically hard problems, like the element distinctness problem
or searching in structured databases.
A notable attribute of quantum walks is their capacity for universal quantum
computation without time-dependent control. This means that by setting up
the right initial conditions and letting the system evolve, the quantum walk
can simulate any quantum operation, making it a dynamic tool in the
toolbox of quantum algorithm designers.
In practical applications, this allows quantum computers to perform
complex computations with fewer resources and less intervention, making
quantum walks an interesting quantum phenomenon and a pivotal technique
that propels the quantum computing field further.
Mastering the Quantum Data Flow
In our quantum journey, imagine data as our primary cargo, with some
items (data) designed for the quantum skies, while others belong to the
classical realm. Just as specific tools ensure that the cargo is suitable for
various phases of flight, in the quantum computing world, these tools are
embodied by TensorFlow Quantum, quantization, and dequantization.
TensorFlow Quantum
This is the main control panel in our aircraft, ensuring everything runs
smoothly. Developed by Google in collaboration with quantum experts, it's
an open-source tool that allows classical neural networks and quantum
circuits to interact seamlessly. If our flight involves hopping between
classical and quantum skies, TensorFlow Quantum ensures we do so
without turbulence.
Quantization
Before taking off into the quantum realm, we need to make sure our
classical data (or cargo) is properly packaged or encoded for quantum
processing. Quantization is this packaging process. It converts classical data
into a format (quantum states) suitable for quantum circuits. Think of it as
adjusting the weight and size of cargo to guarantee it fits perfectly in our
quantum aircraft.
Dequantization
Once our quantum flight concludes and we need to land back in the
classical world, our quantum-formatted data has to be translated back into a
format that classical computers understand. Dequantization is this
translation process, converting quantum results back into classical data. It's
like unpacking and preparing our cargo for its next destination in the
classical realm.
Both quantization and dequantization processes have intricacies, depending
on the nature of the data and the specific quantum operations applied. They
play a critical role in providing an efficient transition between classical and
quantum domains, with minimal data loss. By mastering TensorFlow
Quantum and the processes of quantization and dequantization, we establish
that our leaps between the classical and quantum skies are smooth and
optimized for the best results.
Navigating Through Quantum Turbulence
As we soar through the quantum skies, it's not always smooth sailing. Just
as pilots must encounter atmospheric disturbances, we must address the
challenges of Quantum Machine Learning (QML).
Qubit Noise, Error Correction, and Decoherence
Remember the delicate balance required in quantum physics and
architecture? In QML, this balance is just as pressing. Qubit noise refers to
the unforeseen disturbances that might affect our quantum data. Think of it
as gusts of wind that can throw our plane slightly off course. While qubits
have the power to process information in ways classical bits can't, they are
sensitive and can be influenced by their surroundings.
Decoherence, meanwhile, is like the fading strength of a radio signal over a
long flight. Over time, qubits can lose their quantum properties if not
properly maintained. Just as pilots use instruments to counteract the effects
of turbulence and maintain a plane's altitude, in QML, we use error
correction techniques to make sure our quantum data remains accurate,
even in the face of these disturbances.
Large-Scale Quantum Computers
Similar to the first-generation aircraft, current quantum computers are
remarkable engineering feats but have limitations. They're still smaller than
we'd like and error-prone for some complex machine learning tasks. We're
still in the early days, refining our technology and verifying that our
quantum planes can handle longer, more intricate expeditions.
Trade-offs and Comparisons
Every choice in aviation comes with a trade-off. The same holds for QML.
While quantum algorithms can handle tasks that classical ones find
impossible, they require specialized hardware and are still being refined.
Classical machine learning, like well-established commercial airlines, is
mature, with infrastructure and years of optimization. On the other hand,
Quantum machine learning is like an emerging breed of aircraft – faster and
more powerful, but still being tested for all terrains and weather conditions.
Weighing QML against its classical counterpart is about understanding
when to deploy each for maximum efficiency and impact.
Learning From Mistakes
It's essential to acknowledge and navigate these challenges. Recognizing
them allows us to innovate and refine, assisting in our continued progress in
the skies of Quantum Machine Learning.
Looking Forward
Charting New Territories in the Quantum Skies
As we reach the concluding leg of our journey through the chapter, it's great
to understand where we currently soar and anticipate the horizons waiting
to be explored. The present landscape of quantum machine learning is like
dawn breaking over an expansive sky, casting the first light on regions with
potential.
The current state of QML is marked by an energetic blend of academics,
industry innovations, and a global community of researchers passionately
pushing boundaries. Pilot programs, early-stage quantum computers, and
budding algorithms are like the initial test flights—each one providing
impressive data, refining our approach, and ensuring the skies are safer and
more efficient for following journeys. Collaborations between tech giants,
quantum startups, and academic institutions fuel this momentum, fostering
an ecosystem of rapid learning and adaptation.
With advancements in quantum hardware and deeper insights into quantum
algorithms, we're on the brink of a digital shift that some have labeled as
The Forth Revolution. The coming decade promises QML models that can
decode the most intricate patterns, from climate modeling to space
exploration. As quantum computing power strengthens, we can anticipate
breakthroughs that were the stuff of science fiction just a few years ago.
Imagine machine learning models that can predict and understand climate
changes with unprecedented accuracy, or healthcare algorithms that can
model and analyze every single molecular interaction in real-time,
potentially unlocking cures for the world's most challenging ailments.
Before we close the cabin doors on this chapter, let's make a pit stop back at
Machine Learning (ML) Central. Think of it like that end-of-trip luggage
check – 'Did I pack my socks? My charger? My understanding of ML?' It's
more than just algorithms and computations; it's the unsung hero of the
digital empire. Final call for Machine Learning (ML) in simple terms:
Definition: A subset of AI. It's a method of data analysis that
automates analytical model building. ML allows computers to
learn from data without being explicitly programmed for that
task.
Scope: Specifically deals with the development of algorithms
that can learn from and make predictions or decisions based
on data.
Goal: The primary purpose is to enable machines to learn
from data so that they can give accurate predictions and
decisions.
Learning: ML learns from data; hence, it adjusts its
predictions and decisions as it receives more data.
Types: Can be categorized based on how learning is achieved,
such as supervised learning, unsupervised learning, and
reinforcement learning.
While this chapter concludes, the quantum quest is far from over. As we
segue into considerations of ethics in our upcoming explorations, it's
paramount to approach the future of QML with a blend of optimism,
responsibility, and a relentless search for knowledge. The quantum future is
filled with potential and promise, and the path ahead will be as stimulating
as the chapters we've traversed together.
Chapter 9
Ethics and Social Implications
"Ethics is knowing the difference between what you have a right to do and what is right to do."
– Potter Stewart
In the rapidly evolving world of quantum computing, this profound
statement by Justice Stewart resonates more than ever. As we stand at the
dawn of a new technological era, this chapter discusses the ethical maze
surrounding quantum advancements.
The brilliance of quantum's potential is matched only by the weight of
responsibility it brings. Pioneering this frontier, I often grapple with
questions that extend beyond physical technology. Who truly bears the
mantle of moral and ethical stewardship? Is it the relentless scientists
pushing the boundaries of what we know? The corporations, with their vast
resources and influential reach? The governments entrusted with the
welfare of its citizens? Or is it us, the collective society, the end-users and
beneficiaries of this futuristic technology?
The fabric of responsibility is entangled among all these entities, creating a
complex puzzle that's challenging to solve. Having navigated the intricate
corridors of technology, business, and policy, I've realized that a singular,
definitive answer still needs to be reached. However, one conviction stands
unwavering: Ethical considerations are not mere add-ons to the quantum
narrative. They are its very soul, our "why", guiding its evolution at every
step.
While I stand firmly as a Quantum Computing-AI optimist, boundless
potential should be paired with thoughtful reflection. I encourage you, not
to blindly share in my enthusiasm but to bring your own critical thought and
inquiry to the table. As we navigate this groundbreaking era, it's essential
that we ask questions, demand responsibility, and participate actively in the
blending of this technology with our lives. Let's venture on this exploration
together, making room for wonder, analysis, and the ethical considerations
that will guide the next wave of innovation.
Ethical Implications of Quantum Computing
Quantum Computing and Cryptography
In today's digital age, the security of our online data is paramount.
Everything from our emails to our bank details and medical records is
protected using cryptography. It's like an intricate digital lock and key
system, where data is locked away, and only the correct key can unlock and
access it.
Traditional computers, even the fastest ones we have today, would need
thousands of years to try out all the possible keys and crack the encryption.
But here's the catch: Quantum computers, with their incredible processing
power, might one day be able to do this in a matter of hours or even
minutes.
The Balance
User Privacy vs. National Security
A leading quantum physicist, Dr. Alicia Cortez, once remarked, "The same
quantum technology that promises medical breakthroughs and ultra-
efficient logistical solutions also has the power to undermine the very fabric
of online security."
On one hand, quantum computing poses a potential threat to individual
privacy. If malicious actors get access to quantum capabilities before
protective measures are in place, everything we thought was secure online
could be at risk.
Yet, on the other side of the debate stands national security. Governments
argue that quantum capabilities could be a crucial tool in fighting crime,
terrorism, and other threats. The ability to decrypt communications quickly
could, in some cases, mean the difference between life and death.
We must recognize that the debate isn't black and white. Here are a few
thought-starters:
How do we strike a balance between individual privacy rights
and collective security needs?
Are there ethical considerations in potentially creating
quantum-proof encryptions, making it impossible for
governments to access data even when it's deemed necessary?
What mechanisms and checks should be in place to prevent
misuse, and who gets to decide?
The world of quantum computing is as much about these critical
philosophical and ethical questions as it is about the technology itself. The
choices we make in navigating these challenges will shape the digital
landscape of our quantum future.
As we give thought to quantum computing and cryptography, it becomes
apparent that the reach of this technology stretches far beyond just securing
our emails. The broader implications of quantum computing will impact
privacy, ownership, security, and fairness.
In an age where data has often been dubbed the "new oil," the right to
personal data privacy becomes paramount. Quantum computing could
break today's cryptographic standards, making what was once private
suddenly very public. The question arises: How do we ensure that the
quantum tools that promise to improve our lives in so many ways don't
simultaneously strip away our right to privacy?
Ownership in the quantum realm is complicated. At a superficial level,
who owns quantum technology? Is it large tech corporations, academia, or
governments? And as these entities gain the power to access exceptional
amounts of data, how do we draw the boundaries of digital ownership?
More philosophically, if quantum computing can simulate complex drug
interactions or design new materials, who owns these findings and the
subsequent real-world applications?
While the cryptographic concerns form the tip of the security iceberg,
there's much more lurking beneath the surface. Quantum computers could
boost the field of cybersecurity, both strengthening defenses and
empowering attackers. The arms race between hackers and protectors could
escalate to unforeseen levels, making our navigation of the digital realm a
constantly evolving challenge.
The start of any trailblazing technology invariably begs the question: Who
has access? If quantum computing remains in the hands of a select few—
nations or corporations—it could lead to vast power imbalances. The digital
divide could widen, with those harnessing quantum capabilities having a
significant advantage over those who don't. Fairness, in this context,
encompasses not just access to the technology but also the benefits it brings
and the potential consequences of its misuse.
As quantum technology matures, we find ourselves at an inflection point.
Dr. Sameer Rao, a tech ethicist, puts it succinctly: "Quantum computing
isn't just about faster processing speeds or groundbreaking algorithms. It's a
mirror reflecting our societal values, aspirations, and fears. In this mirror,
we must see not just the promise but also the responsibility."
In our rapidly digitizing world, trust is paramount. Quantum computing
stands at the nexus of this trust, poised with the power to process vast
swathes of information. However, it's crucial to remember that quantum
computers only know and interpret the data they're fed; they're not
conscious entities discerning truth from falsehood on their own.
This brings us to a pressing moral dilemma: In a world armed with quantum
capabilities, who becomes the arbiter of truth? Entrusting this immense
responsibility to a centralized authority can be dangerous. History teaches
us that such power, when unchecked, can suppress legitimate voices,
disguising censorship as a fight against "fake news". Yet, a passive
approach might leave us vulnerable to amplified disinformation campaigns,
further straining the bonds of societal trust.
The optimal path forward lies in championing transparency,
decentralization, and open-source principles. By distributing the power of
truth verification and ensuring that quantum systems' inner workings remain
accessible to all, we can encourage an environment where trust is built
collaboratively, not dictated from above. In this quantum-augmented future,
the question isn't just who should steer our collective narrative, but how we
all can play a part in ensuring its integrity.
Our exploration of the quantum domain is just beginning, and these ethical
concerns will shape its trajectory. Engaging with them proactively, rather
than reactively, could be the difference between a quantum future that
empowers humanity and one that challenges the very essence of a just
society.
Ethical Risks
Treading with Caution in a Quantum Future
While the possibilities of quantum computing seem limitless, it's mindful to
address the ethical challenges that emerge in its wake. The world is on the
threshold of a quantum leap, and with every leap comes the risk of a
stumble. Next are only a few concerns that experts, including insights from
Deloitte, have raised regarding the rapid climb of quantum technologies.
Job Displacement
Quantum computers, with their unmatched problem-solving ability, promise
efficiency levels currently unseen in modern computing. This capability
isn't merely about crunching numbers faster but potentially rendering whole
professions and industries redundant. Financial modeling, complex
simulations, intricate design work – fields that currently need significant
manpower might soon be within the control of a quantum machine working
in mere seconds. What becomes of the workforce in such scenarios? The
displacement isn't just a question of jobs but of identity and purpose for
countless individuals.
Corporate Monopolization
A pressing concern echoed by many is the possible monopolization of
quantum technologies by a select few corporations. With significant entry
barriers in terms of cost and expertise, quantum computing could become a
playground for the elite, resulting in the centralization of power and
influence. This concentration could restrain innovation, creating barriers for
startups and researchers who have groundbreaking ideas but need more
resources to bring them to market.
Uneven Global Advancement
The global landscape is dotted with disparities, and quantum computing
could make them worse. Developed nations and tech hubs, with their
financial might and technical know-how, might leap ahead, leaving others
in the quantum dust. This isn't merely about economic supremacy but about
disproportionately influencing global policies, regulations, and norms
surrounding quantum technology.
Misuse and Malintent
With great power comes great responsibility, and quantum computing is no
exception. In the hands of malicious actors, be it individuals, corporations,
or nations, quantum capabilities could be weaponized, leading to
unforeseen threats. The ethical dilemma here extends to developers and
regulators: How do we ensure responsible development and deployment of
quantum technologies?
Drawing from Deloitte's analysis, it becomes evident that the promise of
quantum computing is linked with ethical challenges. Their report
underscores a sentiment echoed by many in the tech community: it's not just
about what quantum computing can do, but what it should do.
Navigating the quantum future will require a spectrum of voices –
policymakers, ethicists, technologists, and the general public – to weave
together a narrative that celebrates innovation while ensuring it doesn't
come at the cost of our societal values and shared humanity.
Access For All
Quantum computing, with its potential to affect various sectors, also holds
the promise (or peril) of reshaping our societal structures. One of the
primary concerns as we transition into the quantum age is access. History
provides ample evidence that groundbreaking technologies, while
transformative, only sometimes distribute their benefits consistently. With
the onset of quantum computing, we find ourselves at another technological
crossroads.
Digital Divide Revisited
The introduction of the internet in the 90s brought with it hopes of
democratizing information. In many ways, it succeeded. However, it also
unveiled a 'digital divide', where some had access to these vast reservoirs of
knowledge, and others did not. Quantum computing presents a similar
scenario. Its computational capabilities can drive immense societal
progress, but who gets to wield this power? And more importantly, who
decides?
Economic Implications
The capital-intensive nature of quantum research and infrastructure means
that early access may be restricted to entities with significant resources.
This could result in an economic divide, where certain businesses or regions
that can't afford quantum technology are left behind, leading to potential
technological and economic power centralization.
Unintended Consequences
Like any technological revolution, the quantum leap is also riddled with
unintended consequences. For instance, if quantum technology remains
concentrated in the hands of a few, it might influence decision-making
processes at both micro and macro levels, from local policies to
international diplomatic relations. Moreover, rapid technological
advancements could result in skill redundancy, challenging the current job
market dynamics.
The journey into the quantum age offers many challenges and opportunities.
The question isn't just about how quantum computing will reshape our
world, but how we, as a global society, will ensure its benefits are reaped
widely and its challenges resolved thoughtfully.
Communication and Teaching
As the horizon of the quantum era emerges, we are presented with a
compelling question: how might the interconnectedness of our world
evolve? The start of the quantum internet could dissolve the very notions of
time zones and geographical barriers, making one wonder if the essence of
"distance" will soon be an out-of-date concept. Could we soon find
ourselves in a world where global collaborations and friendships become as
instantaneous as the blink of an eye?
Imagine quantum-driven virtual realities where the classroom extends far
beyond four walls. A student in Chicago might walk through the bustling
streets of ancient Rome, and another in Tokyo could engage with the
mysteries of deep-sea ecosystems. At the same time, a curious mind in
Cairo delves into the abstract wonders of quantum mechanics, all from the
comfort of their homes.
Yet, while we envision these awe-inspiring prospects, we must also
entertain some warning signs. As new quantum platforms reshape our social
interactions, offering emotion-sensitive virtual meetings or groundbreaking
social media experiences, will we unknowingly craft a divide between those
immersed in this new world and those left on the fringes? Will a child who
thrives in traditional learning environments feel lost in this sea of
technological marvels? And as we navigate this quantum-woven world,
might we risk losing the cherished nuances of face-to-face interactions?
The quantum age is not just a technical revolution; it's a profound shift in
our societal fabric. As we stand on the cusp of innovation, we're challenged
to ensure an ethical and vibrant future for all.
The Challenge of Explanation
Quantum computing often feels like a riddle wrapped in a riddle. Its
principles, stemming from the strange world of quantum mechanics, stand
in stark contrast to our everyday experiences. How does one articulate the
idea of superposition – where a quantum bit, or qubit, can exist in multiple
states simultaneously – to someone accustomed to a binary world of 0s and
1s?
The challenge to simplify quantum concepts can be as difficult as
describing the color blue to someone who's only ever seen in black and
white. Yet, this isn't just an academic exercise or a puzzle for the curious-
minded. The deeper issue lies in fostering trust and acceptance in a society
already wary of technologies they barely understand. If the intricacies of
today's algorithms leave many feeling in the dark, the seemingly
otherworldly nature of quantum operations might seem like an impenetrable
black box.
But this opacity poses a risk. When people don't understand something,
they're less likely to trust it. And without trust, how can society fully
embrace and integrate the potential wonders of quantum computing? For
quantum tech to flourish, there's an urgency not just to advance it, but to
make its complexities understandable, relatable, and ultimately, trustworthy
to the masses.
Quantum and Business
A New Paradigm
Bill Gates once said, "We're still in the early days of what these computers
can do." Now, think of quantum computing as the next big step after our
classical computers. Imagine a young entrepreneur, like today's tech-savvy
teenager, starting a business with the power of quantum computers. They
could do things like planning the best routes for delivery trucks in an instant
or finding the perfect song for you based on your mood and the weather
outside!
But it's not just the good guys who see opportunities. Where there's a
chance to make money or gain power, a few people always try to cheat the
system. What if scammers could use these super-fast computers to trick
people out of their hard-earned cash? One known scam is AI voices that
sound like loved ones and are panicked for your help. One unsuspecting
grandmother sent thousands of dollars to who she believed to be her
granddaughter in danger, to find out later she had been scammed.
Experts like Dr. Jane Smith from QuantumTech Institute often talk about
the "double-edged sword" of technology. On one side, in healthcare, we
could find cures for diseases faster or predict if someone might get sick in
the future. But, what if someone else could access your health data? Would
you be comfortable with strangers knowing so much about you?
In the field of money and banks, these computers might help predict which
stocks will go up or down. But could that same power also allow a few
people to control and manipulate the market unfairly? And when we talk
about robots or AIs making decisions, we have to ask: Who's in charge if
they make a mistake?
So, what do you think? As we step into this new world of business powered
by quantum computers, should we dive in headfirst or take a moment to
think about the right and wrong ways to use this power?
Remember, as we explore these new horizons, it's always a good idea to ask
questions and stay curious. After all, it's our future we're shaping!
Thought Experiment
Quantum-AI Optimism vs. Reality
Let's imagine for a moment a world flooded with Quantum-AI integration.
Through the lens of personal stories, let's glimpse the potential benefits and
pitfalls of quantum technology.
The Good: A Mother's Relief
It's 2035, and Jenna rushes her six-year-old daughter, Mia, to the local
health center after she falls severely ill. As the clock ticks and Jenna's
anxiety mounts, the medical team employs a quantum computer to cross-
reference Mia's symptoms and genetic makeup with global health
databases. Within minutes, the computer identifies a rare ailment and
recommends a tailored treatment plan. Mia recovers fully, and Jenna is
forever grateful to the quantum age for giving her precious time with her
daughter.
Reflect: Imagine if every child, parent, or loved one had the gift of time and
health through quantum-aided diagnosis. How would that reshape our
personal narratives and families?
The Bad: The Blackout Days
The year is 2028. A chilly evening turns sinister as major cities around the
globe plunge into darkness. Quantum-empowered hackers bypass even the
most advanced defenses, seizing control of power grids and nuclear
facilities. Panic ensues. The world stands on the abyss, wondering if these
faceless entities now possess the power to instigate global catastrophe.
Reflect: Picture your hometown or city in utter darkness, dependent on
faceless hackers' whims. How vital is it for us to ensure our defensive
technologies evolve as fast, if not faster, than our quantum capabilities?
The Ugly: The Artist and the Quantum
In 2030, a renowned architect, Sophia, witnesses the unveiling of quantum-
driven AI design software. This software can generate building designs in
seconds, accounting for local climates, geographies, and cultures. While
these structures are efficient, they lack the human touch—Sophia's creative
flair and understanding of emotional resonance. Soon, her intricate designs
are sidelined for quantum-generated blueprints. A world of artistic passion
risks being overshadowed by quantum AI's sheer speed and efficiency.
Reflect: What if emotionless quantum algorithms suddenly outpaced the
music, art, or designs you cherished? How do we balance human creativity
with computational efficiency in the future?
While these scenarios may seem far-fetched, we must remember our
resilience as a society. Much like the challenges posed by the invention of
computers and the internet, we will learn, adapt, and harness the potential of
upcoming technologies. Historically, every technological leap has not only
presented challenges but also prompted innovation, creating millions of
jobs and vast wealth. With their unparalleled analytical capabilities,
Quantum computers may even guide us in forecasting and mitigating
potential pitfalls. As with any great advancement, it's up to us to steer its
potential toward the collective good.
Embracing the Quantum Future
As we prepare to usher in a new era powered by quantum technology, the
responsibility of molding this future rests not just with the experts and
pioneers but with each one of us. Your curiosity, questions, and pursuit of
knowledge will play a defining role in how society understands and
integrates quantum advancements.
Have you considered how quantum technology could impact your daily life
or profession? How might it revolutionize industries, from entertainment to
finance? What precautions do we need to establish today to ensure a rewa
tomorrow?
The quantum domain is vast, intricate, and undoubtedly overwhelming at
times, but the rewards of understanding even its basics can be immensely
fulfilling. Start your journey now. Dive into resources like [Quantum
Wonders for the Curious Mind] or attend local quantum computing
workshops. Discuss with friends, join online forums, and participate in
community debates.
The future is not just something that happens to us; it's something we can
influence. How will you shape the quantum era? The first step is to begin
exploring, questioning, and learning.
Thoughtful Questions for Reflection
Each of these questions invites deep reflection on the role of technology in
society and the moral imperatives that come with superior power. Give
them some thought.
The Privacy Paradox
If quantum computing can decrypt most modern encryption, at what point
does an individual's right to privacy become a luxury rather than a
fundamental human right?
The Quantum Divide
As quantum advancements progress, will we unwittingly create a two-tiered
society—those with quantum access and those without? Is this division
morally acceptable?
Job Ethics
If quantum computing can render certain jobs obsolete almost overnight, is
it ethical for companies to pursue such capabilities without plans for
workforce retraining or support?
The Economic Balance
With quantum computing having the potential to reshape entire industries,
do we risk placing too much power in the hands of a few corporate giants,
thereby challenging the very fabric of free-market principles?
Moral Transparency
Given the inherent complexity of quantum mechanics, is it ethical to
introduce quantum-driven solutions to the public without ensuring a basic
level of understanding and informed consent?
Corporate Quantum Responsibility
If a corporation holds significant quantum power, does it bear a moral
responsibility to use it for societal good over profit? Where is the line
drawn?
Priority Ethics
In a world with limited quantum resources, who gets to decide which global
challenges—like climate change, health crises, poverty, or hunger—are
addressed first? Is it fair to prioritize one over the other?
Safety Versus Innovation
At what point does the pursuit of quantum breakthroughs overshadow
potential safety concerns? Is pushing the boundary of knowledge worth
potential societal risks?
Eternal Memory
With quantum-driven data storage solutions, is it morally right to have the
capacity to store information indefinitely, potentially long past its relevance
or even the lifespan of the individual it concerns?
The Moral Compass of Quantum Innovation
As we stand at the cusp of quantum advancements, do we have a moral duty
to ask not just if we can, but if we should?
The Ethical Balance of Innovation and Regulation
In the quest for quantum advancements, is it moral to prioritize free-market
innovation over potential regulatory safeguards, risking the suppression of
revolutionary breakthroughs? Where does one draw the line between
preserving the freedom to innovate and ensuring the collective well-being
of society?
In conclusion, I recommend three actionable steps for you to engage with
quantum computing and its ethical outcomes. First, get educated on this
cutting-edge technology and its potential. By gaining a deeper
understanding of quantum computing, you will contribute to informed
discussions and debates surrounding its development and use.
Second, reach out to policymakers and industry leaders to advocate for
responsible development and use of quantum computing. As previously
discussed, numerous entities are involved in the evolution of quantum
technology, and collective action is necessary to ensure that it is designed
and used ethically and responsibly.
Finally, stay informed about the latest changes in quantum computing and
its ethical implications by following reputable news sources and attending
relevant conferences or events. By staying up-to-date on emerging trends
and debates, readers can continue to engage with this groundbreaking
technology and contribute to its responsible evolution and use.
Conclusion
Dear Explorer,
Wow, what an adventure we've been on! From the mystical realms of
quantum mechanics to the promising future of machine learning, we've
traversed the thrilling landscape of tomorrow's tech. Your curiosity,
perseverance, and enthusiasm have made this worth every minute.
But remember, your quantum story doesn't end here. You're now equipped
with a powerful Quantum Toolkit (Chapter 6). Your new knowledge about
applications, misconceptions, and algorithms can empower you to tap into
the quantum world in ways you might not have imagined. I hope the
insights shared in this book illuminate your path. If they did, and you found
value in this book, I'd be grateful if you could leave a book review. Sharing
your experiences and feedback will help guide future explorers on their
quantum quest.
This is the conclusion of our book, but it's only the beginning. Keep
exploring, keep questioning, and keep dreaming big.
There are infinite quantum resources available, and you picked this one.
Thank you for the opportunity. To further fuel your passion and stay in the
loop with the ever-evolving world of quantum developments, news, and
intriguing puzzles, we warmly invite you to join our newsletter or vibrant
Facebook community.
Discover more free resources at pantheonspace.com or engage with us at
facebook.com/pantheonspace.
Our deepest gratitude,
The Pantheon Space Academy
References
06-TrainingNNsBackprop. (n.d.). https://srdas.github.io/DLBook2/TrainingNNsBackprop.html
10Web AI Website Builder. (2023, August 15). Home. 10Web. https://10web.io/
Abioye, S., Oyedele, L. O., Akanbi, L., Ajayi, A. O., Bilal, M., Akinadé, O. O., & Ahmed, A. (2021).
Artificial intelligence in the construction industry: A review of present status, opportunities and
future challenges. Journal of Building Engineering, 44, 103299.
https://doi.org/10.1016/j.jobe.2021.103299
Ahmadi, H. (2012). Quantum Algorithms For: Quantum Phase Estimation [Pdf]. University of
Central Florida. https://core.ac.uk/download/236257998.pdf
AI for Small Business: A Beginner’s Guide. (n.d.-a). https://www.microsoft.com/en-us/microsoft-
365/business-insights-ideas/resources/how-ai-help-small-business
AI for Small Business: A Beginner’s Guide. (n.d.-b). https://www.microsoft.com/en-us/microsoft-
365/business-insights-ideas/resources/how-ai-help-small-business
Aïmeur, E., Brassard, G., & Gambs, S. (2012). Quantum speed-up for unsupervised learning.
Machine Learning, 90(2), 261–287. https://doi.org/10.1007/s10994-012-5316-5
Allison, P. R. (2023). Quantum computing: What are the data storage challenges?
ComputerWeekly.com. https://www.computerweekly.com/feature/Quantum-computing-What-are-the-
data-storage-challenges Altman, E., Brown, K. R., Carleo, G., Carr, L. D., Demler, E., Chin, C.,
DeMarco, B., Economou, S. E., Eriksson, M. A., Fu, K. C., Greiner, M., Hazzard, K. R. A., Hulet, R.
G., Kollár, A. J., Lev, B., Lukin, M. D., Ma, R., Mi, X., Misra, S., . . . Zwierlein, M. (2021). Quantum
Simulators: architectures and opportunities. PRX Quantum, 2(1).
https://doi.org/10.1103/prxquantum.2.017003
American Institute of Physics. (2022, April 26). Christopher Monroe. https://www.aip.org/history-
programs/niels-bohr-library/oral-histories/46948
Amin, M. H. S., Andriyash, E., Rolfe, J. T., Kulchytskyy, B., & Melko, R. G. (2018). Quantum
Boltzmann Machine. Physical Review X, 8(2). https://doi.org/10.1103/physrevx.8.021050
Aminpour, M., Montemagno, C., & Tuszyński, J. A. (2019). An Overview of Molecular Modeling for
Drug Discovery with Specific Illustrative Examples of Applications. Molecules, 24(9), 1693.
https://doi.org/10.3390/molecules24091693
Anderson, M. (2023). 5 Ways Artificial intelligence helps in improving website usability. IEEE
Computer Society. https://www.computer.org/publications/tech-news/trends/5-ways-artificial-
intelligence-helps-in-improving-website-usability/
Artificial Intelligence (AI) services & Solutions. (n.d.-a). Accenture. https://www.accenture.com/us-
en/services/ai-artificial-intelligence-index
Artificial Intelligence (AI) services & Solutions. (n.d.-b). Accenture. https://www.accenture.com/us-
en/services/ai-artificial-intelligence-index
Atske, S. (2020, July 9). 4. The internet will continue to make life better | Pew Research Center. Pew
Research Center: Internet, Science & Tech. https://www.pewresearch.org/internet/2019/10/28/4-the-
internet-will-continue-to-make-life-better/
Atske, S. (2022a, September 15). 3. Improvements ahead: How humans and AI might evolve together
in the next decade | Pew Research Center. Pew Research Center: Internet, Science & Tech.
https://www.pewresearch.org/internet/2018/12/10/improvements-ahead-how-humans-and-ai-might-
evolve-together-in-the-next-decade/
Atske, S. (2022b, September 15). 3. Improvements ahead: How humans and AI might evolve together
in the next decade | Pew Research Center. Pew Research Center: Internet, Science & Tech.
https://www.pewresearch.org/internet/2018/12/10/improvements-ahead-how-humans-and-ai-might-
evolve-together-in-the-next-decade/
Azure Quantum - Quantum Cloud Computing Service | Microsoft Azure. (n.d.).
https://azure.microsoft.com/en-us/products/quantum
Baksh, M. (2023). NIST selects 12 companies for implementing Post-Quantum cryptography.
Nextgov.com. https://www.nextgov.com/cybersecurity/2022/07/nist-selects-12-companies-
implementing-post-quantum-cryptography/374601/
Bender, A., & Cortés-Ciriano, I. (2021). Artificial intelligence in drug discovery: what is realistic,
what are illusions? Part 1: Ways to make an impact, and why we are not there yet. Drug Discovery
Today, 26(2), 511–524. https://doi.org/10.1016/j.drudis.2020.12.009
Bijaya. (2023, September 5). Quantum computing accelerating drug discovery & development.
Experion Technologies – Software Product Engineering Services.
https://experionglobal.com/quantum-computing-drug-discovery-process/
BioExcel CoE. (2022, April 6). Application of molecular dynamics simulations in the field of drug
discovery [Video]. YouTube. https://www.youtube.com/watch?v=yNGS_mv1-94
Biswal, A. (2023). AI applications: Top 18 artificial intelligence applications in 2023.
Simplilearn.com. https://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/artificial-
intelligence-applications Brown, K. L., Munro, W. J., & Kendon, V. (2010). Using quantum
computers for quantum simulation. Entropy, 12(11), 2268–2307. https://doi.org/10.3390/e12112268
Buchholz, S., & Ammanath, B. (2022). Quantum computing may create ethical risks for businesses.
It’s time to prepare. Deloitte Insights. https://www2.deloitte.com/uk/en/insights/topics/cyber-
risk/quantum-computing-ethics-risks.html Buntz, B. (2023). Quantum computing promises new
frontier in drug discovery and bioinformatics. Drug Discovery and Development.
https://www.drugdiscoverytrends.com/quantum-computing-drug-discovery/
Buvailo, A. (n.d.). 12 Companies Using Quantum Theory To Accelerate Drug Discovery.
www.linkedin.com. https://www.linkedin.com/pulse/13-companies-using-quantum-theory-accelerate-
drug-andrii-buvailo-Capgemini. (2022, July 22). What if quantum could transform & shorten the
drug discovery lifecycle? [Video]. YouTube. https://www.youtube.com/watch?v=ZzGo79mjbRk
Centre for Quantum Technologies, National University of Singapore, Clarendon Laboratory,
University of Oxford, Keble College, Institute for Scientific Interchange, Johnson, T., Clark, S., &
Jaksch, D. (2023). What is a quantum simulator? [Pdf]. Johnson et al.
https://www3.physics.ox.ac.uk/groups/qubit/fetch.asp?url=groupwebsite/papers/paper311.pdf Chow,
J., Greplova, E., Heijman, F., Kuchkovsky, C., O’Halloran, D., Pointing, J., Shutko, G., & Williams,
C. (2022). State of Quantum Computing: Building a Quantum Economy [Pdf]. World Economic
Forum. https://www3.weforum.org/docs/WEF_State_of_Quantum_Computing_2022.pdf Chui, M.,
Roberts, R., Rodchenko, T., Singla, A., Sukharevsky, A., Yee, L., & Zurkiya, D. (2023). What every
CEO should know about generative AI. McKinsey & Company.
https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know-
about-generative-ai Chúláin, A. N. (2023, April 12). What is quantum computing and how will
quantum computers change the world? Euronews. https://www.euronews.com/next/2023/04/12/what-
is-quantum-computing-and-what-does-a-quantum-computer-do Ciliberto, C., Herbster, M., Ialongo,
A. D., Pontil, M., Rocchetto, A., Severini, S., & Wossnig, L. (2018). Quantum machine learning: a
classical perspective. Proceedings of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 474(2209), 20170551. https://doi.org/10.1098/rspa.2017.0551
Cocchi, M., Minuto, C., Tonello, L., Gabrielli, F., Bernroider, G., Tuszyński, J. A., Cappello, F., &
Rasenick, M. M. (2017). Linoleic acid: Is this the key that unlocks the quantum brain? Insights
linking broken symmetries in molecular biology, mood disorders and personalistic emergentism.
BMC Neuroscience, 18(1). https://doi.org/10.1186/s12868-017-0356-1
Coleman, S. (2023). Who Are You? How Bank Apps Confirm Your Identity Without Meeting You In
Person. TecSmash. https://tecsmash.com/how-bank-apps-confirm-your-identity/
Connected World. (2023). Success stories: Quantum sensors monitor brains. Connected World - IoT
and Digital Transformation. https://connectedworld.com/success-stories-quantum-sensors-monitor-
brains/
Contributor, E. (2022). Leveraging physics and quantum computing to accelerate drug discovery.
Labiotech.eu. https://www.labiotech.eu/interview/quantum-computing-drug-discovery/
Cox, T. (2023, July 13). Researchers demonstrate the power of quantum computing in drug design.
phys.org. https://phys.org/news/2023-07-power-quantum-drug.html
Dallaire-Demers, P., & Killoran, N. (2018). Quantum generative adversarial networks. Physical
Review. https://doi.org/10.1103/physreva.98.012324
Dang, Y., Jiang, N., Hu, H., Ji, Z., & Zhang, W. (2018). Image classification based on quantum K-
Nearest-Neighbor algorithm. Quantum Information Processing, 17(9).
https://doi.org/10.1007/s11128-018-2004-9
Dargan, J. (2022a). 8 online Quantum computing courses to start career in 2022. The Quantum
Insider. https://thequantuminsider.com/2020/01/07/quantum-computing-course/
Dargan, J. (2022b). 25 Quantum Cryptography & Encryption Companies [2022]. The Quantum
Insider. https://thequantuminsider.com/2021/01/11/25-companies-building-the-quantum-
cryptography-communications-markets/
Dargan, J. (2023a). Top 63 Quantum Computer Simulators For 2022. The Quantum Insider.
https://thequantuminsider.com/2022/06/14/top-63-quantum-computer-simulators-for-2022/
Dargan, J. (2023b). 81 Quantum computing companies: An ultimate 2023 list. The Quantum Insider.
https://thequantuminsider.com/2022/09/05/quantum-computing-companies-ultimate-list-for-2022/
Das Sarma, S., Deng, D., & Duan, L. (2019, March). Machine learning meets quantum physics.
pubs.aip.org. https://pubs.aip.org/physicstoday/article/72/3/48/915959/Machine-learning-meets-
quantum-physicsThe-marriage DataRobot AI Platform | Deliver Value from AI. (2023, October 5).
DataRobot AI Platform. https://www.datarobot.com/
De Wolf, R. (2023). Quantum Computing: Lecture Notes [Pdf]. Independent via Amsterdam U.
Deloitte. (2022, May 17). 3 ways quantum computing may create ethical risks. WSJ.
https://deloitte.wsj.com/cio/3-ways-quantum-computing-may-create-ethical-risks-01652815643
Diving deep into quantum computing: modern cryptography. (n.d.).
https://www.trendmicro.com/vinfo/us/security/news/security-technology/diving-deep-into-quantum-
computing-modern-cryptography Dong, D., Chen, C., Li, H., & Tarn, T. (2008). Quantum
Reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, 38(5), 1207–1220.
https://doi.org/10.1109/tsmcb.2008.925743
Dougfinke. (2023). Public companies. Quantum Computing Report.
https://quantumcomputingreport.com/public-companies/
DSpace@MIT. (2021). Quantum Simulators: Architectures and Opportunities. American Physical
Society. https://dspace.mit.edu/bitstream/handle/1721.1/142317/PRXQuantum.2.017003.pdf?
sequence=2&isAllowed=y Durrant, J. D., & McCammon, J. A. (2011). Molecular dynamics
simulations and drug discovery. BMC Biology, 9(1). https://doi.org/10.1186/1741-7007-9-71
EastMojo, T. (2023, May 12). Quantum AI chronicles: success stories and teachable moments.
EastMojo. https://www.eastmojo.com/science-tech/2023/05/12/quantum-ai-chronicles-success-
stories-and-teachable-moments/
Editor. (n.d.). Intel releases Quantum SDK. https://www.i-programmer.info/news/90-tools/16134-
intel-releases-quantum-sdk.html
Education. (n.d.). Google Quantum AI. https://quantumai.google/education
edX. (n.d.). Delft University of Technology. https://www.edx.org/school/delftx
Elnagar, G. N., & Kazemi, M. (1998). Pseudospectral Legendre-based optimal computation of
nonlinear constrained variational problems. Journal of Computational and Applied Mathematics,
88(2), 363–375. https://doi.org/10.1016/s0377-0427(97)00225-2
Evans, D. (2023, June 26). 5 Websites Designed by Artificial Intelligence. Hubspot.
https://blog.hubspot.com/website/ai-website-design-examples
Evers, M., Heid, A., & Ostojic, I. (2021). Pharma’s digital Rx: Quantum computing in drug research
and development. McKinsey & Company. https://www.mckinsey.com/industries/life-sciences/our-
insights/pharmas-digital-rx-quantum-computing-in-drug-research-and-development First step
towards Global-Scale Quantum Internet Taken. (n.d.). A2D CONSULTING. http://www.a2d-
consulting.com/digital-feed1/first-step-towards-global-scale-quantum-internet-taken Flöther, F. F.
(2023). The state of quantum computing applications in health and medicine. arXiv (Cornell
University). https://doi.org/10.1017/qut.2023.4
Fulton, S., III. (2020, November 10). What is quantum computing today? The how, why, and when of
a paradigm shift. ZDNET. https://www.zdnet.com/article/what-is-quantum-computing-understanding-
the-how-why-and-when-of-quantum-computers/
Generative AI: What is it, tools, models, applications and use cases. (n.d.). Gartner.
https://www.gartner.com/en/topics/generative-ai
Gil, D., Mantas, J., Sutor, R., Townes, L., Flother, F., & Schnabel, C. (2023). Five strategies to
prepare for paradigm-shifting quantum technology. IBM.com.
Google Scholar. (n.d.-a). https://scholar.google.com/scholar_lookup?title=Quantum-
inspired+neural+networks&conference=Proceedings+of+the+Neural+Information+Processing+Syste
ms+95&author=Menneer,+T.&author=Narayanan,+A.&publication_year=1995
Google Scholar. (n.d.-b). https://scholar.google.com/scholar_lookup?
title=Algorithms+for+Quantum+Computation:+Discrete+Logarithms+and+Factoring&conference=P
roceedings+of+the+35th+Annual+Symposium+on+Foundation+of+Computer+Science&author=Sho
r,+P.W.&publication_year=1994&pages=124%E2%80%93134
Google Scholar. (n.d.-c). https://scholar.google.com/scholar_lookup?
title=Quantum+algorithms+for+supervised+and+unsupervised+machine+learning&author=Lloyd,+S
.&author=Mohseni,+M.&author=Rebentrost,+P.&publication_year=2013&journal=arXiv Google
Scholar. (n.d.-d). https://scholar.google.com/scholar_lookup?title=q-
means:+A+quantum+algorithm+for+unsupervised+machine+learning&author=Kerenidis,+I.&author
=Landman,+J.&author=Luongo,+A.&author=Prakash,+A.&publication_year=2018&journal=arXiv
Google Scholar. (n.d.-e). https://scholar.google.com/scholar_lookup?
title=Quantum+algorithms+for+solving+dynamic+programming+problems&author=Ronagh,+P.&pu
blication_year=2019&journal=arXiv Google Scholar. (n.d.-f).
https://scholar.google.com/scholar_lookup?
title=Automated+quantum+programming+via+reinforcement+learning+for+combinatorial+optimizat
ion&author=McKiernan,+K.A.&author=Davis,+E.&author=Alam,+M.S.&author=Rigetti,+C.&publi
cation_year=2019&journal=arXiv Google Scholar. (n.d.-g).
https://scholar.google.com/scholar_lookup?
title=Quantum+Wasserstein+generative+adversarial+networks&author=Chakrabarti,+S.&author=Yi
ming,+H.&author=Li,+T.&author=Feizi,+S.&author=Wu,+X.&publication_year=2019&journal=arX
iv GreyB, T. (2022). Top 10 companies researching quantum computing technology. GreyB.
https://www.greyb.com/blog/quantum-computing-companies/
Grover, L. K. (1997). Quantum mechanics helps in searching for a needle in a haystack. Physical
Review Letters, 79(2), 325–328. https://doi.org/10.1103/physrevlett.79.325
Gupta, S., Modgil, S., Bhatt, P. C., Jabbour, C. J. C., & Kamble, S. S. (2023). Quantum computing
led innovation for achieving a more sustainable Covid-19 healthcare industry. Technovation, 120,
102544. https://doi.org/10.1016/j.technovation.2022.102544
Haney, B. (2021). Quantum Machine Learning: A Patent Review (Vol. 12) [Pdf]. Journal of Law,
Technology & The Internet. https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?
article=1131&context=jolti Harrow, A. W., Hassidim, A., & Lloyd, S. (2009). Quantum Algorithm
for linear systems of equations. Physical Review Letters, 103(15).
https://doi.org/10.1103/physrevlett.103.150502
Henderson, M., Shakya, S., Pradhan, S., & Cook, T. (2020). Quanvolutional neural networks:
powering image recognition with quantum circuits. Quantum Machine Intelligence, 2(1).
https://doi.org/10.1007/s42484-020-00012-y Home. (n.d.). https://www.oecd-
ilibrary.org/sites/bb167041-en/1/3/11/index.html?itemId=/content/publication/bb167041-
en&_csp_=509e10cb8ea8559b6f9cc53015e8814d&itemIGO=oecd&itemContentType=book How
artificial intelligence is transforming the world | Brookings. (2023, June 27). Brookings.
https://www.brookings.edu/articles/how-artificial-intelligence-is-transforming-the-world/
Huang, H., Du, Y., Gong, M., Zhao, Y., Wu, Y., Wang, C., Li, S., Liang, F., Lin, J., Xu, Y., Yang, R.,
Liu, T., Hsieh, M., Deng, H., Rong, H., Peng, C., Lu, C., Chen, Y., Tao, D., . . . Pan, J. (2021).
Experimental quantum generative adversarial networks for image Generation. Physical Review
Applied, 16(2). https://doi.org/10.1103/physrevapplied.16.024051
Ibm. (n.d.). The quest to understand what sews the universe together | IBM. IBM.
https://www.ibm.com/case-studies/cern/
IBM Quantum Computing. (n.d.). https://www.ibm.com/quantum
Johnson, T. H., Clark, S. R. L., & Jaksch, D. (2014). What is a quantum simulator? EPJ Quantum
Technology, 1(1). https://doi.org/10.1140/epjqt10
Kavanagh, C. (2019). New tech, new threats, and new governance challenges: an opportunity to craft
smarter responses? Carnegie Endowment for International Peace.
https://carnegieendowment.org/2019/08/28/new-tech-new-threats-and-new-governance-challenges-
opportunity-to-craft-smarter-responses-pub-79736
Kieferová, M., & Wiebe, N. (2017). Tomography and generative training with quantum Boltzmann
machines. Physical Review, 96(6). https://doi.org/10.1103/physreva.96.062327
Ksupasate. (2023, March 12). Unleashing the Power of Quantum Computing for Machine Learning:
How Quantum Machine Learning is Solving Problems Faster and Better Than Ever Before. Medium.
https://ksupasate.medium.com/unleashing-the-power-of-quantum-computing-for-machine-learning-
how-quantum-machine-learning-is-5bea3922133b?source=post_internal_links---------2-----------------
-----------
Learn Quantum Computing on Brilliant. (n.d.). https://brilliant.org/courses/quantum-computing/
Lewis, J. A., & Wood, G. (2023). Quantum Technology: applications and implications.
https://www.csis.org/analysis/quantum-technology-applications-and-implications
Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory,
28(2), 129–137. https://doi.org/10.1109/tit.1982.1056489
Lloyd, S., Mohseni, M., & Rebentrost, P. (2014). Quantum principal component analysis. Nature
Physics, 10(9), 631–633. https://doi.org/10.1038/nphys3029
Lloyd, S., & Weedbrook, C. (2018). Quantum Generative Adversarial Learning. Physical Review
Letters, 121(4). https://doi.org/10.1103/physrevlett.121.040502
Lu, S., & Braunstein, S. L. (2013). Quantum decision tree classifier. Quantum Information
Processing, 13(3), 757–770. https://doi.org/10.1007/s11128-013-0687-5
McKinsey & Company. (2021). Quantum Computing: An Emerging Ecosystem and Industry Use
Cases [Pdf].
https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20in
sights/quantum%20computing%20use%20cases%20are%20getting%20real%20what%20you%20nee
d%20to%20know/quantum-computing-an-emerging-ecosystem.pdf McLean, D. (2023a). 7 Best AI
SEO tools in 2023 (Reviewed & compared). Elegant Themes Blog.
https://www.elegantthemes.com/blog/business/best-ai-seo-tools
McLean, D. (2023b). 7 Best AI SEO tools in 2023 (Reviewed & compared). Elegant Themes Blog.
https://www.elegantthemes.com/blog/business/best-ai-seo-tools
Michigan State University Artificial Intelligence Boot Camp - Michigan State University boot camps.
(2023, August 1). Michigan State University Boot Camps. https://bootcamp.msu.edu/artificial-
intelligence/
Musti, A. (2022). Quantum Machine Learning Is The Next Big Thing. The Quantum Insider.
https://thequantuminsider.com/2020/05/28/quantum-machine-learning-is-the-next-big-thing/
National Academies of Sciences, Engineering, and Medicine. (2019). Quantum Computing: Progress
and Prospects [Pdf]. The National Academy of Sciences. https://doi.org/10.17226/25196
Obata, A. (2023, February 1). 10 Online Courses You Can Take To Help You Understand Quantum
Computing — Quantum Zeitgeist. Quantum Zeitgeist. https://quantumzeitgeist.com/10-online-
courses-you-can-take-to-help-you-understand-quantum-computing/
O’Connell, B., & McVearry, R. (2023, September 19). 8 Best Quantum Computing Stocks to buy in
2023. US News & World Report. https://money.usnews.com/investing/stock-market-
news/articles/best-quantum-computing-stocks-to-buy Parker, E., Gonzales, D., Kochhar, A., Litterer,
S., O’Connor, K., Schmid, J., Scholl, K., Silberglitt, R., Chang, J., Eusebi, C., & Harold, S. (2022).
An Assessment of the U.S. and Chinese Industrial Bases in Quantum Technology [Pdf]. RAND
Corporation. https://www.rand.org/content/dam/rand/pubs/research_reports/RRA800/RRA869-
1/RAND_RRA869-1.pdf Pijselman, M. (2022). Why innovation leaders must consider quantum
ethics. www.ey.com. https://www.ey.com/en_uk/emerging-technologies/why-innovation-leaders-
must-consider-quantum-ethics
Possati, L. M. (2023). Ethics of Quantum Computing: an Outline. Philosophy & Technology, 36(3).
https://doi.org/10.1007/s13347-023-00651-6
Qiskit. (2022a, January 6). How Do You Explain Quantum Computing To Your Dog (And Other
Important People in Your Life)? Medium. https://medium.com/qiskit/how-do-you-explain-quantum-
computing-to-your-dog-and-other-important-people-in-your-life-22f5fdacaf11
Qiskit. (2022b, January 7). Building a quantum variational classifier using Real-World data. Medium.
https://medium.com/qiskit/building-a-quantum-variational-classifier-using-real-world-data-
809c59eb17c2
Quantum Computers: Opportunities, risks, and challenges for policymakers. (2021, November 16).
American University. https://www.american.edu/sis/centers/security-technology/quantum-
computers.cfm Quantum computing. (n.d.). BCG Global. https://www.bcg.com/capabilities/digital-
technology-data/emerging-technologies/quantum-computing
Quantum computing use cases are getting real—what you need to know. (2021). In McKinsey &
Company. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/quantum-
computing-use-cases-are-getting-real-what-you-need-to-know Quantum Information Science and
Engineering Research at NSF | NSF - National Science Foundation. (n.d.).
https://www.nsf.gov/mps/quantum/quantum_research_at_nsf.jsp
Quantum Learning Machine - ATOS. (2023, July 26). Atos. https://atos.net/en/solutions/quantum-
learning-machine
Quantum Science, networking, and Communications. (2023, October 5). University of Chicago
Professional Education. https://professional.uchicago.edu/find-your-fit/courses/quantum-science-
networking-and-communications Real World Drug Discovery: A Chemist’s Guide to Biotech and
Pharmaceutical research: Rydzewski, Robert M.: 9780080466170: Amazon.com: Books. (n.d.).
https://www.amazon.com/Real-World-Drug-Discovery-Pharmaceutical/dp/0080466176
Rebentrost, P., Mohseni, M., & Lloyd, S. (2014). Quantum Support vector machine for big data
classification. Physical Review Letters, 113(13). https://doi.org/10.1103/physrevlett.113.130503
Researchers take a step toward novel quantum simulators | SLAC National Accelerator Laboratory.
(2023, January 30). SLAC National Accelerator Laboratory.
https://www6.slac.stanford.edu/news/2023-01-30-researchers-take-step-toward-novel-quantum-
simulators Roundy, J. (2023). 10 companies building quantum computers. Data Center.
https://www.techtarget.com/searchdatacenter/feature/Companies-building-quantum-computers
Ruane, J. (2021, December 14). Quantum computing for business leaders. Harvard Business Review.
https://hbr.org/2022/01/quantum-computing-for-business-leaders
Salo‐Ahen, O. M. H., Alanko, I., Bhadane, R., Bonvin, A. M. J. J., Honorato, R. V., Hossain, S.,
Juffer, A. H., Kabedev, A., Lahtela‐Kakkonen, M., Larsen, A. S., Lescrinier, E., Marimuthu, P.,
Mirza, M. U., Mustafa, G., Nunes-Alves, A., Pantsar, T., Saadabadi, A., Singaravelu, K., & Vanmeert,
M. (2020). Molecular dynamics simulations in drug discovery and pharmaceutical development.
Processes, 9(1), 71. https://doi.org/10.3390/pr9010071
Schuld, M., Sinayskiy, I., & Petruccione, F. (2016). Prediction by linear regression on a quantum
computer. Physical Review, 94(2). https://doi.org/10.1103/physreva.94.022342
Simons Institute. (2018, June 12). Quantum simulators: Where do we stand? [Video]. YouTube.
https://www.youtube.com/watch?v=3EvW4_bAQDo
Situ, H., He, Z., Wang, Y., Li, L., & Zheng, S. (2020). Quantum generative adversarial network for
generating discrete distribution. Information Sciences, 538, 193–208.
https://doi.org/10.1016/j.ins.2020.05.127
Socialty Pro. (2022, September 17). 10 INSANE AI Websites that will Change the way we do
Business. . .FOREVER [Video]. YouTube. https://www.youtube.com/watch?v=iM8Fjh3pE5w
SoniaLopezBravo. (2023, June 21). Understanding quantum computing - Azure Quantum. Microsoft
Learn. https://learn.microsoft.com/en-us/azure/quantum/overview-understanding-quantum-
computing Steer-Stephenson, C. (2022). Healthcare, finance and the impact of quantum computing
tech. Technology Magazine. https://technologymagazine.com/enterprise-it/healthcare-finance-and-
the-impact-of-quantum-computing-tech Swayne, M. (2021). TQD Exclusive: John Martinis on
quantum computing ethics and how they may offer Real-World solutions to world’s biggest
challenges. The Quantum Insider. https://thequantuminsider.com/2021/04/26/tqd-exclusive-john-
martinis-on-quantum-computing-ethics-and-how-they-may-offer-real-world-solutions-to-worlds-
biggest-challenges/
Swayne, M. (2023a). What Are The Remaining Challenges of Quantum Computing? The Quantum
Insider. https://thequantuminsider.com/2023/03/24/quantum-computing-challenges/
Swayne, M. (2023b). BTQ patents innovative method and system for generating private
cryptographic keys. The Quantum Insider. https://thequantuminsider.com/2023/06/27/btq-patents-
innovative-method-and-system-for-generating-private-cryptographic-keys/
The rise of quantum computing. (2023, May 19). McKinsey & Company.
https://www.mckinsey.com/featured-insights/the-rise-of-quantum-computing
Vakoch, D. (2012). Archaeology, Anthropology, and Interstellar Communication [Pdf]. National
Aeronautics and Space Administration.
https://www.nasa.gov/sites/default/files/files/Archaeology_Anthropology_and_Interstellar_Communi
cation_TAGGED.pdf Vasques, X., Paik, H., & Cif, L. (2023). Application of quantum machine
learning using quantum kernel algorithms on multiclass neuron M-type classification. Scientific
Reports, 13(1). https://doi.org/10.1038/s41598-023-38558-z Walsh, B. (2021). Charting an ethical
path for quantum computing. Axios. https://www.axios.com/2021/12/11/quantum-computing-ethical-
questions
What Are The Most Promising Real-World Applications For Quantum Machine Learning. (n.d.).
Quantum Computing Stack Exchange.
https://quantumcomputing.stackexchange.com/questions/11530/what-are-the-most-promising-real-
world-applications-for-quantum-machine-learning What is Quantum Computing? | IBM. (n.d.).
https://www.ibm.com/topics/quantum-computing
What is Quantum Computing? - Quantum Computing Explained - AWS. (n.d.). Amazon Web
Services, Inc. https://aws.amazon.com/what-is/quantum-computing/
What Makes Quantum Computing So Hard to Explain? (2021, November 30). Quanta Magazine.
https://www.quantamagazine.org/why-is-quantum-computing-so-hard-to-explain-20210608/
Wiebe, N., Kapoor, A., & Svore, K. M. (2015). Quantum algorithms for nearest-neighbor methods
for supervised and unsupervised learning. Quantum Information & Computation, 15(3 & 4), 316–
356. https://doi.org/10.26421/qic15.3-4-7
Wikipedia contributors. (2023a). Quantum simulator. Wikipedia.
https://en.wikipedia.org/wiki/Quantum_simulator
Wikipedia contributors. (2023b). Quantum simulator. Wikipedia.
https://en.wikipedia.org/wiki/Quantum_simulator
Wikipedia contributors. (2023c). List of companies involved in quantum computing or
communication. Wikipedia.
https://en.wikipedia.org/wiki/List_of_companies_involved_in_quantum_computing_or_communicati
on WIRED. (2018, June 25). Quantum computing expert explains one concept in 5 levels of difficulty
| WIRED [Video]. YouTube. https://www.youtube.com/watch?v=OWJCfOvochA
Wong, T. (2022). Introduction to Classical Computing. Rooted Grove.
https://www.thomaswong.net/introduction-to-classical-and-quantum-computing-1e3p.pdf
Zapata Computing. (2022, December 7). Zapata Computing earns two new patents for quantum
Cybersecurity Threat Intelligence - Zapata AI. Zapata AI. https://zapata.ai/news/zapata-computing-
earns-two-new-patents-for-quantum-cybersecurity-threat-intelligence/
Zeguendry, A., Jarir, Z., & Quafafou, M. (2023). Quantum Machine Learning: a review and case
studies. Entropy, 25(2), 287. https://doi.org/10.3390/e25020287

============================================================
Quantum Computing for Everyone (Bernhardt)
============================================================

Quantum Computing for Everyone
Quantum Computing for Everyone
Chris Bernhardt
The MIT Press
Cambridge, Massachusetts
London, England
© 2019 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any 
electronic or mechanical means (including photocopying, recording, or information 
storage and retrieval) without permission in writing from the publisher.
This book was set in ITC Stone Sans Std and ITC Stone Serif Std by Toppan Best-set 
Premedia Limited. Printed and bound in the United States of America.
Library of Congress Cataloging-in-Publication Data
Names: Bernhardt, Chris, author.
Title: Quantum computing for everyone / Chris Bernhardt.
Description: Cambridge, MA : The MIT Press, [2019] | Includes bibliographical 
references and index.
Identifiers: LCCN 2018018398 | ISBN 9780262039253 (hardcover : alk. paper)
Subjects:  LCSH: Quantum computing--Popular works.
Classification: LCC QA76.889 .B47 2019 | DDC 006.3/843--dc23 LC record available 
at https://lccn.loc.gov/2018018398
10  9  8  7  6  5  4  3  2  1
To Henryka
Contents
Acknowledgments  xi
Introduction  xiii
1	 Spin  1
The Quantum Clock  6
Measurements in the Same Direction  7
Measurements in Different Directions  7
Measurements  9
Randomness  10
Photons and Polarization  11
Conclusions  15
2	 Linear Algebra  17
Complex Numbers versus Real Numbers  17
Vectors  19
Diagrams of Vectors  19
Lengths of Vectors  20
Scalar Multiplication  21
Vector Addition  21
Orthogonal Vectors  23
Multiplying a Bra by a Ket  23
Bra-Kets and Lengths  24
Bra-Kets and Orthogonality  24
Orthonormal Bases  25
Vectors as Linear Combinations of Basis Vectors  27
Ordered Bases  29
Length of Vectors  30
Matrices  30
viii 
Contents
Matrix Computations  33
Orthogonal and Unitary Matrices  34
Linear Algebra Toolbox  35
3	 Spin and Qubits  37
Probability  37
Mathematics of Quantum Spin  38
Equivalent State Vectors  41
The Basis Associated with a Given Spin Direction  43
Rotating the Apparatus through 60°  45
The Mathematical Model for Photon Polarization  46
The Basis Associated with a Given Polarization Direction  47
The Polarized Filters Experiments  47
Qubits  49
Alice, Bob, and Eve  50
Probability Amplitudes and Interference  52
Alice, Bob, Eve, and the BB84 Protocol  53
4	 Entanglement  57
Alice and Bob’s Qubits Are Not Entangled  57
Unentangled Qubits Calculation  59
Entangled Qubits Calculation  61
Superluminal Communication  62
The Standard Basis for Tensor Products  64
How Do You Entangle Qubits?  65
Using the CNOT Gate to Entangle Qubits  67
Entangled Quantum Clocks  68
5	 Bell’s Inequality  71
Entangled Qubits in Different Bases  72
Proof That 1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


 Equals 
1
2
1
2
0
0
1
1
b
b
b
b
⊗
+
⊗
  73
Einstein and Local Realism  75
Einstein and Hidden Variables  77
A Classical Explanation of Entanglement  78
Bell’s Inequality  79
The Answer of Quantum Mechanics  80
Contents 
ix
The Classical Answer  81
Measurement  84
The Ekert Protocol for Quantum Key Distribution  86
6	 Classical Logic, Gates, and Circuits  89
Logic  90
Boolean Algebra  91
Functional Completeness  94
Gates  98
Circuits  99
NAND Is a Universal Gate  100
Gates and Computation  101
Memory  103
Reversible Computation  103
Billiard Ball Computing  111
7	 Quantum Gates and Circuits  117
Qubits  118
The CNOT Gate  118
Quantum Gates  120
Quantum Gates Acting on One Qubit  121
Are There Universal Quantum Gates?  123
No Cloning Theorem  124
Quantum Computation versus Classical Computation  126
The Bell Circuit  127
Superdense Coding  129
Quantum Teleportation  132
Error Correction  135
8	 Quantum Algorithms  141
The Complexity Classes P and NP  142
Are Quantum Algorithms Faster Than Classical Ones?  144
Query Complexity  145
Deutsch’s Algorithm  145
The Kronecker Product of Hadamard Matrices  149
The Deutsch-Jozsa Algorithm  152
Simon’s Algorithm  157
Complexity Classes  166
Quantum Algorithms  168
x 
Contents
9	 Impact of Quantum Computing  171
Shor’s Algorithm and Cryptanalysis  172
Grover’s Algorithm and Searching Data  176
Chemistry and Simulation  181
Hardware  182
Quantum Supremacy and Parallel Universes  186
Computation  187
Index  191
Acknowledgments
© Massachusetts Institute of TechnologyAll Rights Reserved
I am very grateful to a number of people for their help with this book. Matt 
Coleman, Steve LeMay, Dan Ryan, Chris Staecker, and three anonymous 
reviewers read through various drafts with great care. Their suggestions and 
corrections have improved this book beyond measure. I also thank Marie 
Lee and her team at MIT Press for all of their support and work in turning a 
rough proposal into this book.
Introduction
I
n
t
r
o
d
u
c
t
i
o
n
I
n
t
r
o
d
u
c
t
i
o
n
© Massachusetts Institute of TechnologyAll Rights Reserved
The aim of this book is to give an introduction to quantum computing that 
anyone who is comfortable with high school mathematics and is willing 
to put in a little work can understand. We will study qubits, entanglement, 
quantum teleportation, and quantum algorithms, among other quantum-
related topics. The goal is not to give some vague idea of these concepts but 
to make them crystal clear.
Quantum computing is often in the news: China teleported a qubit 
from earth to a satellite; Shor’s algorithm has put our current encryption 
methods at risk; quantum key distribution will make encryption safe again; 
Grover’s algorithm will speed up data searches. But what does all this really 
mean? How does it all work? All of this will be explained.
Can this be done without using mathematics? No, not if we want to 
really understand what is going on. The underlying ideas come from quan­
tum mechanics and are often counterintuitive. Attempts to describe these 
in words don’t work because we have no experience of them in our every­
day lives. Even worse, verbal descriptions often give the impression that we 
have understood something when we really haven’t. The good news is that 
we really do not need to introduce much mathematics. My role as a math­
ematician is to simplify the mathematics as much as possible—just sticking 
to the absolute essentials—and to give elementary examples to illustrate 
both how it is used and what it means. That said, the book probably con­
tains mathematical ideas that you have not seen before, and, as with all 
mathematics, new concepts can seem strange at first. It is important not to 
gloss over the examples but to read them carefully, following each step of 
the calculations.
Quantum computing is a beautiful fusion of quantum physics with com­
puter science. It incorporates some of the most stunning ideas of physics 
xiv 
Introduction
from the twentieth century into an entirely new way of thinking about 
computation. The basic unit of quantum computing is the qubit. We will 
see what qubits are and what happens when we measure them. A classical 
bit is either 0 or 1. If it’s 0 and we measure it, we get 0. If it’s 1 and we mea­
sure 1, we get 1. In both cases the bit remains unchanged. The situation is 
totally different for qubits. A qubit can be in one of an infinite number of 
states—a superposition of both 0 and 1—but when we measure it, as in the 
classical case, we just get one of two values, either 0 or 1. The act of mea­
surement changes the qubit. A simple mathematical model describes all of 
this precisely.
Qubits can also be entangled. When we make a measurement of one of 
them, it affects the state of the other. Again, this is something that we don’t 
experience in our daily lives, but it is described perfectly by our mathemati­
cal model.
These three things—superposition, measurement, and entanglement—
are the key quantum mechanical ideas. Once we know what they mean, 
we can see how they may be used in computations. It is here that human 
ingenuity enters the picture.
Mathematicians often describe proofs as being beautiful, often contain­
ing unexpected insights. I feel exactly the same way about many of the 
topics we will look at. Bell’s theorem, quantum teleportation, superdense 
coding, all are gems. The error correcting circuit and Grover’s algorithm are 
absolutely amazing.
By the end of the book, you should understand the basic ideas that 
underlie quantum computing, and you will have seen some ingenious 
and beautiful constructions. You will also come to realize that quantum 
computing and classical computing are not two distinct disciplines, but 
that quantum computing is the more fundamental form of computing—
anything that can be computed classically can be computed on a quantum 
computer. The qubit is the basic unit of computation, not the bit. Compu­
tation, in its essence, really means quantum computation.
Finally, it should be emphasized that this book is about the theory of 
quantum computation. It is about software, not hardware. We briefly men­
tion hardware in places and occasionally talk about how to physically 
entangle qubits, but these topics are just asides. The book is not about how 
to build a quantum computer, but how to use one.
Here’s a brief description of the book’s contents.
Introduction 
xv
Chapter 1. The basic unit of classical computing is the bit. Bits can be repre­
sented by anything that can be in one of two possible states. The standard 
example is an electrical switch that can be either on or off. The basic unit 
of quantum computing is the qubit. This can be represented by the spin of 
an electron or the polarization of a photon, but the properties of spin and 
polarization are not nearly as familiar to us as a switch being in the on or 
off position.
We look at the basic properties of spin, starting with Otto Stern and 
Walther Gerlach’s classic experiment in which they studied the magnetic 
properties of silver atoms. We see what happens when we measure spin in a 
number of different directions. The act of making a measurement can affect 
the state of a qubit. There is also an underlying randomness associated with 
some of the measurements that we will need to explain.
The chapter concludes by showing that experiments analogous to those 
for spin can be performed using polarized filters and ordinary light.
Chapter 2. Quantum computing is based on an area of mathematics called 
linear algebra. Fortunately, we only need a few concepts. This chapter 
introduces and describes the linear algebra we need and illustrates how it is 
going to be used in the later chapters.
We introduce vectors and matrices and show how to calculate the length 
of vectors and how to tell whether or not two vectors are perpendicular. 
The chapter starts by just considering elementary operations on vectors 
and then shows how matrices give a simple way of doing a number of these 
calculations simultaneously.
It is not initially apparent that this material is going to be useful, but it 
is. Linear algebra forms the foundation of quantum computing. Since the 
rest of the book uses the mathematics introduced here, this chapter needs 
to be read carefully.
Chapter 3. This chapter shows how the previous two chapters are con­
nected. The mathematical model of spin or, equivalently, that of polariza­
tion is given using linear algebra. This enables us to give the definition of a 
qubit and to describe exactly what happens when we measure it.
Several examples of measuring qubits in different directions are given. 
The chapter ends with an introduction to quantum cryptography, describ­
ing the BB84 protocol.
xvi 
Introduction
Chapter 4. This chapter describes what it means for two qubits to be entan­
gled. Entanglement is difficult to describe in words, but it is easy to describe 
mathematically. The new mathematical idea is the tensor product. This is 
the simplest way of combining mathematical models of individual qubits 
to give one model that describes a collection of qubits.
Though the mathematics is straightforward, entanglement is not some­
thing that we experience in everyday life. When one of a pair of entangled 
qubits is measured, it affects the second qubit. This is what Albert Einstein, 
who disliked it, called “spooky action at a distance.” We look at several 
examples.
The chapter concludes by showing that we can’t use entanglement to 
communicate faster than the speed of light.
Chapter 5. We look at Einstein’s concerns with entanglement and whether 
a hidden variable theory can preserve local realism. We go through the 
mathematics of Bell’s inequality—a remarkable result that gives an experi­
mental way of determining whether or not Einstein’s argument is correct. 
As most people know, Einstein’s view was wrong, but even Bell thought he 
would be proved correct.
Artur Ekert realized that the setup for the test of Bell’s inequality could 
also be used both to generate a secure key to be used for cryptography and 
at the same time to test whether any eavesdroppers are present. We con­
clude the chapter with a description of this cryptographic protocol.
Chapter 6. The chapter starts with standard topics in computation: bits, 
gates, and logic. Then we briefly look at reversible computation and the 
ideas of Ed Fredkin. We show that both the Fredkin gate and the Toffoli 
gate are universal—you can build a complete computer using only Fredkin 
gates (or Toffoli gates). The chapter concludes with Fredkin’s billiard-ball 
computer. This is not really needed for the rest of our story, but its sheer 
ingenuity demands that it be included.
This computer consists of balls colliding with one another and off vari­
ous walls. It conjures up images of particles interacting. This is one of the 
ideas that inspired Richard Feynman to become interested in the idea of 
quantum computing. Feynman wrote some of the earliest papers on the 
subject.
Introduction 
xvii
Chapter 7. This chapter begins the study of quantum computing using 
quantum circuits. Quantum gates are defined. We see how a quantum gate 
acts on a qubit and realize that we have been considering these ideas all 
along. We just need to change our perspective. We no longer think of an 
orthogonal matrix as acting on our measuring device, but as acting on the 
qubit. We also prove some amazing results concerning superdense coding, 
quantum teleportation, cloning, and error correction.
Chapter 8. This is probably the most challenging chapter. In it we look at 
some quantum algorithms and show how quickly they can compute an 
answer compared to classical algorithms. To talk about the speed of algo­
rithms we need to introduce various ideas from complexity theory. Once 
we have defined something called query complexity, we study three quan­
tum algorithms and show that they are faster with respect to this type of 
complexity than their classical counterparts.
Quantum algorithms exploit the underlying structure of the prob­
lem that is being solved. It is much more than just the idea of quantum 
parallelism—putting the input into a superposition of all possible states. 
This chapter introduces the last piece of mathematical machinery, the 
Kronecker product of matrices. But the difficulty of the material is really 
caused by the fact that we are computing in a completely new way and we 
have no experience of thinking about solving problems using these novel 
ideas.
Chapter 9. The last chapter looks at the impact that quantum computing 
is going to have on our lives. We start by giving brief descriptions of two 
important algorithms, one invented Peter Shor, the other by Lov Grover.
Shor’s algorithm gives a way of factoring a large number into the product 
of its prime factors. This might not seem that important, but our Internet 
security depends on this problem being hard to solve. Being able to factor 
products of large primes threatens our current methods of securing transac­
tions between computers. It might be some time until we have quantum 
computers powerful enough to factor the large numbers that are currently 
in use, but the threat is real, and it is already forcing us to think about how 
to redesign the ways that computers can securely talk to one another.
Grover’s algorithm is for special types of data searches. We show how it 
works for a small case and indicate how it works in general. Both Grover’s 
and Shor’s algorithms are important, not only for the problems they can 
xviii 
Introduction
solve but also for the new ideas they introduce. These underlying ideas 
have been and are being incorporated into a new generation of algorithms.
After looking at algorithms, we switch gears and briefly look at how 
quantum computation can be used to simulate quantum processes. Chemis­
try, at its most basic level, is quantum mechanical. Classical computational 
chemistry works by taking quantum mechanical equations and simulating 
them using classical computers. These simulations are approximations and 
ignore the fine details. This works well in many cases, but in some cases it 
doesn’t. In these cases you need the fine details, and quantum computers 
should be able to give them.
This chapter also briefly looks at building actual machines. This is a very 
fast-growing area. The first machines are being offered for sale. There is 
even one machine available on the cloud that everyone can use for free. 
It looks likely that we will soon enter the age of quantum supremacy. (We 
explain what this means.)
The book concludes with the realization that quantum computation is 
not a new type of computation but is the discovery of the true nature of 
computation.
1  Spin
Chapter 
1
S
p
i
n
© Massachusetts Institute of TechnologyAll Rights Reserved
All computations involve inputting data, manipulating it according to cer­
tain rules, and then outputting the final answer. For classical computations, 
the bit is the basic unit of data. For quantum computations, this unit is the 
quantum bit—usually shortened to qubit.
A classical bit corresponds to one of two alternatives. Anything that 
can be in exactly one of two states can represent a bit. Later we will see 
various examples, which include the truth or falsity of a logical statement, 
a switch being in the on or off position, and even the presence or absence 
of a billiard ball.
A qubit, like a bit, includes these two alternatives, but—quite unlike a 
bit—it can also be in a combination of these two states. What does this 
mean? What exactly is a combination of two states, and what are physical 
objects that can represent qubits? What is the quantum computation ana­
log to the switch?
A qubit can be represented by the spin of an electron or the polarization 
of a photon. This, though true, does not seem particularly helpful as spins 
of electrons and polarizations of photons are not things that most of us 
have knowledge about, let alone experience with. Let’s start with a basic 
introduction to describe spin and polarization. To do this we describe the 
foundational experiment performed by Otto Stern and Walther Gerlach on 
the spin of silver atoms.
In 1922, Niels Bohr’s planetary model described the current understand­
ing of atoms. In this model an atom consisted of a positive nucleus orbited 
by negative electrons. These orbits were circular and were constrained to 
certain radii. The innermost orbit could contain at most two electrons. 
Once this was filled, electrons would start filling the next level, where at 
most eight electrons could be held. Silver atoms have 47 electrons. Two of 
2 
Chapter 1
these are in the innermost orbit, then eight in the next orbit, then eighteen 
more electrons in both the third and fourth levels. This leaves one lone 
electron in the outermost orbit.
Now, electrons moving in circular orbits generate magnetic fields. The 
electrons in the inner orbits are paired, each of the pair rotating in the 
opposite direction to its partner, resulting in their magnetic fields cancel­
ing. However, the single electron in the outer orbit generates a magnetic 
field that is not canceled by other electrons. This means that the atom as 
a whole can be considered as a little magnet with both a south pole and a 
north pole.
Stern and Gerlach designed an experiment to test whether the north–
south axes of these magnets could have any direction whatsoever or 
whether they were constrained to certain directions. They did this by send­
ing a beam of silver atoms through a pair of magnets as is depicted in figure 
1.1. The vee-shaped design of the magnets makes the south magnet act 
more strongly than the north. If the silver atom is a magnet with north on 
top and south on bottom, it will be attracted to both the magnets of the 
apparatus, but the south magnet wins and the particle is deflected upward. 
Similarly if the silver atom is a magnet with south on top and north on bot­
tom, it will be repelled by both the magnets of the apparatus, but again the 
South
North
N
S
N
S
N
S
Source
Figure 1.1
Stern-Gerlach apparatus.
Spin 
3
south magnet wins and the particle is deflected downward. After passing 
through the apparatus, the atoms are collected on a screen.
From the classical viewpoint, the magnetic poles of the atom could be 
aligned in any direction. If they were aligned horizontally, there would be 
no deflection, and in general, the size of the deflection would correspond 
to the amount the magnetic axis of the atom differs from the horizontal, 
with maximum deflections occurring when the magnetic poles of the atom 
are aligned vertically.
If the classical viewpoint is correct, when we send a large number of 
silver atoms through the machine we ought to see a continuous line on the 
screen going from the top point to the bottom. But this is not what Stern 
and Gerlach found. When they looked at the screen, they found just two 
dots: one at the extreme top and the other at the extreme bottom. All of the 
atoms behaved like little bar magnets that were aligned vertically. None of 
them had any other orientation. How could this be?
But before we start analyzing what is going on in more detail, we will 
shift our attention from atoms to electrons. Not only do atoms act like 
little magnets, but so also do their components. When we discuss quantum 
computers we will often talk about electrons and their spins. As with silver 
atoms, if you measure spin* in the vertical direction, you find that the elec­
tron is either deflected in the north direction or the south direction. Again, 
like silver atoms, you find that electrons are little magnets with their north 
and south poles perfectly aligned in the vertical direction. None of them 
have any other orientation.
In practice, you can’t actually measure electron spin of a free electron 
using the Stern-Gerlach apparatus in the way we have shown because elec­
trons have a negative charge and magnetic fields deflect moving charged 
particles. That said, the following diagrams give useful pictorial representa­
tions of the results of measuring spin in various directions. The idea behind 
this diagram is that you are the source; the magnets are lined up between 
you and this book. The dot shows how the electron gets deflected. In figure 
1.2, the picture on the left shows the deflection by the magnets. The one on 
the right gives a depiction of the electron as a magnet with the north and 
south poles marked. We will describe this situation as saying the electron 
*  We will keep using the term spin because it is the standard terminology. But we are 
just determining the axis of the poles of a magnet.
4 
Chapter 1
has spin N in the vertical direction. Figure 1.3 shows the other possibility, 
where the electron has spin S in the vertical direction.
To understand the deflection, it helps to remember that the south mag­
net acts more strongly than the north, and so to calculate the direction 
of deflection you just consider the effect of this magnet. If the electron 
is aligned with its north pole closest to the south magnet, then it will be 
attracted and the deflection will be in the direction of the south magnet. If 
the electron is aligned with its south pole closest to the south magnet, then 
it will be repelled, and the deflection will be in the direction of the north 
magnet.
Of course, there is nothing special about the vertical direction. For 
example, we can rotate the magnets through 90°. The electrons will still 
be deflected in the direction given by either the north magnet or the south 
magnet. In this case, the electrons now behave as magnets with their 
north and south poles aligned in the horizontal direction, as is depicted in 
figures 1.4 and 1.5.
S
N
(a)  Outcome of experiment
N
S
(b)  Diagram of electron
Figure 1.2
Electron with spin N in the vertical direction.
(a)  Outcome of experiment
(b)  Diagram of electron
S
N
S
N
Figure 1.3
Electron with spin S in the vertical direction.
Spin 
5
In the following chapters we will want to rotate the magnets through 
various angles. We will measure angles in the clockwise direction with 0° 
denoting the upward vertical direction and θ measuring the angle from the 
upward vertical. Figure 1.6 depicts an electron with spin N in the direction 
of a general angle θ°.
Sometimes spin is described as being up, down, left, or right. Our descrip­
tion of an electron being N in the direction 0° seems somewhat cumber­
some, but it is unambiguous and avoids some of the pitfalls of using up, 
down, and so on, especially when we rotate the apparatus through 180°. For 
example, both of the situations pictured in figure 1.7 represent an electron 
having spin N in direction 0° or equivalently spin S in direction 180°.
(a)  Outcome of experiment
(b)  Diagram of electron
S
N
N
S
Figure 1.4
Electron with spin N in the 90° direction.
(a)  Outcome of experiment
(b)  Diagram of electron
S
N
S
N
Figure 1.5
Electron with spin S in the 90° direction.
N
S
Figure 1.6
Electron with spin N in the θ° direction.
6 
Chapter 1
Before we proceed with our study of electron spin, we will pause and 
look at an analogy that we will use in several places.
The Quantum Clock
Imagine that you have a clock with a dial marked with hours in the stan­
dard positions. It also has a hand. You are, however, forbidden to look at 
the face of the clock. You can only ask it questions. You want to know in 
which direction the hand is pointing, so you would like to ask the clock 
this seemingly simple question. But it is not allowed. You are only allowed 
to ask whether the hand is pointing at a particular number on the face. So, 
for example, you can ask if the hand is pointing to twelve, or you can ask 
if it is pointing to four. Now, if this were a regular clock you would have to 
be extremely lucky to get a yes answer. Most of the time the hand would 
be pointing in a completely different direction. But the quantum clock is 
not like a regular clock. It either answers yes or it tells you that the hand is 
pointing in the direction exactly opposite the one you asked about. If we 
ask if the hand is pointing in the direction of twelve, it will tell us either 
that it is or that it is pointing in the direction of six. If we ask if the hand is 
pointing in the direction of four, it will either tell us it is, or that it is point­
ing in the direction of ten. This is a very curious state of affairs, but it is 
exactly analogous to electron spin.
As we said, electron spin is going to be the idea that motivates the defi­
nition of the qubit. If we are going to do computations, we need to under­
stand the rules that govern spin measurements. We start by considering 
what happens when we measure more than once.
(a)  Outcome of experiment
(b)  Outcome of experiment
(c)  Diagram of electron
S
N
S
N
N
S
Figure 1.7
Electron with spin N in the 0° direction.
Spin 
7
Measurements in the Same Direction
Measurements are repeatable. If we repeat exactly the same measurement, 
we get exactly the same result. For example, suppose that we decide to 
measure an electron’s spin in the vertical direction. We then repeat exactly 
the same experiment by positioning two more sets of our apparatus behind 
the first one. One is positioned in exactly the right place to catch elec­
trons that are deflected upward by the first apparatus. The other is placed 
to catch the electrons deflected downward. The electrons that are deflected 
upward by the first apparatus are deflected up by the second, and the ones 
deflected down by the first apparatus are deflected down by the second. 
This means that electrons measured to have spin N in direction 0° initially 
also have spin N in direction 0° when we repeat the experiment. Similarly, if 
an electron is initially measured to have spin S in direction 0° and we repeat 
exactly the same experiment, it will still have spin S in direction 0°. For our 
clock analogy, if we repeatedly ask if the hand is pointing at twelve, we will 
repeatedly get the same answer: that either it is always pointing toward 
twelve or it is always pointing toward six.
There is, of course, nothing special about the vertical direction. If we 
start by measuring in direction θ°, and then repeatedly measure in the same 
direction, we will obtain the exactly the same result each time. We will end 
up with a string of letters consisting entirely of Ns or one entirely of Ss.
The next thing to consider is what happens if we don’t repeat the same 
measurement. For example, what happens if we first measure vertically and 
then horizontally?
Measurements in Different Directions
We will measure the electron’s spin first in the vertical direction, then in 
the horizontal direction. We will send a stream of electrons through the 
first detector—measuring spin in the vertical direction. As before, we have 
two more detectors behind the first one in the appropriate positions to 
catch the electrons coming from the first detector. The difference is that 
these two detectors are rotated through 90° and measure spin in the hori­
zontal direction.
First we look at the stream of electrons that are deflected upward by the 
first detector—these have spin N in direction 0°. When they go through the 
8 
Chapter 1
second detector, we find that half of them have spin N and half have spin S 
in direction 90°. The sequence of north and south spins in direction 90° is 
completely random. There is no way of telling whether an electron that had 
spin N in direction 0° will have either spin S or N when we measure it again 
in direction 90°. Similar results hold for the electrons that the first detector 
shows have spin S in the vertical direction—exactly half have spin N in the 
horizontal direction, and the other half have spin S in the horizontal direc­
tion. Again, the sequence of Ns and Ss is completely random.
The analogous questions for our clock are asking about whether the 
hand is pointing in the direction of twelve and then asking if it is pointing 
in the direction of three. If we have a large number of clocks and ask them 
these two questions, the answers to the second questions will be random. 
Half of the clocks will say the hand is pointing in the direction of three. The 
other half will say in the direction of nine. The answers to the first question 
have no bearing on the answers to the second question.
Finally, we will look at what happens when we make three measure­
ments. First we measure vertically, then horizontally, and then vertically 
once more. Consider a stream of electrons coming from the first detector 
that have spin N in direction 0°. We know that half of them will have 
spin N and half have spin S when we measure spin in direction 90°. We 
will restrict attention to the stream that corresponds to N for the first two 
measurements and then, for the third measurement, measure spin in the 
vertical direction. We find that exactly half of these electrons have spin N 
in direction 0° and half have spin S. Once more the sequence of Ns and Ss 
is completely random. The fact that the electrons initially had spin N in the 
vertical direction has no bearing on whether or not they have will still have 
spin N when we again measure in the vertical direction.
What conclusions can we draw from these results? There are three. And 
they are all important.
First, if we keep repeating exactly the same question we get exactly the 
same answer. This tells us that sometimes there are definite answers. We are 
not getting random answers to every question.
Second, randomness does seem to occur. If we ask a sequence of ques­
tions, the final results can be random.
Third, measurements affect outcomes. We saw that if we ask the same 
question three times, we get the exactly the same answer three times. But 
if the first and third questions are identical and the second is different, the 
Spin 
9
answers to the first and third questions need not be the same. For example, 
if we ask three times in a row if the hand is pointing toward twelve, we 
will get exactly the same answer each time, but if we ask first if it is point­
ing toward twelve, then whether it is pointing to three, and finally again 
whether it is pointing toward twelve, the answers to the first and third ques­
tion need not be the same. The only difference between the two scenarios 
is the second question, so that question must be affecting the outcome of 
the following question. We will say a little more about these observations, 
starting with measurements.
Measurements
In classical mechanics, we might consider the path of a ball thrown into the 
air. The path can be calculated using calculus, but in order to perform the 
calculation we need to know certain quantities such as the mass of the ball 
and its initial velocity. How we measure these is not part of the theory. We 
just assume that they are known. The implicit assumption is that the act 
of measuring is not important to the problem—that taking a measurement 
does not affect the system being modeled. For the example of a ball being 
thrown into the air, this makes sense. We can measure its initial velocity 
using a radar gun, for example. This involves bouncing photons off the 
ball and, though bouncing photons will have an effect on the ball, it is 
negligible. This is the philosophy underlying classical mechanics: Measure­
ments will affect the objects being studied, but experiments can be designed 
so the effect of measurement is negligible and so can consequently be 
ignored.
In quantum mechanics, we are often considering tiny particles like 
atoms or electrons. Here bouncing photons off them has an effect that is no 
longer negligible. In order to perform some measurement, we have to inter­
act with the system. These interactions are going to perturb our system, so 
we can no longer ignore them. It should not seem surprising that measure­
ment becomes a basic component of the theory, but what is surprising is 
how this is done. For example, consider the case where we measure the spin 
of an electron first in the vertical direction and then in the horizontal one. 
We have seen that exactly half of the electrons that have spin N in direction 
0° after passing through the first detector will have spin N in direction 90° 
when measured by the second detector. It might seem that the strength of 
10 
Chapter 1
the magnets might be having some effect on the outcome, perhaps they 
are so strong that they are causing the magnetic axes of the electrons to 
twist to align with the magnetic field of the measuring device, and that if 
we had weaker magnets the twisting would be lessened and we might get 
a different result. However, this is not how measurement is incorporated 
into the theory. As we shall see, our model does not take into account the 
“strength” of the measurement. Rather, it is the actual process of taking the 
measurement, however it is done, that has an effect on the system. Later we 
will describe the mathematics that models how measuring spin is treated 
in quantum mechanics. Each time a measurement is made, we will see that 
the system is changed in certain prescribed ways; these prescribed ways 
depend on the type of measurement being made but not on the strength 
of the measurement.
Incorporating measurements into the theory is one on the differences 
between classical and quantum mechanics. Another difference concerns 
randomness.
Randomness
Quantum mechanics involves randomness. For example, if we first measure 
the spin of a stream of electrons in the vertical direction, then in the hori­
zontal direction, and record the results from the second measuring device, 
we will obtain a string of Ns and Ss. This sequence of spins is completely 
random. For example, it might look something like NSSNNNSS. …
The classical experiment for generating a random sequence of two sym­
bols each associated with probability of a half is that of tossing a fair coin. If 
we toss a fair coin we might get a sequence HTTHHHTT. …  Although these 
two examples yield similar results, there is a big difference in how random­
ness is interpreted in the two theories.
Tossing a coin is something that is described by classical mechanics. It 
can be modeled using calculus. To compute whether the coins lands heads 
or tails up, you need first to carefully measure the initial conditions: the 
weight of the coin, the height above the ground, the force of the impact 
of the thumb on the coin, the exact location on the coin where the thumb 
hits, the position of the coin, and so forth and so on. Given all of these 
values exactly, the theory will tell us which way up the coin lands. There 
is no actual randomness involved. Tossing a coin seems random because 
Spin 
11
each time we do it the initial conditions vary slightly. These slight varia­
tions can change the outcome from heads to tails and vice versa. There is 
no real randomness in classical mechanics, just what is often called sensitive 
dependence to initial conditions—a small change in the input can get ampli­
fied and produce an entirely different outcome. The underlying idea con­
cerning randomness in quantum mechanics is different. The randomness 
is true randomness.
The sequence NSSNNNSS … that we obtained from measuring spin in two 
directions is considered to be truly random, as we shall see. The sequence 
of coin tosses, HTTHHHTT … appears random, but the classical laws of 
physics are deterministic and this apparent randomness would disappear if 
we could make our measurements with infinite accuracy.
At this stage it is natural to question this. Einstein certainly did not like 
this interpretation, famously saying that God does not play dice. Couldn’t 
there be a deeper theory? If we knew more information about the initial 
configurations of our electrons, couldn’t it be the case that the final results 
would no longer be random but completely determined? Couldn’t there be 
hidden variables—once we know the values of these variables, the apparent 
randomness disappears? In what follows we will present the mathemati­
cal theory in which true randomness is used. Later we will return to these 
questions. We will describe a clever experiment to distinguish between the 
hidden variable and the true randomness hypotheses. This experiment has 
been performed several times. The outcomes have always shown that the 
randomness is real and that there is no simple hidden variable theory that 
can eliminate it.
We started this chapter by saying that a qubit can be represented by the 
spin of an electron or the polarization of a photon. We will show how the 
models for spin and polarization are related.
Photons and Polarization
It is often said that we are not aware of the strange quantum phenom­
ena because they only occur at incredibly small scales and are not appar­
ent at the scales of our everyday life. There is some truth to this, but 
there is an experiment that is completely analogous to measuring spin 
of electrons that can be performed with very little apparatus. It concerns 
polarized light.
12 
Chapter 1
To perform these experiments you need three squares of linear polar­
ized film. Start by taking two of the squares and putting one in front of 
the other. Keep one square fixed and rotate the other by ninety degrees. 
You will find that light passes through the pair of filters when they are 
aligned in one direction, but is completely blocked when one of the filters 
is rotated by ninety degrees. This is not particularly exciting. But now rotate 
the two filters so that no light passes through, take the third filter, rotate it 
by forty-five degrees, and slide it between the other two. Amazingly, light 
passes through the region where the three filters overlap—no light passes 
through the overlap of just the original two filters, but it does where all 
three overlap.
I heard about this experiment with three filters several years ago. I asked 
a friend who is a physicist if he had any polarized sheet. He invited me to 
his lab, where he had an enormous roll of it. He cut a piece off and gave 
it to me. I used scissors to cut it into three squares of about an inch by an 
inch and performed the experiment—and it worked! This experiment is so 
simple and yet so surprising. I have kept the three squares in my wallet ever 
since.
When we measure polarization we find that photons are polarized in 
two perpendicular directions, both of which are perpendicular to the direc­
tion of travel of the photon. The polarized square lets through photons 
that are polarized in one of the two directions and absorb the photons that 
are polarized in the other. The polarized squares correspond to the Stern-
Gerlach apparatus. Sending light through a square can be considered mak­
ing a measurement. As with spin, there are two possible outcomes: Either 
the direction of polarization is directly aligned with the orientation of the 
square, in which case the photon passes through, or the direction of polar­
ization is perpendicular to the orientation of the square, in which case the 
photon is absorbed.
We start by assuming that our square has vertical orientation so that it 
lets through photons with vertical polarization and absorbs the ones with 
horizontal polarization, and consider a number of experiments that corre­
spond to the ones we described for electron spin.
First, suppose that we have two squares, both with the same orienta­
tion, so they both let through photons with vertical polarization. If we 
look at the squares individually they look gray, as is expected. They are 
both absorbing some photons—those with horizontal polarization. If we 
Spin 
13
then slide one of the squares over the other, there is minimal change. The 
amount of light let through the two overlapping squares is about the same 
as the amount that comes through each square when they are not overlap­
ping. This is depicted in figure 1.8.
We will now rotate one of the squares through ninety degrees. Assuming 
we are not looking at light reflecting off a shiny surface, or light coming 
directly from a computer screen, but we are in normal light conditions, the 
proportion of horizontally polarized photons is equal to the proportion 
of vertically polarized ones, and both squares will look equally gray. We 
repeat the experiment of overlapping these squares. This time no light is let 
through the region of overlap, as depicted in figure 1.9.
The third experiment is to take the third sheet and rotate it through 
forty-five degrees. Under normal light conditions nothing appears to hap­
pen as we rotate the square. It maintains the same shade of gray. We now 
slide this square between the other two squares, one of which has verti­
cal orientation, and the other has horizontal orientation. The result, as we 
noted earlier, is both surprising and unintuitive. Some light comes through 
the region of overlap of all three squares. (This is depicted in figure 1.10.) 
These polarized squares are sometimes called filters, but clearly they are 
(a)  Two polarized sheets
(b)  Slightly overlapping
(c)  Fully overlapping
Figure 1.8
Two linear polarized squares with the same orientation.
(a)  Two polarized sheets
(b)  Slightly overlapping
(c)  Fully overlapping
Figure 1.9
Two linear polarized squares with different orientations.
14 
Chapter 1
not acting in the conventional ways that filters work. More light comes 
through three filters than comes through two!
We will give a brief description of what is happening. Later we will see 
the mathematical model that describes both spin and polarization.
Recall our quantum clock. We can ask if the hand is pointing at twelve, 
or we can ask if the hand is pointing at six. The information we gain from 
either question tells us which of the numbers 12 or 6 the hand is pointing 
to, but the Yes/No answers are reversed. For the polarized squares the analo­
gous questions are asked by rotating the square by ninety degrees—not one 
hundred and eighty. The information we obtain is the same. The difference 
is that if the answer is yes, the photon passes through the filter and we can 
perform more measurements on it, but if the answer is no, the filter absorbs 
the photon, so we cannot ask it further questions.
The first two experiments involved just two sheets and are telling us 
exactly the same thing: When we repeat a measurement, we get the same 
result. In both experiments we are measuring the polarization in the verti­
cal and horizontal directions two times. In these experiments, the photons 
that pass through the first filter have vertical orientations. The first experi­
ment, where the second filter also has vertical orientation, we are asking 
the question, “Is the photon vertically polarized?” twice and we receive 
the answer “Yes” twice. In the second experiment, the second question is 
changed to “Is the photon horizontally polarized?” and receives the answer 
“No.” Both experiments give us the same information, but the negative 
answer for the second question in the second experiment means that the 
photon is absorbed and so, unlike the first experiment, it is not available 
for further questioning.
In the third experiment, the filter that has been rotated through forty-
five degrees is now measuring the polarization at angles of 45° and 135°. We 
Figure 1.10
Three linear polarized squares with different orientations.
Spin 
15
know that the photons coming through the first filter are polarized verti­
cally. When measured by the second filter, half of the photons are found to 
be polarized in the 45° direction and half in the 135° directions. The ones 
with 45° polarization pass through the filter, and the others are absorbed. 
The third filter again measures the polarization in the vertical and hori­
zontal directions. The photons entering have 45° polarization, and when 
measured in the vertical and horizontal directions, half will have vertical 
polarization and half will have horizontal polarization. The filter absorbs 
the vertically polarized photons and lets through those that are polarized 
horizontally.
Conclusions
We started this chapter by saying that classical bits can be represented by 
everyday objects like switches in the on or off position, but that qubits are 
generally represented by the spin of electrons or the polarization of pho­
tons. Spin and polarization are not nearly so familiar to us and have proper­
ties that are quite unlike their classical counterparts.
To measure spin, you first have to choose a direction and then measure 
it in that direction. Spin is quantized: When measured, it gives just two pos­
sible answers—not a continuous range of answers. We can assign classical 
bits to these results. For example, if we obtain an N we can consider it to be 
the binary digit 0, and if we obtain an S we can consider it to be the binary 
digit 1. This is exactly how we get answers from a quantum computation. 
The last stage of the computation is to take a measurement. The result will 
be one of two things, which will be interpreted as either 0 or 1. Although 
the actual computation will involve qubits, the final answer will be in terms 
of classical bits.
We have only just started our study, so we are quite limited in what we 
can do. We can, however, generate random strings of binary digits. The 
experiment that generated random strings of Ns and Ss can be rewritten 
as a string of 0s and 1s. Consequently measuring spins of electrons first in 
the vertical and then in the horizontal direction gives a random string of 
0s and 1s. This is probably the simplest thing that we can do with qubits, 
but surprisingly this is something that cannot be done with a classical 
computer. Classical computers are deterministic. They can compute strings 
that pass various tests for randomness, but these are pseudorandom, not 
16 
Chapter 1
random. They are computed by some deterministic function, and if you 
know the function and the initial seed input, you can calculate exactly the 
same string. There are no classical computer algorithms that generate truly 
random strings. Thus, already we can see that quantum computations have 
some advantages over classical ones.
Before we start to describe other quantum computations we need to 
develop a precise mathematical model that describes what happens when 
we measure spin in various directions. This is started in the next chapter 
where we study linear algebra—the study of the algebra associated with 
vectors.
2  Linear Algebra
Chapter 
2
Linear 
Algebra
© Massachusetts Institute of TechnologyAll Rights Reserved
Quantum mechanics is based on linear algebra. The general theory uses 
infinite dimensional vector spaces. Fortunately for us, to describe spin or 
polarization we need only finite dimensions, which makes things much 
easier. In fact, we need only a few tools. At the end of this chapter I have 
given a list. The rest of the chapter explains how to use these tools and what 
the calculations mean. There are many examples. It is important to work 
carefully through all of them. The mathematics introduced here is essential 
to everything that follows. Like much mathematics, it can seem compli­
cated when it is first introduced, but it becomes almost second nature with 
practice. The actual computations only involve addition and multiplica­
tion of numbers, along with an occasional square root and trigonometric 
function.
We will be using Paul Dirac’s notation. Dirac was one of the founders 
of quantum mechanics, and his notation is used extensively through­
out both quantum mechanics and quantum computing. It is not widely 
used outside these disciplines, which is surprising given how elegant and 
useful it is.
But first, we begin with a brief description of the numbers we will be 
using. These are real numbers—the standard decimal numbers with which 
we are all familiar. Practically every other book on quantum computation 
uses complex numbers—these involve the square root of negative one. So, 
let’s start by explaining why we are not going to be using them.
Complex Numbers versus Real Numbers
Real numbers are straightforward to use. Complex numbers are—well, more 
complicated. To talk about these numbers we would have to talk about 
18 
Chapter 2
their moduli and explain why we have to take conjugates. For what we are 
going to do, complex numbers are not needed and would only add another 
layer of difficulty. Why then, you ask, do all the other books use complex 
numbers? What can you do with complex numbers that you cannot do 
with real ones? Let’s briefly address these questions.
Recall that we measured the spin of an electron at various angles. These 
angles are all in one plane, but we live in a three-dimensional world. We 
compared measuring spin to using our quantum clock. We could only ask 
about directions given by the hand moving around the two-dimensional 
face. If we move to three dimensions, our analog would not be a clock 
face, but a globe with the hand at its center pointing to locations on the 
surface. We could ask, for example, if the hand is pointing to New York. 
The answer would be either that it is, or that it is pointing to the point 
diametrically opposite New York. The mathematical model for spin in 
three dimensions uses complex numbers. The computations involv­
ing qubits that we will look at, however, need to measure spin in only 
two dimensions. So, though our description using real numbers is not 
quite as encompassing as that using complex numbers, it is all that we 
need.
Finally, complex numbers provide an elegant way of connecting trigo­
nometric and exponential functions. At the very end of the book we will 
look at Shor’s algorithm. This would be hard to explain without using 
complex numbers. But this algorithm also needs continued fractions, 
along with results from number theory and results about the speed of an 
algorithm for determining whether a number is prime. There would be a 
significant jump in the level of mathematical sophistication and knowl­
edge needed if we were to describe Shor’s algorithm in full detail. Instead 
we will describe the basic ideas that underlie the algorithm, indicat­
ing how these fit together. Once again, our description will use only real 
numbers.
So, for what we are going to do, complex numbers are not needed. If, 
however, after reading this book, you want to continue studying quantum 
computation, they will be needed for more advanced topics.
Now that we have explained why we are going to stay with the real num­
bers, we begin our study of vectors and matrices.
Linear Algebra 
19
Vectors
A vector is just a list of numbers. The dimension of the vector is the number 
of numbers in the list. If the lists are written vertically, we call them column 
vectors or kets. If the lists are written horizontally, we call them row vectors 
or bras. The numbers that make up a vector are often called entries. To illus­
trate, here is a three-dimensional ket and a four-dimensional bra:
2
0 5
3
1
0
23
.
,
−










−
[
]
π
.
The names bra and ket come from Paul Dirac. He also introduced nota­
tion for naming these two types of vectors: a ket with name v is denoted by 
v ; a bra with name w is denoted by w . So we might write
v =
−










2
0 5
3
.
 and w =
−
[
]
1
0
23
π
.
Later we will see why we use two different symbols to surround the 
name, and the reason that tells us which side the angled bracket goes. But, 
for now, the important thing is to remember that kets refer to columns 
(think of the repeated “k” sound) and that bras, as usual, have their entries 
arranged horizontally.
Diagrams of Vectors
Vectors in two or three dimensions can be pictured as arrows. We will look
at an example using a = 



3
1 . (In what follows we will often use kets for 
our examples, but if you like you can replace them with bras.) The first 
entry, 3 in this example, gives the change in the x-coordinate from the ini­
tial point to the terminal point. The second entry gives the change in the 
y-coordinate going from the initial point to terminal point. We can draw 
this vector with any initial point—if we choose (a, b) as the coordinates of 
its initial point, then the coordinates of its terminal point will be at (a+3, 
b+1). Notice that if the initial point is drawn at the origin, the terminal 
point has coordinates given by the entries of the vector. This is convenient, 
20 
Chapter 2
and we will often draw them in this position. Figure 2.1 shows the same ket 
drawn with different initial points.
Lengths of Vectors
The length of a vector is, as might be expected, the distance from its initial 
point to its terminal point. This is the square root of the sum of squares 
of the entries. (This comes from the Pythagorean theorem.) We denote the 
length of a ket a  by a , so for a = 



3
1  we have a =
+
=
3
1
10
2
2
.
More generally, if a
a
a
an
=












1
2

, then a
a
a
an
=
+
+…+
1
2
2
2
2 .
Vectors of length 1 are called unit vectors. Later we will see that qubits 
are represented by unit vectors.
x
1
2
3
4
y
1
2
3
4
a =
3
1
a =
3
1
a =
3
1
Figure 2.1
Same ket drawn in different positions.
Linear Algebra 
21
Scalar Multiplication
We can multiply a vector by a number. (In linear algebra, numbers are often 
called scalars. Scalar multiplication just refers to multiplying by a number.) 
We do this by multiplying each of the entries by the given number. For 
example, multiplying the ket a
a
a
an
=












1
2

 by the number c gives c
ca
ca
ca
a
n
=












1
2

.
It is straightforward to check that multiplying a vector by a positive 
number c multiplies its length by a factor of c. We can use this fact to enable 
us to get vectors of different lengths pointing in the same direction. In 
particular, we will often want to have a unit vector pointing in the direc­
tion given by a non–unit vector. Given any non-zero vector a , its length 
is a . If we multiply a  by the reciprocal of its length, we obtain a unit
vector. For example, as we have already seen, if a = 



3
1  then a =
10. 
If we let 
u =



=












1
10
3
1
3
10
1
10
,
then 
u =




+ 



=
+
=
=
3
10
1
10
9
10
1
10
1
1
2
2
.
Consequently, u  is a unit vector that points in the same direction as a .
Vector Addition
Given two vectors that have the same type—they are both bras or both 
kets—and they have the same dimension, we can add them to get a new 
vector of the same type and dimension. The first entry of this vector just 
comes from adding the first entries of the two vectors, the second entry 
22 
Chapter 2
from adding the two second entries, and so on. For example, if a
a
a
an
=












1
2
  
and b
b
b
bn
=












1
2
 , then a
b
a
b
a
b
a
b
n
n
+
=
+
+
+












1
1
2
2

.
Vector addition can be pictured by what is often called the parallelo­
gram law for vector addition. If the vector b  is drawn so that its initial 
point is at the terminal point of a , then the vector that goes from the 
initial point of a  to the terminal point of b  is a
b
+
. This can be drawn 
giving a triangle.
We can interchange the roles of a  and b , drawing the initial point of 
a  at the terminal point of b . The vector that goes from the initial point 
of b  to the terminal point of a  is b
a
+
. Again, this gives a triangle. But 
we know that a
b
b
a
+
+
=
. So if we draw the triangle construction for 
a
b
+
 and b
a
+
 where both the vectors have the same initial and terminal 
points, the two triangles connect to give us a parallelogram with the diago­
nal representing both a
b
+
 and b
a
+
. Figure 2.2 illustrates this where 
a = 



3
1 , b = 



1
2 , and consequently a
b
b
a
+
+
=
 = 4
3



.
x
1
2
3
4
y
1
2
3
4
a
b
a
b
Figure 2.2
Parallelogram law for vector addition.
Linear Algebra 
23
Orthogonal Vectors
Figure 2.2 helps us visualize some basic properties of vector addition. One 
of the most important comes from the Pythagorean theorem. We know 
that if a, b, and c represent the lengths of the three sides of a triangle, 
then a
b
c
2
2
2
+
=
 if and only if the triangle is a right triangle. The picture 
then tells us that two vectors a  and b  are perpendicular if and only if 
a
b
a
b
2
2
2
+
=
+
.
The word orthogonal means exactly the same thing as perpendicular, 
and it is the word that is usually used in linear algebra. We can restate 
our observation: Two vectors a  and b  are orthogonal if and only if 
a
b
a
b
2
2
2
+
=
+
.
Multiplying a Bra by a Ket
If we have a bra and a ket of the same dimension, we can multiply them—
the bra on the left and the ket on the right—to obtain a number. This is 
done in the following way, where we suppose that both a  and b  are 
n-dimensional:
a
a
a
an
= [
]
1
2

  and 
b
b
b
bn
=












1
2
 .
We use concatenation to denote the product. This just means that we 
write down the terms side by side with no symbol between them. So the 
product is written a b . By squeezing the symbols even closer the vertical 
lines coincide and we get a b , which is the notation we will use. The defi­
nition of the bra-ket product is
a b
a
a
a
b
b
b
a b
a b
a b
n
n
n
n
= [
]












=
+
+
+
1
2
1
2
1 1
2
2



.
The vertical lines of the bras and kets are “pushed together,” which helps 
us to remember that the bra has the vertical line on the right side and the 
ket has it on the left. The result consists of terms sandwiched between angle 
brackets. The names “bra” and “ket” come from “bracket,” which is almost 
24 
Chapter 2
the concatenation of the two names. Though this is a rather weak play on 
words, it does help us to remember that, for this product, that the “bra” is 
to the left of the “ket.”
In linear algebra this product is often called the inner product or the dot 
product, but the bra-ket notation is the one used in quantum mechanics, 
and it is the one that we will use throughout the book.
Now that we have defined the bra-ket product, let’s see what we can do 
with it. We start by revisiting lengths.
Bra-kets and Lengths
If we have a ket denoted by a , then the bra a  with the same name is 
defined in the obvious way. They both have exactly the same entries, but 
for a  they are arranged vertically, and for a  horizontally.
a
a
a
a
a
a
a
a
n
n
=












= [
]
1
2
1
2


.
Consequently, a a
a
a
an
=
+
+ … +
1
2
2
2
2, and so the length of a  can be 
written succinctly as a
a a
=
.
To illustrate, we return to the example where we found the length of 
a = 



3
1 : a a = [
]


=
+
=
3
1
3
1
3
1
10
2
2
. Then we take the square root to 
obtain a =
10.
Unit vectors are going to become very important in our study. To see 
whether a vector is unit—has length 1—we will repeatedly use the fact that 
a ket a  is a unit vector if and only if a a = 1.
Another important concept is orthogonality. The bra-ket product can 
also tell us when two vectors are orthogonal.
Bra-kets and Orthogonality
The key result is: Two kets a  and b  are orthogonal if and only if 
a b = 0. We will look at a couple of examples and then give an explanation 
of why this result is true.
Linear Algebra 
25
Let a = 



3
1 , b = 



1
2  and c = −




2
6
. We calculate a b  and a c .
a b = [
]


=
+
=
3
1
1
2
3
2
5
a c = [
]
−



= −
+
=
3
1
2
6
6
6
0
Since a b ≠0, we know that a  and b  are not orthogonal. Since 
a c = 0, we know that a  and c  are orthogonal.
Why does this work? Here is an explanation for two-dimensional kets.
Let a
a
a
= 



1
2
 and b
b
b
= 



1
2
, then a
b
a
b
a
b
+
=
+
+




1
1
2
2
. We calculate the 
square of the length of a
b
+
.
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
a
+
=
+
+
[
]
+
+




=
+
(
) +
+
(
)
=
+
2
1
1
2
2
1
1
2
2
1
1
2
2
2
2
1
2
2
1 1
1
2
2
2
2
2
2
2
1
2
2
2
1
2
2
2
1 1
2
2
2
2
b
b
a
a b
b
a
a
b
b
a b
a b
+
(
) +
+
+
(
)
=
+
(
) +
+
(
) +
+
(
)
=
+
+
a
b
a b
2
2
2
Clearly this number equals a
b
2
2
+
 if and only if 2
0
a b =
. Now 
recall our observation that two vectors a  and b  are orthogonal if and 
only if a
b
a
b
2
2
2
+
=
+
. We can restate this observation using our cal­
culation for the square of the length of a
b
+
 to say: Two vectors a  and 
b  are orthogonal if and only if a b = 0.
Though we have shown this for two-dimensional kets, the same argu­
ment can be extended to kets of any size.
Orthonormal Bases
The word “orthonormal” has two parts; ortho from orthogonal, and nor­
mal from normalized which, in this instance, means unit. If we are working 
with two-dimensional kets, an orthonormal basis will consist of a set of two 
unit kets that are orthogonal to one another. In general, if we are working 
with n-dimensional kets, an orthonormal basis consists of a set of n unit 
kets that are mutually orthogonal to one another.
26 
Chapter 2
We begin by looking at two-dimensional kets. The set of all two-
dimensional vectors is denoted by 2. An orthonormal basis for 2 consists 
of a set containing two unit vectors b1  and b2  that are orthogonal. So, 
given a pair of kets, to check whether they form an orthonormal basis, we 
must check first to see if they are unit vectors, and then check whether they 
are orthogonal. We can check both of these conditions using bra-kets. We 
need b b
1
1
1
= , b
b
2
2
1
=
, and b b
1
2
0
=
.
The standard example, which is called the standard basis, is to take 
b1
1
0
= 


 and b2
0
1
= 


. It is straightforward to check that the two bra-ket 
properties are satisfied. While 
1
0
0
1










,
 is a particularly easy basis to find, 
there are infinitely many other possible choices. Two of these are
1
2
1
2
1
2
1
2
−




































,
 and 
1
2
3
2
3
2
1
2












−
























,
.
In the last chapter we considered measuring the spin of a particle. We 
looked at spin measured in the vertical direction and in the horizontal 
direction. The mathematical model for measuring spin in the vertical direc­
tion will be given using the standard basis. Rotating the measuring appara­
tus will be described mathematically by choosing a new orthonormal basis. 
The three two-dimensional bases* that we have listed will all have impor­
tant interpretations concerning spin, so instead of naming the vectors in 
the bases with letters we will use arrows, with the direction of the arrow 
related to the direction of spin. Here are the names we are going to use:
↑= 



↓= 



→=
−












1
0
0
1
1
2
1
2
,
,
, ←=












1
2
1
2
, ↗=
−












1
2
3
2
, 
and ↙=












3
2
1
2
.
*  Note that the word bases is the plural of basis and of base. The word is pronounced 
differently depending on what the singular term is. We will always be using it for 
the plural of basis. In this case, it is pronounced “BAY-sees.”
Linear Algebra 
27
Our three bases can be written more succinctly as 
↑
↓
{
}
,
, 
→
←
{
}
,
 
and ↗
↙
,
.
{
}  Since these are orthonormal, we have the following bra-ket 
values.
↑↑=
↓↓=
↑↓=
↓↑=
→→=
←←=
→←=
←→=
=
=
=
=
1
1
0
0
1
1
0
0
1
1
0
0
↗↗
↙↙
↗↙
↙↗
Vectors as Linear Combinations of Basis Vectors
Given a ket and an orthonormal basis, we can express the ket as a weighted 
sum of the basis vectors. Although at this stage it is not clear that this is use­
ful, we will see later that this is one of the basic ideas on which our math­
ematical model is based. We start by looking at two-dimensional examples.
Any vector v  in 2 can be written as a multiple of ↑ plus a multiple 
of ↓. This is equivalent to the rather obvious fact that for any numbers c 
and d the equation
c
d
x
x



=



+




1
2
1
0
0
1
has a solution. Clearly, this has a solution of x
c
1 =
 and x
d
2 =
, and this is 
the only solution.
Can any vector v  in 2 be written as a multiple of → plus a multiple 
of ←? Equivalently, does the following equation have a solution for any 
numbers c and d?
c
d
x
x



=
→+
←
1
2
.
How do we solve this? We could replace the kets with their two-
dimensional column vectors and then solve the resulting system of two 
linear equations in two unknowns. But there is a far easier way of doing 
this using bras and kets.
First, take the equation and multiply both sides on the left by the bra → 
This gives us the following equation.
→


= →
→+
←
(
)
c
d
x
x
1
2
Next, distribute the terms on the right side of the equation.
28 
Chapter 2
→


=
→→+
→←
c
d
x
x
1
2
We know both of the bra-kets on the right side. The first is 1. The second 
is 0. This immediately tells us that x1 is equal to →



|
.
c
d
 So, we just need 
to evaluate this product.
→


=
−





= (
) −(
)
=
−
(
)
|
/
/
/
/
/
c
d
c
d
c
d
c
d
1
2
1
2
1
2
1
2
2 .
Consequently, x
c
d
1
2
=
−
(
) /
.
We can use exactly the same method to find x2. We start with the same 
initial equation c
d
x
x



=
→+
←
1
2
 and multiply both sides on the left by 
the bra ←|.
←


=
←→+
←←=
+
| c
d
x
x
x
x
1
2
1
2
0
1.
So, x
c
d
c
d
c
d
2
1
2
1
2
1
2
1
2
2
= 




= (
) +(
)
=
+
(
)
/
/
/
/
/
.
This means that we can write
c
d
c
d
c
d



=
−
(
) →+
+
(
) ←
2
2
.
The sum on the right consists of multiplying the basis vectors by cer­
tain scalars and then adding the resulting vectors. I described it earlier as 
a weighted sum of the basis vectors, but you have to be careful with this 
interpretation. There is no reason for the scalars to be positive. They can be 
negative. In our example, if c were to equal –3 and d were to equal 1, both of 
the weights, c
d
−
(
) /
2  and c
d
+
(
) /
2, would be negative. For this reason 
the term linear combination of the basis vectors is used instead of weighted 
sum.
Now let’s move to n dimensions. Suppose that we are given an 
n-dimensional ket | v  and an orthonormal basis 
b
b
bn
1
2
,
,
,

{
}. Can we 
write | v  as a linear combination of the basis vectors? If so, is there a unique 
way of doing this? Equivalently, does the equation
Linear Algebra 
29
v
x b
x b
x b
x b
i
i
n
n
=
+
+
+
+
+
1
1
2
2


have a unique solution? Again, the answer is yes. To see this we will show 
how to find the value for xi. The calculation follows exactly the same 
method we used in two dimensions. Start by multiplying both sides of 
the equation by bi . We know that b b
i
k  equals 0 if i
k
≠
 and equals 1 if 
i
k
=
. So, after multiplying by the bra, the right side simplifies to just xi, 
and we obtain that b
v
x
i
i
|
=
. This tells us that x
b
v
1
1
=
|
, x
b
v
2
2
=
|
, etc. 
Consequently, we can write | v  as a linear combination of the basis vectors:
v
b v b
b v b
b v b
b v b
i
i
n
n
=
+
+
+
+
+
1
1
2
2


At this stage, this all seems somewhat abstract, but it will all become 
clear in the next chapter. Different orthonormal bases correspond to choos­
ing different orientations to measure spin. The numbers given by the bra-
kets like b
v
i |
 are called probability amplitudes. The square of b
v
i |
 will 
give us the probability of v  jumping to bi  when we measure it. This will 
all be explained, but understanding the equation written above is crucial 
to what follows.
Ordered Bases
An ordered basis is a basis in which the vectors have been given an order, 
that is, there is a first vector, a second vector, and so on. If b
b
bn
1
2
,
,
,

{
} 
is a basis, we will denote the ordered basis by b
b
bn
1
2
,
,
,

(
)—we change 
the brackets from curly to round. For an example, we will look at 2. Recall 
that the standard basis is ↑
↓
{
}
,
. Two sets are equal if they have the same 
elements—the order of the elements does not matter, so ↑
↓
{
} =
↓
↑
{
}
,
,
. 
The two sets are identical.
However, for an ordered basis the order the basis vectors are given mat­
ters. ↑
↓
(
) ≠↓
↑
(
)
,
,
. The first vector in the ordered basis on the left is not 
equal to the first vector in the ordered basis on the right, so the two ordered 
bases are distinct.
The difference between unordered bases and ordered bases might seem 
rather pedantic, but it is not. We will see several examples where we have 
the same set of basis vectors in which the order is different. The permuta­
tion of the basis vectors will give us important information.
30 
Chapter 2
As an example, earlier we noted that the standard basis ↑
↓
{
}
,
 cor­
responds to measuring the spin of an electron in the vertical direction. 
The ordered basis ↑
↓
(
)
,
 will correspond to measuring the spin when the 
south magnet is on top of our measuring apparatus. If we flip the apparatus 
through 180°,we will also flip the basis elements and use the ordered basis 
↓
↑
(
)
,
.
Length of Vectors
Supposing that we have been given a ket | v  and an orthonormal basis 
b
b
bn
1
2
,
,
,

{
}, we know how to write | v  as a linear combination of 
the basis vectors. We end up with v
b v b
b
v b
b v b
i
i
=
+
+
+
+
1
1
2
2
|
|
|

 
b
v b
n
n
+
|

. To simplify things, we will write this as v
c b
c b
=
+
+
1
1
2
2

c b
c b
i
i
n
n
+
+
+

. There is a useful formula for the length of v . It’s 
v
c
c
c
c
i
n
2
1
2
2
2
2
2
=
+
+
+
+



.
Let’s quickly see why this is true. We know that v
v v
2 =
|
.
Using v
c
b
c
b
c
b
n
n
=
+
+
+
1
1
2
2 |

 we obtain
v v
c
b
c
b
c
b
c b
c b
c b
n
n
n
n
|
=
+
+
+
(
)
+
+
+
(
)
1
1
2
2
1
1
2
2


.
The next step is to expand the product of the terms in the parentheses. 
This looks as though it is going to be messy, but it is not. We again use the 
facts that b b
i
k  equals 0 if i
k
≠
 and equals 1 if i
k
=
. All the bra-ket prod­
ucts with different subscripts are 0. The only bra-kets that are nonzero are 
the ones where the same subscript is repeated, and these are all 1. Conse­
quently, we end up with v v
c
c
c
c
i
n
|
=
+
+
+
+
1
2
2
2
2
2


.
Matrices
Matrices are rectangular arrays of numbers. A matrix M with m rows and n 
columns is called an m
n
×
 matrix. Here are a couple of examples:
A
B
=
−




=










1
4
2
2
3
0
1
2
7
5
6
1
A has two rows and three columns so it is a 2
3
×
 matrix. B is a 3
2
×
 matrix. 
We can think of bras and kets as being special types of matrices: bras have 
just one row, and kets have just one column.
Linear Algebra 
31
The transpose of a m
n
×
 matrix M , denoted M T, is the n
m
×
 matrix 
formed by interchanging the rows and the columns of M. The ith row of M 
becomes the ith column of M T, and the jth column of M becomes the jth 
row of M T. For our matrices A and B we have:
A
B
T
T
= −










= 



1
2
4
3
2
0
1
7
6
2
5
1
Column vectors can be considered as matrices with just one column, 
and row vectors can be considered as matrices with just one row. With this 
interpretation, the relation between bras and kets with the same name is 
given by a
a
T
|=
 and a
a T
=
| .
Given a general matrix that has multiple rows and columns, we think of 
the rows as denoting bras and the columns as denoting kets. In our exam­
ple, we can think of A as consisting of two bras stacked on one another or 
as three kets side by side. Similarly, B can be considered as three bras stacked 
on one another or as two kets side by side.
The product of the matrices A and B uses this idea. The product is 
denoted by AB. It’s calculated by thinking of A as consisting of bras and B 
of kets. (Remember that bras always come before kets.)
A
a
a
= 



1
2
, where a1
1
4
2
=
−
[
] and a2
2
3
0
= [
].
B
b
b
= [
]
|
|
1
2 , where b1
1
7
6
=










 and b2
2
5
1
=










.
The product AB is calculated as follows:
AB
a
a
b
b
a b
a b
a b
a b
= 


[
] = 



=
×
−
×
+
1
2
1
2
1
1
1
2
2
1
2
2
1
1
4
7
2
|
|
|
|
×
×
−
×
+
×
×
+
×
+
×
×
+
×
+
×




= −
−




6
1
2
4
5
2
1
2
1
3
7
0
6
2
2
3
5
0
1
15
16
23
19 
Notice that the dimension of the bras in A is equal to the dimension of 
the kets in B. We need to have this in order for the bra-ket products to be 
32 
Chapter 2
defined. Also notice that AB
BA
≠
. In our example, BA is a 3
3
×
 matrix, so 
it is not even the same size as AB.
In general, given an m
r
×  matrix A and an r
n
×
 matrix B, write A in 
terms of r-dimensional bras and B in terms of r-dimensional kets.
A
a
a
am
=












1
2
|
|
|

B
b
b
bn
= [
]
|
|
|
1
2

,   
The product AB is the m
n
×
 matrix that has a b
i
j
|
 as the entry in the ith 
row and jth column, that is,
AB
a b
a b
a b
a b
a b
a b
a b
a b
a b
j
n
j
n
i
=
1
1
1
2
1
1
2
1
2
2
2
2
1
|
|
|
|
|
|
|
|
|










a b
a b
a b
a
b
a
b
a
b
a
b
i
i
j
i
n
m
m
m
j
m
n
|
|
|
|
|
|
|
2
1
2






























Reversing the order of multiplication gives BA, but we cannot even begin 
the calculation if m is not equal to n because the bras and kets would have 
different dimensions. Even if m is equal to n, and we can multiply them, 
we would end up with a matrix that has size r
r
× . This is not equal to 
AB, which has size n
n
×
, if n is not equal to r. Even in the case when n, 
m and r are all equal to one another, it is usually not the case that AB will 
equal BA. We say that matrix multiplication is not commutative to indicate 
this fact.
Matrices with the same number of rows as columns are called square 
matrices. The main diagonal of a square matrix consists of the elements on 
the diagonal going from the top left of the matrix to the bottom right. A 
square matrix that has all leading diagonal entries equal to 1 and all other 
entries equal to 0 is called an identity matrix. The n
n
×
 identity matrix is 
denoted by In.
I
I
2
3
1
0
0
1
1
0
0
0
1
0
0
0
1
= 



=










…
,
,
Linear Algebra 
33
The identity matrix gets its name from the fact that multiplying matrices 
by the identity is analogous to multiplying numbers by 1. Suppose that A is 
an m
n
×
 matrix. Then I A
AI
A
m
n
=
=
.
Matrices give us a convenient way of doing computations that involve 
bras and kets. The next section shows how we will be using them.
Matrix Computations
Suppose that we are given a set of n-dimensional kets b
b
bn
1
2
,
,
,

{
} and 
we want to check to see if it is an orthonormal basis. First, we have to check 
that they are all unit vectors. Then we have to check that the vectors are 
mutually orthogonal to one another. We have seen how to check both of 
these conditions using bras and kets, but the calculation can be expressed 
simply using matrices.
We begin by forming the n
n
×
 matrix A
b
b
bn
= [
]
1
2

, then take 
its transpose.
A
b
b
b
T
n
=












1
2
|
|
|

Then we take the product A A
T
.
A A
b
b
b
b
b
b
b b
b b
b b
b b
b
T
n
n
n
=












[
] =
…
1
2
1
2
1
1
1
2
1
2
1
|
|
|


|
|
|
|
2
2
2
1
2
|
|
|
|
|
b
b b
b b
b b
b b
n
n
n
n
n
…
…
















Notice that the entries down the main diagonal are exactly what we 
need to calculate in order to find if the kets are unit. And the entries off 
the diagonal are what we have to calculate to see if the kets are mutually 
orthogonal. This means that the set of vectors is an orthonormal basis if 
and only if A A
I
T
n
=
. This equation gives a succinct way of writing down 
everything that we need to check.
Though it is a concise expression, we still need to do all the calculations 
to find the entries. We need to calculate all the entries along the main 
diagonal in order to check that the vectors are unit. However, we don’t need 
to calculate the entries below the main diagonal. If i
j
≠ then one of b
b
i
k
|
 
34 
Chapter 2
and b
b
k
i
|
 will be above and the other below the main diagonal. These two 
bra-ket products are equal, and once we have calculated one we don’t need 
to calculate the other. So, after we have checked that all the main diagonal 
entries are 1, we just need to check that all the entries above (or below) the 
diagonal are 0.
Now that we have checked that 
b
b
bn
1
2
,
,
,

{
} is an orthonormal 
basis, suppose that we are given a ket v  and want to express it as a linear 
combination of the basis vectors. We know how to do this.
v
b v b
b v b
b v b
b v b
i
i
n
n
=
+
+
+
+
+
1
1
2
2


Everything can be calculated using the matrix AT.
A
v
b
b
b
v
b
v
b
v
b
v
T
n
n
=












=












1
2
1
2
|
|
|
|
|
|


This has been a long chapter in which much mathematical machinery 
has been introduced. But the mathematics has been building, and we now 
have a number of ways for performing calculations. Three key ideas that we 
will need later are summarized in the final section. (They are at the end of 
the chapter for easy reference.) Before we conclude we look at some naming 
conventions.
Orthogonal and Unitary Matrices
A square matrix M that has real entries and has the property that M M
T
 is 
equal to the identity matrix is called an orthogonal matrix.
As we saw in the last section, we can check to see whether we have an 
orthonormal basis by forming the matrix of the kets and then checking 
whether the resulting matrix is orthogonal. Orthogonal matrices will also 
be important when we look at quantum logic gates. These gates also cor­
respond to orthogonal matrices.
Two important orthogonal matrices are 
1
2
1
2
1
2
1
2
−












 and 
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0












.
Linear Algebra 
35
The 2
2
×
 matrix corresponds to the ordered basis ←
→
(
)
,
, which we 
will meet in the next chapter where we will see how it is connected to 
measuring spin in the horizontal direction. We will also meet exactly the 
same matrix later. It is the matrix corresponding to a special gate, called the 
Hadamard gate.
The 4
4
×
 matrix corresponds to taking the standard basis for 4 and 
ordering with the last two vectors interchanged. This matrix is associated 
with the CNOT gate. We will explain later exactly what gates are, but practi­
cally all of our quantum circuits will be composed of just these two types of 
gates. So, these orthogonal matrices are important!
(If we were working with complex numbers, the matrix entries could 
be complex numbers. Matrices with complex entries that correspond to 
orthogonal matrices are called unitary.** Real numbers are a subset of the 
complex numbers, so all orthogonal matrices are unitary. If you look at 
practically every other book on quantum computing, they will call the 
matrices describing the CNOT gate and the Hadamard gate unitary, but we 
are calling them orthogonal. Both are correct.)
Linear Algebra Toolbox
Here is a list of three tasks that we will need to perform repeatedly. These are 
all easy to do. The methods for tackling each task are given.
(1)	 Given a set of n-dimensional kets b
b
bn
1
2
,
,
,

{
}, check to see if it is 
an orthonormal basis.
To do this, first construct A
b
b
bn
= [
]
1
2

. Then compute A A
T
. 
If this is the identity matrix, we have an orthonormal basis. If it isn’t, 
we don’t.
(2)	 Given an orthonormal basis b
b
bn
1
2
,
,
,

{
} and a ket v , express the 
ket as a linear combination of the basis vectors, that is, solve
v
x b
x b
x b
x b
i
i
n
n
=
+
+
+
1
1
2
2


.
To do this, construct A
b
b
bn
= [
]
1
2

. Then
**  A matrix M is unitary if M M
†
 is the identity matrix, where M † means first trans­
pose M, then take the conjugate of all the entries.
36 
Chapter 2
x
x
x
A
v
b
v
b
v
b
v
n
T
n
1
2
1
2














=
=












|
|
|
(3)	 Given an orthonormal basis b
b
bn
1
2
,
,
,

{
} and 
v
c b
c b
c b
c b
i
i
n
n
=
+
+
+
1
1
2
2


,  find the length of v .
To do this, use v
c
c
c
c
i
n
2
1
2
2
2
2
2
=
+
+
+
+



.
Now that we have the tools, we return to the study of spin.
3  Spin and Qubits
Chapter 
3
Spin 
and 
Qubits
© Massachusetts Institute of TechnologyAll Rights Reserved
The first chapter described measurements involving the spin of an electron. 
We saw that if you measure the spin in the vertical direction, you don’t 
obtain a continuum of values, but just two of them: Either the electron has 
its north pole vertically upward, or it is vertically downward. If we measure 
the spin first in the vertical direction and then once more in the same direc­
tion, we obtain exactly the same result for both measurements. If the first 
measurement shows that the electron has its north pole upward, then so will 
the second measurement. We also saw that if we measure first in the vertical 
direction and then in the horizontal direction, the electrons will have spin 
N and S in direction 90° each with probability of one half. It doesn’t matter 
what the first measurement is; the second measurement will give a random 
choice of either N or S. The second chapter introduced the mathematics of 
linear algebra. The goal of this chapter is to combine these previous two 
chapters, giving a mathematical model that describes the measurement of 
spin. We will then show how this relates to qubits. But before we start this 
description, we introduce the mathematics of probability.
Probability
Imagine that we have a coin and we repeatedly toss it, counting both the 
number of tosses and the number of times it comes up heads. If the coin is 
fair—equally likely to land heads up as tails up—the ratio of the number of 
heads to the number of tosses, after tossing it a large number of times, will 
be close to one half. We say that the probability of the outcome “heads” 
is 0.5.
In general, we perform an experiment—often we will call it making 
a measurement—that has a finite number of possible outcomes. We will 
38 
Chapter 3
denote these by E
E
En
1
2
,
,
,
…
. The underlying assumption is that the result 
of the experiment, or measurement, will be one and only one of these n 
outcomes. Associated with outcome Ei is a probability pi. Probabilities must 
be numbers between 0 and 1 that sum to 1. In the case of tossing a coin, 
the two outcomes are getting a head and getting a tail. If the coin is fair, the 
probability of each event is 1/2.
We return to the experiments involving the spin of a particle from the 
first chapter using a slightly more formal notation to describe them. Sup­
pose that we are going to measure the spin in direction 0°. There are two 
possible outcomes that we will denote as N and S. Both of these outcomes 
will have an associated probability. We will denote by pN the probability 
of obtaining N, and pS the probability of obtaining S. If we already know 
that our electron has spin N in direction 0°, then we know that when we 
measure again in this direction we will get the same result, so, in this case, 
pN = 1 and pS = 0. On the other hand, if we know our electron has spin N 
in direction 90° and we now measure in direction 0°, then we are equally 
likely to obtain N and S as the outcome, so, in this case p
p
N
S
=
= 0 5
. .
Mathematics of Quantum Spin
We will now present the mathematical model that describes quantum spin. 
It uses both probabilities and vectors.
The basic model is given by a vector space. When we make a measure­
ment there will be a number of possible outcomes. The number of out­
comes determines the dimension of this underlying vector space. For 
spin, there are just two possible outcomes from any measurement, so 
the underlying vector space is two-dimensional. We will take the space 
to be 2—this is the standard two-dimensional plane with which we are 
all familiar. This is fine for our purposes because we are only rotating our 
measuring apparatus in the plane. If we also wanted to consider all pos­
sible three-dimensional rotations of the apparatus, the underlying space 
would still be two-dimensional—two is still the number of possible out­
comes for each measurement—but instead of using vectors with real coef­
ficients, we would have to use vectors that involve complex numbers. The 
underlying vector space would then be the two-dimensional complex space 
denoted 2. For the reasons listed in the previous chapter, 2 is fine for 
our needs.
Spin and Qubits 
39
We will not consider all of the vectors in 2, just the unit vectors. For 
kets, this means we are restricting to kets of the form v
c
c
= 



1
2
, where 
c
c
1
2
2
2
1
+
= .
Choosing a direction to measure spin corresponds to choosing an 
ordered, orthonormal basis b
b
1
2
,
(
). The two vectors in the basis corre­
spond to the two possible outcomes for the measurements. We will always 
associate N with the first basis vector and S with the second. Before we 
measure the spin, the particle will be in a spin state given by a linear combi­
nation of b1  and b2 , that is, it has the form c
b
b
c
1
1
2
2
+
. We will some­
times refer to this as a state vector and sometimes just call it a state. After 
we measure, its state vector will jump to either b1  or b2 . This is one of 
the major ideas in quantum mechanics: Measurement causes the state vec­
tor to change. The new state is one of the basis vectors associated with the 
measurement. The probability of getting a particular basis vector is given 
by the initial state. The probability of its being b1  is c1
2; the probability 
of b2  is c2
2. The numbers c1 and c2 are called the probability amplitudes. 
It’s important to remember that the probability amplitudes are not prob­
abilities. They can be positive or negative. It’s the squares of these numbers 
that are probabilities. To make everything more concrete, we will return 
to the experiments where we measured in spin the vertical and horizontal 
directions.
As we mentioned in the previous chapter, the ordered orthonormal 
basis corresponding to measuring spin in the vertical direction is given by 
↑
↓
(
)
,
, where ↑= 



1
0  and ↓= 



0
1 . The first vector listed in the basis 
corresponds to the electron having spin N in direction 0° and the second 
vector to S in direction 0°.
The spin in the horizontal direction is given by the ordered orthonormal 
basis →
←
(
)
,
, where →=
−












1
2
1
2
 and ←=












1
2
1
2
. The first vector listed 
in the basis corresponds to the electron having spin N in direction 90° and 
the second vector to S in direction 90°.
We first measure spin in the vertical direction. Initially, we might not 
know the spin state of the incoming electron, but it must be a unit vector 
40 
Chapter 3
and so can be written as c
c
1
2
↑+
↓, where c
c
1
2
2
2
1
+
=
. We now perform 
the measurement. Either the electron is diverted upward in which case the 
state jumps to ↑ or it is diverted downward in which case its state jumps 
to ↓. The probability of it being diverted upward is c1
2 and the probability 
of it being diverted downward is c2
2.
We now repeat exactly the same experiment, measuring the spin 
once more in the vertical direction. Suppose that the electron was 
deflected upward by the first set of magnets. We know it is in spin state 
↑=
↑+
↓
1
0
. When we measure again, the state jumps to ↑ with 
probability 1
1
2 =
, or to ↓ with probability 0
0
2 =
. This just means that it 
remains in state ↑, and so is deflected upward once more.
Similarly, if the electron was deflected downward it will be in state 
↓=
↑+
↓
0
1
. No matter how many times we measure it in the vertical 
direction it will remain in this state, telling us that however many times we 
repeat the experiment the electron will keep being deflected downward. As 
we noted in the first chapter, if we repeat exactly the same experiment, we 
get exactly the same outcome.
Instead of repeatedly measuring spin in the vertical direction, we will 
first measure spin in the vertical direction and then measure spin in the 
horizontal direction. Suppose that we have just performed the first mea­
surement. We have measured spin in the vertical direction, and let us sup­
pose that the electron has spin N in direction 0°. Its state vector is now 
↑. Since we are next going to measure spin in the horizontal direction, we 
have to write this vector in terms of the orthonormal basis that corresponds 
to this direction, which means we must find the values of x1 and x2 that 
solve ↑=
→+
←
x
x
1
2
. We know how to do this: It’s the second tool in 
the toolbox listed at the end of the last chapter.
First construct the matrix A by stacking the kets that form the orthonor­
mal basis side by side.
A = [
] =
−












→←
1
2
1
2
1
2
1
2
Then calculate AT ↑ to get the probability amplitudes with respect to 
the new basis.
Spin and Qubits 
41
AT ↑=
−















=












1
2
1
2
1
2
1
2
1
0
1
2
1
2
This tells us that ↑=
→+
←
1
2
1
2
.
When we measure in the horizontal direction the state will jump to 
→ with probability 
1
2
1
2
2




=
, or it will jump to ← with probability 
1
2
1
2
2




=
. This tells us that the probability that the electron has spin N
in the 90° direction is equal to the probability that it has spin S in the 
90°direction; both probabilities are exactly one-half.
Notice that we didn’t really need to calculate the matrix A to do this 
calculation. The matrix that we need to use is AT . We can calculate this by 
stacking the bras that correspond to the orthonormal basis on top of one 
another. We must, of course, keep things in the same order. The left to right 
ordering of kets corresponds to the top to bottom ordering of bras, so the 
first element of the basis is the topmost bra.
In the first chapter we measured the spin three times. The first and third 
measurements were in the vertical direction, the second was in the hori­
zontal direction. We will describe the mathematics that corresponds to the 
third measurement. After the second measurement, the state vector of our 
electron will have one of two values. It will be either → or ←. We are 
now going to measure the spin in the vertical direction, so we need to 
express these as linear combinations of the vertical orthonormal basis. This 
gives →=
↑−
↓
1
2
1
2
 and ←=
↑+
↓
1
2
1
2
. In either case, when 
we measure spin in the vertical direction the state vector will jump to either 
↑ or to ↓, each occurring with probability one-half.
Equivalent State Vectors
Suppose that we are given a number of electrons and are told that their 
spins are given by either ↑ or by −↑. Can we distinguish between the 
two cases? Is there any measurement that we can perform that would tell 
them apart? The answer is that there is not.
42 
Chapter 3
To see this, let’s suppose that we choose a direction in which to measure 
spin. This is equivalent to choosing an ordered, orthonormal basis. We will 
denote this basis by b
b
1
2
,
(
).
Suppose our electron has state ↑. We have to find the values of a and b 
that solve the equation ↑=
+
a b
b b
1
2 . When we perform the measure­
ment, the probability of the spin being N is a2, and the probability of the 
spin being S is b2.
Suppose our electron has state −↑. For exactly the same values of a and 
b, we have −↑= −
−
a b
b b
1
2 . When we perform the measurement the 
probability of the spin being N is −
(
) =
a
a
2
2 and the probability of the spin 
being S is −
(
) =
b
b
2
2.
We get exactly the same probabilities for both cases, so there is no mea­
surement that can distinguish electrons with state vectors of form ↑ from 
those of −↑.
Similarly, given electrons with state v  there is no way to distinguish 
them from electrons with state −v . Since these states are indistinguish­
able, they are considered equivalent. Saying that an electron has spin given 
by v  means exactly the same as saying that it has spin given by −v .
To help illustrate this point further, consider these four kets:
1
2
1
2
↑+
↓
−
↑−
↓
1
2
1
2
1
2
1
2
↑−
↓
−
↑+
↓
1
2
1
2
By the preceding remarks, we know that 1
2
1
2
↑+
↓ and −
↑−
1
2
↓
1
2
 are equivalent, and that 1
2
1
2
↑−
↓ and −
↑+
↓
1
2
1
2
 are 
equivalent. So, these four kets describe at most two distinguishable states. 
But what about 1
2
1
2
↑+
↓ and 1
2
1
2
↑−
↓? Do these describe the 
same state, or are they distinguishable?
We do have to be a little careful. If we choose to measure the spin in 
the vertical direction, these two kets are not distinguishable. In both cases, 
we get ↑ or ↓ each occurring with probability of a half. But we know 
that 1
2
1
2
↑+
↓= ← and 1
2
1
2
↑−
↓= →. Consequently, if we 
measure in the 90° direction, we will obtain S for the first ket and N for 
the second. This choice of basis does distinguish them, and so they are not 
equivalent.
Spin and Qubits 
43
One thing that is probably not clear at the moment is how the basis asso­
ciated with a direction of measurement is chosen. We have seen that the 
basis associated with measuring in the vertical (0°) direction is 
1
0
0
1












,
 
and with the horizontal (90°) direction is 
1
2
1
2
1
2
1
2
−


































,
.
But where did these bases come from? Later, when we come to Bell’s 
theorem, we will need the bases associated with 120° and 240°. What are 
these? We answer these questions in the next section.
The Basis Associated with a Given Spin Direction
We begin with our measurement apparatus. We take the vertical direction 
as the starting point and start rotating in the clockwise direction. As we 
have already noted, when it has been rotated through 90°, we are measur­
ing in the horizontal direction. By the time it’s rotated through 180°, we are 
measuring the vertical direction once more. An electron that has spin N in 
direction 0° will have spin S in direction 180°, and an electron that has spin 
S in direction 0° will have spin N in direction 180°. Clearly, saying a magnet 
has its north pole in one direction conveys exactly the same information as 
saying the magnet has its south pole in the opposite direction, and conse­
quently we need only to rotate our apparatus through an angle between 0° 
and 180° to cover all possible directions.
We will now consider bases. We take the standard basis 
1
0
0
1












,
 as our 
starting point. This can be pictured as two vectors in the plane, as shown 
in figure 3.1.
Now we rotate these vectors. The general picture, with rotation of α° is 
depicted in figure 3.2. The vector 1
0



 rotates to cos
sin
α
α
( )
−
( )



, and 0
1



 rotates 
to sin
cos
α
α
( )
( )



.
Rotating through α° changes our initial ordered, orthonormal basis from 
1
0
0
1












,
 to 
cos
sin
sin
cos
α
α
α
α
( )
−
( )




( )
( )








,
.
44 
Chapter 3
cos(a)
–sin(a)
x
y
a
a
1
1
sin(a)
cos(a)
Figure 3.2
The standard basis rotated by α°.
x
1
y
1
0
1
1
0
Figure 3.1
The standard basis.
If the basis is rotated through 90° it becomes 
cos
sin
sin
cos
90
90
90
90
°
(
)
−
°
(
)




°
(
)
°
(
)








,
, 
which simplifies to 
0
1
1
0
−












,
. As we previously noted, 0
1
−



 is equivalent 
to 0
1



, so rotating through 90° brings us back to a basis equivalent to the 
original one, except that the order of the basis elements has been inter­
changed (i.e., N and S have been interchanged).
Spin and Qubits 
45
We will let θ denote the angle we are rotating our measurement appa­
ratus and α the angle we rotate our basis vectors. We have seen that we 
get a complete set of directions as θ goes from 0° to 180°, and that we get 
a complete set of rotated bases as α goes from 0° to 90°. Once we reach 
θ = 180° or equivalently α = 90°, N and S measured in direction 0° are 
interchanged.
We make the natural definition that θ = 2α. Consequently, the basis 
associated with rotating our apparatus by θ is 
cos
sin
sin
cos
θ
θ
θ
θ
/
/
,
/
/
2
2
2
2
(
)
−
(
)




(
)
(
)







. 
Figure 3.3 illustrates this.
Rotating the Apparatus through 60°
As an example to illustrate our formula, we look at what happens when 
we rotate our measuring apparatus by 60°. Suppose that we first measured 
our electron to have spin N in direction 0°. We will measure it again using 
the apparatus turned through 60°. What is the probability that it gives 
a result of N?
In this case the associated basis to the rotated apparatus is 
cos
sin
sin
cos
30
30
30
30
°
(
)
−
°
(
)




°
(
)
°
(
)








,
 which simplifies to 
3
2
3
2
/
,
/
−












1/2
1/2
.
Since the electron initially was measured to have spin N in direction 0°, 
its state vector after the initial measurement was 1
0



. We must now express 
this as a linear combination of the new basis vectors. To get the coordinates 
relative to the new basis we can multiply the state vector on the left by the 
matrix consisting of the bras of the basis. This gives:
N
S
(a)  Measurement angle
cos
2
sin
2
,
sin
2
cos
2
(b)  Basis
Figure 3.3
Rotating measuring apparatus by θ°.
46 
Chapter 3
3
2
3
2
1
0
3
2
/
/
/
−











= 



1/2
1/2
1/2
,
telling us that
1
0
3
2
3
2
3
2



=
−



+




/
/
/
1/2
1/2
1/2
.
So the probability of getting N when we measure in the 60° direction is 
3 2
3 4
2
(
) =
/ .
The Mathematical Model for Photon Polarization
In most of the book we will restrict our attention to measuring spin of 
electrons, but in the first chapter we said that we could rewrite everything 
in terms of the polarization of photons. In the next few sections we will 
explain the analogy between electron spin and photon polarization and 
give the mathematical model of polarization.
We start by associating the angle of 0° with a polarized filter in the verti­
cal direction, that is, a filter that lets through photons that are polarized 
vertically, which means that horizontally polarized photons are absorbed 
by the filter. As with the spin of electrons, we associate the standard basis 
1
0
0
1












,
 to the angle of 0°. The vector 1
0



 corresponds to a vertically 
polarized photon and the vector 0
1



 to a horizontally polarized one.
We will rotate the filter through an angle β°. It now lets through pho­
tons that are polarized in direction β° and blocks photons that are polarized 
perpendicularly to β°.
The mathematical model follows that for the spin of electrons. For each 
direction, there is an ordered orthonormal basis b
b
1
2
,
(
) associated with 
making a polarization measurement in this direction. The ket b1  corre­
sponds to a photon that is polarized in the given direction—that is, that 
passes through the filter. The ket b2  corresponds to a photon that is polar­
ized orthogonally to the given direction—that is absorbed by the filter.
A photon has a polarization state given by a ket, v . This can be written 
as a linear combination of the vectors in the basis: v
d
b
d
b
=
+
1
2
2
1
.
Spin and Qubits 
47
When the polarization is measured in the direction given by the ordered 
basis, the result will be that the photon is polarized in the given direction 
with probability d1
2 and polarized perpendicularly with probability d2
2; that 
is, the probability the photon passes through the filter is d1
2, and the prob­
ability it is absorbed is d2
2.
If the result of the measurement is that the photon is polarized in the 
given direction—it passes through the filter—then the state of the photon 
becomes b1 .
The Basis Associated with a Given Polarization Direction
Recall that if we start with our standard basis 
1
0
0
1












,
 and rotate 
these vectors though an angle α, we obtain the new orthonormal basis 
cos
sin
sin
cos
α
α
α
α
( )
−
( )




( )
( )








,
. Also recall that rotating through an angle of 90° 
brings us back to the same basis as the original, except that the order of the 
basis elements has been interchanged.
Now consider rotating a polarized filter through an angle β. When β 
is 0°, we are measuring in the vertical and horizontal direction. The ver­
tically polarized photons pass through the filter, and the horizontally 
polarized photons are absorbed. Once β reaches 90° we will be measuring 
photons in the vertical and horizontal direction, but now the vertically 
polarized photons are absorbed and the horizontally polarized ones pass 
through. In this case, β = 90° corresponds to α = 90°, and, in general, we can 
take α = β.
In conclusion, the ordered orthonormal basis associated with rotating a 
polarized filter through an angle β is 
cos
sin
sin
cos
β
β
β
β
( )
−
( )




( )
( )








,
.
The Polarized Filters Experiments
Using our model, we describe the experiments that we looked at in the first 
chapter.
In the first experiment we have two polarized squares. One measures 
polarization in direction 0° and the other in direction 90°. No light is let 
through the region of overlap, as depicted in figure 3.4.
48 
Chapter 3
The basis associated with 0° is the standard orthonormal basis. The 
basis associated with 90° is the same, except the order of the elements 
has been changed. A photon that passes through the first filter has had a 
measurement made—it is vertically polarized—and so is now in state 1
0



. 
We now measure it with the second filter. This lets through photons with 
state vector 0
1



and absorbs photons with state vector 1
0



. Consequently, 
any photon that passes through the first filter is absorbed by the second.
In the three-filter experiment we have the two filters arranged as above. 
We take the third sheet and rotate it through 45°, and slide this sheet 
between the other two. Some light comes through the region of overlap of 
all three squares. This is depicted in figure 3.5.
The ordered bases for the three filters are 
1
0
0
1












,
, 
1
2
1
2
1
2
1
2
−


































,
 and 
0
1
1
0












,
. A photon that passes through all three filters will have had 
Figure 3.5
Three polarized squares.
(a)  Two polarized sheets
(b)  Slightly overlapping
(c)  Fully overlapping
Figure 3.4
Two polarized squares.
Spin and Qubits 
49
three measurements made. Photons that pass through the first filter will 
be in state 1
0



.
The second measurement corresponds to passing through the filter 
rotated by 45°. We need to rewrite the state of the photon using the appro­
priate basis.
1
0
1
2
1
2
1
2
1
2
1
2
1
2



=
−












+












The probability of a photon passing through the second filter once it has 
gone through the first is 
1
2
1
2
2




=
. Consequently, half the photons that 
pass through the first filter will pass through the second filter. Those that 
do will now be in state 
1
2
1
2
−












.
The third filter corresponds to making a measurement using the third 
basis. We must rewrite the state of our photon using this basis.
1
2
1
2
1
2
0
1
1
2
1
0
−












= −



+




The third filter lets through photons corresponding to state 0
1



. The 
probability of this is −




=
1
2
1
2
2
. Consequently, half the photons that pass 
through the first two filters will pass through the third filter.
We have shown how the mathematical model relates the spin of an elec­
tron to the polarization of a photon. This model is also exactly what we 
need to describe qubits.
Qubits
A classical bit is either 0 or 1. It can be represented by anything that has two 
mutually exclusive states. The standard example is a switch that can be in 
50 
Chapter 3
either the on or off position. In classical computer science the measurement 
of bits does not enter the picture. A bit is a bit. It is either 0 or it is 1, and 
that is all there is to it. But for qubits the situation is more complicated, and 
measurement is a crucial part of the mathematical description.
We define a qubit to be any unit ket in 2. Usually, given a qubit, we will 
want to measure it. If we are going to measure it, we also need to include a 
direction of measurement. This is done by introducing an ordered ortho­
normal basis b
b
0
1
,
(
). The qubit can be written as a linear combination—
often called a linear superposition—of the basis vectors. In general, it will 
have the form d
b
b
d
0
0
1
1
+
. After we measure, its state will jump to either 
b0  or b1 . The probability of its being b0  is d0
2; the probability of b1  is 
d1
2. This is exactly the same model we have been using, but now we connect 
the classical bits 0 and 1 to the basis vectors. We will associate the b0  basis 
vector with the bit 0 and the b1  basis vector with the bit 1. So when we 
measure the qubit d
b
b
d
0
0
1
1
+
 we will obtain 0 with probability d0
2 and 1 
with probability d1
2 .
Since a qubit can be any unit ket and there are infinitely many unit kets, 
there are infinitely many possible values for a qubit. This is quite unlike 
classical computation, where we just have two bits. It is important, how­
ever, to notice that to get information out of a qubit we have to measure it. 
When we measure it we will get either 0 or 1, so the result is a classical bit.
We will give some illustrative examples using Alice, Bob, and Eve.
Alice, Bob, and Eve
Alice, Bob, and Eve are three characters that often appear in cryptogra­
phy. Alice wants to send a confidential message to Bob. Unfortunately, Eve 
wants to eavesdrop with evil intent. How should Alice encrypt her messages 
so that Bob can read them but Eve cannot? This is the central question of 
cryptography. We will look at it later. But for the moment we will just con­
centrate on Alice sending Bob a stream of qubits.
Alice measures qubits using her orthonormal basis, which we will denote 
as a
a
0
1
,
(
). Bob measures the qubits that Alice sends to him using his 
orthonormal basis b
b
0
1
,
(
).
Suppose that Alice wants to send 0. She can use her measuring apparatus 
to sort qubits into either state a0  or a1 . Since she wants to send 0, she 
sends a qubit in state a0 . Bob is measuring with respect to his ordered 
Spin and Qubits 
51
basis. To calculate what happens we must write a0  as a linear combina­
tion of Bob’s basis vectors. It will have the form a
d
b
d
b
0
0
0
1
1
=
+
. When 
Bob measures the qubit one of two things happens: Either it jumps to state 
b0  with probability d0
2 and he writes down 0, or it jumps to state b1  with 
probability d1
2 and he writes down 1.
You might be wondering why Bob and Alice don’t choose to use the 
same basis. If they did this, Bob would receive 0 with certainty whenever 
Alice sent 0 and receive 1 with certainty whenever Alice sent 1. This is 
true, but remember Eve. If she also chooses the same basis, then she too 
will receive exactly the same message as Bob. We will see later that there 
are good reasons for why Alice and Bob might choose different bases to 
thwart Eve.
For an example, Alice and Bob might to choose to measure their qubits 
using either the basis 
1
0
0
1












,
 or the basis 
1
2
1
2
1
2
1
2
−


































,
. The calculations 
are exactly as before, where we were considering spin in the vertical and 
horizontal directions. The only change is that we replace N with 0 and S 
with 1. Only if Alice and Bob choose to use the same basis will Bob end up 
with exactly the bit that Alice wanted to send. If they choose to use differ­
ent bases, then half of the time Bob gets the correct bit, but half of the time 
he gets the wrong one. This might not seem very useful, but we will see at 
the end of this chapter that Alice and Bob can use these two bases to secure 
their communications.
A couple of chapters from now, Alice and Bob will each choose one of 
three bases at random. These correspond to measuring the spin of an elec­
tron in the directions of 0°, 120°, or 240°. We will need to analyze all the 
possibilities, but now, to give a concrete example, we will have Alice mea­
sure in the 240° direction and Bob in the 120° direction.
We know the orthonormal basis in direction θ is 
cos
sin
θ
θ
/
/
,
2
2
(
)
−
(
)






sin
cos
θ
θ
/
/
2
2
(
)
(
)





. Consequently, Alice’s basis is 
1/2
1/2
−
−




−








3
2
3
2
/
,
/
 and Bob’s is 
1/2
1/2
−












3
2
3
2
/
,
/
. Since multiplying a ket by –1 gives an equivalent 
ket, we can simplify Alice’s basis to 
1/2
1/2
3
2
3
2
/
,
/




−







. (Notice that this 
52 
Chapter 3
is the basis for direction 60° that we looked at earlier, with the order of the 
basis vectors switched. There is nothing surprising about this. In fact, that’s 
exactly what we expect. Measuring N in direction 240° is exactly the same 
as measuring S in direction 60°.)
If Alice wants to send 0, she sends the qubit 
1/2
3
2
/



. To calculate what 
Bob measures we need to write this as a linear superposition of his basis 
vectors. We can get the probability amplitudes by forming the matrix con­
sisting of the bras of his basis vectors and then multiplying the qubit by 
this matrix.
1/2
1/2
1/2
1/2
−











=
−




3
2
3
2
3
2
3
2
/
/
/
/
.
This tells us that 
1/2
1/2
1/2
1/2
3 2
3 2
3 2
3 2
/
/
/
/



= −
−



+



.
This means that when Bob measures the qubit, he gets 0 with probability 
1/4 and 1 with probability 3/4. Similarly, it can be checked that if Alice 
sends 1, Bob will get 1 with probability 1/4 and 0 with probability 3/4.
It can also be checked, and it is an excellent exercise, that if Alice and 
Bob choose from the three bases, where the third is the standard basis, and 
end up with different bases, then Bob always gets the correct bit with prob­
ability 1/4.
Probability Amplitudes and Interference
If you drop a stone into a pond, waves propagate outward from where the 
stone hits the water. If you drop two stones, the waves propagating form 
one stone can interfere with the waves coming from the other one. If the 
waves are in phase—the peaks or the troughs coincide—then you get con­
structive interference: The amplitude of the resulting wave increases. If the 
waves are out of phase—the peak of one meets the trough of the other—
then you get destructive interference: The amplitude of the resulting wave 
decreases.
A qubit has the form d
b
d
b
0
0
1
1
+
, where d0 and d1 are the probability 
amplitudes. The square of these numbers gives the probabilities that the 
Spin and Qubits 
53
qubit jumps to the corresponding basis vector. Probabilities are not allowed 
to be negative, but probability amplitudes can be. This fact allows both 
constructive and destructive interference to take place.
As an example, consider the qubits that we are denoting by ← and → 
If we measure either of them in the standard basis, they will jump to either 
↑ or ↓. Each has a probability of 1/2 of occurring. If we are translating 
this back to bits, we will get either a 0 or a 1 with equal probability. We now 
take a superposition of the original two qubits, v =
←+
→
1
2
1
2
. If 
we were to measure v  in the horizontal direction, we would get either ← 
or → with equal probability. But if we measure in the vertical direction we 
get 0 with certainty, because
v =
←+
→=












+
−












=




1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1 1
0 +




0 0
1 .
The terms in ← and → that give 0 have interfered constructively, and 
the terms that give 1 have interfered destructively.
This will be important when it comes to quantum algorithms. We want 
to choose linear combinations carefully so that terms that we are not inter­
ested in cancel, but terms that we are interested in are amplified.
There are a very limited number of things that we can do with one qubit, 
but one thing we can do is to enable Alice and Bob to communicate securely.
Alice, Bob, Eve, and the BB84 Protocol
We often want to send secure messages. All Internet commerce depends on 
it. The standard way that messages are encrypted and decrypted uses two 
steps. The first step is when first contact is made. The two parties agree on a 
key—a long string of binary digits. Once they both have the same key, they 
then use it to both encode and decode messages from one another. The 
security of the method comes from the key. It is impossible to decode the 
messages between the two parties without knowing the key.
Alice and Bob want to communicate securely. Eve wants to eavesdrop. 
Alice and Bob want to agree on a key, but they need to be sure that Eve does 
not know it.
54 
Chapter 3
The BB84 protocol derives its name from its inventors, Charles Bennett 
and Gilles Brassard, and the year that it was invented, 1984. It uses two sets 
of ordered, orthonormal bases: the standard one, 
1
0
0
1












,
, that we used 
for measuring spin in the vertical direction, and so is denoted by V, and 
1
2
1
2
1
2
1
2
−


































,
 that we used for measuring spin in the horizontal direction, 
and so is denoted by H. In both cases, the classical bit 0 will correspond to 
the first vector in the ordered basis and 1 to the second.
Alice chooses the key that she wants to send to Bob. This is a string of 
classical bits. For each bit, Alice chooses one of the two bases V and H at 
random and with equal probability. She then sends Bob the qubit consist­
ing of the appropriate basis vector. For example, if she wants to send 0 and 
chooses V, she will send 1
0



, if she chooses H, she will send 
1
2
1
2
−












. She 
follows the same process for each bit, keeping a record of which basis she 
has used for each bit. If the string is 4n binary digits long, she will end up 
with a string of length 4n consisting of Vs and Hs. (The reason we are using 
4n and not n will become clear in a moment, but n should be a fairly large 
number.)
Bob also chooses between the two bases at random and with equal prob­
ability. He then measures the qubit in his chosen basis. Bob does this for 
each bit, and he keeps a record of which basis he has used. At the end of the 
transmission he also ends up with two strings of length 4n, one consisting 
of 0s and 1s from his measurements, the other consisting of Vs and Hs cor­
responding to the bases he chose.
Alice and Bob are choosing the basis for each bit at random. Half the 
time they end up using the same basis, while half the time they use differ­
ent bases. If they both choose the same basis, then Bob will obtain the bit 
that Alice is sending with certainty. If they choose different bases, then half 
the time Bob gets the right bit, but half the time it is the wrong bit—no 
information is transmitted when they choose different bases.
Alice and Bob now compare their strings of Vs and Hs over an unen­
crypted line. They keep the bits corresponding to the times when they both 
Spin and Qubits 
55
used the same basis and erase the bits that correspond to times that they 
used different bases. If Eve is not intercepting the message, they both end 
up with the same string of binary digits that has length about 2n. They now 
must check to see if Eve was listening in.
If Eve intercepts the qubit on the way from Alice to Bob, she would 
really like to clone it, sending one copy on to Bob and measuring the other 
qubit. Unfortunately for Eve, this is impossible. To obtain any information, 
she has to measure the qubit that Alice has sent, and this could change the 
qubit—it will end up as one of the basis vectors in the basis with which she 
chooses to measure. The best she can do is to choose one of the two bases 
at random, measure the qubit, and then send the qubit on to Bob. Let’s see 
what happens.
Alice and Bob are interested only in the measurements where they chose 
the same basis. We will restrict our attention to these times. When Alice 
and Bob agree on the basis, half the time Eve will also agree, and half the 
time she will choose the other basis. If all three agree on the basis, then they 
will all get the same bit as the measurement. If Eve chooses the wrong basis, 
then she will send a qubit that is in a superposition of Bob’s basis states. 
When Bob measures this qubit he will get 0 and 1 with equal probability; 
he will get the right bit one half of the time.
We now return to Alice and Bob and their strings of bits of length, at 
the moment, of 2n. They know that if Eve is not intercepting qubits, these 
strings will be identical. But they know that if Eve is intercepting qubits, 
she is going to choose the wrong basis half the time, and in these cases Bob 
will end up with the wrong bit half of the time. So, if Eve is intercepting 
qubits, a quarter of Bob’s bits will disagree with Alice’s. They now compare 
half of the 2n bits over an unencrypted line. If they agree on all of them, 
they know Eve is not listening in and can use the other n bits as the key. 
If they disagree on a quarter of the bits, they know that Eve is intercepting 
their qubits. They know that they need to find another way to secure their 
communication.
This is a nice example of sending one qubit at a time. There are, how­
ever, very few things that we can do with qubits that don’t interact with 
one another. In the next chapter we look at what happens when we have 
two or more qubits. In particular, we look at yet another phenomenon that 
is not part of our classical worldview but that plays an essential part of the 
quantum world: entanglement.
4  Entanglement
Chapter 
4
E
n
t
a
n
g
l
e
m
e
n
t
© Massachusetts Institute of TechnologyAll Rights Reserved
In this chapter we study the mathematics of entanglement. To do this, we 
need to introduce one more idea from linear algebra: the tensor product. 
We start by looking at two systems with no interaction between them. 
Since there is no interaction, we could study each system by itself, without 
any reference to the other system, but we will show how we can combine 
the two systems using tensor products. Then we introduce the tensor prod­
uct of two vector spaces and show that most of the vectors in this product 
represent what are referred to as entangled states.
Throughout this chapter there will be two qubits. Alice has one, and 
Bob has another. We will begin our study by examining a case where there 
is no interaction between Alice’s and Bob’s systems. This analysis initially 
might seem to make something that is very simple look somewhat compli­
cated, but once we have described everything in terms of tensor products it 
becomes fairly straightforward to extend the underlying ideas to the gen­
eral entangled case.
The approach we take, however, is not the approach we have taken so 
far. Instead of presenting physical experiments and then deriving a mathe­
matical model, we proceed in the other direction. We will extend our model 
in the simplest way possible and then see what the model predicts should 
be found when we perform experiments. We find that the model predicts 
the experiments accurately, but the conclusions are quite surprising.
Alice and Bob’s Qubits Are Not Entangled
We suppose that Alice is measuring using the orthonormal basis a
a
0
1
,
(
) 
and Bob is measuring with orthonormal basis 
b
b
0
1
,
(
). A typical qubit 
for Alice is v
c
a
c
a
=
+
0
0
1
1 , and for Bob is w
d
d
b
b
=
+
0
0
1
1 . We can 
58 
Chapter 4
combine these two state vectors using a new type of product that we call a 
tensor product, giving us a new vector denoted by v
w
⊗
.
Now v
w
c
a
c
a
d
d
b
b
⊗
=
+
(
) ⊗
+
(
)
0
0
1
0
0
1
1
1
. How do we multiply 
these two terms using this new product? Well, we do it the most natu­
ral way possible. We expand it in the usual way we multiply out algebraic 
expressions of the form a
b
c
d
+
(
)
+
(
). We write
c
a
c
a
d
d
c d
a
c d a
c d
a
c d
b
b
b
b
b
0
0
1
0
0
1
0
0
0
0
0
1
0
1
0
0
1
1
1
1
1
1
+
(
) ⊗
+
(
)
=
⊗
+
⊗
+
⊗
+
a
b
1
1
⊗
If you are familiar with the FOIL method, you should recognize that this is 
exactly what we have done. To make the terminology even simpler we will 
use juxtaposition of two kets to mean the tensor product, so v
w
⊗
 will 
be denoted as v
w .
v
w
c
a
c
a
d
d
c d
a
c d a
c d
a
c d a
b
b
b
b
b
=
+
(
)
+
(
)
=
+
+
+
0
0
1
0
0
1
0
0
0
0
0
1
0
1
0
0
1
1
1
1
1
1
1
1b
Though this is just the standard way of multiplying out two expressions, 
there is one thing that we have to be very careful about: The first ket in the 
tensor product belongs to Alice, and the second ket belongs to Bob. For 
example, v
w  means that v  belongs to Alice and that w  belongs to Bob. 
The product w v  means that w  belongs to Alice and that v  belongs to 
Bob. So, in general, v
w  will not equal w v . The technical term for this 
is to say the tensor product is not commutative.
Alice is measuring with her orthonormal basis a
a
0
1
,
(
) and Bob is mea­
suring with his orthonormal basis b
b
0
1
,
(
). We are describing both Alice’s 
and Bob’s qubits using tensor notation. This description involves the four 
tensor products that come from the basis vectors: a
b
0
0 , a
b
0
1 , a
b
1
0 , 
and a
b
1
1 . These four products form an orthonormal basis for the tensor 
product of Alice’s and Bob’s systems: Each of these products is a unit vector, 
and they are orthogonal to one another.
At this stage, though we have introduced new notation, we have not 
introduced anything new in terms of concepts. It is just the information 
that we already knew, but in a different package. For example, the number 
c d
0
0 is a probability amplitude. Its square gives the probability that when 
both Alice and Bob measure their qubits Alice’s qubit jumps to a0 , that is, 
she reads 0, and Bob’s qubit jumps to b0 , that is, he reads 0. But we already 
knew that the probability that Alice’s qubit would jump to a0  is c0
2 and the 
Entanglement 
59
probability that Bob’s would jump to b0  is d0
2. So we really knew that the 
probability that both would occur is c d
0
2
0
2, which is, of course, the same as 
c d
0
0
2
(
) . In a similar way, the numbers c d
0
2
1
2, c d
1
2
0
2, and c d
1
2
1
2 give the probabili­
ties that Alice and Bob read 01, 10, and 11, respectively. (Remember that 
Alice’s bit is always listed before Bob’s.)
Next we will replace these probability amplitudes using just one sym­
bol instead of two. We will let r
c d
=
0
0, s
c d
=
0
1, t
c d
=
1
0, and u
c d
=
1
1, so 
v
w
r a
b
s a
b
t a
b
u a
b
=
+
+
+
0
0
0
1
1
0
1
1 . We know that r
s
2
2
+
+
t
u
2
2
1
+
= , because they are probability amplitudes. We also know that 
ru = st, because both ru and st equal c c d d
0 1
0
1. Now we come to the new idea. 
We are going to describe the states of Alice’s and Bob’s qubits by tensors of 
the form r a
b
s a
b
t a
b
u a
b
0
0
0
1
1
0
1
1
+
+
+
. Again we stipulate that 
r
s
t
u
2
2
2
2
1
+
+
+
= , so that we can treat r, s, t, and u as probability ampli­
tudes. But we no longer insist that ru = st. We allow any values of r, s, t, and 
u just as long as the sum of their squares is 1.
Given a tensor of the form r a
b
s a
b
t a
b
u a
b
0
0
0
1
1
0
1
1
+
+
+
 with 
r
s
t
u
2
2
2
2
1
+
+
+
= , there are two cases. The first case is when ru = st. In this 
case we say that Alice’s and Bob’s qubits are not entangled. The second case is 
when ru
st
≠
. In this case we say that Alice’s and Bob’s qubits are entangled. 
This rule is easy to remember if the terms are written out with the subscripts 
in the order we have presented them: 00, 01, 10, 11. In this order, ru are 
the outer terms and st are the inner, so the qubits are not entangled if the 
product of the outer terms is equal to the product of the inner ones, and 
they are entangled if the products are not equal.
We will look at examples that illustrate both of these cases.
Unentangled Qubits Calculation
Suppose we are told that Alice and Bob’s qubits are given by
1
2 2
3
2 2
1
2 2
3
2 2
0
0
0
1
1
0
1
1
a
b
a
b
a
b
a
b
+
+
+
.
We quickly calculate the products of the outer and inner probability 
amplitudes. Both products equal 3 8 , so the qubits are unentangled.
The probability amplitudes tell us what happens when Alice and Bob 
both make measurements. They will get 00 with probability 1/8, 01 with 
probability 3/8, 10 with probability 1/8 and 11 with probability 3/8.
60 
Chapter 4
What is slightly trickier is to see what happens if just one of them makes 
a measurement. We start by assuming that Alice is going to make a mea­
surement, but Bob is not. To do this we begin by pulling out common fac­
tors from Alice’s perspective. We rewrite the tensor product as
a
b
b
a
b
b
0
0
1
1
0
1
1
2 2
3
2 2
1
2 2
3
2 2
+



+
+



.
Next, we want the expressions in the parentheses to be unit vectors, so 
we divide by their lengths inside the parentheses and multiply by their 
lengths outside, which gives us
1
2
1
2
3
2
1
2
1
2
3
2
0
0
1
1
0
1
a
b
b
a
b
b
+



+
+



.
We can then pull out the common factor in the parentheses. (But 
remember that it’s Bob’s, so we must keep it on the right.)
1
2
1
2
1
2
3
2
0
1
0
1
a
a
b
b
+




+



.
Written in this form, it becomes explicit that the states are not entan­
gled. We have a tensor product of a qubit that belongs to Alice with a qubit 
that belongs to Bob.
From this we can deduce that if Alice measures first she will obtain 0 and 
1 with equal probability. This measurement has no effect on the state of 
Bob’s qubit. It was and remains
1
2
3
2
0
1
b
b
+



.
We can also read from the factored expression that if Bob measures first 
he will get 0 with probability 1/4 and 1 with probability 3/4. Again, it is 
clear that Bob’s measurement has no effect on Alice’s qubit.
When the qubits are unentangled, a measurement of one of the qubits 
has absolutely no effect on the other qubit. The situation is completely 
different with entangled qubits. If qubits are entangled, the measurement 
of one will have an effect on the other one. We will illustrate this with an 
example.
Entanglement 
61
Entangled Qubits Calculation
Suppose we are told that Alice’s and Bob’s qubits are given by
1
2
1
2
1
2
0
0
0
0
1
1
0
1
1
a
a
a
a
b
b
b
b
+
+
+
.
We quickly calculate the products of the outer and inner probability. The 
product of the outer terms is 0. Since the product of the inner terms is not 
equal to 0, the two qubits are entangled.
Usually both Alice and Bob will make measurements. As in the previous 
example, we can use the probability amplitudes to tell us what happens 
when Alice and Bob both measure their qubits. They will get 00 with prob­
ability 1/4, 01 with probability 1/4, 10 with probability 1/2, and 11 with 
probability 0. Notice that there is nothing strange going on. This is exactly 
the same calculation as in the unentangled case.
We will now see what happens if just one of them makes a measurement. 
We start by assuming that Alice is going to make a measurement, but Bob 
is not. To do this we begin by pulling out common factors from Alice’s per­
spective. We rewrite the tensor product as
a
a
b
b
b
b
0
0
1
1
0
1
1
2
1
2
1
2
0
+
(
) +
+



.
As before, we want the expressions in the parentheses to be unit vectors, 
so we divide by their lengths inside the parentheses and multiply by their 
lengths outside, which gives us
1
2
1
2
1
2
1
2
1
0
0
0
1
1
0
1
a
a
b
b
b
b
+



+
+
(
).
In the previous example, the terms in the parentheses were the same, 
and we could pull this common term out as a common factor. But in this 
case the terms in the parentheses are different. This is what it means to be 
entangled.
The probability amplitudes in front of Alice’s kets tell us that when she 
measures she will get 0 and 1 with equal probability. But when Alice gets 0, 
her qubit jumps to a0 . The combined system jumps to the unentangled 
state a
b
b
0
0
1
1
2
1
2
+
(
), and Bob’s qubit is no longer entangled with 
Alice’s. It is 
1
2
1
2
0
1
b
b
+
(
). When Alice gets 1, again Bob’s qubit is no 
longer entangled with Alice’s. It becomes b0 .
62 
Chapter 4
The result of Alice’s measurement affects Bob’s qubit. If she gets 0, Bob’s 
qubit becomes 
1
2
1
2
0
1
b
b
+
(
). If she gets 1, his qubit becomes b0 . This 
does seem strange. Alice and Bob could be far apart. As soon as she makes 
a measurement Bob’s qubit becomes unentangled, but exactly what it is 
depends on Alice’s outcome.
For completeness, we will see what happens when Bob measures first.
We start with the initial tensor product.
1
2
1
2
1
2
0
0
0
0
1
1
0
1
1
a
a
a
a
b
b
b
b
+
+
+
Rewriting from Bob’s perspective gives
1
2
1
2
1
2
0
0
1
0
0
1
1
a
a
a
a
b
b
+




+
+




.
As always, we want the expressions in the parentheses to be unit vectors, 
so we divide by their lengths inside the parentheses and multiply by their 
lengths outside, which gives us
1
3
2
1
0
1
2
3
2
3
0
1
0
0
1
1
a
a
a
a
b
b
+




+
+
(
)
.
When Bob measures his qubit he gets 0 with probability 3/4 and 
1 with probability 1/4. When Bob gets 0, Alice’s qubit jumps to state 
1
3
3
2
0
1
a
a
+
(
). When he gets 1, her qubit becomes a0 .
When the first person measures her or his qubit, the second person’s 
qubit immediately jumps to one of two states. These states depend on the 
result of the first person’s measurement. This is quite unlike our everyday 
experience. Later we will see clever ways of exploiting entangled qubits, but 
first we consider superluminal communication.
Superluminal Communication
Superluminal communication is communication faster than the speed 
of light. Two apparently contradictory inferences seem to be able to be 
deduced concerning this. The first is that Einstein’s special theory of rela­
tivity tells us that as you travel faster, approaching the speed of light, time 
slows down. If you could travel at the speed of light, time stops. And if you 
Entanglement 
63
travel faster than the speed of light, time would go backward. The theory 
also tells us that as we approach the speed of light our mass increases with­
out bound, which means that we can never reach that speed. Also, it seems 
unlikely that we could go back in time. If we could, then we run into all the 
science fiction scenarios in which we can prevent some history-changing 
event from happening. Time travel seems to lead to contradictions. It’s not 
just physical travel that seems to be ruled out, but also communication. 
If we could send messages back in time, we could still change the course 
of history—we could still design scenarios in which the communication 
causes some dramatic change in the present—for example, prevents us 
being born. So, one of our immediate thoughts is that superluminal com­
munication should not be possible.
On the other hand, suppose that Alice and Bob are on opposite sides of 
the universe and have a number of entangled qubits. These are electrons 
whose spin states are entangled. Alice has one of each entangled pair of 
electrons in her possession, and Bob has the other one. (Though we are 
talking about entangled electrons, we should be clear that the actual elec­
trons are totally separate. It’s their spin states that are entangled.)
When Alice makes a measurement on one of her electrons, the spin state 
of the corresponding electron in Bob’s possession instantaneously jumps 
into one of two distinct states. Instantaneous is clearly faster than the speed 
of light! Can’t entanglement be used for instantaneous communication?
Let’s suppose that each pair of the entangled electrons is in the entan­
gled spin state that we have just studied:
1
2
1
2
1
0
2
0
0
0
1
1
0
1
1
a
a
a
a
b
b
b
b
+
+
+
.
Suppose that Alice measures the spins of her electrons before Bob mea­
sures the spin of their partners. We have seen that she gets a random string 
of 0s and 1s, with each symbol occurring with equal probability.
Suppose instead that Bob measures his spins before Alice. Then Alice 
measures the spins. What does she now get? When Alice makes her mea­
surement, they will both have made measurements, so we can use the 
probability amplitudes of the initial expression. We know that they will 
obtain 00 and 01 with probability 1/4, 10 with probability 1/2 and 11 with 
probability 0. Consequently, Alice will get 0 with probability of 1
4
1
4
1
2
+
=
, and 1 
with probability 1
2
1
2
0
+
=
. So, Alice gets a random string of 0s and 1s, with 
64 
Chapter 4
each symbol occurring with equal probability. But this is exactly the same 
situation as when she measured first. So Alice cannot tell from her measure­
ments whether they were made before or after Bob’s. All entangled states 
behave this way. If there is no way of Alice and Bob being able to tell from 
their measurements who went first, there certainly can be no way of send­
ing any information from one to the other.
We have shown that Alice and Bob cannot send information when their 
qubits have a particular entangled state, but the argument generalizes to 
any entangled state. No matter what states Alice’s and Bob’s qubits have, 
it is impossible for them to send information by solely measuring their 
qubits.
Now that we have seen that superluminal communication is not pos­
sible, we turn to the more prosaic task of writing tensor products using 
standard bases. But afterward we will return to our exploration of entangled 
qubits using the quantum clock example from the previous chapters.
The Standard Basis for Tensor Products
The standard basis for 2 is 
1
0
0
1












,
. When both Alice and Bob use the 
standard basis, tensor products have the form:
r
s
t
u
1
0
1
0
1
0
0
1
0
1
1
0



⊗


+



⊗


+ 


⊗


+
0
1
0
1



⊗



Therefore, the standard ordered basis for 

2
2
⊗
 is
1
0
1
0
1
0
0
1
0
1
1
0
0
1



⊗






⊗






⊗





,
,
,

⊗







0
1
.
Since it has four vectors in the basis, it is a four-dimensional space. The 
standard four-dimensional space is 4 with ordered basis is
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1







































,
,
,




















.
We identify the basis vectors in 

2
2
⊗
 with those in 4, making sure to 
respect the ordering.
Entanglement 
65
1
0
0
0
1
0
1
0












= 


⊗


, 
0
1
0
0
1
0
0
1












= 


⊗


, 
0
0
1
0
0
1
1
0












= 


⊗


, 
0
0
0
1
0
1
0
1












= 


⊗


.      
The easiest way to remember this is by the following construction.
a
a
b
b
a
b
b
a
b
b
0
1
0
1
0
0
1
1
0
1



⊗


=




















=












a b
a b
a b
a b
0
0
0
1
1
0
1 1
.
Notice also that the subscripts follow the standard binary ordering: 00, 
01, 10, 11.
How Do You Entangle Qubits?
This book is about the mathematics that underlies quantum computing. 
It is not about how to physically create a quantum computer. We are not 
going to spend much time on the details of physical experiments, but the 
question of how physicists create entangled particles is such an important 
one that we will briefly address it. We can represent entangled qubits by 
either entangled photons or electrons. Though we often say the particles 
are entangled, what we really mean is that the vector describing their states, 
a tensor in 

2
2
⊗
, is entangled. The actual particles are separate and, as 
we have just noted, can be very far apart. That said, the question remains: 
How do you go about creating a pair of particles whose state vector is entan­
gled? First, we look at how physical experiments create entangled particles. 
Then we look at how quantum gates create entangled qubits.
The most commonly used method at this time involves photons. The 
process is called spontaneous parametric down-conversion. A laser beam 
sends photons through a special crystal. Most of the photons just pass 
through, but some photons split into two. Energy and momentum must be 
conserved—the total energy and momentum of the two resulting photons 
must equal the energy and momentum of the initial photon. The conser­
vation laws guarantee that the state describing the polarization of the two 
photons is entangled.
In the universe, electrons are often entangled. At the start of the book 
we described Stern and Gerlach’s experiment on silver atoms. Recall that 
66 
Chapter 4
the electron spins in the inner orbits canceled, leaving the lone electron 
in the outer orbit to give its spin to the atom. The innermost orbit has two 
electrons. These are entangled so that their spins cancel. We can think of 
the state vector describing the spin of these electrons as
1
2
1
0
0
1
1
2
0
1
1
0



⊗


−



⊗


.
Entangled electrons also occur in superconductors, and these electrons 
have been used in experiments. However, often we want to have entangled 
particles that are far apart—as we will see later when we talk about the 
Bell test.
The main problem with using entangled electrons that are near one 
another and then separating them is that they have a tendency to interact 
with the environment. It is difficult to separate them without this happen­
ing. On the other hand, entangled photons are much easier to separate, 
though more difficult to measure. It is possible, however, to get the best of 
both worlds. This has been done by an international team based at the Delft 
University of Technology in what they describe as a loophole free Bell test. 
They used two diamonds separated by 1.3 kilometers. Each diamond had 
slight imperfections—nitrogen atoms altered the carbon atom lattice struc­
ture in places. Electrons become trapped in the defects. A laser excited an 
electron in each of the diamonds in such a way that both electrons emitted 
photons. The emitted photons were entangled with the spins of the elec­
trons that they were emitted from. The photons then traveled toward one 
another through a fiber optic cable and met in a beam splitter—a standard 
piece of equipment that is usually used to split a beam of photons in two, 
but here it is used to entangle the two photons. The photons were then 
measured. The result was that the two electrons were now entangled with 
one another.* (We will explain why the team was doing this experiment in 
the next chapter.)
In quantum computing, we will usually input unentangled qubits and 
entangle them using the CNOT gate. Later we will explain exactly what 
gates are, but the actual computations involve just matrix multiplication. 
We briefly look at this.
*  There is a short video on this at https://www.youtube.com/watch?v=AE8Ma
QJkRcg/.
Entanglement 
67
Using the CNOT Gate to Entangle Qubits
We leave the actual definition of what a quantum gate is until later, but 
we’ll just make the comment now that they correspond to orthonormal 
bases or, equivalently, to orthogonal matrices.
The standard ordered basis for four-dimensional space is 4 is
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1







































,
,
,




















.
The CNOT gate comes from interchanging the order of the last two ele­
ments. This results in the matrix for the CNOT gate.
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0












This gate acts on pairs of qubits. To use the matrix, everything must be 
written using four-dimensional vectors. We look at an example.
We start by taking the unentangled tensor product
1
2
1
1
1
0
1
2
1
0
1
0



⊗


=












.
When we send qubits through the gate, they are changed. The resulting 
qubits are obtained by multiplying by the matrix.
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
1
2
0
1
2
0
1
2
0
0
1






























=
2
1
2
1
0
0
1


















=












This last vector corresponds to a pair of entangled qubits—the product 
of the inner amplitudes is zero, which is not equal to the product of the 
outer amplitudes. This can be rewritten as
68 
Chapter 4
1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


.
We will often use entangled qubits in this state. It has the very nice prop­
erty that if Alice and Bob measure in the standard basis, they will both get 
1
0



, corresponding to 0, or they will both get 0
1



, corresponding to 1. The 
two cases are equally probable.**
We examine this further with a quantum clock analogy.
Entangled Quantum Clocks
Recall the quantum clock metaphor. We can ask only about whether the 
hand is pointing in a certain direction, and the clock will answer either that 
it is or that it is pointing in the opposite direction.
We let the vector 1
0



 correspond to pointing to twelve, and 
0
1



 
to pointing to six. Consider a pair of clocks in the entangled state 
1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


. In fact, consider one hundred pairs of clocks, 
each pair of which is in this state. Suppose that you have one hundred of 
these clocks, and I have the hundred partners. We are both going to ask the 
same question repeatedly: Is the hand pointing toward twelve?
In the first scenario, we don’t contact one another. We just go through 
the clocks one at a time and ask the question. Each time the clock will 
answer either yes or no. We will write 1 if it is yes, and 0 if it is no. After we 
have finished asking questions, we have a string of 0s and 1s. I analyze my 
string and you analyze yours. Both strings are a random sequence of 0s and 
1s. Both digits occur about the same number of times. We now contact one 
another and compare strings. Both your string and my string are identical. 
In all one hundred places the strings agree.
In the second scenario, we again each have one hundred clocks. This time 
we make an agreement that you will measure first. You will ask your ques­
tion on the hour, and I will ask mine half an hour later. During these half-
hours between our questions you will call me and tell me what my clock’s 
**  In the next chapter, we will see that Alice and Bob don’t need to stick to the 
standard basis. If they both use the same orthonormal basis, no matter which one, 
they will still get exactly the same results.
Entanglement 
69
answer will be. At the end of the experiment we both have a string of 0s and 
1s. Both strings agree in every place. Every time you called me and told me 
what the result of my measurement was going to be you were exactly right. 
Can we conclude that your measurements were affecting mine?
Well, suppose that I now tell you that I was cheating. I didn’t follow the 
rules. In fact, I was asking the questions of the clock half an hour before 
you asked yours. I knew your answer before you did. Your calls were just 
confirming what I knew.
There is no way from the data that you can tell whether or not I was fol­
lowing the rules or I was cheating. There is no way you can tell whether I 
am asking my questions before or after you asked yours.
There is no causation here, just correlation. As we saw earlier, we can­
not use these entangled clocks to send messages between us. But the pro­
cess is still mysterious. Albert Einstein described entanglement as implying 
spooky action at a distance. Nowadays many people would say that there is 
no action, just correlation. Of course, we can quibble about the definition 
of “action,” but even if we agree that there is no action, there seems to be 
something spooky going on.
Suppose that you and I have a pair of the entangled quantum clocks, 
and we are talking on the phone to one another. Neither of us has asked 
our clock a question, so they are still entangled. In this state, if you were to 
ask your clock the question, you would have an equal chance of getting an 
answer that the hand was pointing to twelve or six. But as soon as I ask my 
clock a question, you no longer have an equal chance of getting one of the 
two answers. You will get exactly the same answer as mine.
This correlation would not be spooky if when our clocks were entangled 
it was decided, but unknown to us, whether both our hands were pointing 
at either twelve or six. We had to wait until one of us asked the question, 
and as soon as one of us knows the answer so does the other.
But this is not what our model describes. Our model says that the deci­
sion on which direction our hands are pointing is not made beforehand. 
It’s made only when the first of us asks our question. This is what makes it 
spooky.
In the next chapter we will look at this in detail. We will look at a model 
that incorporates correlation in an intuitive and nonspooky way. Unfortu­
nately, it is wrong. John Stewart Bell came up with an ingenious test that 
shows that the simple explanation is not correct and that the mysterious 
spookiness has to remain.
5  Bell’s Inequality
Chapter 
5
Bell’s 
Inequality
© Massachusetts Institute of TechnologyAll Rights Reserved
We have seen a mathematical model of a small portion of quantum mechan­
ics that concerns the spin of particles or the polarization of photons, and 
that gives us the mathematics describing qubits. This is the standard model, 
often called the Copenhagen interpretation after the city where Niels Bohr was 
living and working.
Some of the great physicists of the early twentieth century, including 
Albert Einstein and Irwin Schrödinger, didn’t like this model, with its inter­
pretation of states jumping with given probabilities to basis states. They 
objected to both the use of probability and to the concept of action at a 
distance. They thought that there should be a better model using “hidden 
variables” and “local realism.” They weren’t objecting to using the Copen­
hagen model for doing calculations, but they thought that there should be 
a deeper theory that would explain why the calculations were producing 
correct answers—a theory that eliminated the randomness and explained 
the mystery.
Bohr and Einstein were both interested in the philosophy of quantum 
mechanics and had a series of debates about the true meaning of the theory. 
In this chapter we will look at their two different viewpoints. You might be 
wondering if we are digressing and that the philosophical underpinnings 
are not necessary to understand quantum computation. We all now know 
that Einstein and Schrödinger’s view was wrong and that the Copenhagen 
model is regarded as the standard description. But Einstein and Schrödinger 
were both brilliant scientists, and there are a number of reasons to study 
their arguments.
The first reason is that the debates between Bohr and Einstein focused on 
local realism. We will explain more about this in a moment, but essentially 
local realism means that a particle can only be influenced by something 
72 
Chapter 5
changing in its vicinity. Practically all of us are local realists, but quantum 
mechanics shows us that we are wrong. Einstein’s model seems to us to be 
the natural and correct model—at least it does to me. When I first heard 
of quantum entanglement, my natural assumption was to assume a model 
similar to Einstein’s. You too might be thinking about entanglement incor­
rectly. These arguments are important to the philosophy of physics and 
help us to understand that the mysteriousness cannot be eliminated.
John Stewart Bell was an Irish physicist. He devised an ingenious test that 
could distinguish between the two models. Many were surprised that the 
models were not just philosophies, but testable theories. We have learned 
only a small portion of the mathematics needed for quantum mechanics, 
but it is exactly what is needed to understand Bell’s result. His test has been 
carried out several times. It is tricky to eliminate all possible biases in the 
setup of this experiment, but more and more possible loopholes have been 
excluded. The results have always been in accordance with the Copenha­
gen interpretation. Since Bell’s result is one of the most important of the 
twentieth century and we have the mathematical machinery lined up, it 
makes sense to look at it.
You may still be wondering what any of this has to do with quantum 
computation. We will see at the end of this chapter that the ideas behind 
Bell’s inequality can be used for sending encrypted messages. Also, the 
entangled qubits that Bell uses will reappear when we look at quantum 
algorithms. So this chapter has connections to quantum computation. But 
the main reason for this chapter is that I find this material fascinating, and 
I hope you will too.
We start by looking at the entangled qubits we introduced in the last 
chapter and see what happens if we measure them in different bases. We 
begin our analysis using the standard model—the Copenhagen model—
that we have seen in the earlier chapters.
Entangled Qubits in Different Bases
In the last chapter we looked at two entangled clocks in the state
1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


.
We observed that if Alice and Bob each had one of the clocks, and both 
asked whether the hand was pointing toward twelve, both would either get 
Bell’s Inequality 
73
the answer that it was or that it was pointing toward six. Both possibilities 
were equally likely, but both Alice and Bob get exactly the same answer. We 
now ask what happens if Alice and Bob change the direction in which they 
are measuring. For example, what happens if they both ask whether the 
hands are pointing to four? We know that the clocks will answer that the 
hands are pointing either toward four or to ten, but will Alice and Bob get 
exactly the same answer? Are both answers equally likely?
First, we give an intuitive argument for two qubits in the entangled state
1
2
1
0
0
1
1
2
0
1
1
0



⊗


+



⊗


.
Two electrons might represent this state. Suppose that Alice and Bob 
measure the spin of their electrons in direction 0°. If Alice gets N, Bob gets 
S. If Alice gets S, Bob gets N. As we mentioned earlier, this might represent 
two electrons in an atom where the spins cancel. But we would expect the 
spins to cancel in every direction, so we would expect that if Alice and Bob 
chose a new basis for measurements they would still get spins in the oppo­
site direction. Symmetry also seems to imply that both directions should 
be equally likely.
This intuitive argument leads us to conjecture that if we have entangled 
qubits in the state
1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗



and then rewrite this state using a new orthonormal basis b
b
0
1
,
(
), we 
ought to get 
1
2
1
2
0
0
1
1
b
b
b
b
⊗
+
⊗
. Of course, our argument is 
intuitive and clearly making intuitive arguments about something as coun­
terintuitive as quantum mechanics is not entirely persuasive, but in this 
case we are correct, as we will now prove.
Proof That 1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


 Equals 1
2
1
2
0
0
1
1
b
b
b
b
⊗
+
⊗
We start by writing the kets b0  and b1  as column vectors. We let 
b
a
b
0 = 


 and b
c
d
1 = 


. Next we express our standard basis vectors as 
linear combinations of the new basis vectors. We do this in the standard 
74 
Chapter 5
way (using the second tool at the end of chapter 2). We start with 1
0



. The 
equation
a
b
c
d
a
c







= 



1
0
tells us that
1
0



=



+




a a
b
c c
d .
Consequently,
1
0
1
0
1
0



⊗


=



+







⊗



a a
b
c c
d
.
Rearranging the terms on the right gives
a a
b
c c
d



⊗


+



⊗



1
0
1
0 ,
which can be rewritten as
a
b
a
c
d
c



⊗


+ 


⊗



0
0 .
Thus, 1
0
1
0
0
0



⊗


= 


⊗


+ 


⊗



a
b
a
c
d
c .
A similar calculation shows that
0
1
0
1
0
0



⊗


= 


⊗


+ 


⊗



a
b
b
c
d
d .
Adding these two results gives us
1
0
1
0
0
1
0
1
0
0



⊗


+ 


⊗


= 


⊗


+ 

a
b
a
b






+ 


⊗


+ 







c
d
c
d
0
0
.
This simplifies to
a
b
a
b
c
d
c
d



⊗


+ 


⊗


,
which is just b
b
b
b
0
0
1
1
⊗
+
⊗
.
Bell’s Inequality 
75
So 1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


 does equal 1
2
1
2
0
0
1
1
b
b
b
b
⊗
+
⊗
.
This result tells us that if Alice and Bob have qubits that are entangled 
with state 1
2
1
0
1
0
1
2
0
1
0
1



⊗


+



⊗


, and if they both choose to measure 
their qubits with respect to an orthonormal basis b
b
0
1
,
(
), the entangled 
state can be rewritten as 1
2
1
2
0
0
1
1
b
b
b
b
+
. When the first measurement 
is made, the state jumps to either b
b
0
0  or to b
b
1
1 , where both of these 
now unentangled states are equally likely to occur. The consequence is that 
when Alice and Bob have both measured their qubits they will both get 0 or 
they will both get 1, and both outcomes are equally likely.
For Bell’s result we want to measure the entangled qubits using three 
different bases. These are the bases that correspond to rotating our mea­
suring device through 0°, 120°, and 240°. For our entangled clocks, we are 
asking one of three questions, whether the hand is pointing to twelve, 
to four, or to eight. If we denote these bases by ↑
↓
(
)
,
, ↘
↖
,
(
), and 
↙
↗
,
(
), then the following are three descriptions of exactly the same 
entangled state:
1
2
1
2
↑↑+
↓↓
1
2
1
2
↘↘
↖↖
+
1
2
1
2
↙↙
↗↗
+
We now turn to Einstein and see how he viewed these entangled states.
Einstein and Local Realism
Gravity provides a good example to explain local realism. Newton’s law of 
gravity gives a formula that tells us the strength of the force between two 
masses. If you plug in the size of the masses, the distance they are apart, and 
the gravitational constant, the formula gives the size of the attractive force. 
Newton’s law transformed physics. It can be used, for example, to show 
that a planet orbiting a star moves in an elliptical orbit. But though it tells 
us the value of the force, it does not tell us the mechanism that connects 
the planet to the sun.
Although Newton’s law of gravitation was useful for calculations, it did 
not explain how gravity worked. Newton, himself, was concerned about 
76 
Chapter 5
this. Everyone thought that there should be some deeper theory that 
explained the action of gravity. Various proposals were made, often involv­
ing an “aether” that was supposed to permeate the universe. Though there 
was no consensus on how the mechanism behind gravity worked, there was 
consensus that gravity was not spooky action at a distance and that some 
explanation would be found. There was a belief in what we now call local 
realism.
Newton’s law of gravitation was superseded by Einstein’s general theory 
of gravitation. Einstein’s theory not only improved on Newton’s in terms of 
accurately predicting astronomical observations that could not be deduced 
using Newton’s theory, but it also gave an explanation as to how gravity 
worked. It described the warping of space-time. A planet moved according 
to the shape of space-time where it was located. There was no spooky action 
at a distance. Einstein’s theory was not only more precise, but it also gave a 
description of how gravity worked, and this description was local. A planet 
moves according to the shape of space in its vicinity.
The Copenhagen interpretation of quantum mechanics, of course, rein­
troduced this idea of spooky action at a distance. When you measure a pair 
of entangled qubits the state immediately changes, even if the qubits are 
physically far apart. Einstein’s philosophy seems entirely natural. He had 
just eliminated spooky action from the theory of gravity, and now it was 
being proposed again. The difference now was that Bohr didn’t believe that 
there was some deeper theory that could explain the mechanism behind 
this action. Einstein disagreed.
Einstein believed he could prove that Bohr was wrong. With Boris Podol­
sky and Nathan Rosen, he wrote a paper pointing out that his special theory 
of relativity implied that information could not travel faster than the speed 
of light, but instantaneous action at a distance would mean that informa­
tion could be sent from Alice to Bob instantaneously. This problem became 
known as the EPR paradox, for Einstein-Podolsky-Rosen.
Nowadays, the EPR paradox is usually described in terms of spin, and 
this is how we will do it, but this was not how Einstein et al. described the 
problem. They considered position and momentum of two entangled par­
ticles. It was David Bohm who reformulated the problem in terms of spin. 
Bohm’s formulation is the one that is practically always used now, and it 
is the formulation that John Stewart Bell used to calculate his important 
Bell’s Inequality 
77
inequality. Even though Bohm played a major role in describing and refor­
mulating the paradox, his name is usually omitted.
In the last chapter we pointed out that the Copenhagen interpretation 
does not allow information to be transmitted faster than the speed of light, 
and so although the EPR paradox is not really a paradox, there is still the 
question of whether there can be an explanation that eliminates the spooky 
action.
Einstein and Hidden Variables
In the classical view, physics is deterministic—if you know all the initial 
conditions to infinite precision, then you can predict the outcome with 
certainty. Of course, you can only know initial conditions to some finite 
precision, meaning that there will always be some small error in what is 
measured—a small difference between the measured value and the true 
value. As time progresses this error can grow until we are unable to make 
any sensible prediction for what happens in the long run. This idea forms 
the basis for what is commonly known as sensitive dependence on initial 
conditions. It explains why forecasting the weather for more than a week 
or so is very unreliable. It is important to remember, however, that the 
underlying theory is deterministic. The weather seems unpredictable, but 
this is not due to some inherent randomness, it is just that we cannot make 
measurements that are sufficiently accurate.
Another area where probability is incorporated into classical physics is in 
laws concerning gases—the laws of thermodynamics—but again the under­
lying theory is still deterministic. If we know exactly the velocities and 
masses of every molecule in the gas, in theory we can predict with com­
plete accuracy what happens to each molecule in the future. In practice, of 
course, there are far too many molecules to consider them one by one, and 
so we take average values and look at the gas from a statistical viewpoint.
This classical, deterministic view was what Einstein was referring to 
when he famously said that God does not play dice with the universe. He 
felt that the use of probability in quantum mechanics showed that the the­
ory was not complete. There should be a deeper theory, perhaps involving 
new variables, that is deterministic but looks probabilistic if you don’t con­
sider all of these as yet unknown variables. These as yet unknown variables 
became known as hidden variables.
78 
Chapter 5
A Classical Explanation of Entanglement
We begin with our quantum clocks in state 1
2
1
2
↑↑+
↓↓. Alice and 
Bob are going to ask the question about whether the hand is pointing 
to twelve. The quantum model says that Alice and Bob will get exactly 
the same answer: that it’s pointing to twelve or it’s pointing to six. Both 
answers are equally likely. We can actually perform experiments measur­
ing the spin of entangled electrons. The experimental outcomes are exactly 
what the quantum model predicts. How do we explain this with a classical 
model?
The classical interpretation for the preceding situation is quite simple. 
Electrons have a definite spin in any direction. Entangled electrons become 
entangled through some local interaction. Again, we appeal to hidden vari­
ables and a deeper theory. We don’t know exactly what happens, but there 
is some local process that puts the electrons in exactly the same spin con­
figuration state. When they are entangled, a direction of spin is chosen for 
both electrons.
This can be compared to our being given a deck of cards that we shuffle. 
We then take out one card without looking at it. We cut the card in two 
and put the halves in two envelopes, all the time without any knowledge of 
which card has been chosen. We then send the cards to Bob and Alice, who 
live in different parts of the universe. Alice and Bob have no idea which 
card they have. It could be anyone of the fifty-two, but as soon as Alice 
opens her envelope and sees the jack of diamonds she knows that Bob’s 
card is also the jack of diamonds. There is no action at a distance, and there 
is nothing spooky going on.
For Bell’s result, we need to measure our entangled qubits in three dif­
ferent directions. We return to our entangled clock analogy. We will be ask­
ing one of three questions, about whether the hand is pointing to twelve, 
to four, or to eight. The quantum theoretical model predicts that for each 
question the answer will be either that the hand is pointing in the direction 
asked or that it is pointing in the opposite direction. For each question both 
answers are equally likely. But when Alice and Bob ask exactly the same 
question, they will both get exactly the same answer. We can describe this 
classically by giving essentially the same answer as before.
There is some local process that entangles the clocks. We don’t attempt to 
describe exactly how this is done, but just appeal to hidden variables—there 
Bell’s Inequality 
79
is some deeper theory that explains it. But when the clocks are entangled, 
definite answers to the three questions are chosen. This can be compared to 
our having three decks of cards, each with different colored backs. We take 
a card from the blue deck, from the red deck and the green deck. We cut 
these three cards in half and mail three halves to Alice and the other three 
halves to Bob. If Alice looks at her green card and sees the jack of diamonds, 
she knows that Bob’s green card is also the jack of diamonds.
For our quantum clocks, the classical theory says that there is a definite 
answer to each question that is already determined before we ask it. Quan­
tum theory says, contrarily, that the answer to the question is not deter­
mined up until the time we ask it.
Bell’s Inequality
Imagine that we are generating a stream of pairs of qubits that we are 
sending to Alice and Bob. Each pair of qubits is in the entangled state 
1
2
1
2
↑↑+
↓↓. Alice randomly chooses to measure her qubit in 
direction 0°, 120°, or 240°. Each of these directions is chosen randomly, 
each with probability 1/3. Alice doesn’t bother to keep track of the direc­
tions she has chosen, but she does write down whether she gets 0 or 1 as 
the answer. (Remember, 0 corresponds to the first basis vector and 1 to the 
second.) Shortly after Alice has measured her qubit, Bob chooses one of 
the same three directions at random, each with probability 1/3, and mea­
sures his qubit. Like Alice, he doesn’t record the direction, just the result of 
whether he obtained either 0 or 1.
In this way, both Alice and Bob generate a long string of 0s and 1s. 
They then compare their strings symbol by symbol. If they agree on the 
first symbol, they write down A. If they disagree on the first symbol, they 
write down D. Then they look at the second symbol and write down A or D 
depending on whether the symbols agree or disagree. They continue in this 
way through their entire strings.
In this way they generate a new string consisting of As and Ds. What 
proportion of the string is made up of As? Bell realized that the quantum 
mechanics model and the classical model gave different numbers for the 
answer.
80 
Chapter 5
The Answer of Quantum Mechanics
The qubits are in the entangled spin state 1
2
1
2
↑↑+
↓↓. We have 
already observed that if Alice and Bob both choose the same measurement 
direction, then they will get the same answer. The question is what happens 
if they choose different bases.
We will take the case when Alice chooses ↘
↖
,
(
) and Bob chooses 
↙
↗
,
(
). The entangled state is 
1
2
1
2
↑↑+
↓↓, which can be 
written in Alice’s basis as 
1
2
1
2
↘↘
↖↖
+
. When Alice makes 
her measurement, the state jumps to either ↘↘ or ↖↖; both are 
equally likely. If it jumps to ↘↘, she will write down 0. If it jumps to 
↖↖, she will write down 1.
Bob must now make his measurement. Suppose after Alice’s measure­
ment that the qubits are in state ↘↘, so Bob’s qubit is in state ↘. To 
calculate the result of Bob’s measurement we have to rewrite this using 
Bob’s basis. (We did a similar calculation on page 51.)
Writing everything using two-dimensional kets, we have:
↘=
−








12
3 2
↙=
−
−








12
3 2
↗=
−










3
2
12
We multiply ↘ by the matrix with rows given by the bras of Bob’s basis.
−
−
−









−







=








12
3 2
3
2
12
12
3 2
12
3 2
This tells us that ↘
↙
↗
=
+
1
2
3
2
. When Bob makes his measurement, 
he will get 0 with probability 1/4 and 1 with probability 3/4. So, when Alice 
gets 0, Bob will get 0 with probability 1/4. It is easy to check the other case. 
If Alice gets 1, Bob’s probability of also getting 1 is 1/4.
The other cases are all similar: If Bob and Alice measure in different 
directions, they will agree 1/4 of the time and disagree 3/4 of the time.
Bell’s Inequality 
81
To summarize: One-third of the time they measure in the same direction 
and agree each time; two-thirds of the time they measure in different direc­
tions and agree on just one-quarter of these measurements. This gives the 
proportion of As in the string consisting of As and Ds as
1
3
1
2
3
1
4
1
2
×
+
×
=
.
The conclusion is that the quantum mechanics model gives the answer 
that in the long run the proportion of As should be one-half.
We now look at the classical model.
The Classical Answer
The classical view is that the measurements in all directions are determined 
right from the start. There are three directions. A measurement in each 
direction can yield either a 0 or a 1. This gives us eight configurations: 000, 
001, 010, 011, 100, 101, 110, 111, where the leftmost digit gives us the 
answer if we were to measure in the basis ↑
↓
(
)
,
, the middle digit gives us 
the answer if we were to measure in the basis ↘
↖
,
(
), and the rightmost 
digit gives us that answer if we were to measure in the basis ↙
↗
,
(
).
The entanglement just means that the configurations for Alice’s and 
Bob’s qubits are identical—if Alice’s qubit has configuration 001, then so 
does Bob’s. We now have to figure out what happens when Alice and Bob 
choose a direction. For example, if their electrons are in configuration 001 
and Alice measures using basis ↑
↓
(
)
,
, and Bob measures using the third 
basis, then Alice will get a measurement of 0 and Bob a measurement of 1, 
and they will disagree.
The table below gives all the possibilities. The left column gives the 
configurations, and the top row gives the possibilities for Alice and Bob’s 
measurement bases. We will use letters to represent the bases. We denote 
↑
↓
(
)
,
 by a, ↘
↖
,
(
) by b, and ↙
↗
,
(
) by c. We will list Alice’s basis 
first and then Bob’s. So, for example, (b, c) means Alice is choosing ↘
↖
,
(
) 
and Bob is choosing ↙
↗
,
(
). The entries in the table show whether the 
measurements agree or disagree.
82 
Chapter 5
We do not know the probabilities that should be assigned to the con­
figurations. There are eight possible configurations, so it might seem plau­
sible that they each occur with probability 1/8, but they perhaps are not 
all equal. Our mathematical analysis will make no assumption about these 
probabilities’ values. We can, however, assign definite probabilities to the 
measurement directions. Both Bob and Alice are choosing each of their 
three bases with equal probability, so each of the nine possible pairs of bases 
occurs with probability 1/9.
Notice that each row contains at least five As, telling us that given a pair 
of qubits with any configuration the probability of getting an A is at least 
5/9. Since the probability of getting an A is at least 5/9 for each of the spin 
configurations, we can deduce that the probability overall must be at least 
5/9, no matter what proportion of time we get any one configuration.
We have now derived Bell’s result. The quantum theory model tells us 
that Alice’s and Bob’s sequences will agree exactly half the time. The classi­
cal model tells us that Alice’s and Bob’s sequences will agree at least 5/9ths 
of the time. It gives us a test to distinguish between the two theories.
Bell published his inequality in 1964. Sadly, this was after the death of 
both Einstein and Bohr, so neither ever realized that there would be an 
experimental way of deciding their debate.
Actually carrying out the experiment is tricky. John Clauser and Stuart 
Freedman first performed it in 1972. It showed that the quantum mechani­
cal predictions were correct. The experimenters, however, had to make 
some assumptions that could not be checked, leaving some chance that the 
classical view could still be correct. The experiment has since been repeated 
Measurement directions
Config.
a a
,
(
)
a b
,
(
)
a c
,
(
)
b a
,
(
)
b b
,
(
)
b c
,
(
)
c a
,
(
)
c b
,
(
)
c c
,
(
)
000
A
A
A
A
A
A
A
A
A
001
A
A
D
A
A
D
D
D
A
010
A
D
A
D
A
D
A
D
A
011
A
D
D
D
A
A
D
A
A
100
A
D
D
D
A
A
D
A
A
101
A
D
A
D
A
D
A
D
A
110
A
A
D
A
A
D
D
D
A
111
A
A
A
A
A
A
A
A
A
Bell’s Inequality 
83
with increasing sophistication. It has always been in agreement with quan­
tum mechanics, and there seems little doubt now that the classical model 
is wrong.
There were three potential problems with the earliest experiments. The 
first was that Alice and Bob were too close to one another. The second was 
that their measurements were missing too many entangled particles. The 
third was that Alice’s and Bob’s choices of measurement direction were not 
really random. If the experimenters are close to one another, it is theoreti­
cally possible that the measurements could be influenced by some other 
mechanism. For example, as soon as the first measurement is made, a pho­
ton travels to influence the second measurement. To ensure that this is not 
occurring, the measurers need to be far enough apart to know that the time 
interval between their measurements is less than the time it takes for a pho­
ton to travel between them. To counter this loophole, entangled photons 
are used. Unlike entangled electrons, entangled photons can travel long 
distances without interacting with the outside world.
Unfortunately, this property of not interacting readily with the outside 
world makes it difficult to measure them. In experiments involving pho­
tons, many of the entangled photons escape measurement, so it is theo­
retically possible that there is some selection bias going on—the results 
are reflecting the properties of a nonrepresentative sample. To counter the 
selection bias loophole, electrons have been used. But if electrons are used, 
how do you get the entangled electrons far enough apart before you mea­
sure them?
This is exactly the problem that the team from Delft, which we men­
tioned in the previous chapter, solved using electrons trapped in diamonds 
that are entangled with photons. Their experiment seems to have closed 
both loopholes simultaneously.*
The problem of randomness is harder. If the Copenhagen interpretation 
is correct, producing streams of random numbers is easy. If we are question­
ing this interpretation as it relates to randomness, however, we need to test 
strings of numbers and see if they are random. There are many tests to look 
for underlying patterns among the numbers. These tests, unfortunately, can 
prove only a negative. If a string fails a test, then we know the string is not 
random. Passing the test is a good sign, but it is not proof that the string is 
*  The paper “Loophole-free Bell inequality violation using electron spins separated 
by 1.3 kilometres” by B. Hensen et al. was published in Nature in 2015.
84 
Chapter 5
random. All we can say is that no quantum mechanical generated string has 
failed a test for randomness.
Clever ways have been chosen to ensure that the direction Alice chooses 
to measure is not correlated to Bob’s. But again, it is not possible to rule 
out the possibility that some hidden variable theory determines what, we 
think, are uncorrelated random outcomes.
Most people consider that Einstein has been proved wrong, but that his 
theory made sense. Bell, in particular, believed that the classical theory was 
the better of the two theories up until he saw the results of the experiments, 
saying, “This is so rational that I think that when Einstein saw that, and 
the others refused to see it, he was the rational man. The other people, 
although history has justified them, were burying their heads in the sand. 
… So for me, it is a pity that Einstein’s idea doesn’t work. The reasonable 
thing just doesn’t work.”**
I am in total agreement with Bell. When you first meet these ideas, it 
seems to me that Einstein’s view is the natural one to take. I am surprised 
that Bohr was so convinced that it was wrong. Bell’s result, often called 
Bell’s theorem, resulted in Bell’s being nominated for the Nobel Prize in 
physics. Many people think that if he hadn’t died of a stroke at the rela­
tively young age of sixty-one he would have received it. Interestingly, there 
is a street in Belfast named after Bell’s theorem—this might be the only 
theorem that you can enter into Google Maps and get a location.
We have to abandon the standard assumption of local reality. When par­
ticles are entangled, but perhaps far apart, we should not think of spin as a 
local property associated with each of the particles separately; it is a global 
property that has to be considered in terms of the pair of particles.
Before we leave our discussion of quantum mechanics, we should also 
look at one other unusual aspect of the theory.
Measurement
In our description of quantum mechanics we describe a state vector as 
jumping to a basis vector when we make a measurement. Everything is 
deterministic until we make a measurement, and then it jumps to one of 
the basis vectors. The probabilities for jumping to each of the basis vectors 
**  J. Bernstein, Quantum Profiles (Princeton: Princeton University Press, 1991), 84.
Bell’s Inequality 
85
are known exactly, but they are still probabilities. The theory changes from 
being deterministic to probabilistic when we make a measurement.
In the general theory of quantum mechanics it is the solution of the 
Schrödinger’s wave equation that collapses when a measurement is made. 
Erwin Schrödinger, of the eponymous equation, was very uncomfortable 
with this idea of waves collapsing to states given by probabilities.
A significant problem is that what we mean by measurement is not 
defined. It is not part of quantum mechanics. Measurements cause jumps, 
but what do we mean by measurement? Sometimes the word observation 
has been used instead of measurement, and this has led some people to talk 
about consciousness causing the jump, but this seems unlikely. The stan­
dard explanation is that the measurement involves an interaction with a 
macroscopic device. The measuring device is large enough that it can be 
described using classical physics and does not have to be incorporated into 
the quantum theoretical analysis—that whenever we make a measurement 
we have to interact physically with the object being measured, and this 
interaction causes the jump. But this explanation is not entirely satisfac­
tory. It seems a plausible description, but it lacks mathematical precision.
Various interpretations of quantum mechanics have been proposed, 
each trying to eliminate something that seems problematic in the Copen­
hagen interpretation.
The many-worlds interpretation deals with the measurement problem by 
saying that it only appears that the state vector jumps to one of the possi­
bilities, but in fact there are different universes and each of the possibilities 
is an actual occurrence in one of the many universes. The version of you in 
this universe sees one outcome, but there are other versions of you in other 
universes that see the other outcomes.
Bohmian mechanics tackles the introduction of probabilities. It is a 
deterministic theory in which particles behave like classical particles, but 
there is also a new entity called the pilot wave that gives the nonlocality 
properties.
There are many ardent believers in each of these theories. For exam­
ple, David Deutsch, whom we will meet later, believes in the many-worlds 
view. But at the moment there are no scientific tests that have shown that 
one set of beliefs is preferable to another, unlike the local hidden-variable 
theory that the Bell’s inequality experiments have shown to be wrong. All 
of the interpretations are consistent with our mathematical theory. Each 
86 
Chapter 5
interpretation is a way of trying to explain how the mathematical theory 
relates to reality. Perhaps, at some point there will be an insightful genius 
like Bell who can show that the different interpretations lead to different 
conclusions that can be experimentally differentiated, and that experi­
ments will then give us some reason for choosing one interpretation over 
another. But at this point, most physicists subscribe to the Copenhagen 
interpretation. There is no convincing reason not to use this interpretation, 
so we shall use it without further comment from now on.
The final topic of this chapter shows that Bell’s theorem is not just of 
academic interest. It can actually be used to give a secure way of sharing a 
key to be used in cryptography.
The Ekert Protocol for Quantum Key Distribution
In 1991, Artur Ekert proposed a method based on entangled qubits used in 
Bell’s test. There are many slight variations. We will present a version that 
uses our presentation of Bell’s result.
Alice and Bob receive a stream of qubits. For each pair, Alice receives one 
and Bob receives the other. The spin states are entangled. They are always 
in the state 1
2
1
2
↑↑+
↓↓.
If Alice and Bob measure their respective qubit using the same orthonor­
mal basis, then we saw that they will get either 0 or 1with equal probability, 
but they will both get exactly the same answer.
We could imagine a protocol where Alice and Bob both decide to mea­
sure their qubits in the standard basis every time. They will end up with 
exactly the same string of bits, and the string will be a random sequence of 
0s and 1s, which seems like a great way of both choosing and communicat­
ing a key. The problem, of course, is that it is not the least bit secure. If Eve 
is intercepting Bob’s qubits, she can measure them in the standard basis 
and then send the resulting unentangled qubit on to Bob. The result is that 
Alice, Bob, and Eve all end up with identical strings of bits.
The solution is to measure the qubits using a random choice of three 
bases—exactly as we did with the Bell test. As in the BB84 protocol, for 
each measurement Alice and Bob write down both the result and the basis 
that they chose. After they have made 3n measurements, they compare 
the sequences of bases that they chose. This can be done on an insecure 
Bell’s Inequality 
87
channel—they are only revealing the basis, not the result. They will agree 
on approximately n of them. In each place they have chosen the same basis 
they will have made the same measurement. They will either both have 0, 
or both have 1. This gives them a string of n 0s and 1s. This will be their key 
if Eve is not listening in.
They now test for Eve. If Eve is eavesdropping, she will have to make 
measurements. Whenever she does, the entangled states become unen­
tangled. Alice and Bob look at the strings of 0s and 1s that come from the 
times when they chose different bases. This gives two strings of 0s and 1s 
with length about 2n. From the Bell inequality calculation, they know that 
if their states are entangled, in each place they should only agree 1/4 of the 
time. However, if Eve is measuring one of the qubits the proportion of times 
they agree changes. For example, if Eve measures a qubit before Alice and 
Bob have made their measurements, it is fairly straightforward to check all 
the possibilities to show that the proportion of times that Alice and Bob 
will agree increases to 3/8. This gives them a test for the presence of Eve. 
They calculate the proportion of agreement. If it is 1/4, they can conclude 
that nobody has interfered and use the key.
The Ekert protocol has the useful feature that the process generates the 
key. No digits need to be generated and stored beforehand, thus eliminat­
ing one of the main security threats to encryption. This protocol has been 
successfully carried out in the lab using entangled photons.
Having concluded the introduction to quantum concepts, the next topic 
to introduce is classical computation. This is the topic of the next chapter.
6  Classical Logic, Gates, and Circuits
Chapter 
6
Classical 
Logic, 
Gates, 
and 
Circuits
© Massachusetts Institute of TechnologyAll Rights Reserved
In this chapter we briefly study classical computation, presenting the ideas 
in roughly chronological order. We start with boolean functions and logic, 
first introduced by George Boole in the late nineteenth century. In the 
1930s, Claude Shannon studied boolean algebra and realized that boolean 
functions could be described using electrical switches. The electrical com­
ponents that correspond to boolean functions are called logic gates. Com­
posing boolean functions becomes the study of circuits involving these 
gates. We will begin by studying boolean functions in terms of logic; then 
we will show how to translate everything into circuits and gates. The mate­
rial, up to this point, is now considered standard and is contained in every 
introductory computer science text. But after this we look at some ideas 
that are usually not part of the standard introduction.
In the 1970s, the Nobel Prize–winning physicist Richard Feynman 
became interested in computing and, for a few years in the early 1980s, 
he gave a course on computation at the California Institute of Technology. 
These lectures were eventually written up as Feynman Lectures on Computa­
tion. Feynman’s interest in computation was partly due to his interaction 
with Edward Fredkin and Fredkin’s idiosyncratic views of physics and com­
putation. Fredkin believes that the universe is a computer, and that since 
the laws of physics are reversible we ought to study reversible computa­
tion and reversible gates. But even though Fredkin’s overarching thesis is 
not widely accepted in the physics community, he is recognized for hav­
ing some brilliant and unconventional ideas. One of these is the billiard 
ball computer. Feynman’s book includes a discussion of reversible gates and 
shows how any computation can be performed by bouncing balls off one 
another.
90 
Chapter 6
We take Feynman’s approach. It turns out that reversible gates are exactly 
what we need for quantum computing. The billiard ball computer led Feyn­
man to think of particles interacting instead of balls. It was the inspiration 
for his work on quantum computing, but we include it here mainly because 
of its sheer simplicity and ingenuity.
Logic
In the late nineteenth century George Boole realized that certain parts of 
logic could be treated algebraically—that there were laws of logic that could 
be expressed in terms of algebra. We adopt the now standard way of intro­
ducing boolean logic by using truth tables for the three basic operations 
not, and, and or.
Negation
If a statement is true, then its negation is false, and conversely, if a state­
ment is false, then its negation is true. For example, the statement 2 + 2 
= 4 is true, and its negation 2 + 2 ≠ 4 is false. Instead of giving concrete 
examples of statements, we often let the symbols P, Q, and R stand in for 
them. So, for example, 2 + 2 = 4 might be represented by P. The symbol ¬ 
stands for not; if P represents the statement 2 + 2 = 4, then ¬P stands for 2 
+ 2 ≠ 4. We can then summarize the basic properties of negation using our 
symbols: If P is true, then ¬P is false. If P is false, then ¬P is true.
To make things even more concise we can use the symbols T and F to 
denote true and false respectively. We can then define the properties using 
a table.
P
¬P
T
F
F
T
And
The symbol for and is ∧. If we have two statements P and Q, we can com­
bine them to form P
Q
∧
. The statementP
Q
∧
 is true if and only if both of 
the component statements P and Q are true. We define and by the following 
table, where the first two columns give the possibilities for the truth-values 
of P and Q and the third column gives us the corresponding truth-value of 
P
Q
∧
.
Classical Logic, Gates, and Circuits 
91
Or
The symbol for or is ∨ and is defined by the following table.
P
Q
P
Q
∧
T
T
T
T
F
F
F
T
F
F
F
F
P
Q
P
Q
∨
T
T
T
T
F
T
F
T
T
F
F
F
P
Q
P
Q
⊕
T
T
F
T
F
T
F
T
T
F
F
F
Notice that P
Q
∨
 is true if both P and Q are true, so P
Q
∨
 is true if either 
one of P or Q is true and also if both are true. This is the or that is used 
in mathematics and is sometimes called the inclusive or. The exclusive or is 
defined to be true if either one, but not both, of P and Q is true. It is false if 
they are both false, but is also false if they are both true. The exclusive or is 
denoted by ⊕. Its truth table is below.
(Later we will see why the symbol for the exclusive or resembles a plus sign—
it corresponds to addition modulo two.)
Boolean Algebra
We start by showing how to construct the truth table for any binary expres­
sion. For concreteness we will construct the truth table of ¬ ¬
∧¬
(
)
P
Q . This 
is done in several steps. First we write down the table for the possibilities 
for P and Q.
92 
Chapter 6
Then we attach columns for ¬P and ¬Q , writing in the appropriate truth-
values in each case.
P
Q
T
T
T
F
F
T
F
F
P
Q
¬P
¬Q
T
T
F
F
T
F
F
T
F
T
T
F
F
F
T
T
P
Q
¬P
¬Q
	
¬
∧¬
P
Q
T
T
F
F
F
T
F
F
T
F
F
T
T
F
F
F
F
T
T
T
P
Q
¬P
¬Q
	
¬
∧¬
P
Q
	
¬ ¬
∧¬
(
)
P
Q
T
T
F
F
F
T
T
F
F
T
F
T
F
T
T
F
F
T
F
F
T
T
T
F
Next we add a column for ¬
∧¬
P
Q . This is true only in the case when both 
¬P and ¬Q  are true.
Finally, we get to the column associated with ¬ ¬
∧¬
(
)
P
Q . This statement 
is true if and only if ¬
∧¬
P
Q  is false.
Omitting the intermediate columns corresponding to the intermediate 
steps gives the following table.
Classical Logic, Gates, and Circuits 
93
We look for entries of T in the third column. The first occurs when P has 
value T and Q has value F. An expression that gives us a value of T only for 
those particular truth-values of P and Q is P
Q
∧¬
.
The next value of T in the third column occurs when P has value F and 
Q has value T. An expression that gives us a value of T only for those par­
ticular truth-values of P and Q is ¬
∧
P
Q.
These are the only places where T occurs in the third column. To get an 
expression equivalent to the one that we want, we now join all the expres­
sions we have generated so far using ∨ s, so
P
Q
P
Q
P
Q
⊕
≡
∧¬
(
) ∨¬
∧
(
).
P
Q
¬ ¬
∧¬
(
)
P
Q
T
T
T
T
F
T
F
T
T
F
F
F
P
Q
P
Q
⊕
T
T
F
T
F
T
F
T
T
F
F
F
Logical Equivalence
Notice that the truth-values in the table for ¬ ¬
∧¬
(
)
P
Q  are identical to the 
truth-values in the table for P
Q
∨
. They have exactly the same truth-values 
in every case. We say that the statements P
Q
∨
 and ¬ ¬
∧¬
(
)
P
Q  are logically 
equivalent. We write:
P
Q
P
Q
∨
≡¬ ¬
∧¬
(
)
This means that we need never use or. Every case where or occurs can be 
replaced using expressions involving ¬ and ∧.
What about the exclusive or, which we write with ⊕? Can we replace this 
by an expression involving only the use of ¬ and ∧? We can, and we will 
now show how to do this.
We consider the truth table for ⊕.
94 
Chapter 6
We know
P
Q
P
Q
∨
≡¬ ¬
∧¬
(
).
Using this to replace ∨ gives
P
Q
P
Q
P
Q
⊕
≡¬ ¬
∧¬
(
) ∧¬ ¬
∧
(
)
(
)
(
).
Again, this means that we that we need never use ⊕. Every case where ⊕
occurs can be replaced using expressions involving ¬ and ∧. The method 
we have just used for replacing ⊕ using ¬ and ∧ works quite generally.
Functional Completeness
We can think of the logical operators that we have introduced as functions. 
For example, ∧ is a function that has two inputs, P and Q, and gives us one 
output; ¬ has one input and one output.
We could invent our own function that has a number of inputs that take 
on values of T and F and in each of the cases gives us a value of either T or F; 
such a function is called a boolean function. To make things more concrete, 
we will invent a function that has three inputs that we will label P, Q, and 
R. We call our function f P Q R
,
,
(
). To define our function, we have to com­
plete the third column in the following table.
P
Q
R
f P Q R
,
,
(
)
T
T
T
T
T
F
T
F
T
T
F
F
F
T
T
F
T
F
F
F
T
F
F
F
There are eight values that need to be filled in. There are two choices for 
each value, giving us a total of 28 possible functions. We will show that no 
matter how we choose our function, we can find an equivalent expression 
that uses only the functions ¬ and ∧.
We use exactly the same method that we used to show that
Classical Logic, Gates, and Circuits 
95
The first T occurs when P and R have values T, and Q has value F. A function 
that gives us a value of T for only this set of truth-values is P
Q
R
∧¬
∧
. The 
next T occurs when P and R have values F and Q has value T. A function 
that gives us a value of T for only this set of truth-values is ¬
∧
∧¬
P
Q
R. The 
final T occurs when P, Q, and R all have value F. A function that gives us a 
value of T for only this set of truth-values is ¬
∧¬
∧¬
P
Q
R.
An expression that takes on value T in just these three cases is
P
Q
R
P
Q
R
P
Q
R
∧¬
∧
(
) ∨¬
∧
∧¬
(
) ∨¬
∧¬
∧¬
(
),
so
f P Q R
P
Q
R
P
Q
R
P
Q
R
,
,
(
) ≡
∧¬
∧
(
) ∨¬
∧
∧¬
(
) ∨¬
∧¬
∧¬
(
).
The final step is to replace ∨ using the fact that
P
Q
P
Q
∨
≡¬ ¬
∧¬
(
).
Replacing the first occurrence gives
f P Q R
P
Q
R
P
Q
R
P
Q
R
,
,
(
) ≡¬ ¬
∧¬
∧
(
) ∧¬ ¬
∧
∧¬
(
)
(
) ∨¬
∧¬
∧¬
(
).
Replacing the second occurrence tells us that f P Q R
,
,
(
) is logically equiva­
lent to
¬ ¬ ¬ ¬
∧¬
∧
(
) ∧¬ ¬
∧
∧¬
(
)
(
)
[
] ∧¬ ¬
∧¬
∧¬
[
]
(
)
P
Q
R
P
Q
R
P
Q
R .
P
Q
R
f P Q R
,
,
(
)
T
T
T
F
T
T
F
F
T
F
T
T
T
F
F
F
F
T
T
F
F
T
F
T
F
F
T
F
F
F
F
T
P
Q
P
Q
P
Q
⊕
≡
∧¬
(
) ∨¬
∧
(
).
We begin by looking for values of T in the last column. To help make things 
easier to follow we will use the specific function given by the following 
table, but the method we use will work for any boolean function.
96 
Chapter 6
This method works in general. If f is a function that is defined by a truth 
table, then f is logically equivalent to some expression that involves only 
the functions ¬ and ∧. Since we can generate any boolean function what­
soever using just these two functions, we say that ¬ ∧
{
}
,
 is a functionally 
complete set of boolean operators.
It might seem surprising that we can generate any function defined by 
a truth table using just ¬ and ∧, but incredibly, we can do even better. 
There is a binary operator called Nand, and any boolean function is logi­
cally equivalent to some expression that only uses the Nand operator.
Nand
Nand is a portmanteau word formed from combining not and and. It is 
denoted by ↑. It can be defined by
P
Q
P
Q
↑
= ¬
∧
(
),
or, equivalently, by the following truth table:
P
Q
P
Q
↑
T
T
F
T
F
T
F
T
T
F
F
T
P
P
P
∧
¬
∧
(
)
P
P
T
T
F
F
F
T
We know that ¬ ∧
{
}
,
 is a functionally complete set of operators, so to show 
that Nand by itself is functionally complete—that any boolean operator can 
be rewritten as an equivalent function that just uses Nand — we just need 
to show that both and and not have equivalent expressions that are written 
solely in terms of Nand.
Consider the following truth table, which considers just the statement P, 
then P
P
∧
 and finally ¬
∧
(
)
P
P .
Notice that the final column has the same truth-values as ¬P, telling us
¬
∧
(
) ≡¬
P
P
P,
Classical Logic, Gates, and Circuits 
97
but ¬
∧
(
)
P
P  is just P
P
↑
, so
P
P
P
↑
≡¬ .
This shows that we can replace all occurrences of not with Nand. We now 
turn our attention to and.
Observe that
P
Q
P
Q
∧
≡¬¬
∧
(
).
Now, ¬
∧
(
) ≡
↑
P
Q
P
Q, so
P
Q
P
Q
∧
≡¬
↑
(
).
We can now replace not using the preceding identity to obtain
P
Q
P
Q
P
Q
∧
≡
↑
(
) ↑
↑
(
).
Henry M. Sheffer, in 1913, first published the fact that Nand by itself is 
functionally complete. Charles Sanders Peirce also knew this fact in the late 
nineteenth century, but like much of his highly original work it remained 
unpublished until much later. (Sheffer used the symbol | for Nand. Many 
authors use, or have used, Sheffer’s symbol instead of ↑. It is called the Shef­
fer stroke.)
Boolean variables take on one of two values. We have been using T and 
F for these, but we can use any two symbols. In particular, we can use 0 and 
1. The advantage of replacing T and F by 0 and 1 is that we can then think 
of boolean functions as operating on bits. This is what we will do from 
now on.
There are two choices for how we could do the substitution. The conven­
tion is that 0 replaces F and 1 replaces T, and this what we shall use. Notice 
that conventionally we list T before F, but 0 before 1. Consequently, truth 
tables written in terms of 0 and 1 reverse the order of the rows when written 
in terms of T and F. This shouldn’t cause any confusion, but just to hammer 
home the point, here are the two tables for P
Q
∨
.
P
Q
P
Q
∨
P
Q
P
Q
∨
T
T
T
0
0
0
T
F
T
0
1
1
F
T
T
1
0
1
F
F
F
1
1
1
98 
Chapter 6
Gates
Various people realized that if logic could be expressed in terms of algebra, 
then machines could be designed to perform logical operations, but the 
most influential by far was Claude Shannon, who showed that all of bool­
ean algebra could be performed using electrical switches. This is one of the 
fundamental ideas underlying the circuit design of all modern computers. 
Remarkably, he did this while still a master’s student at MIT.
At discrete time intervals either a pulse of electricity is transmitted or it is 
not. If at the appropriate time interval we receive a pulse of electricity, then 
we think of this as representing the truth-value T or, equivalently, bit value 
1. If at the appropriate time interval we do not receive a pulse of electricity, 
then we think of this as representing the truth-value F or, equivalently, bit 
value 0.
The combinations of switches that correspond to our binary operators 
are called gates. The common gates have special diagrams associated to 
them. We look at some of these.
The NOT Gate
Figure 6.1 shows the symbol for the NOT gate. This can be thought of as a 
wire entering from the left and leaving from the right. If we input 1, we get 
output 0. If we input 0, we get output 1.
The AND Gate
Figure 6.2 shows the symbol for the AND gate. Again, it is read from left to 
right. It has two inputs that can be either 0 or 1 and one output. Figure 6.3 
shows the four cases.
Figure 6.1
The NOT gate.
Figure 6.2
The AND gate.
Classical Logic, Gates, and Circuits 
99
The OR Gate
Figure 6.4 shows the symbol for the OR gate, along with the inputs and 
output for the four cases.
The NAND Gate
Figure 6.5 shows the symbol for the NAND gate, along with the inputs and 
output for the four cases.
Circuits
We can connect the gates together to form a circuit. Despite the name, 
there is nothing circular about circuits. They are linear and are read from 
left to right. We input our bits into the wires on the left and read the output 
from the wires on the right. We will look at examples that correspond to the 
boolean functions that we looked at earlier.
We start with the boolean expression ¬ ¬
∧¬
(
)
P
Q . The corresponding 
circuit can be given using gates. This is shown in figure 6.6, where the 
wires entering and leaving the gates have been labeled with the appropriate 
expressions. Recall that P
Q
P
Q
∨
≡¬ ¬
∧¬
(
), so the circuit in figure 6.6 is 
equivalent to the OR gate.
1
1
1
1
0
0
0
1
0
0
0
0
Figure 6.3
The four possibilities for inputs to the AND gate.
1
1
1
1
0
1
0
1
1
0
0
0
Figure 6.4
The OR gate.
1
1
0
1
0
1
0
1
1
0
0
1
Figure 6.5
The NAND gate.
100 
Chapter 6
Our next example is P
P
↑
. We want to enter the same value, P, into both 
the inputs of our NAND gate. Splitting the input signal into two by con­
necting an additional wire achieves this. This process of splitting a signal 
into multiple copies is called fan-out. Figure 6.7 shows the circuit.
We know that P
P
P
↑
≡¬ , so the circuit in figure 6.7 is equivalent to the 
NOT gate.
Our final example is the binary expression P
Q
P
Q
↑
(
) ↑
↑
(
). To get 
the two copies of P
Q
↑
, we again need to use fan-out. Figure 6.8 shows 
the circuit.
We know that P
Q
P
Q
P
Q
∧
≡
↑
(
) ↑
↑
(
), so the circuit in figure 6.8 is 
equivalent to the AND gate.
NAND Is a Universal Gate
Earlier we showed that the boolean function Nand was functionally com­
plete. In this section we repeat the argument using gates.
P
Q
(
P
Q)
P
Q
P
Q
Figure 6.6
A circuit for ¬ ¬
∧¬
(
)
P
Q .
P
P
P
P
P
Figure 6.7
A circuit for P
P
↑
.
P
Q
P
Q
(P
Q)
(P
Q)
Figure 6.8
A circuit for P
Q
P
Q
↑
(
) ↑
↑
(
).
Classical Logic, Gates, and Circuits 
101
Our argument started by showing that we could replace any occurrence 
of or by using the identity
P
Q
P
Q
∨
≡¬ ¬
∧¬
(
).
The corresponding circuit, shown in figure 6.6, shows that we need never 
use the OR gate.
The argument continued by showing that any boolean function could 
be constructed using combinations of not and and. Consequently, we can 
construct a circuit that computes any boolean function using just NOT and 
AND gates.
Then we showed that both not and and could be generated by Nand 
showing that Nand by itself was functionally complete. The analogous 
statement is true for the NAND gate. You can implement any boolean func­
tion using a circuit that just uses NAND gates. Instead of using the term 
functionally complete, the standard term for gates is universal, so NAND is a 
universal gate. But let’s look at this in a little more detail.
The circuits in figures 6.7 and 6.8 show how to get rid of NOT and AND 
gates, replacing them with NAND gates. But notice that we also have to 
use fan-out. This operation takes one bit of information and outputs two 
output bits that are identical to the input bit. It might seem obvious that 
we can do this; it just requires connecting one piece of wire to another, but 
we will see later that we cannot perform this operation when it comes to 
quantum bits.
Gates and Computation
Gates are the fundamental building blocks of the modern computer. In 
addition to performing logical operations we can use gates to compute. We 
won’t show how this can be done. (The interested reader should see the 
wonderful book Code by Charles Petzold, where he starts with switches and 
shows how to construct a computer.) But we will give an example to help 
illustrate how the ideas underlying addition can be implemented.
Recall the exclusive or, denoted ⊕. It’s defined by:
0
0
0
⊕
=
,
0
1
1
⊕
= ,
1
0
1
⊕
= ,
1
1
0
⊕
=
.
even + even = even,
even + odd = odd,
odd + even = odd,
odd + odd = even.
This can be compared to adding odd and even whole numbers. We know:
102 
Chapter 6
This addition of “oddness” and “evenness” is often called addition mod­
ulo 2. If we let 0 stand for “even” and 1 stand for “odd,” addition modulo 
2 is given by ⊕. This is why the symbol contains a plus sign. (It is often 
easier to calculate with ⊕ thinking of addition, rather than use the 
exclusive or.)
The exclusive or gate is called XOR and is denoted by the symbol shown 
in figure 6.9.
We will use this gate to construct what is called a half-adder. This is a 
circuit that adds two binary digits. To understand what is going on we will 
compare it to a decimal half-adder. If we have two digits that sum to less 
than ten, then we just add them. So, for example, 2 + 4 = 6, 3 + 5 = 8.
If the digits sum to more than ten, however, we write down the appro­
priate digit, but we must remember to carry one for the next step in the 
computation. So, for example, 7 + 5 = 2, and we have a carry of 1.
A binary half-adder does the analogous computation. We can construct 
it using an XOR gate and an AND gate. The XOR gate computes the digit 
part, and the AND gate computes the carry.
Figure 6.9
The XOR gate.
0 + 0 = 0, with carry = 0;
0 + 1 = 1, with carry = 0;
1 + 0 = 1, with carry = 0;
1 + 1 = 0, with carry = 1.
A circuit that performs this is shown in figure 6.10. (In this picture the 
crossings of the wires that have dots indicate fan-out operations. The 
crossings without dots mean that the wires cross one another, but are not 
connected.)
The reason that this is called a half-adder, and not just an adder, is that 
it doesn’t take into account that we might have a carry coming in from the 
step before. We look at an example where we are adding standard decimal 
numbers. Suppose that the calculation is to add the following four-digit 
numbers, where the stars represent unknown digits.
Classical Logic, Gates, and Circuits 
103
To add the 6 and 5 we might get a digit of 1 and a carry of 1, but it is pos­
sible that we might have a carry of 1 from the first step of the calculation, 
in which case, the digit would be 2 and the carry 1. A full adder takes into 
account the possibility of an incoming carry from the step before.
We won’t draw the circuit for a full binary adder, but it can be done. 
Since all of our gates can be replaced with NAND gates, we can build an 
adder just using NAND gates and fan-outs. Indeed, we can build a whole 
computer just using these two components.
Memory
We have shown how to use gates for logic and indicated how we can use 
gates to do arithmetic, but to build a computer we also need to be able to 
store data. This can also be done using gates. It will take us too far afield 
to describe in detail how to do this, but the key idea is to build a flip-flop. 
These can be built out of gates using feedback. The outputs of the gates are 
fed back into inputs. An example using two NAND gates is shown in figure 
6.11. We won’t describe how to implement these, but we will end by com­
menting that once we start using feedback it is important to get the timing 
of inputs and outputs exactly right. This is where the clock comes in, send­
ing pulses of electricity at constant time intervals.
Reversible Computation
Now that we have given some idea of how a computer can be built from 
classical gates, we begin our study of reversible gates.
Input
Input
Digit
Carry
Figure 6.10
A half-adder circuit.
**6*
+
**5*
104 
Chapter 6
Gates can be considered as boolean functions. For example, the AND 
gate takes two boolean inputs and gives a boolean output. Often the easiest 
way of representing this is through a table. (This table is exactly the same as 
what we have been calling a truth table.)
Figure 6.11
A flip-flop using two NAND gates.
AND
Input
Output
0
0
0
0
1
0
1
0
0
1
1
1
Half-adder
Input
Output
digit
carry
0
0
0
0
0
1
1
0
1
0
1
0
1
1
0
1
We can also represent the half-adder using a table. This time there are two 
inputs and two outputs.
In this section we will look at reversible gates. These correspond to invert­
ible functions. Given an output, can we determine what the input was? If 
we can in every case, the function is invertible—the gate is reversible.
Classical Logic, Gates, and Circuits 
105
Looking at AND, if we get an output of 1, then we know that the input 
values must have been both 1, but if we get an output value of 0 there 
are three pairs of input values that have this output, and if we are not 
given any other information, we have no way of knowing which one of 
the three possibilities was actually input. Consequently, AND is not a 
reversible gate.
The half-adder is also not reversible. There are two pairs of input values 
that give a digit of 1 and a carry of 0. In both of these cases we have two bits 
of input, but are not getting two bits of output. We have lost some informa­
tion doing the computation.
The study of reversible gates and reversible computation began by look­
ing at the thermodynamics of computation. Shannon defined entropy for 
information. Entropy is also defined in thermodynamics. In fact, this is 
where Shannon got the idea. How closely are these two entropies related 
to one another? Can some of the theory of computation be expressed in 
terms of thermodynamics? In particular, can one talk about the minimum 
energy required performing a calculation? John von Neumann conjectured 
that when information was lost energy is expended—it dissipates as heat. 
Rolf Landauer proved the result and gave the minimum possible amount 
of energy to erase one bit of information. This amount of energy is called 
the Landauer limit.
If the computation is reversible, however, no information is lost and 
theoretically it can be performed with no energy loss.
We will look at three reversible gates: the CNOT, Toffoli, and Fredkin 
gates.
Controlled Not Gate
The controlled not gate or CNOT gate takes two inputs and gives two outputs. 
The first input is called the control bit. If it is 0, then it has no effect on the 
second bit. If the control bit is 1, it acts like the NOT gate on the second bit. 
The control bit is the first input bit and denoted by x. This bit is not changed 
and becomes the first output. The second output equals the second input if 
the control bit is 0, but it is flipped when the control bit is 1. This function 
is given by f x y
x x
y
,
,
,
,
.
(
) =
⊕
(
)or equivalently by the following table
106 
Chapter 6
Notice that this operation is invertible. Given any pair of output values, 
there is exactly one pair of input values that corresponds to it.
We can build a circuit that performs this operation using a fan-out and 
an XOR gate. This is shown in figure 6.12.
This, however, is not the picture that is most commonly used. The usual 
picture is the simplified version shown in figure 6.13.
The CNOT gate is not just invertible, but it also has the nice property 
that it is its own inverse. This means that if you put two CNOT gates in 
series, where the output of the first gate becomes the input of the second 
gate, the output from the second gate is identical to the input to the first 
gate. The second gate undoes what the first gate does. To see this, we know 
that applying the CNOT gate once is given by
f x y
x x
y
,
,
.
(
) =
⊕
(
)
CNOT
Input
Output
x
y
x
x
y
⊕
0
0
0
0
0
1
0
1
1
0
1
1
1
1
1
0
x
y
x
y
x
Figure 6.12
A circuit for CNOT.
x
x
y
x
y
Figure 6.13
Usual representation of CNOT gate.
Classical Logic, Gates, and Circuits 
107
Using this output as the input of another CNOT gate gives
f x x
y
x x
x
y
x y
,
,
,
⊕
(
) =
⊕
⊕
(
) = (
) .
Here we have used the facts that x
x
⊕
= 0 and 0 ⊕
=
y
y.
We started with the input x y
,
(
) and the output after going through the 
gate twice is x y
,
(
), back where we started.
The Toffoli Gate
The Toffoli gate, invented by Tommaso Toffoli, has three inputs and three 
outputs. The first two inputs are control bits. They flip the third bit if they 
are both 1, otherwise the third bit remains the same. Since this gate is like 
the CNOT gate, but has two control bits, it is sometimes called a CCNOT gate. 
The function describing what this gate does is: T x y z
x y
x
y
z
, ,
, ,
.
(
) =
∧
(
) ⊕
(
)
This can also be given in tabular form.
Toffoli gate
Input
Output
x
y
z
x
y
x
y
z
∧
(
) ⊕)
0
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
1
1
1
0
0
1
0
0
1
0
1
1
0
1
1
1
0
1
1
1
1
1
1
1
1
0
The standard diagram for this gate comes from the diagram of the CNOT 
gate (figure 6.14).
We can see from the table that the Toffoli gate is invertible—each triple 
of output values corresponds to exactly one triple of input values. Like the 
CNOT gate, this gate also has the property that it is its own inverse.
We know that T x y z
x y
x
y
z
, ,
, ,
.
(
) =
∧
(
) ⊕
(
)  Now, using the output as the 
new input and applying T again gives:
T x y
x
y
z
x y
x
y
x
y
z
x y z
, ,
, ,
, ,
.
∧
(
) ⊕
(
) =
∧
(
) ⊕
∧
(
) ⊕
(
) = (
)
Here we use the facts that x
y
x
y
∧
(
) ⊕
∧
(
) = 0 and 0 ⊕
=
z
z.
108 
Chapter 6
The Toffoli gate is also universal. Recall that we can construct any bool­
ean circuit using just NAND gates and fan-outs. To show that the Toffoli 
gate is universal, it is enough if we can show how to use it to compute both 
of these.
The NAND gate is described by f x y
x
y
,
(
) = ¬
∧
(
), so we want a way of 
inputting x and y and getting an output of ¬
∧
(
)
x
y . Since we are using the 
Toffoli gate, we will be inputting three values and getting an output of three 
values. Now ¬
∧
(
)
x
y  is logically equivalent to x
y
∧
(
) ⊕1. We can choose 
the third input value to always be 1, and we can ignore extra output values. 
We use
T x y
x y
x
y
x y
x
y
, ,
, ,
, ,
1
1
(
) =
∧
(
) ⊕
(
) =
¬
∧
(
)
(
)
to show that we can emulate the NAND gate by inputting x and y and read­
ing off the third entry of the output.
We can use a similar idea for fan-out. We want to input just one value 
x and receive two outputs that are both x. Again, the Toffoli gate has three 
inputs and three outputs. We can choose the two other inputs apart from x 
to be fixed and as long as we get xs for two of the outputs we can ignore the 
third. This can be done by
T x
x
x
, ,
, ,
.
1 0
1
(
) = (
)
Consequently, any boolean circuit can be constructed using just Toffoli 
gates.
These constructions illustrate something that often arises when we use 
reversible gates. The number of inputs must equal the number of outputs, 
but often we want to compute things where the number of inputs and out­
puts differ. We can always do this by adding extra bits, often called ancilla 
bits, to the inputs, or by ignoring bits that are output. Output bits that are 
x
x
y
y
z
(x
y)
z
Figure 6.14
Toffoli gate.
Classical Logic, Gates, and Circuits 
109
ignored are sometimes called garbage bits. In the example where we showed 
that fan-out could be done using the Toffoli gate we had T x
x
x
, ,
, ,
.
1 0
1
(
) = (
)  
The 1 and 0 in the input are ancilla bits, and the 1 in the output is a 
garbage bit.
The Fredkin Gate
The Fredkin gate also has three inputs and three outputs. The first input is 
a control bit. If it is 0, the second and third inputs are unchanged. If the 
control bit is 1, it swaps the second and third inputs—the second output 
is the third input and the third output is the second input. It is defined by
F
y z
y z
F
y z
z y
0
0
1
1
, ,
, ,
,
, ,
, ,
.
(
) = (
)
(
) = (
)
Equivalently, it is given by the following table.
It is easily seen from the table that the Fredkin gate is invertible and that, 
like both the CNOT and Toffoli gates, it is its own inverse. The table also has 
the property that the number of 1s for each input is equal to the number 
of 1s in the corresponding output. We will make use of this fact later when 
we construct a Fredkin gate using billiard balls. (When constructing billiard 
ball gates, you want them to have the property that the number of balls 
entering is equal to the number of balls leaving.) Figure 6.15 shows the 
diagram for this gate.
Notice that F 0 0 1
0 0 1
, ,
, ,
(
) = (
) and F 1 0 1
1 1 0
, ,
, ,
,
(
) = (
)  so for both possible 
values of x,
F x
x x
x
, ,
, ,
0 1
(
) =
¬
(
),
Fredkin gate
Input
Output
x
y
z
x
0
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
1
1
1
0
0
1
0
0
1
0
1
1
1
0
1
1
0
1
0
1
1
1
1
1
1
1
110 
Chapter 6
telling us that we can use the Fredkin gate for both fan-out and negation. 
For fan-out, we think of ¬ x as a garbage bit. For negation, we think of both 
the xs as garbage bits.
If we put z equal to 0 we obtain:
×
×
Figure 6.15
The Fredkin gate.
F(0,0,0) = (0,0,0),
F(0,1,0)=(0,1,0),
F(1,0,0) = (1,0,0),
F(1,1,0) = (1,0,1).
We can write this more succinctly as
F x y
x
x
y x
y
, ,
,
,
0
(
) =
¬
∧
∧
(
).
This tells us that we can use the Fredkin gate to construct the AND gate (0 
is an ancilla bit, and both x and ¬
∧
x
y are garbage bits).
Since any boolean circuit can be constructed using just NOT and AND 
gates along with fan-out, we can construct any boolean circuit using just 
Fredkin gates. Like the Toffoli gate, the Fredkin gate is universal.
We defined the Fredkin gate by
F
y z
y z
F
y z
z y
0
0
1
1
, ,
, ,
,
, ,
, ,
,
(
) = (
)
(
) = (
)
but we will give another equivalent definition.
This gate outputs three numbers. The first number output is always equal 
to the first input x. The second number will be 1 if either x = 0 and y = 1 or if 
x = 1 and z = 1, which we can express as ¬
∧
(
) ∨
∧
(
)
x
y
x
z . The third output 
will be 1 if either x = 0 and z = 1 or if x = 1 and y = 1, which we can express 
as ¬
∧
(
) ∨
∧
(
)
x
z
x
y . Consequently, we can define this gate by
F x y z
x
x
y
x
z
x
z
x
y
, ,
,
,
(
) =
¬
∧
(
) ∨
∧
(
)
¬
∧
(
) ∨
∧
(
)
(
) .
This looks somewhat intimidating and seems much more complicated 
than just remembering that if x = 0, then both y and z are unchanged; if 
x = 1, then y and z get switched. However, there is one place where this 
Classical Logic, Gates, and Circuits 
111
complicated formula is useful, and that is in the next section, when we 
show how to construct this gate using billiard balls.
Billiard Ball Computing
We haven’t discussed how to actually build gates. They can all be built 
from switches and wires with electric potential or its absence representing 
the bits 1 and 0. Fredkin showed that they could also be built using billiard 
balls that bounce off one another and strategically placed mirrors. A mirror 
is just a solid wall that the ball bounces off. (They are called mirrors because 
the angle of incidence is equal to the angle of reflection.) Billiard ball gates 
are theoretical devices; it is assumed that all collisions are totally elastic—
that no energy is lost. An example of a simple gate, called the switch gate, 
is shown in figure 6.16. In these pictures the solid lines represent walls; the 
grid lines are drawn to help keep track of the centers of the balls.
In the picture on the left a ball has just entered via Input 1. Since we 
haven’t entered a ball into Input 2, the ball just rolls unhindered and exits 
via Output 1. The picture on the right shows the analogous situation when 
one ball enters via Input 2 and we don’t send a ball through Input 1: It rolls 
unhindered out of Output 2A.
There are two other possibilities for sending ball through the two input 
slots. Unsurprisingly, if we don’t enter any balls, then no balls exit. The 
final and most complicated case is when balls are sent through both inputs. 
The assumption is that the balls have the same size, mass, speed and are 
entered simultaneously. Figure 6.17 indicates what happens.
In 2
In 1
Out 1
Out 2A
Out 2B
In 2
In 1
Out 1
Out 2A
Out 2B
Figure 6.16
Billiard ball switch gate.
112 
Chapter 6
First the balls collide with one another, then they both bounce off the 
diagonal walls (or mirrors), then they collide again. Finally, they exit. One 
leaves via Output 1 and the other by Output 2 B. (The paths of the centers 
of the balls are indicated by the bold arrows.)
We can denote the presence of a ball by 1 and the absence by 0, and then 
we can summarize what the gate does in a table.
In 2
In 1
Out 1
Out 2A
Out 2B
Figure 6.17
Two balls entering switch gate.
Switch gate
Input
Output
1
2
1
2A
2B
0
0
0
0
0
0
1
0
1
0
1
0
1
0
0
1
1
1
0
1
x
y
x
¬
∧
x
y
x
y
∧
0
0
0
0
0
0
1
0
1
0
1
0
1
0
0
1
1
1
0
1
We can construct a table with the same values using the statements x, y, 
¬
∧
x
y, and x
y
∧
.
Classical Logic, Gates, and Circuits 
113
This enables us to depict the switch as a black box with the inputs and out­
puts appropriately labeled, as is depicted in figure 6.18.
This picture tells us where balls enter and leave the gate. If a ball enters 
via x, a ball must leave via x. If a ball enters via y, a ball will leave via the 
¬
∧
x
y exit if there is no ball entering via x and will leave via the x
y
∧
 exit 
if there is also a ball entering via x. At this point you might be slightly wor­
ried by the fact that in the case when two balls enter, the balls get switched 
because the ball that exits via x is the ball that entered via y and the ball 
that exits from x
y
∧
 is the one that entered from x. But this is not a prob­
lem. We regard the balls as being indistinguishable—we just keep track of 
where there are balls, not where the balls originally came from.
We can also reverse the gate as is depicted in figure 6.19. We have to be 
slightly careful interpreting this. If a ball enters via ¬
∧
x
y, then there won’t 
be a ball entering via x, and so the ball sails directly across. If a ball enters 
x
x
x
y
x
y
y
Figure 6.18
Switch gate with inputs and outputs labeled.
x
x
x
y
x
y
y
(
x
y)
(x
y)
Figure 6.19
Switch gate with inputs and outputs interchanged.
114 
Chapter 6
via x
y
∧
, then there will be a ball entering via x, and consequently they 
will collide. One ball exits through the top of the gate and one exits via the 
output on the left. This means that a ball will exit through the left output if 
either ¬ ∧
x
y  or x
y
∧
, so this exit can be labeled ¬ ∧
(
) ∨
∧
(
)
x
y
x
y . However, 
¬ ∧
(
) ∨
∧
(
)
x
y
x
y  is logically equivalent to y, which means that reversing the 
gate just reverses the arrows but leaves all the labels the same.
We are now in a position to construct a Fredkin gate. Recall that
F x y z
x
x
y
x
z
x
z
x
y
, ,
,
,
(
) =
¬
∧
(
) ∨
∧
(
)
¬
∧
(
) ∨
∧
(
)
(
).
We need a construction that inputs x, y and z and outputs x, 
¬
∧
(
) ∨
∧
(
)
x
y
x
z  and ¬
∧
(
) ∨
∧
(
)
x
z
x
y ). This can be done with four switch 
gates and a lot of ingenuity. It is depicted in figure 6.20.
In this picture, the right angles in the paths are obtained by bouncing off 
diagonally placed mirrors. The only other interactions occur in the switch 
gates. Paths crossing don’t indicate collisions; the balls pass through the 
intersection points at different times. To make sure that balls don’t collide 
where they shouldn’t and do collide where they should, we can always add 
delays to paths by adding little detours to paths using mirrors. For example, 
x
x
x
x
x
y
z
x
z
x
z
x
y
x
y
(x
y)
(
x
z)
(x
z)
(
x
y)
Figure 6.20
Fredkin gate constructed from switch gates.
Classical Logic, Gates, and Circuits 
115
we can add a little delay by changing a straight-line path to one like the one 
depicted in figure 6.21.
By putting mirrors in the appropriate places and adding delays, we can 
construct the gate so that the outputs are lined up with the inputs and 
when balls enter at the same time they leave at the same time. (This is 
depicted in figure 6.22.) We can then form circuits that contain more than 
one Fredkin gate.* Since the Fredkin gate is universal, it can be used to 
construct any boolean circuit. Consequently, any boolean circuit can be 
constructed using just billiard balls and mirrors.
Fredkin believes that the universe is a computer. He didn’t convince 
Feynman of this, but the billiard ball computer did impress him. As they 
both realized, any slight error in the position or velocity of a ball would 
result in an error that would propagate and get amplified. Collisions are 
never perfectly elastic; there is always friction and heat is lost. The billiard 
ball computer is clearly just a theoretical machine, not something that can 
be constructed in practice. But this machine does conjure images of atoms 
bouncing off one another, and it led Feynman to consider gates based on 
quantum mechanics rather than classical mechanics. We look at this idea 
in the next chapter.
Figure 6.21
Delay added to straight-line path.
z
y
x
x
z
x
y
x
y
x
z
x
Figure 6.22
Billiard-ball Fredkin gate to be used in circuits.
*  There is a great animation showing this gate with balls entering and leaving on 
the website http://www.bubblycloud.com/billiard/fredkin-from-switches.html.
7  Quantum Gates and Circuits
Chapter 
7
Quantum 
Gates 
and 
Circuits
© Massachusetts Institute of TechnologyAll Rights Reserved
Quantum gates and circuits are a natural extension of both classical gates 
and circuits. They are also another way of thinking about the mathematics 
that describes sending qubits from Alice to Bob.
I commute by train. Often the train I am on is stationary, with another 
train also at a standstill just inches away from my window. One train will 
move slowly. Sometimes it is impossible to tell whether it is my train or 
the other one that is moving without turning to look out of the window 
on the opposite side. We could be inching forward, or the train moving in 
the opposite direction could be inching forward. Both scenarios fit. The 
same analysis applies to Bob’s measurements. We can either think of Bob 
as rotating his measuring apparatus, or we can think of Bob as keeping 
his apparatus in the same direction as Alice, but somehow the qubit gets 
rotated on the trip from Alice to Bob. When Alice and Bob are far apart, it 
often makes sense to think of Bob’s apparatus as being rotated. But we are 
going to send qubits to ourselves. We could think of our apparatus rotating 
during the travel time, but it is more natural to think of the apparatus as 
fixed and the qubit as being rotated. We think of the rotation as happen­
ing between the time it is sent and the time it is measured. Sending the 
qubits through a quantum gate performs this rotation. Previously we said 
that choosing directions to measure our qubits correspond to choosing an 
orthogonal matrix. Now we think of the directions we are measuring as 
being fixed and the orthogonal matrix as corresponding to a gate that the 
qubits pass through. Before we look at examples, we will introduce some 
new names for our basis kets.
118 
Chapter 7
Qubits
Since we are going to think of our measuring device as being fixed, we need 
to use only one ordered basis for both sending and receiving qubits. The 
natural basis to choose is the standard one 
1
0
0
1












,
. Earlier we denoted 
this as ↑
↓
(
)
,
. But we also associated the first vector in the ordered basis 
to the bit 0 and the second vector to 1. Now that we are solely going to use 
this basis it makes sense to give our kets new names that reflect how they 
relate to bits. We let 0  denote 1
0



 and 1  denote 0
1



.
In general a qubit will have the form a
a
0
1
0
1
+
, where a
a
0
2
1
2
1
+
= . 
When we measure it, either the state jumps to 0  and we read 0, or the 
state jumps to 1  and we read 1. The first occurs with probability a0
2, the 
second with probability a1
2.
Usually we have a system with more than one qubit, which means that 
we have to form tensor products. For a system with two qubits the underly­
ing ordered basis is
1
0
1
0
1
0
0
1
0
1
1
0
0
1



⊗






⊗






⊗





,
,
,

⊗







0
1
.
This can be written as 
0
0
0
1
1
0
1
1
⊗
⊗
⊗
⊗
(
)
,
,
,
. As we noted 
before, we often suppress the tensor product symbols, and so we write 
the product even more succinctly as 0 0
0 1
1 0
1 1
,
,
,
(
). Finally, we 
make the convention that we let ab  denote a b , giving the representa­
tion 00
01
10
11
,
,
,
(
) that is short and easy to read.
How does this connect to gates? This is what we will consider next. We 
start by reexamining the CNOT gate.
The CNOT Gate
As we saw, the classical CNOT gate takes two input bits and gives two out­
put bits. It’s defined by the table:
Quantum Gates and Circuits 
119
The table tells us what happens to the basis vectors. We then extend to lin­
ear combinations of the basis vectors in the obvious way.
CNOT r
s
t
u
r
s
u
t
00
01
10
11
00
01
10
11
+
+
+
(
) =
+
+
+
It just flips the probability amplitudes of 10  and 11 .
We keep using the diagram we used previously for the CNOT gate, but we 
must be careful about how we interpret it. For classical bits, the bit entering 
the top wire on the left, leaves the top wire on the right unchanged. This 
CNOT
Input
Output
x
y
x
x
y
⊕
0
0
0
0
0
1
0
1
1
0
1
1
1
1
1
0
CNOT
Input
Output
x
Y
x
x
y
⊕
0
0
0
0
0
1
0
1
1
0
1
1
1
1
1
0
CNOT
Input
Output
00
00
01
01
10
11
11
10
We extend this to qubits in the natural way—replacing 0 by 0 , and 1 by 
1 . The table becomes:
This can be written more succinctly using our compact notation for tensor 
products.
120 
Chapter 7
For example, we take 1
0
1
1
2
2
+
 as the top qubit and 0  for the 
bottom one.
The input is 
1
0
1
1
0
1
00
1
10
2
2
2
2
+



⊗
=
+
. This is sent by the 
CNOT gate to 1
00
1
11
2
2
+
.
This state, as we recognize from the EPR experiment, is an entangled 
state. Consequently, we cannot assign individual states to the top and bot­
tom wires on the right side. We draw the diagram in the following way.
0
0
1
2
1
2
1
00 +
11
1
2
1
2
is still true for qubits if the top qubit is either 0  or 1 , but it is not true for 
other qubits.
The wires represent our electrons or photons. These are separate objects 
and can be far apart. We will often talk about the top qubit and the bot­
tom qubit and think of them as being far apart. But, remember, if they are 
entangled, a measurement on one will affect the state of the other.
This example illustrates how we will often use this gate. We can input 
two unentangled qubits and use the gate to entangle them.
Quantum Gates
Notice that the CNOT gate permutes the basis vectors. Permuting the basis 
vectors in an ordered orthonormal basis gives another ordered orthonormal 
basis, and we know that associated with any of these bases is an orthogo­
nal matrix. Consequently, the matrix corresponding to the CNOT gate is 
orthogonal. In fact, all of the reversible gates that we introduced in the last 
chapter permute basis vectors. They all correspond to orthogonal matrices. 
Quantum Gates and Circuits 
121
This gives us the definition of quantum gates. They are just operations that 
can be described by orthogonal matrices.
Just as for classical computation, we want to assemble a small collection 
of simple gates that we can connect together to form circuits. We start by 
looking at the simplest gates, those that act on just one qubit.
Quantum Gates Acting on One Qubit
In classical, reversible computation there are only two possible boolean 
operators that act on one bit: the identity that leaves the bit unchanged, 
and NOT, which flips the values of 0 and 1. For qubits there are infinitely 
many possible gates!
We begin by looking at the two quantum gates that correspond to the 
classical identity and that both leave the qubits 0  and 1  unchanged. 
Then we will look at the two quantum gates corresponding to flipping the 
qubits 0  and 1 . These four gates are named after Wolfgang Pauli and are 
called the Pauli transformations.
The Gates I and Z
The gate I is just the identity matrix 1
0
0
1



.
We will see how I acts on an arbitrary qubit a
a
0
1
0
1
+
.
I a
a
a
a
a
a
a
a
0
1
0
1
0
1
0
1
0
1
0
1
1
0
0
1
+
(
) = 






= 


=
+
.
Unsurprisingly, I acts as the identity and leaves qubits totally unchanged.
The gate Z is defined by the matrix 1
0
0
1
−



.
Again, let’s see how Z acts on an arbitrary qubit a
a
0
1
0
1
+
.
Z a
a
a
a
a
a
a
a
0
1
0
1
0
1
0
1
0
1
0
1
1
0
0
1
+
(
) =
−







= −



=
−
.
So, Z leaves the probability amplitude of 0  unchanged, but it changes the 
sign of the probability amplitude of 1 . But let’s look at what Z does a little 
more carefully.
122 
Chapter 7
First, we will look at how it acts on the basis vectors. We have Z
0
0
(
) =
 
and Z 1
1
(
) = −
. But recall that a state vector is equivalent to that state vec­
tor multiplied by −1, so −1  is equivalent to 1 ; consequently, Z preserves 
both of the basis vectors, but it is not the identity. If we apply Z to the 
qubit 1
0
1
1
2
2
+
, we obtain
1
0
1
1
2
2
−
,
and, as we showed earlier, 1
0
1
1
2
2
+
 is distinguishable from, not 
equivalent to, 1
0
1
1
2
2
−
.
Even though the transformation Z preserves both the basis vectors, 
it changes every other qubit! This operation of changing the sign of a 
probability amplitude is sometimes called changing the relative phase of 
the qubit.
The Gates X and Y
The gates X and Y are given by:*
They both correspond to NOT in that they interchange 0  and 1 . The gate 
X just flips, while Y flips and changes the relative phase.
X = 



0
1
1
0
Y = −




0
1
1
0 .
*  Most authors define the matrix Y to be −i times the matrix we have given. We 
have chosen not to use any complex numbers. Our choice for Y simplifies things 
slightly when we consider superdense coding and quantum teleportation.
The Hadamard Gate
The last and most important gate that acts on one bit is the Hadamard gate, 
H. It is defined by
H =












=
−




−
1
1
1
1
1
2
2
2
2
2
1
1
1
1 .
Quantum Gates and Circuits 
123
We have named five quantum gates that act on just one qubit. Of course, 
there are infinitely more. Any rotation will give us an orthogonal matrix, 
and there are infinitely many of these, all of which can be considered as 
gates.
Are There Universal Quantum Gates?
In classical computing, we found that every boolean function could be 
given by a circuit that used only Fredkin gates, telling us that the Fredkin 
gate is universal. We also saw that NAND, along with fan-out, was universal. 
Are there universal quantum gates?
In the classical case, there are only finitely many boolean functions with 
a given number of variables. There are just two boolean functions of one 
variable. There are four of two variables. In general, there are 2n possible 
functions with n variables. Things are very different with quantum gates. 
As we have seen there are infinitely many possible gates that can act on 
just one qubit. If we take a finite number of gates and connect them in a 
finite number of ways, we will end up with a finite number of circuits. So, 
it is just not possible to have a finite number of gates generate an infinite 
number of circuits.
The short answer to the question of whether or not there is a finite set 
of quantum gates that is universal is just “no.” However, even though it 
is impossible to have a finite number of quantum gates that will generate 
every other possible quantum circuit, people have shown there is a finite 
collection of gates that can be used to approximate every possible circuit, but 
H
This gate is often used to put the standard basis vectors into superpositions:
H(
)
0
1
0
1
2
=
(
)
+
	
H(
)
1
1
0
1
2
=
(
)
−
     
In diagrams, gates that act on one qubit are denoted by a square with the 
appropriate letter drawn in the center. For example, the Hadamard gate act­
ing on one bit is denoted by the following.
124 
Chapter 7
we will not go into this. All of the circuits that we need can be constructed 
from the gates that we have introduced; five that act on just one qubit, and 
one, the CNOT gate, that acts on two qubits.
No Cloning Theorem
We first came across the fan-out operation when we were looking at clas­
sical circuits. One input wire is connected to two output wires. The input 
signal is split into two identical copies.
We then looked at reversible gates. For these, if you have two outputs, 
then you must also have two inputs. We could get the fan-out operation by 
using an ancilla bit—taking the second input always to be 0. One way of 
doing this is with the CNOT gate.
CNOT
0 0
0 0
(
) =
, CNOT
1 0
1 1
(
) =
, so CNOT
x
x x
0
(
) =
, if 
x  is either 0  or 1 . Unfortunately, if x  is not 0  or 1 , we don’t end 
up with two copies. We saw this when we input 
1
0
1
1
0
2
2
+




 into 
the CNOT  gate. It resulted in an entangled state, not two copies of the left 
qubit. We can use CNOT  to copy classical bits, but not general qubits.
The term fan-out is restricted to classical computing. We use the word 
cloning for the analogous idea in quantum computing. Cloning is like fan-
out, but for qubits. We want to make copies not just of classical bits but also 
of qubits. We want a gate that inputs a general qubit x  and a fixed second 
input 0  (an ancilla bit) and outputs two copies of x . A diagram of our 
desired gate follows.
The question of cloning becomes the question of whether or not the gate 
G can exist. We will show that it cannot, showing that it is impossible to 
clone general qubits. We do this by supposing that there is such a gate and 
then showing that two contradictory consequences follow logically from 
G
x
x
x
0
Quantum Gates and Circuits 
125
this assumption. Since our argument is logically sound and contradictions 
should not occur, we conclude that our initial assumption that G existed 
was false. Here’s the argument.
If G exists, we know that its cloning property gives:
1.  G 0 0
0 0
(
) =
.
2.  G 1 0
1 1
(
) =
.
3.  G
1
0
1
1
0
1
0
1
1
1
0
1
1
2
2
2
2
2
2
+







=
+




+



.
These three statements can be restated to give:
1.  G 00
00
(
) =
.
2.  G 00
11
(
) =
.
3.  G
1
00
1
10
1
00
01
10
11
2
2
2
+



=
+
+
+
(
).
The gate G, like all matrix operators, must be linear, which means that
G
G
G
1
00
1
10
1
00
1
10
2
2
2
2
+



=
(
) +
(
).
Replacing G 00
(
)and G 10
(
) using statements (1) and (2) gives
G
1
00
1
10
1
00
1
11
2
2
2
2
+



=
+
.
But statement (3) says that
G
1
1
1
00
01
10
11
2
00
2
10
2
+



=
+
+
+
(
).
However,
1
00
1
11
1
00
01
10
11
2
2
2
+
≠
+
+
+
(
).
So we have shown that if G exists then two things that are not equal must 
be equal. This is a contradiction. The only logical conclusion is that G can­
not exist, and it is impossible to construct a gate that clones general qubits. 
The argument we have given used 0  for the ancilla bit. There is nothing 
special about this. Exactly the same argument can be used whatever value 
is chosen for this bit.
126 
Chapter 7
The inability to clone a qubit has many important consequences. We 
want to be able to back up files and send copies of files to other people. 
Copying is ubiquitous. Our everyday computers are based on von Neu­
mann architecture, which is heavily based on the ability to copy. When 
we run a program we are always copying bits from one place to another. In 
quantum computing this is not possible for general qubits. So, if program­
mable quantum computers are designed they will not be based on our cur­
rent architecture.
At first, the fact that we cannot clone qubits seems like a serious 
drawback, but there are a couple of important comments that need to be 
made.
Often we want to prevent copying. We want to secure our data—we 
don’t want our communications to be tapped. Here, as we saw with Eve, 
the fact that we cannot clone qubits can be used to our advantage, prevent­
ing unwanted copies from being made.
The second comment is so important it deserves its own section.
Quantum Computation versus Classical Computation
The qubits 0  and 1  correspond to the bits 0 and 1. If we run our quantum 
CNOT gate just using the qubits 0  and 1 , and not any superpositions, 
then the computation is exactly the same as running a classical CNOT gate 
with 0 and 1. The same is true of the quantum version of the Fredkin gate. 
Since the classical Fredkin gate is universal and the quantum Fredkin gate 
using just 0  and 1  is equivalent to the classical gate, we can see that a 
quantum circuit can calculate anything that can be calculated by a classical 
circuit. The no-cloning property may seem worrisome, but it doesn’t restrict 
us from doing classical computations in any way.
This is a deep result. It shows that if we compare classical and quantum 
computation, we shouldn’t think of them as different types of computa­
tion. Quantum computation includes all of classical computation. It is the 
more general form of computation. The qubit is the basic unit of computa­
tion, not the bit.
Now that we have seen some basic gates, we will start to connect them 
together to form circuits.
Quantum Gates and Circuits 
127
The Bell Circuit
We call the following quantum circuit the Bell circuit.
H
To see what it does, we will input the four pairs of qubits that form the 
standard basis. We start with 00
0 0
=
. The first qubit is acted on by the 
Hadamard gate’s changing it to 1
0
1
1
2
2
+
, so the system of two qubits 
has state
1
0
1
1
0
1
00
1
10
2
2
2
2
+




=
+
at this stage. We now apply the CNOT gate. This flips 10  to 11 , giving the 
final state 1
00
1
11
2
2
+
.
We can represent the situation by the following picture.
0
0
00
1
2
1
2
11
H
We will summarize this by
B 00
1
00
1
11
2
2
(
) =
+
.
Convince yourself that
B 01
1
01
1
10
2
2
(
) =
+
.
B 10
1
00
1 11
2
2
(
) =
−
.
B 11
1
01
1 10
2
2
(
) =
−
.
128 
Chapter 7
Each of these outputs is entangled. Since the inputs form an orthonormal 
basis for 4, the outputs must also form an orthonormal basis. This basis, 
consisting of four entangled kets, is called the Bell basis.
Recall that the way to tell whether a square matrix A is orthogonal is 
by calculating A A
T
, where AT  is the transpose matrix obtained from A by 
interchanging the rows and columns. If we get the identity matrix I, then 
the matrix is orthogonal and the columns of the matrix give us an ortho­
normal basis. If we don’t get the identity, then the matrix is not orthogonal. 
We have defined our gates to be orthogonal, so they all have this prop­
erty. In fact, all the gates we have introduced in this chapter, with the one 
exception of the Pauli matrix Y, also have the property that when you take 
the transpose matrix you end up with exactly the same matrix you started 
with.** Consequently, for all of these gates, AA
I
= . This tells us that if we 
apply the gate twice in a row we end up with an output that is unchanged 
from the input. The second time we apply the gate, it undoes what we did 
when we applied it the first time.
We will see a couple of uses of the Bell circuit in a moment, but first we 
make use of the fact that the Hadamard gate and the CNOT gate are their 
own inverses. Consider the following circuit:
H
H
If we send a pair of qubits through the circuit, the first thing that hap­
pens is that the Hadamard gate is applied, and then we apply the CNOT 
gate. This action is immediately undone by the second application of the 
CNOT gate. Finally, the second application of the Hadamard gate undoes 
the action done by the initial Hadamard gate. The result is that the cir­
cuit doesn’t change anything. The qubits output are identical to the qubits 
that entered. The second half of the circuit reverses what the first half 
does.
This means that the following circuit, which we will call the reverse Bell 
circuit, reverses the action of the Bell circuit.
**  Matrices with the property that A
A
T =
 are called symmetric. They are symmetric 
with respect to the main diagonal.
Quantum Gates and Circuits 
129
H
In particular, we know what happens if we input vectors from the Bell basis. 
It is going to give us vectors in the standard basis.
If we input 1
00
1
11
2
2
+
, it will output 00 .
If we input 1
01
1
10
2
2
+
, it will output 01 .
If we input 1
00
1 11
2
2
−
, it will output 10 .
If we input 1
01
1 10
2
2
−
, it will output 11 .
Now that we have the basic properties of the Bell circuit, we will see how it 
can be applied to do some very interesting things. We look at superdense 
coding and quantum teleportation.
Superdense Coding
The initial setup for both superdense coding and quantum teleportation 
is the same. Two electrons have the entangled spin state 1
00
1
11
2
2
+
. 
One of the electrons is given to Alice and the other to Bob. They then travel 
far apart, both being careful not to make any measurement of their respec­
tive electron, preserving the entangled state.
In superdense coding, Alice wants to send Bob two classical bits of infor­
mation, that is, one out of the following possibilities: 00, 01, 10, 11. She is 
going to do this by sending Bob one qubit—her electron. We will outline 
the exact procedure in a moment, but first we will analyze the problem to 
see what we want to do.
Initially, it seems as though the solution should be easy. Alice is going 
to send Bob a qubit a
a
0
1
0
1
+
. There are infinitely many choices for the 
qubit, anything that satisfies a
a
0
2
1
2
1
+
=  will do. Surely, it must be easy to 
construct a way of transmitting two bits of information—one out of four 
130 
Chapter 7
possibilities—if you are allowed to send something that can be one of an 
infinite number of things. The problem is, of course, that Bob can never 
know what the qubit is. He can get information only by measuring. He will 
measure the spin in the standard basis and get either 0  or 1 . If Alice sends 
him a
a
0
1
0
1
+
, he will get 0  with probability a0
2 and 1  with probability 
a1
2. If he gets 0 , he knows nothing about a0, except for the fact that it is 
nonzero. Bob can get at most one bit of information from each qubit. In 
order to get two bits of information he will have to extract one bit from the 
particle that Alice is sending him, but he must also extract one bit from the 
particle in his possession.
Alice and Bob initially have one electron each. Eventually Bob is going 
to have both electrons and is going to measure their spins. Bob will have 
some quantum circuit with two wires exiting. If Alice wants to send 00, 
we need to arrange things so that just before Bob starts measuring, the top 
electron is in state 0  and the bottom electron is in state 0 , that is, the 
pair of electrons is in the unentangled state 00  just before Bob measures 
their spins. Similarly, if Alice wants to send 01, we want the pair of elec­
trons to be in the state 01  just before Bob makes his measurements. The 
final state should be 10  if Alice wants to send 10, and 11  if Alice wants to 
send 11.
The final observation is that Bob must do the same thing to every pair 
of electrons that he receives. He cannot do different things depending on 
what Alice is trying to send, because he doesn’t know what she is trying to 
send. That’s the whole point!
The idea behind the method is that Alice will act on her electron in one 
of four ways. Each way will result in the state of the qubits being one of the 
basis vectors in the Bell basis. Bob will then run the pair of qubits through 
the reverse Bell circuit to get the correct unentangled state.
Alice has four quantum circuits, one for each of the two-bit choices. Each 
circuit uses Pauli gates. The circuits are given below.
I
Circuit for 00
X
Circuit for 01
Z
Circuit for 10
Y
Circuit for 11
Quantum Gates and Circuits 
131
We will look at what happens to the qubits in each case. Initially, Alice’s 
and Bob’s qubits are entangled. They are in state 1
00
1
11
2
2
+
 which we 
will write as
1
0
0
1
1
1
2
2
⊗
+
⊗
.
When Alice sends her electron through the appropriate circuit, her kets 
change. Note that Alice’s circuits do not affect Bob’s electron in any way. 
We will do the calculation in each case.
If Alice wants to send 00, then she does nothing. The resultant state for 
the qubits remains as state 1
00
1
11
2
2
+
.
If Alice wants to send 01, she applies X. This interchanges her 0  and 
her 1 . The new state will be 1
1
0
1
0
1
2
2
⊗
+
⊗
, which we can write 
as 1
10
1
01
2
2
+
.
If Alice wants to send 10, she applies Z. This interchanges leaves 0  alone 
but changes her 1  to −1 . The new state will be 1
0
0
1
1
1
2
2
⊗
+
⊗
−
(
)
 
which we can write as 1
00
1
11
2
2
−
.
If Alice wants to send 11, she applies Y. The qubits end in the entangled 
state 1
10
1
01
2
2
−
.
Notice that these resultant states are exactly what she wants. Each is a 
distinct Bell basis vector. Now she sends Bob her electron. When Bob has 
her electron, he can use a circuit that inputs both the qubit that Alice has 
sent and the one that has always been in his possession. He uses the reverse 
Bell circuit.
If Alice is sending 00, when Bob receives the qubits they will be in state 
1
00
1
11
2
2
+
. He sends this through the reverse Bell circuit. This changes 
the state to 00 . This is unentangled. The top bit is 0  as is the bottom bit. 
Bob now measures the qubits. He gets 00.
If Alice is sending 01, when Bob receives the qubits they will be in 
state 1
10
1
01
2
2
+
 He sends this through the reverse Bell circuit. This 
132 
Chapter 7
changes the state to 01 .This is unentangled. The top bit is 0  and the 
bottom bit is 1 . Bob now measures the qubits. He gets 01. The other cases 
are similar. In each case Bob ends up with the two bits that Alice wants to 
send to him.
Quantum Teleportation
As in superdense coding, Alice and Bob are far apart. They each have one 
electron. The electrons share the entangled state 1
00
1
11
2
2
+
. Alice 
also has another electron. It is in state a
b
0
1
+
. Alice has no idea what 
the probability amplitudes a and b are, but she and Bob want to change 
Bob’s electron so that it has state a
b
0
1
+
. They want to teleport the state 
of Alice’s electron to Bob’s. To do this, we will see that Alice needs to send 
Bob two classical bits, but notice that there are infinitely many possibili­
ties for the initial state of her electron. It’s impressive that we can send 
one of an infinite number of possibilities using only two classical bits. It is 
also interesting that Alice starts with a qubit and Bob ends up with it, but 
neither of them can ever know exactly what it is. To learn about it, they 
have to make a measurement. When they measure, they just get either 
0  or 1 .
We can deduce a few things about how the process will work. Bob is 
going to end up with an electron in the unentangled state a
b
0
1
+
. 
At the start, Bob and Alice’s electrons share an entangled state. To disen­
tangle the state someone has to make a measurement. Clearly, it cannot 
be Bob. If Bob makes a measurement he will end up with an electron in 
state of either 0  or 1 , not the required a
b
0
1
+
, so we know Alice will 
be making a measurement. We also have to get the third electron’s state 
involved. Alice will have to do something to entangle the state of this 
electron with the state of her other electron, which is currently entangled 
with Bob’s. The obvious way of doing this is to send the two qubits that 
she controls through a CNOT gate. That will be the first step. The sec­
ond step will be to apply the Hadamard gate to the top qubit. So, in fact, 
Alice is going to put the two qubits that she controls through a reverse 
Bell circuit. The situation is depicted as follows, where Alice’s qubits are 
shown above Bob’s qubit. The second and third rows depict the entangled 
qubits.
Quantum Gates and Circuits 
133
H
Alice’s qubits
Bob’s qubit
00
1
2
1
2
11
a
b
0
1
We have three qubits, the initial state that describes the three electrons is
a
b
0
1
1
00
1
11
2
2
+
(
) ⊗
+



,
which we can write as
a
a
b
b
2
2
2
2
000
011
100
111
+
+
+
.
Alice is going to act on her qubits, so we write the state emphasizing these.
a
a
b
b
2
2
2
2
00
0
01
1
10
0
11
1
⊗
+
⊗
+
⊗
+
⊗
.
Alice is going to apply the reverse Bell circuit. We will analyze this in two 
steps, first by applying the CNOT gate to the first two qubits and then the 
Hadamard gate to the top bit. Applying the CNOT gate gives:
a
a
b
b
2
2
2
2
00
0
01
1
11
0
10
1
⊗
+
⊗
+
⊗
+
⊗
.
Alice now is going to act on the first qubit, so we write the state emphasiz­
ing this.
a
a
b
b
2
2
2
2
0
0
0
0
1
1
1
1
0
1
0
1
⊗
⊗
+
⊗
⊗
+
⊗
⊗
+
⊗
⊗
.
We now apply the Hadamard gate to the first qubit. This changes 0  to
1
0
1
1
2
2
+
 and 1  to 1
0
1
1
2
2
−
.
This results in the state
a
a
a
a
b
b
b
b
2
2
2
2
2
2
2
2
0
0
0
1
0
0
0
1
1
1
1
1
0
1
0
1
1
0
0
0
1
⊗
⊗
+
⊗
⊗
+
⊗
⊗
+
⊗
⊗
⊗
⊗
⊗
⊗
⊗
⊗
+
−
+
−
1
0
1
⊗
⊗
134 
Chapter 7
This can be slightly simplified to give
1 00
0
1
1 01
1
0
1 10
0
1
1 11
1
0
2
2
2
2
⊗
+
(
) +
⊗
+
(
)
+
⊗
−
(
) +
⊗
−
(
)
a
b
a
b
a
b
a
b
.
Alice now measures her two electrons in the standard basis. She will get one 
of 00 , 01 , 10 , 11 , each with probability 1/4.
If she gets 00 , Bob’s qubit will jump to state a
b
0
1
+
.
If she gets 01 , Bob’s qubit will jump to state a
b
1
0
+
.
If she gets 10 , Bob’s qubit will jump to state a
b
0
1
−
.
If she gets 11 , Bob’s qubit will jump to state a
b
1
0
−
.
Alice and Bob want Bob’s qubit to be in the state a
b
0
1
+
. It is almost 
there, but not quite. To sort things out, Alice has to let Bob know which 
of the four possible situations he is in. She sends Bob two classical bits of 
information, 00, 01, 10, or 11, corresponding to the results of her measure­
ments, to let him know. These bits of information can be sent in any way, 
by text, for example.
If Bob receives 00, he knows that his qubit is in the correct form and so 
does nothing.
If Bob receives 01, he knows that his qubit is a
b
1
0
+
. He applies the 
gate X to it.
If Bob receives 10, he knows that his qubit is a
b
0
1
−
. He applies the 
gate Z to it.
If Bob receives 11, he knows that his qubit is a
b
1
0
−
. He applies 
the gate Y to it.
In every case Bob’s qubit ends in state a
b
0
1
+
, the original state of the 
qubit that Alice wanted to teleport.
It is important to note that there is only one qubit in state a
b
0
1
+
 at 
any point during the process. Initially, Alice has it. At the end Bob has it, 
but as the no cloning theorem tells us, we can’t copy, so only one of them 
can have it at a time.
It is also interesting to observe that when Alice sends her qubits through 
her circuit Bob’s qubit instantaneously jumps to one of the four states. He 
has to wait for Alice to send him the two classical bits before he can deter­
mine which of the four qubits correspond to Alice’s original qubit. It is the 
fact that the two bits have to be sent by some conventional transportation 
method that prevents instantaneous transmission of information.
Quantum Gates and Circuits 
135
Quantum teleportation and superdense coding are sometimes described 
as being inverse operations. For superdense coding, Alice sends Bob one 
qubit to convey two classical bits of information. For quantum teleporta­
tion, Alice sends Bob two classical bits of information to teleport one qubit. 
For superdense coding, Alice encodes using the Pauli transformations, and 
Bob decodes using the reverse Bell circuit. For quantum teleportation, Alice 
encodes using the reverse Bell circuit, and Bob decodes using the Pauli 
transformations.
Quantum teleportation is actually being performed, usually using entan­
gled photons rather than entangled electrons, where it can be done over 
substantial distances. As I write this, it has been announced that a Chinese 
team has teleported a qubit from Earth to a satellite in low Earth orbit. These 
experiments are often mentioned on news broadcasts, mainly because of 
the word “teleportation,” which conjures up images of Star Trek. Unfortu­
nately, quantum teleportation is not something that is readily explained in 
a brief sound bite, and though many people have heard the term, not many 
understand exactly what it is that is being teleported.
Quantum teleportation gives a way of transporting a qubit from one 
place to another without actually transporting the particle that represents 
the qubit. It is used in various ways to correct errors. This is extremely 
important for quantum computations. Qubits have a tendency to interact 
with the environment and get corrupted. We will not study error correction 
in detail but will only look at a simple example.
Error Correction
I was a student before the advent of CDs. We listened to vinyl records. To 
play a record we went through an elaborate ritual. First, the record was 
gently slid from its sleeve, care being taken to hold it by its edges and not 
get any fingerprints on the surface. Then the record was placed on the turn­
table. The next step was to clean it of any dust. This often involved an 
antistatic spray and a special cleaning brush. Finally you lined up the stylus 
and carefully lowered it to the record.
Even with all these precautions, there were often clicks and pops caused 
by unseen dust or some minute imperfection. If you accidently scratched it, 
you would get a pop thirty three times per minute, which made the music 
unlistenable. Then CDs came. Gone were the pops. You could even scratch 
the surface and it still played perfectly. It seemed incredible.
136 
Chapter 7
Vinyl records have no error correction built in. If you damage them, 
you cannot recover the original sound. CDs, on the other hand, incorpo­
rate error correction. If there is some small imperfection, the digital error-
correcting code can often calculate what has gone wrong and correct it.
Encoding digital information involves two essential ideas. The first is to 
eliminate redundancy to compress the information as much as possible—to 
make the message as short as possible. A good example of this is making 
a ZIP file of a document. (Some people don’t like CDs because they think 
the music has been compressed too much, losing the warmth you get from 
vinyl.) The second important idea is to add some redundancy back in, but 
to make it useful redundancy. You want to add in some additional informa­
tion that will help correct errors.
Nowadays, practically all transmissions of digital information use some 
form of error-correcting code. There are so many ways that a message can 
be slightly corrupted, and given a slightly corrupted message, you want to 
be able to correct it.
Error correction is essential for transmissions involving qubits. We are 
using photons and electrons to encode them. These particles can interact 
with the rest of the universe and unwanted interactions may change the 
states of some qubits.
In this section we will look at the most basic classical error-correcting 
code and then show how it can be modified for sending qubits.
The Repetition Code
A simple error-correcting code is just to repeat the symbol that we want to 
send. The simplest case is to repeat it three times. If Alice wants to send 0, 
she sends 000. If she wants to send 1, she sends 111. If Bob keeps getting 
sequences of three 0s and three 1s, he assumes that all is well. If he receives 
something else, say 101, he knows that an error has occurred; the string 
should have been 000 or 111. If the string that Alice sent was 000, then 
two errors must have occurred. If the string was 111, then only one error 
has occurred. If errors are fairly unlikely, it is more probable that one error, 
rather than two errors, have occurred, so Bob assumes that the least number 
of errors have occurred and consequently replaces 101 with 111.
There are eight possibilities of three-bit strings that Bob could receive. 
Four of them are 000, 001, 010, and 100. Bob decodes all of these as 000. The 
other four three-bit strings are 111, 110, 101, and 011. Bob decodes these as 
Quantum Gates and Circuits 
137
111. If the chance of error is very small, then this repetition code corrects 
many errors and reduces the overall error rate. This is fairly straightforward, 
but we will analyze what Bob does in a way that generalizes for qubits. The 
problem with qubits is that to read them, we have to measure them, and 
that can make them jump to a new state. We need a new way of determin­
ing what Bob should do. He is going to perform parity tests.
Now, suppose Bob receives the three bits b b b
0
1
2. We will do some compu­
tations to show which, if any, of the bits should be changed. Bob computes 
b
b
0
1
⊕
 and b
b
0
2
⊕
.
The first sum checks the parity of the first two bits—that is, it checks 
whether they are the same digit or not. The second sum performs a parity 
check on the first and third digits.
If all three bits equal 0, or all equal 1, then he will get 0 for both sums. 
If not all of the bits are equal, then two will be equal and the third will dif­
fer. It will be this third symbol that needs to be flipped from 0 to 1, or from 
1 to 0.
If b
b
b
0
1
2
=
≠
, then b
b
0
1
0
⊕
=
 and b
b
0
2
1
⊕
= .
If b
b
b
0
2
1
=
≠
, then b
b
0
1
1
⊕
=
 and b
b
0
2
0
⊕
=
.
If b
b
b
0
1
2
≠
=
, then b
b
0
1
1
⊕
=
 and b
b
0
2
1
⊕
= .
This means that Bob can look at the pair of bits b
b
0
1
⊕
 and b
b
0
2
⊕
.
If he gets 00 then there is nothing to correct, so he does nothing.
If he gets 01, he flips b2.
If he gets 10, he flips b1.
If he gets 11, he flips b0.
We look at how these error-correcting ideas can be modified for qubits. 
But before we do we make one important observation. It might seem trivial, 
but it is what makes the quantum bit-flip correction code work.
Suppose Bob receives a string and there is an error in the first bit. This 
means that he has received either 011 or 100. After Bob does the parity 
tests, he will get 11 for both strings and will know that there is an error in 
the first bit. The key observation is that the parity tests tell us where the 
error is. They do not tell us whether it is a 0 that needs to be flipped to a 1, 
or a 1 that needs to be flipped to a 0.
Quantum Bit-Flip Correction
Alice wants to send the qubit a
b
0
1
+
 to Bob. There are various types 
of errors that can occur, but we will restrict our attention to bits getting 
flipped. In this case, a
b
0
1
+
 gets changed to a
b
1
0
+
.
138 
Chapter 7
Alice would like to send three copies of her qubit. This, of course, is not 
possible. The no cloning theorem tells us that she cannot make copies. But 
she can perform what is essentially a classical fan-out and replace 0  with 
000  and 1  with 111 . This is done with two CNOT gates. This is shown 
in the circuit below.
She starts with three qubits, the one she wants to encode and two ancilla 
bits that are both 0 , so the initial state is a
b
a
0
1
0 0
0 0 0
+
(
)
=
+
b 1 0 0 . The first CNOT gate changes it to a
b
0 0 0
1 1 0
+
. The sec­
ond gives us the required state a
b
0 0 0
1 1 1
+
.
a
b
0
1
a
b
000
111
0
0
Alice then sends the three qubits to Bob. But the channel is noisy, and 
there is the possibility of a qubit being flipped. Bob might receive the cor­
rect qubits a
b
000
111
+
, or he might receive one of the following incor­
rect versions, a
b
100
011
+
, a
b
010
101
+
 or a
b
001
110
+
, which 
correspond to the error occurring in the first, second, and third qubit, 
respectively. He wants both to detect the error and to correct it. But notice 
that he cannot make any measurements on this entangled state. If he does, 
the state immediately becomes unentangled and he just gets three qubits 
that are some combination of 0 s and 1 s—the values of a and b are lost, 
with no way of recovering them.
It is amazing that Bob can determine which bit is flipped, correct it, and 
yet never make a measurement on the three qubits that Alice sent him! But 
he can. He uses the parity check idea that we used for classical bits.
He adds an additional two qubits in which to perform the parity checks. 
The circuit is given below. It uses four CNOT gates. The two on the fourth 
wire are used to do the b
b
0
1
⊕
 parity calculation; the two on the fifth wire 
do the b
b
0
2
⊕
 calculation. The standard first reaction on seeing this circuit 
is to assume that we end up with five qubits that are hopelessly entangled. 
But I’ve drawn the picture that shows that the bottom two qubits are not 
entangled with the top three. Can that really be the case?
Quantum Gates and Circuits 
139
0
0
Received qubits
Received qubits
Parity qubits
Let us suppose that Bob receives a c c c
b d d d
0 1 2
0
1
2
+
 The key observation 
is that if there is an error, then there will be an error in both c c c
0 1 2 and 
d d d
0
1
2, and it will occur in exactly the same place. When we apply the par­
ity checks, both strings give the same results.
To illustrate what is going on, let’s look at Bob’s circuit, ignoring the fifth 
wire for the moment. The input for the first four qubits is
a c c c
b d d d
a c c c
b d d d
0 1 2
0
1
2
0 1 2
0
1
2
0
0
0
+
(
)
=
+
.
The two CNOT gates attached to the fourth wire perform the parity check 
on the first two digits. But c
c
d
d
0
1
0
1
⊕
=
⊕
, so the four qubits at the right of 
the circuit will be in one of two states. They will be in state
a c c c
b d d d
a c c c
b d d d
0 1 2
0
1
2
0 1 2
0
1
2
0
0
0
+
=
+
(
)
if c
c
d
d
0
1
0
1
0
⊕
=
⊕
=
.
They will be in state
a c c c
b d d d
a c c c
b d d d
0 1 2
0
1
2
0 1 2
0
1
2
1
1
1
+
=
+
(
)
if c
c
d
d
0
1
0
1
1
⊕
=
⊕
= .
In both cases, the fourth qubit is not entangled with the top three.
A similar argument applies to the fifth qubit. It is not entangled with the 
others. It is 0  if c
c
d
d
0
2
0
2
0
⊕
=
⊕
=
, and is 1  if c
c
d
d
0
1
0
1
0
⊕
=
⊕
=
.
Since the bottom two qubits are not entangled with the top three, Bob 
can make measurements on the bottom two qubits, and it will leave the top 
three unchanged. This is what he does:
If he gets 00, then there is nothing to correct, so he does nothing.
If he gets 01, he flips the third qubit using by installing an X gate on the 
third wire.
140 
Chapter 7
If he gets 10, he flips the second qubit using by installing an X gate on 
the second wire.
If he gets 11, he flips the first qubit using by installing an X gate on the 
first wire.
The result is that the bit-flip error is corrected and the qubits are now 
back in the state that Alice sent.
In this chapter we introduced the idea of quantum gates and circuits. 
We have seen some surprising things we can do with just a few quantum 
gates. We have also seen that quantum computation includes all of classical 
computation. This doesn’t mean that we will be using quantum computers 
to perform classical computations, but it does tell us that quantum compu­
tation is the more fundamental form of computation.
The next topic that we look at concerns whether we can use quantum 
circuits to perform calculations faster than can be done with classical cir­
cuits. How do we measure the speed of a computation? Are quantum com­
puters always faster than classical ones? These are some of the questions we 
look at in the next chapter.
8  Quantum Algorithms
Chapter 
8
Quantum 
Algorithms
© Massachusetts Institute of TechnologyAll Rights Reserved
Popular descriptions of quantum algorithms describe them as being much 
faster than regular algorithms. This speedup, it is explained, comes from 
being able to put the input into a superposition of all possible inputs and 
then performing the algorithm on the superposition. Consequently, instead 
of running the algorithm on just one input, as you do classically, you can 
run the algorithm using “quantum parallelism” on all possible inputs at the 
same time. These descriptions often end at this point. But this leaves many 
unanswered questions. We seem to end up with many possible answers all 
superimposed on one another. If we make a measurement, won’t we get 
just one of these answers at random? There are far more likely to be wrong 
answers than right answers, so aren’t we more likely to end up with a wrong 
answer than with the right answer?
Clearly, there has to be more to quantum algorithms than just putting 
everything into a superposition of states. The real art of constructing these 
algorithms is being able to manipulate these superpositions so that when 
we make measurements we get a useful answer. In this chapter, we will 
look at three quantum algorithms and see how they tackle this problem. 
We will see that not every algorithm is susceptible to a quantum speedup. 
Quantum algorithms are not classical algorithms that have been sped up. 
Instead, they involve quantum ideas to see the problem in a new light; the 
algorithms work not by the use of brute force, but by ingenious ways of 
exploiting underlying patterns that can be seen from only the quantum 
viewpoint. 
We will describe three algorithms in detail. All three are ingenious 
exploitations of underlying mathematical patterns. The level of difficulty 
increases as we move through the algorithms. Some mathematics books 
use a star to denote a difficult section and a double star to denote a very 
142 
Chapter 8
difficult section. The Deutsch-Jozsa algorithm probably deserves a star, and 
Simon’s algorithm a double star. 
At the end of the chapter, we will talk a little about the properties that 
questions must have in order for a quantum algorithm to solve them faster 
than a classical one, and why they seem so hard! But first we must describe 
how the speed of algorithms is measured.
The Complexity Classes P and NP
Imagine that you are given the following problems. You are told that you 
are not allowed to use a calculator or computer but have to work them out 
using paper and pencil.
•	 Find two whole numbers bigger than 1 whose product is equal to 35.
•	 Find two whole numbers bigger than 1 whose product is equal to 187.
•	 Find two whole numbers bigger than 1 whose product is equal to 2,407.
•	 Find two whole numbers bigger than 1 whose product is equal to 88,631.
You won’t have much difficulty doing the first question, but each subse­
quent question is harder and will take more steps and consequently more 
time to solve. Before we analyze this in more detail, consider another four 
problems.
•	 Multiply 7 by 5 and check that it equals 35.
•	 Multiply 11 by 17 and check that it equals 187.
•	 Multiply 29 by 83 and check that it equals 2407.
•	 Multiply 337 by 263 and check that it equals 88,631.
These questions are undoubtedly easier than the first series. Again each 
subsequent question takes more time to solve than the previous one, but 
the amount of time is growing more slowly. Even the fourth question takes 
less than a minute to solve by hand.
We will denote the number of digits of the input number by n, so in the 
first set of questions we start with n = 2 and go up to n = 5.
We will let T n
( ) denote the time, or equivalently the number of steps, 
to solve a question of input length n. Complexity looks at how the size of 
T n
( ) grows as n grows. In particular, we ask if we can find some positive 
numbers k and p such that T n
knp
( ) ≤
 for every value of n. If we can, we 
say that the underlying problem can be solved in polynomial time. If, on 
the other hand, we can find positive number k and a number c > 1, such 
Quantum Algorithms 
143
that T n
kcn
( ) >
 for every value of n, we say that the problem requires expo­
nential time. Recall the basic fact concerning polynomial versus exponen­
tial growth: Given enough time, something with exponential growth will 
grow much faster than something with polynomial growth. In computer 
science, questions that can be solved in polynomial time are considered 
tractable, but those with exponential growth are not. Problems that can 
be solved in polynomial time are regarded as easy; those that require expo­
nential time are hard. In practice, it turns out that most polynomial time 
problems involve polynomials with small degree, so even if we don’t have 
the computational power to solve a problem with a large value of n at the 
moment, we should have it in a few years. On the other hand, with an 
exponential time problem, once the size has increased beyond what we 
can currently tackle, increasing the size of n even slightly more produces 
a problem that becomes much harder and is unlikely to be solvable in the 
foreseeable future.
Let’s look at our two sets of problems. The second set involves multiply­
ing two numbers together, but this is easy to do. As n increases it does take 
more time, but it can be shown that this is a polynomial time problem. 
What about the first set of questions? If you tried tackling them, you will 
probably believe that the amount of time needed is exponential in n and 
not polynomial in n, but is this the case? Everybody thinks so, but, on the 
other hand, nobody has found a proof.
In 1991, RSA Laboratories posted a challenge. It listed large numbers, 
each of which was the product of two primes. The challenge was to factor 
them. They went from being about 100 decimal digits long to 600 digits. 
You were of course allowed to use computers! There were prizes for the 
first person to factor them. The 100 digit numbers were factored relatively 
quickly, but the numbers with 300 or more digits still haven’t been factored.
If a problem can be solved in polynomial time we say it belongs to the 
complexity class P. So the problem that consists of multiplying two num­
bers together belongs to P. Suppose that instead of solving the problem, 
someone gives you the answer and you just have to check that the answer 
is correct. If this process of checking that an answer is correct takes polyno­
mial time, then we say the problem belongs to complexity class NP.* The 
*  NP comes from nondeterministic polynomial, which in turn refers to certain types of 
Turing machines that are called nondeterministic Turing machines.
144 
Chapter 8
problem of factoring a large number into the product of two primes belongs 
to NP.
Clearly, checking that an answer is correct is easier than actually find­
ing the answer, so every problem that is in P is also in NP, but what about 
the converse question. Does every NP problem belong to P? Is it true that 
every question whose answer can be checked in polynomial time can also 
be solved in polynomial time? You are probably saying to yourself, “Of 
course not!” Most people would agree that it seems extremely unlikely, 
but nobody has managed to prove that P is not equal to NP. The prob­
lem of factoring a large number into the product of two primes belongs 
to NP, and we don’t think it belongs to P, but nobody has been able to 
prove it.
The problem of whether NP is equal to P is one of the most important 
in computer science. In 2000, the Clay Mathematics Institute listed seven 
“Millennium Prize Problems,” each with a prize of a million dollars. The P 
versus NP problem is one of the seven.
Are Quantum Algorithms Faster Than Classical Ones?
Most quantum computer scientists believe that P is not equal to NP. They 
also think that there are problems that are in NP but not P, which a quan­
tum computer can solve in polynomial time. This means that there are 
problems that a quantum computer can solve in polynomial time that a 
classical computer cannot. To prove this, however, involves the first step 
of showing that some problem belongs to NP but not to P, and as we have 
seen, nobody knows how to do this. So, how can we compare the speed 
of quantum algorithms to classical algorithms? There are two ways: one 
theoretical, the other practical. The theoretical way is to invent a new way 
of measuring complexity that makes it easier to construct proofs. The prac­
tical way is to construct quantum algorithms for solving important real-
world problems in polynomial time that we believe, but have been unable 
to prove, do not belong to P.
An example of the second approach is Shor’s algorithm for factoring the 
product of two primes. Peter Shor constructed a quantum algorithm that 
works in polynomial time. We believe, but have been unable to prove, that 
a classical algorithm cannot do this in polynomial time. Why is that impor­
tant? Well, as we shall see our Internet security depends on this. That said, 
Quantum Algorithms 
145
in the rest of this chapter we will take the first approach—defining a new 
way of calculating complexity.
Query Complexity
All of the algorithms that we are going to look at in this chapter concern 
evaluating functions. The Deutsch and Deutsch-Jozsa algorithms consider 
functions that belong to two classes. We are given a function at random, 
and we have to determine which of the two classes the function belongs 
to. Simon’s algorithm concerns periodic functions of a special type. Again 
we are given one of these functions at random, and we have to determine 
the period.
When we run these algorithms we have to evaluate the functions. The 
query complexity counts the number of times that we have to evaluate the 
function to get our answer. The function is sometimes called a black box 
or an oracle. Instead of saying that we are evaluating the function, we say 
that we are querying the black box or the oracle. The point of this is that 
we don’t have to worry about how to write an algorithm that emulates the 
function, so we don’t have to calculate the number of steps that function 
takes to evaluate the input. We just keep track of the number of questions. 
This is much simpler. To illustrate, we begin with the most elementary 
example.
Deutsch’s Algorithm
David Deutsch is one of the founders of quantum computing. In 1985, 
he published a landmark paper that described quantum Turing machines 
and quantum computation.** This paper also includes the following 
algorithm—the first to show that a quantum algorithm could be faster than 
a classical one.
The problem concerns functions of just one variable. The input can be 
either 0 or 1. The output also just takes the values of 0 or 1. There are four 
of these functions that we will denote f0, f1, f2, and f3:
The function f0 sends both inputs to 0; i.e., f0 0
0
( ) =
 and f0 1
0
( ) =
.
The function f1 sends 0 to 0 and 1 to 1; i.e., f1 0
0
( ) =
 and f1 1
1
( ) = .
**  “Quantum theory, the Church-Turing principle and the universal quantum com­
puter,” Proceedings of the Royal Society A 400 (1818): 97–117.
146 
Chapter 8
The function f2 sends 0 to 1 and 1 to 0; i.e., f2 0
1
( ) =  and f2 1
0
( ) =
.
The function f3 sends both inputs to 1; i.e., f3 0
1
( ) =  and f3 1
1
( ) =
.
The functions f0 and f3 are called constant functions. The output is the 
same value for both inputs—the output is constant. A function is called 
balanced if it sends half its inputs to 0 and the other half to 1. Both f1and 
f2 are balanced.
The question that Deutsch posed is this: Given one of these four func­
tions at random, how many function evaluations do we have to make to 
determine whether the function is constant or balanced? It is important to 
understand what we are asking. We are not interested in which of the four 
functions we have, but solely in whether the given function is constant 
or not.
The classical analysis is as follows. We can evaluate our given function at 
either 0 or 1. Supposing that we choose to evaluate it by plugging in 0, then 
there are two possible outcomes—either we get 0 or we get 1. If we get 0, all 
we know is f 0
0
( ) =
. The function could be either f0 or f1. Since one is con­
stant and the other is balanced, we are forced to evaluate our function again 
to decide between them. Classically, to answer the question we have to plug 
both 0 and 1 into the function. We need to make two function evaluations.
We now look at the quantum version of the question. First, we construct 
gates that correspond to the four functions. The following picture depicts 
the gates, where i can take on the numbers 0, 1, 2, or 3.
x
y
Fi
y
fi (x)
x
This says that:
If we input 0
0
⊗
, it outputs 0
0
⊗
( )
fi
.
If we input 0
1
⊗
, it outputs 0
0
1
⊗
( ) ⊕
fi
.
If we input 1
0
⊗
, it outputs 1
1
⊗
( )
fi
.
If we input 1
1
⊗
, it outputs 1
1
1
⊗
( ) ⊕
fi
.
Notice that for each i, one of fi 0
( ) and fi 0
1
( ) ⊕ is equal to 0 and the 
other is equal to 1, and one of fi 1
( ) and fi 1
1
( ) ⊕ is equal to 0 and the other 
Quantum Algorithms 
147
is equal to 1. This means that the four outputs always give us the standard 
basis elements, telling us the matrix representing our gate is orthogonal—
and so we really do have a gate.
Though we enter two bits of information and get two bits as output, the 
information these gates gives for classical bits, 0  and 1  is exactly the same 
as for the functions evaluated at 0 and 1. The top qubit is exactly what we 
entered, so that piece of output gives us no new information. The choice 
of 0  and 1  for the second input gives us the option of the second output 
giving us the function evaluated on the top input ket or of the opposite 
answer. If we know one of these answers, we know the other.
The quantum computing question that corresponds to the classical 
question is: Given one of these four gates at random, how many times do 
you have to use the gate to determine whether the underlying function fi is 
constant or whether it is balanced?
If we restrict to just entering 0  and 1  into the gate, the analysis is 
exactly the same as before. You have to use the gate twice. But David 
Deutsch showed that if we are allowed to input qubits containing superpo­
sitions of 0  and 1 , the gate only needs to be used once. To show this, he 
used the following circuit.
0
1
H
H
H
Fi
The little meter symbol at the right end of the top wire means that we 
are going to measure this qubit. The lack of the meter symbol on the second 
wire tells us that we won’t be measuring the second output qubit. Let us see 
how this circuit works.
The qubits 0
1
⊗
 are input. They go through the Hadamard gates, 
which puts them in the state
1
2
0
1
1
2
0
1
1
2
00
01
10
11
+
(
) ⊗
−
(
) =
−
+
−
(
).
These then go through the Fi gate. The state becomes
148 
Chapter 8
1
2
0
0
0
0
1
1
1
1
1
1
⊗
( ) −
⊗
( ) ⊕
+
⊗
( ) −
⊗
( ) ⊕
(
)
f
f
f
f
i
i
i
i
.
This can be rearranged to give:
1
2
0
0
0
1
1
1
1
1
⊗
( ) −
( ) ⊕
(
) +
⊗
( ) −
( ) ⊕
(
)
(
)
f
f
f
f
i
i
i
i
.
We now make the observation that f
f
i
i
0
0
1
( ) −
( ) ⊕
 is either 0
1
−
 or 
1
0
−
, depending on whether fi 0
( ) is 0 or 1. But we can be clever about 
this and write
f
f
i
i
fi
0
0
1
1
0
1
0
( ) −
( ) ⊕
= −
(
)
−
(
)
( )
.
We also deduce that
f
f
i
i
fi
1
1
1
1
0
1
1
( ) −
( ) ⊕
= −
(
)
−
(
)
( )
.
The state of our qubits after passing through the Fi gate can then be 
written as
1
2
0
1
0
1
1
1
0
1
0
1
⊗
−
(
)
−
(
)
(
) +
⊗
−
(
)
−
(
)
(
)
(
)
( )
( )
f
f
i
i
.
We can rearrange this to give
1
2
1
0
0
1
1
1
0
1
0
1
−
(
)
⊗
−
(
)
(
) + −
(
)
⊗
−
(
)
(
)
(
)
( )
( )
f
f
i
i
,
then 
1
2
1
0
1
1
0
1
0
1
−
(
)
+ −
(
)
(
) ⊗
−
(
)
( )
( )
f
f
i
i
,
and finally
1
2
1
0
1
1
1
2
0
1
0
1
−
(
)
+ −
(
)
(
) ⊗
−
(
)
( )
( )
f
f
i
i
.
This shows that the two qubits are not entangled, and the top qubit has 
state
1
2
1
0
1
1
0
1
−
(
)
+ −
(
)
(
)
( )
( )
f
f
i
i
.
Quantum Algorithms 
149
Let’s examine this state for each of the four possibilities for fi.
For f0, we have f
f
0
0
0
1
0
( ) =
( ) =
, so the qubit is 1
2
0
1
(
)
+
(
).
For f1, we have f1 0
0
( ) =
 and f1 0
1
( ) = , so the qubit is 1
2
0
1
(
)
−
(
).
For f2, we have f2 0
1
( ) =  and f2 0
0
( ) =
, so the qubit is −(
)
−
(
)
1
2
0
1 .
For f3, we have f
f
3
3
0
1
1
( ) =
( ) =
, so the qubit is −(
)
+
(
)
1
2
0
1 .
The next step in the circuit is to send our qubit through the Hadamard 
gate. This gate sends 1
2
0
1
(
)
+
(
) to 0  and 1
2
0
1
(
)
−
(
) to 1 . So we 
know:
If i = 0, the qubit is 0 .
If i = 1, the qubit is 1 .
If i = 2, the qubit is −1 .
If i = 3, the qubit is −0 .
If we now measure the qubit in the standard basis, we will get 0 if i is 
either 0 or 3, and we will get 1 if i is either 1 or 2. Of course, f0 and f3 are 
the constant functions and f1and f2are the balanced. So, if after measuring 
we get 0, we know with certainty that the original function was constant. If 
we get 1, we know that the original function was balanced.
Consequently, we need to ask the oracle only one question versus two. 
For Deutsch’s problem there is therefore a slight speedup using a quan­
tum algorithm. This algorithm has no real practical applications, but, as 
we noted earlier, it was the first example of proving that there are quantum 
algorithms faster than classical ones.
We will look at two other quantum algorithms in detail. They both 
involve inputting a number of qubits and then sending each one through 
a Hadamard gate. We introduce a little more mathematics to help keep 
the description of many qubits in superposition from becoming too 
unwieldy.
The Kronecker Product of Hadamard Matrices
We know that the matrix for the Hadamard gate is given by
H =
−












=
−




1
2
1
2
1
2
1
2
1
2
1
1
1
1 .
This tells us that
150 
Chapter 8
H(
)
0
1
2
1
1
1
1
1
0
1
2
1
1
1
2
1
0
1
2
0
1
=
−







=



=



+



=
+
1
2
0
1
2
1 ,
and
H(
)
1
1
2
1
1
1
1
0
1
1
2
1
1
1
2
1
0
1
2
0
1
=
−







=
−



=



−



=
−
1
2
0
1
2
1 .
Suppose that we input two qubits and send both through Hadamard gates. 
The four basis vectors will be sent as follows:
0
0
⊗
 goes to
1
2
0
1
2
1
1
2
0
1
2
1
1
2
00
01
10
11
+



⊗
+



=
+
+
+
(
) .
0
1
⊗
 goes to
1
2
0
1
2
1
1
2
0
1
2
1
1
2
00
01
10
11
+



⊗
−



=
−
+
−
(
).
1
0
⊗
 goes to
1
2
0
1
2
1
1
2
0
1
2
1
1
2
00
01
10
11
−



⊗
+



=
+
−
−
(
).
1
1
⊗
 goes to
1
2
0
1
2
1
1
2
0
1
2
1
1
2
00
01
10
11
−



⊗
−



=
−
−
+
(
).
Recall that we can write everything in terms of four-dimensional kets. The 
previous four statements are equivalent to saying:
1
0
0
0












 goes to 1
2
1
1
1
1












,
0
1
0
0












 goes to 1
2
1
1
1
1
−
−












,
Quantum Algorithms 
151
0
0
1
0












 goes to 1
2
1
1
1
1
−
−












,
0
0
0
1












 goes to 1
2
1
1
1
1
−
−












.
This is a description of an orthonormal basis being sent to another ortho­
normal basis. So, we can write the matrix that corresponds to this. We call 
this new matrix H ⊗2.
H ⊗=
−
−
−
−
−
−












2
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
There is an underlying pattern to this matrix that involves H.
H ⊗=
−
−
−
−
−
−












=
−






2
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
2
1
2
1
2
1
2






−












−












−
−
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2




































=
−




H
H
H
H .
This pattern continues. The matrix that corresponds to inputting three 
qubits and sending all three through Hadamard gates can be written 
using H ⊗2.
H
H
H
H
H
⊗
⊗
⊗
⊗
⊗
=
−



=
−
−
−
−
−
−







3
2
2
2
2
1
2
1
2 2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1




−
−
−
−
−
−












−
−
−
−
−
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
−












−
−
−
−
−
−
−






































152 
Chapter 8
=
−
−
−
−
−
−
−
−
1
2 2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
−


























1
As n increases these matrices quickly get large, but it is always true that
H
H
H
H
H
n
n
n
n
n
⊗
⊗
−
(
)
⊗
−
(
)
⊗
−
(
)
⊗
−
(
)
=
−




1
2
1
1
1
1 ,
and this gives us a recursive formula that lets us quickly calculate them. 
These products of matrices telling us how to act on tensor products are 
called Kronecker products.
For Simon’s algorithm we are going to need to study these matrices in 
some detail, but for our next algorithm the key observation is that the 
entries in the top row of each these matrices are all equal to one another; 
for H
n
⊗ they all equal 1
2
(
)
n.
The Deutsch-Jozsa Algorithm
Deutsch’s algorithm looked at functions of one variable. You were given 
one of these and had to determine whether it was a constant or balanced 
function. The Deutsch-Jozsa problem is a generalization of this.
We now have functions of n variables. The inputs for each of these vari­
ables, as before, can be either 0 or 1. The output is either 0 or 1. We are told 
that our function is either constant—all the inputs get sent to 0, or all the 
inputs get sent to 1 — or it is balanced—half the inputs get sent to 0 and 
the other half to 1. If we are given one of these functions at random, how 
many function evaluations do we need to determine whether the function 
belongs to the constant group or to the balanced group?
To illustrate, we consider the case when n = 3. Our function takes three 
inputs, each of which can take two values. This means that there are 23, or 
8, possible inputs:
(0,0,0), (0,0,1), (0,1,0), (0,1,1), (1,0,0), (1,0,1), (1,1,0), (1,1,1).
Quantum Algorithms 
153
Classically, suppose we evaluate f 0 0 0
, ,
(
) and get the answer that 
f 0 0 0
1
, ,
(
) =
. We cannot deduce anything from this piece of information 
alone, so we ask for another function evaluation, say f 0 0 1
, ,
(
). If we get 
f 0 0 1
0
, ,
(
) =
, then we are done. We know that the function cannot be con­
stant, so it must be balanced. On the other hand, if we get f 0 0 1
1
, ,
(
) = , we 
cannot deduce anything from the two pieces of information we have. In 
the worst possible scenario, we could get the same answer for the first four 
questions and still not be able to answer the question. For example, from 
the fact that f 0 0 0
1
, ,
(
) =
, f 0 0 1
1
, ,
(
) = , f 0 1 0
1
, ,
(
) = , f 0 1 1
1
, ,
(
) =  we cannot 
determine whether or not the function is balanced. We need to ask one 
more question. If the answer to the next question is also 1, then we know 
the function is constant. If the answer is 0, then we know the function is 
balanced.
This analysis works in general. Given a function of n variables, there 
will be 2n possible input strings. In the best-case scenario we can obtain 
the answer with just two questions to the oracle, but in the worst case it 
will take us 2
1
1
n−+  questions. Since the n −1 appears as an exponent, the 
function is exponential. In the worst case it requires an exponential num­
ber of inquiries to the oracle. The Deutsch-Jozsa algorithm is a quantum 
algorithm that just requires one question to the oracle, so the speedup is 
substantial!
The first step, as in all of these questions, is to describe the oracle. For each 
of the functions we need to construct an orthogonal matrix that captures 
the essence of the function. We just generalize our previous construction.
Given any function f x
x
xn
0
1
1
,
,
,
…
(
)
−
 that has n boolean inputs and has 
just one boolean output, we construct the gate F given by the following 
circuit, where the slashes with n on the top lines indicate that we have n 
wires in parallel.
Remember that this circuit tells us what happens when each of the kets, 
xi , is either 0  or 1 . The input consists of n + 1 kets, x
x
xn
0
1
1
⊗
⊗…⊗
− 
y
F
n
n
y
f (x0, x1
xn
, . . . ,
1)
x0
xn
1
x1
x0
xn
1
x1
154 
Chapter 8
and y , where the first n correspond to the function variables. The output 
also consists of n + 1 kets, the first n of which are exactly the same as the 
input kets. The last output is the ket f x
x
xn
0
1
1
,
,
,
…
(
)
−
 if y = 0 and the ket of 
the other boolean value when y = 1.
The next step after describing how the black-box function works is to 
give the quantum circuit that incorporates this function. It is the natural 
generalization of the circuit used for Deutsch’s algorithm: all the top qubits 
pass through Hadamard gates on either side of the black box.
As before, we will analyze what this circuit does step by step. We show the 
case when n = 2, just to make things look a little less messy on the page, but 
every step we do works in exactly the same way for every value of n.
Step 1.  The Qubits Pass through the Hadamard Gates
The top n inputs are all 0 . For n = 2, this is 00 . The following calculation 
shows what happens.
H ⊗(
) =
−
−
−
−
−
−






















2
00
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0


=












=
+
+
+
(
)
1
2
1
1
1
1
1
2
00
01
10
11
It gives a superposition of all possible states; each of the basis kets has the 
same probability amplitude (1 2 in this case).
(This calculation works for every value of n. After the n qubits have 
passed through H
n
⊗ they are in superposition of all possible states, each of 
which has the same probability amplitude: 1
2
(
)
n.)
The bottom entry is just 1 . This becomes 1
1
2
0
2 1
(
)
−(
)
 after 
passing through the Hadamard gate. At this stage, our three input qubits 
will be in the following state.
1
F
00
0
n
H
n
n
n
n
H
H
n
Quantum Algorithms 
155
1
2
00
01
10
11
1
2
0
1
2
1
+
+
+
(
) ⊗
−



. We will rewrite this as
1
2 2
00
0
1
1
2 2
01
0
1
1
2 2
10
0
1
1
2 2
11
0
1
⊗
−
(
)
+
⊗
−
(
)
+
⊗
−
(
)
+
⊗
−
(
)
Step 2.  The Qubits Pass through the F Gate
After passing through the F gate the qubits will be in the following state.
  
1
2 2
00
0 0
0 0
1
1
2 2
01
0 1
0 1
1
1
2 2
10
1
⊗
(
) −
(
) ⊕
(
)
+
⊗
(
) −
(
) ⊕
(
)
+
⊗
f
f
f
f
f
,
,
,
,
,
,
,
,
0
1 0
1
1
2 2
11
1 1
1 1
1
(
) −
(
) ⊕
(
)
+
⊗
(
) −
(
) ⊕
(
)
f
f
f
We now use the fact that that if a is either 0 or 1 we have the following
a
a
a
−
⊕
= −
(
)
−
(
)
1
1
0
1
to rewrite the state as
  −
(
)
⊗
−
(
)
−
(
)
⊗
−
(
)
−
(
)
+
+
(
)
(
)
(
)
1
1
2 00
1
2
0
1
1
1
2 01
1
2
0
1
1
0 0
0 1
1 0
f
f
f
,
,
,
1
2 10
1
2
0
1
1
1
2 11
1
2
0
1
1 1
⊗
−
(
)
−
(
)
⊗
−
(
)
+
(
)
f
,
As before, this shows that the bottom qubit is not entangled with the top 
qubits. We just look at the top two qubits. These top two are in state:
1
2
1
00
1
01
1
10
1
11
0 0
0 1
1 0
1 1
−
(
)
+ −
(
)
+ −
(
)
+ −
(
)
(
)
(
)
(
)
(
)
(
)
f
f
f
f
,
,
,
,
156 
Chapter 8
(The argument that we have used works for general n. At this stage you 
obtain a state that is a superposition of all basis kets. Each ket, x x
xn
0
1
1
…
−
 
is multiplied by 1
2
1
0
1
1
(
)
−
(
)
…
(
)
−
n
f
x
xn
x ,
,
,
.)
Step 3.  The Top Qubits Pass through the Hadamard Gates
The standard method is to convert our state to a column vector and then 
multiply by the appropriate Kronecker product of the Hadamard matrix. 
This gives:
1
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0 0
0 1
−
−
−
−
−
−












−
(
)
−
(
)
−
(
)
(
)
f
f
,
,
(
)
−
(
)














(
)
(
)
f
f
1 0
1 1
1
,
,
.
However, we are not going to calculate all of the entries in the resulting col­
umn vector. We are just going to calculate the top entry. This entry comes 
from multiplying the bra corresponding to the top row of the matrix with 
the ket given by the column vector. We get
1
4
1
1
1
1
0 0
0 1
1 0
1 1
−
(
)
+ −
(
)
+ −
(
)
+ −
(
)
(
)
(
)
(
)
(
)
(
)
f
f
f
f
,
,
,
,
.
This is the probability amplitude of the ket 00 . We calculate this ampli­
tude for the possible functions.
If f is constant and sends everything to 0, the probability amplitude is 1.
If f is constant and sends everything to 1, the probability amplitude 
is −1.
For the balanced functions, the probability amplitude is 0.
Step 4.  Measure the Top Qubits
When we measure the top qubits we will get one of 00, 01, 10, or 11. The 
question becomes “do we get 00?” If the function is constant, then we will 
with probability 1. If the function is balanced, we get it with probability 0. 
So, if the result of measuring gives 00, we know our function was constant. 
If the result is not 00, then the function is balanced.
The analysis works for general n. Just before we measure the qubits the 
probability amplitude for 0
0
…
 is
1
2
1
1
1
0 0
0
0 0
1
1 1
1
n
f
f
f
−
(
)
+ −
(
)
+
+ −
(
)
(
)
…
(
)
…
(
)
…
(
)
, ,
,
, ,
,
, ,
,

.
Quantum Algorithms 
157
As with n = 2, this number will be ±1 if f is constant and 0 if f is balanced. 
So, if every measurement gives 0, the function is constant. If at least one of 
the measurements is 1, the function is balanced.
Consequently, we can solve the Deutsch-Jozsa problem for any value n 
with just one use of the circuit. We only need to ask the oracle one ques­
tion. Recall in the classical case, that in the worst case it required 2
1
1
n−+  
questions, so the improvement is dramatic.
Simon’s Algorithm
The two algorithms that we have seen so far have been unusual in that 
we get the final answer with certainty after just one query. Most quantum 
algorithms use a mixture of quantum algorithms and classical algorithms; 
they involve more than one use of a quantum circuit; and they involve 
probability. Simon’s algorithm contains all of these components. However, 
before we describe the algorithm, we need to discuss the problem being 
tackled, and before we can do that, we need to introduce a new way of add­
ing binary strings.
Bitwise Addition of Strings Modulo 2
We defined ⊕ to be the exclusive or XOR, or, equivalently, as addition mod­
ulo 2. Recall
0
0
0
0
1
1 1
0
1 1
1
0
⊕
=
⊕
=
⊕
=
⊕
=
We extend this definition to the addition of binary strings of the same 
length by the formula:
a a
a
b b
b
c c
c
n
n
n
0
1
0
1
0 1



⊕
=
, where
c
a
b
c
a
b
c
a
b
n
n
n
0
0
0
1
1
1
=
⊕
=
⊕
…
=
⊕
,
,
,
.
This is like doing addition in binary, but ignoring any carries. Here’s a con­
crete example of bitwise addition:
1101
0111
1010
⊕
158 
Chapter 8
The Statement of Simon’s Problem
We have a function f that sends binary strings of length n to binary strings 
of length n. It has the property that there is some secret binary string s, such 
that f x
f y
( ) =
( ) if and only if y
x
=
 or y
x
s
=
⊕. We don’t allow s to be the 
string consisting entirely of 0s; this forces pairs of distinct input strings to 
have the same output strings. The problem is to determine the secret string 
s. An example should make all of this clear.
We will take n = 3, so our function f will take binary strings of length 3 
and give other binary strings of length 3. Suppose that the secret string is 
s = 110. Now
000
110
110
001
110
111
010
110
100
011
110
101
100
110
010
101
⊕
=
⊕
=
⊕
=
⊕
=
⊕
=
⊕
=
⊕
=
⊕
=
110
011 110
110
000
111
110
001.
Consequently, for this value of s, we get the following pairings:
f
f
f
f
f
f
f
f
000
110
001
111
010
100
011
101
(
) =
(
)
(
) =
(
)
(
) =
(
)
(
) =
(
).
A function with this property is
f
f
f
f
f
f
f
000
110
101
001
111
010
010
100
111
011
(
) =
(
) =
(
) =
(
) =
(
) =
(
) =
(
) = f 101
000
(
) =
.
Now, of course, we don’t know the function f or the secret string s: We want 
to find s. The question is how many function evaluations need to be made 
to determine this string?
We keep evaluating the function f on strings. We stop as soon as we get a 
repeated answer. Once we have found two input strings that give the same 
output, we can immediately calculate s.
For example, if we find that f
f
011
101
(
) =
(
),then we know that
011
101
0 1 2
⊕
=
s s s
.
Using the fact that
011
011
000
⊕
=
,
bitwise add 011 to the left of both sides of the equation to obtain
s s s
0 1 2
011
101
110
=
⊕
=
.
How many evaluations do we have to make using a classical algorithm? We 
have eight binary strings. It is possible to evaluate four of these and get four 
Quantum Algorithms 
159
different answers, but with the fifth evaluation we are guaranteed to get a 
pair. In general, for strings of length n, there are 2n binary strings and, in the 
worst case, we will need to 2
1
1
n−+  function evaluations to get a repeat. So, 
in the worst case, we will need to ask the oracle 2
1
1
n−+  questions.
Before we look at the quantum algorithm, we need to look at the Kro­
necker product of Hadamard matrices in a little more detail.
The Dot Product and the Hadamard Matrix
Given two binary strings, a
a a
an
=
−
0
1
1

 and b
b b
bn
=
−
0
1
1

 of the same 
length, we define the dot product by
a b
a
b
a
b
a
b
n
n
⋅
=
×
⊕
×
⊕
⊕
×
−
−
0
0
1
1
1
1

, where × denotes our usual 
multiplication.
So, for example, if a = 101 and b = 111, then a b
⋅
=
⊕
⊕
=
1
0
1
0. This opera­
tion can be thought of as multiplying corresponding terms of the sequences, 
then adding and finally determining whether the sum is odd or even.
In computer science, we often start counting at 0, so instead of counting 
from 1 to 4 we count from 0 to 3. Also, we often use binary. The numbers 0, 
1, 2, and 3 are represented in binary by 00, 01, 10, 11. Given a 4
4
×
 matrix, 
we will label both the rows with these numbers, as is shown here:
00
01 10
11
00
01
10
11
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 












The position of an entry in this matrix is given by listing both the row and 
the column in which it appears. If we make the entry in the ith row and jth 
column be i j⋅, we get the following matrix.
00
01 10
11
00
01
10
11
 0 
 0 
 0 
 0 
 0 
 1 
 0 
 1 
 0 
 0 
 1 
 1 
 0 
 1 
 1 
 0 












Compare this matrix to H ⊗2. Notice that the entries that are 1 in our dot 
product matrix are in exactly the same positions as the negative entries in 
H ⊗2. Using the facts that −
(
) =
1
1
0
 and −
(
) = −
1
1
1
, we can write
160 
Chapter 8
H ⊗
⋅
⋅
⋅
⋅
⋅
⋅
=
−
−
−
−
−
−
2
00 00
00 01
00 10
00 11
01 00
01
1
2
1
1
1
1
1
1
(
)
(
)
(
)
(
)
(
)
(
)
01
0110
00 11
10 00
10 01
10 10
10 11
1
1
1
1
1
1
(
)
(
)
(
)
(
)
(
)
(
)
(
−
−
−
−
−
−
⋅
⋅
⋅
⋅
⋅
⋅
−
−
−
−












⋅
⋅
⋅
⋅
1
1
1
1
11 00
11 01
1110
1111
)
(
)
(
)
(
)
.
This method of finding where the positive and negative entries holds in 
general; for example, if we want the entry of H ⊗3 that is in the row with 
number 101 and column with number 111, we calculate the dot product 
and get 0. This tells us that the entry will be positive.
Hadamard Matrices and Simon’s Problem
Now that we know how to find entries of Kronecker products of Hadamard 
matrices, we are going to use this knowledge to see what happens when 
we add two columns of one of these products. We are going to add two 
columns that are paired by the secret string s given in Simon’s problem. If 
one column is labeled by the string b, the other will have label b
s
⊕. We are 
going to add these two columns together.
To illustrate, we will work with strings of length 2, and suppose the 
secret string is 10. We will be adding column 00 to 10, or column 01 to 11.
Here is H ⊗2.
H ⊗=
−
−
−
−
−
−












2
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Adding the 00 column to 10 gives:
1
2
1
1
1
1
1
2
1
1
1
1
1
2
2
2
0
0












+
−
−












=












.
Adding the 01 column to 11 gives:
1
2
1
1
1
1
1
2
1
1
1
1
1
2
2
2
0
0
−
−












+
−
−












=
−












.
Quantum Algorithms 
161
Notice that some probability amplitudes are getting amplified and some are 
canceling. What exactly is going on here?
It is fairly easy to check that products and bitwise addition obey the 
usual law of exponents.
−
(
)
= −
(
)
−
(
)
⋅
⊕
(
)
⋅
⋅
1
1
1
a b
s
a b
a s.
This tells us that −
(
)
⋅
⊕
(
)
1
a b
s  and −
(
)
⋅
1
a b will be equal if a s
⋅
= 0, and that 
−
(
)
⋅
⊕
(
)
1
a b
s  and −
(
)
⋅
1
a b will have opposite signs if a s
⋅
= 1.
We can summarize this as:
−
(
)
+ −
(
)
= ±
⋅
=
−
(
)
+ −
(
)
=
⋅
=
⋅
⊕
(
)
⋅
⋅
⊕
(
)
⋅
1
1
2
0
1
1
0
1
a b
s
a b
a b
s
a b
a s
a s
if
if
.
This tells us that when we add the two columns given by b and b
s
⊕, the 
entry in row a will be 0 if a s
⋅
= 1, and will be either 2 or −2 if a s
⋅
= 0. In 
general, the entries cancel in the rows labeled with strings that have a dot 
product of 1 with the string s.
Looking back at our example, the reason that the bottom two entries are 
0 is that these rows have labels 10 and 11 and these both have a dot prod­
uct of 1 with our secret string s. The nonzero entries occur in the rows with 
labels 00 and 01 and these both have a dot product of 0 with s.
We now have the information needed to understand the quantum cir­
cuit for Simon’s problem. It is going to give us a string whose dot product 
with the secret string s is 0. It is going to do that by adding two columns of 
the Hadamard matrix. Let’s see how it works.
The Quantum Circuit for Simon’s Problem
The first thing is to construct the black box—the gate that acts like f. The 
following circuit gives the construction.
We can think of this as inputting two strings consisting of 0 s and 1 s, 
both of which have the same length. The top string is unchanged. The 
F
n
n
n
n
x0
xn
1
x0
xn
1
y0
yn
1
f (
)
x0
xn
1
y0
yn
1
162 
Chapter 8
bottom string is the function evaluated on the top string added bitwise to 
the bottom string.
The following circuit gives the circuit for the algorithm.
We will illustrate what happens in the case n = 2. Everything we do general­
izes straightforwardly for general n.
The first step consists of the qubits in the top register passing through 
the Hadamard gates. This should now look familiar. The top two qubits are 
initially in state 00 , while after passing through the Hadarmard gates they 
will be in state
1
2
00
01
10
11
+
+
+
(
).
The bottom two qubits remain in state 00 . So, at this stage the four qubits 
are in state:
1
2
00
00
01
00
10
00
11
00
⊗
+
⊗
+
⊗
+
⊗
(
)
The next thing that happens is that the qubits pass through the F gate. This 
changes the state to:
1
2
00
00
01
01
10
10
11
11
⊗
(
) +
⊗
(
) +
⊗
(
) +
⊗
(
)
(
)
f
f
f
f
The top qubits now pass through the Hadamard gates, which changes the 
state to:
1
4
00
01
10
11
00
1
4
00
01
10
11
01
1
4
00
01
10
11
+
+
+
(
) ⊗
(
)
+
−
+
−
(
) ⊗
(
)
+
+
−
−
(
f
f
) ⊗
(
)
+
−
−
+
(
) ⊗
(
)
f
f
10
1
4
00
01
10
11
11
F
|00 . . . 0
n
H⊗n
n
n
H⊗n
n
n
n
00
0
Quantum Algorithms 
163
The pattern of + and − signs comes from the matrix for H ⊗2. We now 
rearrange the terms, solving for the first two qubits, which results in the 
following:
1
4 00
00
01
10
11
1
4 01
00
01
10
1
⊗
(
) +
(
) +
(
) +
(
)
(
)
+
⊗
(
) −
(
) +
(
) −
f
f
f
f
f
f
f
f
1
1
4 10
00
01
10
11
1
4 11
00
01
1
(
)
(
)
+
⊗
(
) +
(
) −
(
) −
(
)
(
)
+
⊗
(
) −
(
) −
f
f
f
f
f
f
f
0
11
(
) +
(
)
(
)
f
This way of writing the state has a couple of nice features. The first is that 
here, also, the pattern of + and − signs comes from the matrix for H ⊗2. The 
second is that the pairs of qubits to the left of the tensor product corre­
spond to the row numbers.
Now we use the fact that we know that f b
f b
s
( ) =
⊕
(
), so f b
f b
s
( ) =
( ) ⊕
. 
We can simplify things, combining these terms, by adding their probability 
amplitudes. This corresponds to the column addition we just looked at. To 
illustrate, suppose that s = 10, then f
f
00
10
(
) =
(
) and f
f
01
11
(
) =
(
). If we 
plug these values into the state, we obtain:
1
4 00
00
01
00
01
1
4 01
00
01
00
0
⊗
(
) +
(
) +
(
) +
(
)
(
)
+
⊗
(
) −
(
) +
(
) −
f
f
f
f
f
f
f
f
1
1
4 10
00
01
00
01
1
4 11
00
01
0
(
)
(
)
+
⊗
(
) +
(
) −
(
) −
(
)
(
)
+
⊗
(
) −
(
) −
f
f
f
f
f
f
f
0
01
(
) +
(
)
(
)
f
which simplifies to
  1
4 00
2
00
2
01
1
4 01
2
00
2
01
1
4 10
0
1
4 1
⊗
(
) +
(
)
(
)
+
⊗
(
) −
(
)
(
)
+
⊗( )
+
f
f
f
f
1
0
⊗( )
.
164 
Chapter 8
The kets to the left of the tensor products are labeled with the row numbers 
of the matrix. The 0s on the right of the tensor products occur in the rows 
whose dot product with s is 1.
We can simplify the state to:
1
2
00
1
2
00
01
1
2
01
1
2
00
01
⊗
(
) +
(
)
(
) +
⊗
(
) −
(
)
(
)
f
f
f
f
.
When we measure the top two qubits we will get either 00 or 01, each with 
probability 1/2.
Though we have only looked at the relatively simple case of n = 2, every­
thing we have done holds for every value of n. At the end of the process we 
will end up with one of the strings whose dot product with the secret string 
is 0. Each of these strings is equally likely.
You might be concerned that after all this work we still don’t know s. 
This is where the classical part of Simon’s algorithm comes in.
The Classical Part of Simon’s Algorithm
We start with an example with n = 5. We know that there is some secret 
number s
s s s s s
=
0 1 2 3 4. We are not allowing 00000, so there are 2
5
1
31
^
−
=
 
possible choices for s. We are going to try to find it using Simon’s quantum 
circuit.
We run it and get 10100 as an answer. We know that the dot product of 
this with s gives 0. So,
1
0
1
0
0
0
0
1
2
3
4
×
⊕
×
⊕
×
⊕
×
⊕
×
=
s
s
s
s
s
.
This tells us that s
s
0
2
0
⊕
=
. Since these digits are either 0 or 1, we deduce 
that s
s
0
2
=
.
We run the circuit again hoping that we don’t get 10100 again. (The 
probability of this happening is 1 16
/
, so we are fairly safe.) We also hope 
that we don’t get 00000, which wouldn’t give us any new information. Sup­
pose we get 00100. Then we know that
0
0
1
0
0
0
0
1
2
3
4
×
⊕
×
⊕
×
⊕
×
⊕
×
=
s
s
s
s
s
.
This shows that s2 must be 0. From the first step, we can now deduce that 
s0 must also be 0. We run the circuit again and get 11110. We know that
1
0
1
1
0
1
0
0
1
3
4
×
⊕
×
⊕
×
⊕
×
⊕
×
=
s
s
s
,
Quantum Algorithms 
165
which tells us that s
s
1
3
=
. Running the circuit again gives 00111, telling us 
that
0
0
0
1
0
1
1
0
1
3
4
×
⊕
×
⊕
×
⊕
×
⊕
×
=
s
s
s
.
Consequently, we must have s
s
3
4
=
 and, since s
s
1
3
=
, we have s
s
s
1
3
4
=
=
.
We know that not all of the digits are 0, so we must have s
s
s
1
3
4
1
=
=
= , 
and consequently s must be 01011. For this example, we made four calls to 
the oracle.
At this point, there are a couple of questions that you might be ask­
ing. The first concerns the algorithm for finding s using the outputs of the 
quantum circuit. We have seen what to do in a specific case, but is there 
an algorithm—a step-by-step procedure—that tells you what to do in every 
case? The second question concerns how we are measuring the number of 
questions we are asking the oracle. When we looked at the classical algo­
rithm, we took the worst possible case and saw that after 2
1
1
n−+  questions 
we would definitely have our answer. But when we come to the quantum 
algorithm, the worst possible case is much worse! We are getting an answer 
at random. The answer does have a dot product of 0 with s, but we could 
get the same answer more than once. We could run our quantum circuit 
2
1
1
n−+  times and get a string of 0s every single time. It is unlikely, but it is 
possible. A string of 0s gives us no information, so it is possible that after 
2
1
1
n−+  questions to the oracle we haven’t deduced anything at all about 
the secret number. We will address both of these concerns.
Each time we run the circuit, we get a number whose dot product with s 
is zero. This gives us a linear equation in the n unknowns. Running the cir­
cuit several times results in our obtaining several—a system of—equations. 
In the previous example, at each stage we got a new equation, but that new 
equation also gave us some new information. The technical term for this is 
that the equation is linearly independent of the previous equations. In order 
to calculate s we need a system of n −1 linearly independent equations.***
Algorithms for solving systems of equations are extremely well known. 
They are studied in courses like Linear Algebra and Matrix Theory and have 
***  You may have seen systems of linear equations before and remember that you 
need n equations to solve a system with n unknowns. This is true when the coeffi­
cients can be real numbers, but in our case the coefficients are either 0 or 1. This 
restriction and the fact that the string of all 0s is not allowed for s make it possible 
for us to reduce the number of equations by one.
166 
Chapter 8
numerous applications. They are so commonly needed that they are pro­
grammed into most scientific calculators. We won’t discuss them here apart 
from mentioning that the number of steps required to solve a system of n 
equations can be bounded above by a quadratic expression involving n. We 
say the system can be solved in quadratic time.
The other question that we need to address is this: How many times do 
we need to run the quantum circuit? As we pointed out, in the worst-case 
scenario, we can keep running our qubits through the circuit and never get 
any useful information. However, this is extremely unlikely. We examine 
this idea in more detail in the next section.
Complexity Classes
In complexity theory, the main classification is between problems that take 
polynomial time to solve and those that need more than polynomial time. 
Polynomial time algorithms are regarded as being practical even for very 
large values of n, but non-polynomial time algorithms are regarded as being 
infeasible for large n.
Problems that classical algorithms can solve in polynomial time are 
denoted by P. Problems that quantum algorithms can solve in polynomial 
time are denoted by QP (sometimes it is denoted by EQP, for exact quantum 
polynomial time). Usually when we use these terms we are referring to the 
number of steps that an algorithm takes, but, remember, we defined a new 
way of measuring complexity—query complexity—that counts the number 
of questions we need to ask an oracle. We saw that the Deutsch-Jozsa prob­
lem was not in the class P, but belonged to QP for query complexity. (The 
constant function is a degree 0 polynomial.) This is sometimes described as 
saying that the Deutsch-Jozsa problem separates P and QP—it is a problem 
that belongs to QP but not to P for query complexity.
However, let’s recall the worst-case scenario for the classical algorithm. 
To make things more concrete, we will take n = 10. We are given a func­
tion that takes 10 inputs and told that it is either balanced or constant. We 
have to keep evaluating our function on specific inputs until we can deduce 
the answer. There are 2
1024
10 =
 possible inputs. The worst-case scenario is 
when the function is balanced, but we get the same answer for the first 512 
evaluations, and then on the 513th evaluation we get the other value. But 
how likely is this to happen?
Quantum Algorithms 
167
If the function is balanced, for each input value we are equally likely to 
get either a 0 or a 1. This can be compared to tossing a fair coin and obtain­
ing a head or a tail. How likely is it to toss a fair coin 512 times and get 
heads every time? The answer is 1 2
512
(
)
, which is less than 1 divided by a 
googol, where a googol is 10100. It is a minute number!
Suppose you were given a coin and asked whether it was fair or was 
double-headed. If you toss it once and get heads, you can’t really answer 
the question. But if you toss it ten times and it comes up heads every time, 
then you can be fairly sure that it is double-headed. Of course, you could be 
wrong, but in practice we are willing to accept being wrong as long as the 
probability of this happening is very small.
This is what we do for the bounded-error complexity classes. We choose 
some bound on the probability of getting an error that we think is accept­
able. Then we look at algorithms that can answer the question within our 
bound for error.
Returning to the Deutsch-Jozsa example, suppose that we want at least a 
99.9 percent success rate, or equivalently an error rate of less than 0.1 per­
cent. If a function is balanced the probability of evaluating the function 11 
times and getting 0 every time is 0.00049 to five decimal places. Similarly, 
the probability of obtaining 1 every time is 0.00049. Consequently, the 
probability of obtaining the same answer 11 times in a row when the func­
tion is even is just less than 0.001. So if we are willing to accept a bound 
on the probability of error of 0.1 percent, we can choose to make at most 
11 function evaluations. If during the process we get both a 0 and a 1, we 
can stop and know with certainty that our function is balanced. If all 11 
evaluations are the same, we will say the function is constant. We could be 
wrong, but our error rate is less than our chosen bound. Notice that this 
argument works for any n. In every case, we need 11 function evaluations 
at most.
Problems that classical algorithms can solve in polynomial time with the 
probability of error within some bound are denoted BPP (for bounded-error 
probabilistic polynomial time). The Deutsch-Jozsa problem is in the class 
BPP.
One thing that you might be worried about is whether a problem could 
be in BPP for one bound on the probability of error, but not in the class BPP 
for a smaller bound. This doesn’t happen. If the problem is in the class BPP, 
it will be there for every choice of the bound.
168 
Chapter 8
We now return to Simon’s algorithm. We need to keep sending qubits 
through the circuit until we have n −1 linearly independent equations. As 
we know, in the worst case this process can go on forever, so Simon’s algo­
rithm is not in class QP. However, let’s choose a bound that we are willing 
to accept on the probability of making an error. Then we can calculate N so 
that 1 2
(
)
N  is less than our bound.
We won’t prove this, but it can be shown that if we run the circuit n
N
+
 
times, the probability of the n
N
+
 equations containing a system n −1 lin­
early independent equations is greater than 1
1 2
−(
)
N .
We can now state Simon’s algorithm. First we decide on a bound on the 
probability of error and calculate the value N. Again, the number N does 
not depend on n. We can use the same value of N in each case. We run 
Simon’s circuit n
N
+
 times. The number of queries is n
N
+
, which, since N 
is fixed, is a linear function of n. We make the assumption that our system 
of n
N
+
 equations contains n −1 independent vectors. We could be wrong, 
but the probability of being wrong is less than the bound that we chose. 
Then we solve the system of n
N
+
 equations using a classical algorithm. 
The time taken will be quadratic in n
N
+
, but because N is a constant, this 
can be expressed as a quadratic in n.
The algorithm as a whole contains the quantum part that takes linear 
time added to the classical part that takes quadratic time, giving quadratic 
time overall. Problems that quantum algorithms can solve in polynomial 
time with the probability of error within some bound are denoted BQP (for 
bounded-error quantum polynomial time). Simon’s algorithm shows the 
problem belongs to BQP for query complexity.
We showed that the classical algorithm, in the worst case, took 2
1
1
n−+  
function evaluations—this is exponential in n, not polynomial, so the 
problem definitely does not belong to P. It can also be shown that even if 
we allow a bound on the probability of error the algorithm is still exponen­
tial, so the problem does not belong to BPP. We say that Simon’s problem 
separates BPP and BQP for query complexity.
Quantum Algorithms
We started this chapter by describing how in many popular descriptions the 
speedup provided by quantum algorithms is said to come solely from quan­
tum parallelism—the fact that we can put the input into a superposition 
Quantum Algorithms 
169
that involves all the basis states. However, we have looked at three algo­
rithms and have seen that though we need to use quantum parallelism, we 
need to do much more. We will briefly look at what is needed and why it 
is hard!
The three algorithms we have studied are the most elementary and con­
sidered standard, but as you have probably noticed they are by no means 
trivial. The dates when they were published tells an important story. David 
Deutsch published his algorithm in his landmark paper of 1985. This was 
the first quantum algorithm, and it showed that a quantum algorithm 
could be faster than a classical one. Deutsch and Jozsa published their gen­
eralization of Deutsch’s algorithm in 1992, seven years later. It might seem 
surprising that what seems to be a fairly straightforward generalization took 
so long to find, but it is important to realize that it is the modern notation 
and presentation that make the generalization seem to be the natural one. 
Deutsch’s paper doesn’t state the problem exactly the way it is stated here, 
and it doesn’t use diagrams for quantum circuits that are now standard. 
That said, there was an incredibly productive period from 1993 to 1995 
when many of the most important algorithms were discovered. Daniel 
Simon’s algorithm was published in this window, as were the algorithms by 
Peter Shor and Lov Grover that we will look at in the next chapter.
Orthogonal matrices represent quantum gates. Quantum circuits con­
sist of combinations of gates. These correspond to multiplying orthogonal 
matrices, and since the product of orthogonal matrices results in an orthog­
onal matrix, any quantum circuit can be described by just one orthogonal 
matrix. As we have seen, an orthogonal matrix corresponds to a change of 
basis—a different way of viewing the problem. This is the key idea. Quan­
tum computing gives us more ways of viewing a problem than classical 
computing does. But in order to be effective, there has to be a view that 
shows the correct answer separated from other possible incorrect answers. 
Problems that quantum computers can solve faster than classical comput­
ers need to have a structure that becomes visible only when we transform it 
using an orthogonal matrix.
The problems that we have looked at are clearly reverse-engineered. 
They are not important problems that people have been considering for 
years and that we have only now discovered that if we view them from 
the right quantum computing perspective they become simpler to solve. 
Rather, they are problems that are specially created using the structure of 
170 
Chapter 8
Kronecker products of Hadamard matrices. Of course, what we really want 
is not to reverse-engineer a problem, but to take a problem that is impor­
tant and then construct a quantum algorithm that is faster than any known 
classical algorithm. This is what Peter Shor achieved in his 1994 landmark 
paper, in which he showed (among other things) how quantum computing 
could be used to break the codes that are currently used for Internet secu­
rity. We will briefly discuss Shor’s algorithm in the next chapter, where we 
look at the impact of quantum computing.
9  Impact of Quantum Computing
Chapter 
9
Impact 
of 
Quantum 
Computing
© Massachusetts Institute of TechnologyAll Rights Reserved
It is, of course, impossible to predict the long-term impact of quantum 
computing with any accuracy. If we look back at the birth of the modern 
computer in the 1950s, nobody could have predicted how much comput­
ers would change society and how dependent we would become on them. 
There are well-known quotes from computer pioneers proclaiming that the 
world would only need a handful of computers and that nobody would 
ever need a computer in their home. These quotes are out of context. The 
authors were generally referring to specific types of computers, but the 
impression they give, though exaggerated, is true. Initially computers were 
massive, had to be in air-conditioned rooms, and were not very reliable. 
Today, I have a laptop, a smartphone, and a tablet. All three are far more 
powerful than the first computers. I think that even visionaries like Alan 
Turing would be amazed at the extent to which computers have thoroughly 
permeated all levels of society. Turing did discuss chess playing and arti­
ficial intelligence, but nobody predicted that the rise of e-commerce and 
social media would come to dominate so much of our lives.
Quantum computing is now in its infancy, and the comparison to the 
first computers seems apt. The machines that have been constructed so 
far tend to be large and not very powerful, and they often involve super­
conductors that need to be cooled to extremely low temperatures. Already 
there are some people saying that there will be no need for many quantum 
computers to be built and that their impact on society will be minimal. 
But, in my opinion, these views are extremely shortsighted. Although it 
is impossible to predict what the world will be like in fifty years time, we 
can look at the dramatic changes in quantum computing over the last few 
years and see the direction in which it is heading. It might be some time 
before we get powerful universal quantum computers, but even before we 
172 
Chapter 9
do, quantum computing looks likely to make a substantial impact on our 
lives. In this chapter we will look at some ways that this could occur. In 
contrast to the previous chapter where we looked at three algorithms in 
considerable depth, we will look at a wide variety of topics at a less detailed 
level.
Shor’s Algorithm and Cryptanalysis
The major result in quantum computing concerning cryptanalysis is Shor’s 
algorithm. To fully understand this algorithm requires a substantial math­
ematics background. It uses Euler’s theorem and continued fraction expan­
sions from number theory. It also requires knowledge of complex analysis 
and the discrete Fourier transform. It marks the place where the theory of 
quantum computation changes from requiring just elementary mathemat­
ics to a more substantial background. Consequently, we won’t be covering 
the algorithm in detail, but its importance means that we should at least 
look at it.
It is an algorithm, like Simon’s algorithm, that has a quantum part and 
a classical part. The quantum part is similar to that of Simon’s algorithm. 
Before we give a brief description, we will look at the problem that Shor 
wanted to tackle.
RSA Encryption
The RSA encryption method is named after its inventors, Ron Rivest, Adi 
Shamir, and Leonard Adleman. They published a paper on it and then pat­
ented it in 1978. Later it became known that Clifford Cocks, working for 
the Government Communications Headquarters (GCHQ), a British intel­
ligence agency, had essentially invented the same algorithm in 1973. The 
British classified it, but they did pass it on to the Americans. It seems, how­
ever, that neither of the American or British intelligence agencies used it or 
realized how important it would become. Nowadays, it is used widely on 
the Internet for encrypting data sent from one computer to another. It is 
used for Internet banking and for electronic purchases using credit cards.
We will show how the encryption algorithm works with an example 
in which we want to share some confidential information with our bank 
and, at the same time, want to protect it from anyone who might be 
eavesdropping.
Impact of Quantum Computing 
173
When you want to communicate with the bank, you want to encrypt 
your data so that if it is intercepted it cannot be read. The actual encryption 
of the data is going to be done using a key that both you and the bank share 
to both encrypt and decrypt—this is called a symmetric key—and must be 
kept secret by both parties. The key is generated on your computer and sent 
to the bank, but, of course, we can’t just send the key without encrypting 
it. We need to encrypt the key that we are going to use to encrypt our com­
munication with the bank. This is where RSA encryption enters the picture. 
It is a way of securely sending the key to the bank.
To start the communication with the bank, your computer generates the 
key that will be used later for encryption and decryption for both you and 
the bank. We will call the key K.
The bank’s computer finds two large prime numbers that we will denote 
by p and q. The primes need to be roughly the same size and the product N 
= pq, called the modulus, should contain at least 300 digits using standard 
decimal numbers (1024 binary digits), which is currently considered large 
enough to ensure security. This is fairly straightforward. There are efficient 
ways of generating these primes and multiplying the two primes to get the 
modulus N is easy.
The second step is for the bank to find a relatively small number e that 
shares no common factors with either p −1 or q −1. This is also easy to 
compute. The bank keeps the primes p and q secret, but sends the numbers 
N and e.
Your computer takes the key K and raises it to the power e taking the 
remainder after dividing by N. Once more, this is easy to do. This is the 
number is called Ke mod N. This is then sent to the bank. The bank knows 
how to factor N into p and q, and this lets it quickly calculate K.
If someone is tapping into the communication, they will know N and e, 
both of which the bank sent, they will also know the number Ke mod N that 
you sent. To calculate K, the eavesdropper needs to know the factors p and 
q of N, but these are being kept secret. The security of the system depends 
on the fact that the eavesdropper will not be able to factor the number N 
to obtain p and q.
The question is how hard is it to factor a number that is the product of 
two large primes? The answer is that it seems hard. All of the other steps 
involved in RSA encryption can be performed with classical algorithms that 
take polynomial time, but nobody has discovered a classical algorithm that 
174 
Chapter 9
can factor a product of two large primes in polynomial time. But, on the 
other hand, nobody has a proof that such an algorithm doesn’t exist.
This is where Shor enters the picture. He constructed a quantum algo­
rithm that does factor a product of large prime numbers. The algorithm 
belongs to class BQP, which means that it works with bounded error in 
polynomial time. One thing that needs to be emphasized is that we are 
no longer talking about query complexity. We are not assuming that we 
can ask questions of an oracle. We are counting the total number of steps 
or, equivalently, the time needed to get from the beginning to the end of 
the computation. Shor is giving a concrete algorithm for each step. The 
fact that the algorithm belongs to BQP means that if it is implemented it 
becomes feasible to factor large numbers, and, more important, it means 
that if the quantum circuit can be actually constructed, then RSA encryp­
tion is no longer secure.
Shor’s Algorithm
Shor’s algorithm involves a significant amount of mathematics. We will just 
give a short and somewhat vague description of the quantum part.
An important part of the algorithm is a gate that is called the quantum 
Fourier transform gate. This can be thought of as a generalization of the Had­
amard gate. In fact, for one qubit the quantum Fourier transform gate is 
exactly H. Recall that we used a recursive formula that told us how to get 
from the matrix for H
n
⊗−1 to the matrix for H
n
⊗. Similarly, we can give a 
recursive formula for the quantum Fourier transform matrix. The major 
difference between H
n
⊗ and the quantum Fourier matrix is that the entries 
in the latter case are generally complex numbers—more specifically, they 
are complex roots of unity. Recall that the entries for H
n
⊗ are either 1 or 
−1. These are the two possible square roots of 1. When we look for fourth 
roots of 1 we again just get ±1 if we are using real numbers, but we get two 
other roots if we use the complex numbers. In general, 1 has n complex nth 
roots. The quantum Fourier transform matrix on n qubits involves all the 
2nth complex roots of unity.
Simon’s algorithm was based on the properties of H
n
⊗. It used interfer­
ence, the amplitudes were either 1 or −1, which meant that when we added 
terms, the kets either canceled or reinforced one another. Shor realized that 
a similar idea applied to the quantum Fourier matrix, only now the ampli­
tudes are given not just by 1 and −1, but also by all the 2nth complex roots 
Impact of Quantum Computing 
175
of unity, which means that we can detect more types of periods than just 
the ones that Simon’s algorithm considers.
Recall that we know the number N and want to factor it into the product 
of the two primes p and q. The algorithm chooses a number a satisfying 
1 <
<
a
N. It checks to see if a shares any factors with N, if it does we can 
deduce that a is a multiple of either p or q. From there it is easy to complete 
the factorization. If a does not share any factors with N, then we calcu­
late a mod(N), a2 mod(N), a3 mod(N), and so on, where ai mod(N) means 
calculate ai and then take the remainder when divided by N. Since these 
numbers are remainders, they will all be less than N. Consequently, this 
sequence of numbers will eventually repeat. There will be some number r 
such that a
N
a
N
rmod
mod
(
) =
(
). The number r can be thought of as the 
period, and it is this number that the quantum part of Shor’s algorithm 
computes. Once r has been found, classical algorithms can use this fact to 
determine the factors of N.
Well, that description was rather sketchy, but it gives some idea of how 
the quantum part of Shor’s algorithm works. The key part is that Simon’s 
algorithm for finding the secret string s can be generalized to find the 
unknown period r.
The algorithm actually has been implemented, but just for small num­
bers. In 2001, it was used to factor 15 and in 2012 it factored 21. Clearly, it 
is nowhere near factoring 300 digit numbers at the moment. But how long 
will it take before a circuit can be built for numbers of this size? It seems to 
be only a matter of time until the RSA encryption scheme will no longer 
be secure.
Over the years other methods of encryption have been developed, but 
Shor’s algorithm also works on many of these. It has become clear that 
we need to develop new cryptographic methods—and these new methods 
should be able to withstand not just classical attacks but also attacks by 
quantum computers.
Post-quantum cryptography is now an extremely active area, with new 
methods of encryption being developed. Of course, there is no reason why 
these have to use quantum computing. We just need the encrypted message 
to be able to withstand being broken by a quantum computer. But quantum 
ideas do give us ways of constructing secure codes.
We have seen two quantum key distribution (QKD) schemes that are 
secure: the BB84 and Ekert’s protocols. Several labs have succeeded in 
176 
Chapter 9
getting QKD systems up and running. There are also a few companies that 
offer QKD systems for sale. One of the first times that QKD was used in 
a real-world setting was in 2007, when ID Quantique set up a system to 
secure the transmission of votes between a counting station and Geneva’s 
main polling office during a Swiss parliamentary election.
Many countries are experimenting with small quantum networks using 
optic fiber. There is the potential of connecting these via satellite and being 
able to form a worldwide quantum network. This work is of great interest 
to financial institutions.
The most impressive results, so far, involve a Chinese satellite that is 
devoted to quantum experiments. It’s named Micius after a Chinese phi­
losopher who did work in optics. This is the satellite that was used for the 
quantum teleportation we mentioned in an earlier chapter. It has also been 
used for QKD. A team in China connected to a team in Austria—the first 
time that intercontinental QKD has been achieved. Once the connection 
was secured, the teams sent pictures to one another. The Chinese team 
sent the Austrians a picture of Micius, and the Austrians sent a picture of 
Schrödinger to the Chinese.
Grover’s Algorithm and Searching Data
We are entering the era of big data. Searching through enormous data sets 
efficiently is now a high priority for many major companies. Grover’s algo­
rithm has the potential to speed up data searches.
Lov Grover invented the algorithm in 1996. Like Deutsch’s and Simon’s 
algorithms, its speedup over classical algorithms is given in terms of query 
complexity. Of course, to implement the algorithm for real-world data 
searches, we don’t have oracles that can answer our questions. We have 
to construct an algorithm that does the work of the oracle. But before we 
begin to discuss how to implement Grover’s algorithm, we will look at what 
it does and how it does it.
Grover’s Algorithm
Imagine that you have four cards in front of you. They are all face down. 
You know that one of them is the ace of hearts, and this is the card you 
want to find. How many cards must you turn over until you know the loca­
tion of the ace of hearts?
Impact of Quantum Computing 
177
You might be lucky and turn it over on the first try, or you might be 
unlucky and turn over three cards, none of which is the ace. If you are 
unlucky and haven’t turned it over after three tries, then you know that 
the last card must be the ace. So, we know where the ace is after turning 
over between one and three cards. On average we have to turn over 2.25 
cards.
This problem is one that Grover’s algorithm tackles. Before we begin 
describing the algorithm, we will reword the problem. We have four binary 
strings: 00, 01, 10, and 11. We have a function f that sends three of these 
strings to 0 and the other one to 1. We want to find the find the binary 
string that is sent to 1. For example, we might have f 00
0
(
) =
, f 01
0
(
) =
, 
f 10
1
(
) = , and f 11
0
(
) =
. The problem now asks how many function evalua­
tions do we need to make before we find that f 10
1
(
) = . We are just restating 
the problem, wording it in terms of functions instead of cards, so we know 
the answer is the same as before: 2.25 on average.
As with all query complexity algorithms we construct an oracle—a gate 
that encapsulates the function. For our example, where we just have four 
binary strings, the oracle is given in figure 9.1.
The circuit for Grover’s algorithm is given in figure 9.2.
The algorithm has two steps. The first is to flip the sign of the prob­
ability amplitude connected to the location we are trying to find. The sec­
ond is to amplify this probability amplitude. We will show how the circuit 
does this.
After going through the Hadamard gates, the top two qubits will be in 
state
1
2
00
01
10
11
+
+
+
(
)
y
y
f (x0, x1)
x0
x1
x0
x1
F
2
2
Figure 9.1
The oracle for f.
178 
Chapter 9
and the bottom qubit will be in state
1
2
0
1
2
1
−
.
We can write the combined state as
1
2
00
1
2
0
1
2
1
01
1
2
0
1
2
1
⊗
−



+
⊗
−






  +
⊗
−



+
⊗
−





10
1
2
0
1
2
1
11
1
2
0
1
2
1
.
The qubits then pass through the F gate. This flips 0  and 1  of the third 
qubit in the location we are trying to find. If we use our example where 
f 10
1
(
) = , we obtain
1
2
00
1
2
0
1
2
1
01
1
2
0
1
2
1
⊗
−



+
⊗
−






  +
⊗
−



+
⊗
−





10
1
2
1
1
2
0
11
1
2
0
1
2
1
.
This can be written as
1
2
00
01
10
11
1
2
0
1
2
1
+
−
+
(
) ⊗
−



.
The result is that the top two qubits are not entangled with the bottom 
qubit, but we have flipped the sign of the probability amplitude of 10 , 
which corresponds to the location we are trying to find.
At this stage, if we were to measure the top two qubits we would get one 
of the four locations, with each of the four answers being equally likely. We 
F
2
H
2
2
2
A
2
H
1
00
Figure 9.2
Grover algorithm circuit.
Impact of Quantum Computing 
179
need another trick and that is amplitude amplification. Amplitude ampli­
fication works by flipping a sequence of numbers about their mean. If a 
number is above the mean, it is flipped below the mean. If a number is 
below the mean, it is flipped above the mean. In each case the distance to 
the mean is preserved. To illustrate, we use the four numbers 1, 1, 1, and 
−1. Their sum is 2, and so their mean is 2/4, which is equal to 1/2. We then 
go through the sequence of numbers. The first is 1. This is 1/2 above the 
mean. When we flip about the mean it becomes 1/2 below the mean. In 
this case it becomes 0. The number −1 is 3/2 below the mean. When we flip 
about the mean it becomes 3/2 above the mean, which is 2.
Our top two qubits are currently in the state
1
2 00
1
2 01
1
2 10
1
2 11
+
−
+
.
If we flip the probability amplitudes about the mean we get 
0 00
0 01
1 10
0 11
10
+
+
+
=
. When we measure this we will get 10 
with certainty, so flipping about the mean does exactly what we want. We 
just need to make sure that there is a gate or, equivalently, an orthogonal 
matrix that performs the flip about the mean. There is. It’s
A =
−
−
−
−












1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
.
When this gate acts on the top two qubits we get
A 1
2 00
1
2 01
1
2 10
1
2 11
1
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
+
−
+



=
−
−
−
−












−












=












=
1
1
1
1
0
0
1
0
10 .
For this example, where we just have two qubits, we only need to use the 
oracle once. We only need to ask one question. So, for n = 2, Grover’s algo­
rithm gives the answer with certainty with just one question, whereas the 
classical case takes 2.25 questions on average.
Exactly the same idea works for n qubits. We start by flipping the sign of 
the probability amplitude that corresponds to the location we are trying to 
find. Then we flip about the mean. However, the amplitude magnification 
180 
Chapter 9
is not as dramatic in general as in the case for just two qubits. For example, 
if we have eight numbers, seven of which are 1 and the other of which is 
−1. Their sum is 6, and so their mean is 6/8. When we flip about the mean, 
the 1s become 1/2s, and the −1 becomes 10/4. The consequence of this is 
that if we have three qubits, after performing the amplitude magnification, 
if we were to measure our qubits, we would get the location we are trying to 
find with higher probability than the other locations. The concern is that 
there is still some significant probability that we will get the wrong answer. 
We want a higher probability of getting the right answer—we want to mag­
nify the amplitude even more before we measure. The solution is that we 
send everything back through the circuit. We flip the sign of the probability 
amplitude associated with the location we are trying to find again and then 
perform the flip about the mean again.
Let’s look at the general case. We want to find something that could be 
in one of m possible locations. To find it classically we need to ask m −1 
questions in the worst-case scenario. The number of questions grows at the 
same rate as the size of m. Grover calculated a formula for the number of 
times you should use his circuit to maximize the chance of getting the cor­
rect answer. The number given by this formula grows at the rate m. This 
is a quadratic speedup.
Applications of Grover’s Algorithm
There are a number of problems with implementing the algorithm. The 
first is that the quadratic speedup is for query complexity. If we need to 
use an oracle, then we need to actually construct it, and if we are not 
careful the number of steps involved with the computation of the ora­
cle will outweigh the number of steps saved by the algorithm, resulting 
in the algorithm being slower rather than faster than the classical one. 
Another problem is that in calculating the speedup we are assuming that 
there is no underlying order to the data set. If there is structure, we can 
often find classical algorithms that exploit the structure and find the solu­
tion more quickly than randomly guessing. The last concern is about the 
speedup. Quadratic speedup is nothing like the exponential speedup we 
have seen with other algorithms. Can’t we do better? Let’s look at these 
concerns.
The concerns involving implementing the oracle and the structure of 
data sets are both valid and show that Grover’s algorithm is not going to 
Impact of Quantum Computing 
181
be practical for most database searches. But in certain cases the structure of 
the data can make it possible to construct an oracle that works efficiently. 
In these cases, the algorithm can give a speedup over classical algorithms. 
The question about whether we can do better than quadratic speedup has 
been answered. It has been proved that Grover’s algorithm is optimal. 
There is no quantum algorithm that can solve the problem with more 
than just quadratic speedup. Quadratic speedup, though not as impressive 
as exponential speedup, is useful. With massive data sets, any speedup can 
be valuable.
The main applications for Grover’s algorithm are probably not going to 
be for the algorithm as has been presented but for variations on it. In par­
ticular, the idea of amplitude amplification is a useful one.
We have only presented a few algorithms, but Shor’s and Grover’s are 
considered the most important. Many other algorithms have built upon 
the ideas in these two.* We now turn our attention from algorithms to 
other applications of quantum computing.
Chemistry and Simulation
In 1929, Paul Dirac wrote about quantum mechanics, saying, “The fun­
damental laws necessary for the mathematical treatment of a large part of 
physics and the whole of chemistry are thus completely known, and the 
difficulty lies only in the fact that the application of these laws leads to 
equations that are too complex to be solved.”
In theory, all of chemistry involves interactions of atoms and configu­
rations of electrons. We know the underlying mathematics—it’s quantum 
mechanics, but although we can write down the equations we cannot 
solve them exactly. In practice, chemists use approximation techniques 
instead of trying to find exact solutions. These approximations ignore fine 
details. Computational chemistry has taken this approach and, in general, 
it has worked well. Classical computers can give us good answers in many 
cases, but there are areas where the current computational techniques 
don’t work. The approximation is not good enough. You need the fine 
details.
*  The online Quantum Algorithm Zoo, found at https://math.nist.gov/quantum/
zoo/, aims to provide a comprehensive catalog of all quantum algorithms.
182 
Chapter 9
Feynman thought that one of the main applications of quantum com­
puters would be to simulate quantum systems. Using quantum computers 
to study chemistry that belongs to the quantum world is a natural idea 
that has great potential. There are a number of areas where it is hoped that 
quantum computing will make important contributions. One of these is to 
understand how an enzyme, nitrogenase, used to make fertilizers actually 
works. The current method of producing fertilizers releases a significant 
amount of greenhouse gases and consumes considerable energy. Quantum 
computers could play a major role in understanding this and other catalytic 
reactions.
There is a group at the University of Chicago that is looking into pho­
tosynthesis. The transfer of sunlight to chemical energy is a process that 
happens quickly and very efficiently. It is a quantum mechanical process. 
The long-term goal is to understand this process and then use it in photo­
voltaic cells.
Superconductivity and magnetism are quantum mechanical phenom­
ena. Quantum computers may help us understand them better. One goal is 
to develop superconductors that don’t need to be cooled to near absolute 
zero.
The actual construction of quantum computers is in its infancy, but even 
with a few qubits it is possible to begin studying chemistry. IBM recently 
simulated the molecule beryllium hydride (BeH2) on a seven-qubit quan­
tum processor. This is a relatively small molecule with just three atoms. The 
simulation does not use the approximations that are used in the classical 
computational approach. However, since IBM’s processor uses just a few 
qubits, it is possible to simulate the quantum processor using a classical 
computer. Consequently, everything that can be done on this quantum 
processor can be done classically. However, as processors incorporate more 
qubits we get to the point where it is no longer possible to simulate them 
classically. We will soon be entering a new era when quantum simulations 
are beyond the power of any classical computer.
Now that we have seen some of the possible applications, we will briefly 
survey some of the ways that are being used to build quantum computers.
Hardware
To actually make practical quantum computers you need to solve a num­
ber of problems, the most serious being decoherence—the problem of your 
Impact of Quantum Computing 
183
qubit interacting with something from the environment that is not part 
of the computation. You need to set a qubit to an initial state and keep it 
in that state until you need to use it. You also need to be able to construct 
gates and circuits. What makes a good qubit?
Photons have the useful properties of being easy to initialize and easy to 
entangle, and they don’t interact very much with the environment, so they 
stay coherent for long times. On the other hand, it is difficult to store pho­
tons and have them ready when you need them. The properties of photons 
make them ideal for communication, but they are more problematic for 
building quantum circuits.
We have often used electron spin as an example. Can this be used? Ear­
lier we mentioned the apparatus used in the loophole-free Bell test. It used 
electrons trapped in synthetic diamonds. These are manipulated by shining 
lasers on them. The problem has been scaling. You can construct one or 
two qubits but, at the moment, it is not possible to generate large numbers. 
Instead of using electrons, spins of the nucleus have also been tried, but 
scalability is again the problem.
Another method uses the energy levels of ions. Ion-trap computing uses 
ions that are held in position by electromagnetic fields. To keep the ions 
trapped vibrations must be minimized; cooling everything to near abso­
lute zero does this. The ions’ energy levels encode the qubits and lasers 
can manipulate these. David Wineland used ion traps to construct the 
first CNOT gate in 1995, for which he received a Nobel Prize, and in 2016 
researchers at NIST entangled more than 200 beryllium ions. Ion-traps do 
have potential to be used in future quantum computers, but a number of 
computers are being constructed using a different approach.
To minimize the interaction of quantum computers with the environ­
ment, they are always protected from light and heat. They are shielded 
against electromagnetic radiation, and they are cooled. One thing that can 
happen in cold places is that certain materials become superconductors—
they lose all electrical resistance—and superconductors have quantum 
properties that can be exploited. These involve things called Cooper pairs 
and Josephson junctions.
The electrons in a superconductor pair up, forming what are called Coo­
per pairs. These pairs of electrons act like individual particles. If you sand­
wich thin layers of a superconductor between thin layers of an insulator, 
184 
Chapter 9
you obtain a Josephson junction.** These junctions are now used in physics 
and engineering to create sensitive instruments for measuring magnetic 
fields. For our purposes, the important fact is that the energy levels of the 
Cooper pairs in a superconducting loop that contains a Josephson junction 
are discrete and can be used to encode qubits.
IBM uses superconducting qubits in its quantum computers. In 2016, 
IBM introduced a five-qubit processor that they have made available to 
everyone for free on the cloud. Anyone can design their own quantum 
circuit, as long as it uses five or fewer qubits, and run it on this computer. 
IBM’s aim is to introduce quantum computing to a wide audience—circuits 
for superdense coding, for Bell’s inequality, and a model of the hydrogen 
atom have all been run on this machine. A primitive version of Battle­
ships has also been run, giving the coder the claim of constructing the first 
quantum computer multiplayer game. At the end of 2017, IBM connected 
a twenty-qubit computer to the cloud. This time it is not for education, but 
it is a commercial venture where companies can buy access.
Google is working on its quantum computer. It also uses superconduct­
ing qubits. Google is expected to announce in the near future that it has a 
computer that uses 72 qubits. What is special about this number?
Classical computers can simulate quantum computers if the quantum 
computer doesn’t have too many qubits, but as the number of qubits 
increases we reach the point where that is no longer possible. Google is 
expected to announce that it has reached or exceeded this number, giving 
them the right to claim quantum supremacy—the first time an algorithm 
has been run on a quantum computer that is impossible to run, or simulate, 
on a classical computer. IBM, however, is not giving up without a fight. Its 
team, using some innovative ideas, has recently found a way to simulate a 
56-qubit system classically, increasing the lower bound on the number of 
qubits needed for quantum supremacy.
As work continues on building quantum computers, we are likely to see 
spinoffs into other areas. Qubits, however we encode them, are sensitive to 
interactions with their surroundings. As we understand these interactions 
better we will be able to build better shields to protect our qubits, but we 
will also be able to design ways our qubits can measure their surroundings. 
**  Brian David Josephson received the Nobel Prize in physics for his work on how 
Cooper pairs can flow through a Josephson junction by quantum tunneling.
Impact of Quantum Computing 
185
An example involves electrons trapped in synthetic diamonds. These are 
very sensitive to magnetic fields. NVision Imaging Technologies is a startup 
that is using this idea to build NMR machines that they hope will be better, 
faster, and cheaper than current ones.
Quantum Annealing
D-Wave has computers for sale. Their latest, the D-Wave 2000Q has, as you 
might guess from its name, 2,000 qubits. However, their computers are not 
general purpose, they are designed for solving certain optimization prob­
lems using quantum annealing. We will give a brief description of this.
Blacksmiths often need to hammer metal and bend metal. In the pro­
cess, it can become hardened—various stresses and deformities occur in 
the crystal structure—making it hard to work. Traditional annealing is a 
method of restoring the uniform crystal structure, making the metal mal­
leable once more. It’s done by heating the piece of metal to a high tempera­
ture and then letting it slowly cool.
Simulated annealing is a standard technique, based on annealing, that 
can be used for solving certain optimization problems. For example, sup­
pose we have the graph given in figure 9.3 and want to find the lowest 
point—the absolute minimum. Think of the graph as being the bottom of 
a two-dimensional bucket. We drop a ball bearing into the bucket. It will 
settle at the bottom of one of the valleys. These are labeled A, B, and C in 
the figure. We want to find C. The ball bearing may not land at the bottom 
A
B
C
Figure 9.3
Graph of function—bottom of bucket.
186 
Chapter 9
of C, but instead it might end up at the bottom of valley at A. The impor­
tant observation in annealing is that the energy required to push the ball 
bearing up the hill and let it drop into valley B is much less than the energy 
needed to push the ball bearing up from B and let it drop into A. So, we 
shake the bucket with an energy level between these two values. The ball 
can move from A to B, but it cannot move back. After a while of shaking at 
this level, it will end up either at the bottom of A or B. But shaking at this 
level can send the ball from C to B. The next step is to shake it again, but 
less energetically, with enough energy to get it up the peak from B to C, but 
not enough to let it get back from C to B.
In practice, you start shaking and gradually reduce the energy. This cor­
responds to gradually cooling your piece of metal in traditional annealing. 
The result is that the ball bearing ends up at the lowest point. You have 
found the absolute minimum of the function.
Quantum annealing adds quantum tunneling. This is a quantum effect 
where the ball bearing can just appear on the other side of a hill. Instead of 
going over, it can go through. Instead of reducing the heights of hills the 
ball can climb, you reduce the length of the tunnels it can tunnel through.
D-Wave has produced a number of commercially available computers 
that use quantum annealing for optimization problems. Initially, they 
were met with some skepticism about whether the computers actually used 
quantum tunneling, but now it is generally agreed that they do. There is 
still some question of whether the computers are faster than classical ones, 
but people are buying. Volkswagen, Google, and Lockheed Martin, among 
others, have all bought D-Wave machines.
After this brief look at hardware, we turn to deeper questions. What does 
quantum computation tell us about us, the universe, and what computa­
tion is at its most fundamental level?
Quantum Supremacy and Parallel Universes
There are 8 possible three-bit combinations: 000, 001, 010, 011, 100, 101, 
110, 111. The number 8 comes from 23. There are two choices for the first 
bit, two for the second and two for the third, and we multiply these three 
2s together. If instead of bits we switch to qubits, each of these 8 three-bit 
strings is associated with a basis vector, so the vector space is 8-dimensional. 
Exactly the same analysis tells us that if we have n qubits, then we will have 
Impact of Quantum Computing 
187
2n basis vectors, and the space will be 2n-dimensional. As the number of 
qubits grows, the number of basis vectors grows exponentially, and things 
quickly get big.
If we have 72 qubits, the number of basis elements is 272. This is about 
4,000,000,000,000,000,000,000. It is a large number and is considered to be 
around the point at which classical computers cannot simulate quantum 
computers. Once quantum computers have more than 72 or so qubits we 
will enter the age of quantum supremacy—when quantum computers can 
do computations that are beyond the ability of any classical computer. As 
we mentioned earlier, it is expected that Google is about to announce that 
this age has been reached. (D-Wave has 2,000 qubits in its latest computer. 
However, this specialized machine has not been able to do anything that 
cannot be done by a conventional computer, so it hasn’t broken the quan­
tum supremacy barrier.)
Let’s consider a machine with 300 qubits. This doesn’t seem an unrea­
sonable number for the not too distant future. But 2300 is an enormous 
number. It’s more than the number of elementary particles in the known 
universe! A computation using 300 qubits would be working with 2300basis 
elements. David Deutsch asks where computations like this, which involve 
more basis elements than there are particles in the universe, are done. He 
believes that we need to introduce parallel universes, each collaborating 
with one another.
This view of quantum mechanics and parallel universes goes back to 
Hugh Everett. Everett’s idea is that, whenever we make a measurement, the 
universe splits into several copies, each containing a different outcome. 
Though this is distinctly a minority view, Deutsch is a firm believer. His 
paper in 1985 is one of the foundational papers in quantum computing, 
and one of Deutsch’s goals with this work was to make a case for parallel 
universes. He hopes that one day that there will be a test, analogous to Bell’s 
test, that will confirm this interpretation.
Computation
Alan Turing is one of the fathers of the theory of computation. In his land­
mark paper of 1936 he carefully thought about computation. He consid­
ered what humans did as they performed computations and broke it down 
188 
Chapter 9
to its most elemental level. He showed that a simple theoretical machine, 
which we now call a Turing machine, could carry out any algorithm. Tur­
ing’s theoretical machines evolved into our modern day computers. They 
are universal computers. Turing’s analysis showed us the most elemental 
operations. These involve the manipulation of bits. But remember, Turing 
was analyzing computation based on what humans do.
Fredkin, Feynman, and Deutsch argue that the universe does 
computations—that computations are part of physics. With quantum com­
putation the focus changes from how humans compute to how the uni­
verse computes. Deutsch’s 1985 paper should also be seen as a landmark 
paper in the theory of computation. In it, he showed that the fundamental 
object is not the bit, but the qubit.
We have seen that we will soon reach the point of quantum supremacy; 
that we will have quantum computers that no classical computers will be 
able to simulate, but what about the converse? Can quantum computers 
simulate classical computers? The answer is that they can. Any classical 
computation can be done on a quantum computer. Consequently, quan­
tum computation is more general than classical computation. Quantum 
computations are not a strange way of doing a few special calculations; 
rather, they are a new way of thinking about computation as a concept. We 
shouldn’t think of quantum and classical computation as two distinct sub­
jects. Computation is really quantum computation. Classical computations 
are just special cases of quantum ones.
In this light, classical computation seems an anthropocentric version 
of what computation really is. Just as Copernicus showed that the Earth 
wasn’t the center of the universe and Darwin showed that humans evolved 
from other animals, we are now beginning to see that computations are 
not centered on humans. Quantum computing represents a true paradigm 
shift.
I am not suggesting that classical computing is going to become obso­
lete, but it will become accepted that there is a more fundamental level of 
computing, and the most elemental level of computing involves qubits, 
entanglement, and superpositions. At the moment, the focus is on showing 
that certain quantum algorithms are faster than classical ones, but this will 
change. Quantum physics has been around longer than quantum computa­
tion. It’s now accepted as its own subject. Physicists don’t try to compare 
quantum physics with classical physics and hope to show that it is in some 
Impact of Quantum Computing 
189
way better. They study quantum physics in its own right. The same shift 
will happen with quantum computation. We have been given new tools 
that change the way we study computation. We will use them to experi­
ment and see what new things we can construct. This has started with tele­
portation and superdense coding, and it will continue.
We are entering a new era, with a new way of thinking about what 
computation really is. What we are going to discover is impossible to say, 
but now is the time for exploration and innovation. The greatest years for 
quantum computation are ahead of us.
Index
I
n
d
e
x
I
n
d
e
x
© Massachusetts Institute of TechnologyAll Rights Reserved
Adder, 102
Adleman, Leonard, 172
Algorithms
Deutsch, 145–149
Deutsch-Jozsa, 152–157
Grover, 176–181
Shor, 174–176
Simon, 157–168
Amplitude amplification, 179
Ancilla bit, 108
BB84 protocol, 53–55
Bell, John Stewart, 72, 76, 84
Bell basis, 128
Bell circuit, 127
Bell’s inequality, 79–84
Bell test, loophole-free, 66, 183
Bennett, Charles, 54
Billiard ball computer, 90, 111–115
Bit
ancilla, 108
classical, 1
garbage, 109
quantum, 1, 50
Bitwise addition, 157
Black box, 145
Bohm, David, 76
Bohmian mechanics, 85
Bohr, Niels, 1, 71, 76, 82, 84
Boole, George, 89, 90
Boolean algebra, 91–93
Boolean function, 94
Bounded quantum polynomial. See 
Complexity classes, BQP
Bra, 19
Bra-ket
length of vectors, 24
notation, 23
orthogonality of vectors, 24–25
Brassard, Gilles, 54
Chemistry. See Computational 
chemistry
Circuit, 99
Classical bit, 1
Classical mechanics, 9–11
Clauser, John, 82
Clay Mathematics Institute, 144
Cloning, 124
Complexity classes
BPP, 167
BQP, 167
EQP, 166
NP, 142–144
P, 142–144
QP, 166
Complex numbers, 17–18, 35, 
38
Computational chemistry, 181–182
Cooper pair, 183
Copenhagen interpretation, 71
Cryptanalysis, 172
192 
Index
Darwin, Charles, 188
Database search, 181
Decoherence, 182
Deutsch, David, 85, 145, 169, 187
Dimension, 19, 38
Dirac, Paul, 17, 19, 181
D-Wave, 185–186
Einstein, Albert, 69, 71, 75–77, 82
Ekert, Artur, 86
Ekert protocol, 86–87
Encryption, RSA, 172–173
Entanglement, 59
Entropy, 105
EPR paradox, 76–77
Equivalent state vectors, 41–42
Error correction, 135–140
Everett, Hugh, 187
Exact quantum polynomial. See 
Complexity classes, EQP
Exponential time, 143
Fan-out, 100
Feynman, Richard, 89, 115, 182
Flip about the mean, 179
Flip-flop, 103
Fredkin, Edward, 89, 111, 115
Freedman, Stuart, 82
Function
balanced, 146
constant, 146
Functional completeness, 94–96
Garbage bit, 109
Gate
AND, 98
controlled not (CNOT), 67, 105
Fredkin, 109
Hadamard, 122
NAND, 99
NOT, 98
OR, 99
Pauli, 121
quantum, 117, 120
reversible, 102
switch, 111
Toffoli, 107
universal, 101, 108, 110, 123
XOR, 102
Gerlach, Walther, 1
Google, 184
Gravity, 75–76
Grover, Lov, 176
Half-adder, 102–103
Hidden variables, 11, 71, 77–78
IBM, 182, 184
ID Quantique, 176
Interference, 52–53, 174
Ion-trap, 183
Josephson junction, 183
Ket, 19
Kronecker product, 149–152
Landauer limit, 105
Linear algebra toolbox, 35
Linearly independent, 165
Linear superposition, 50
Local realism, 71, 75–76
Logical equivalence, 93
Many-worlds view, 85
Matrix, 30
Hadamard, 159–160
identity, 32
Kronecker product, 149–152
main diagonal, 32
multiplication, 31, 32
not commutative, 32
orthogonal, 34
square, 32
transpose, 31
unitary, 34
Index 
193
Micius, 176
Millennium Prize, 144
NMR machine, 185
No cloning theorem, 124–126, 134, 138
Non-commutative operation, 32, 58
NVision Imaging Technologies, 185
Oracle, 145
Ordered basis, 29
Orthonormal basis, 25
Parallelogram law, 22
Parallel universes, 187
Parity check, 137–139
Pauli, Wolfgang, 121
Pauli transformations, 121
Peirce, Charles Sanders, 97
Petzold, Charles, 101
Photosynthesis, 182
Podolsky, Boris, 76
Polarization, 11–15
Polarized filters, 12–15
Polynomial time, 142–144
Post-quantum cryptography, 175
Probability, 37–38
Probability amplitude, 29, 39, 52
Pseudorandom numbers, 15
P versus NP, 144
Pythagorean theorem, 20
Quadratic speedup, 180
Quantized spin, 15
Quantum annealing, 185–186
Quantum bit, 1, 49–50
Quantum bit-flip correction, 137–140
Quantum clock, 6, 14, 68–69, 78
Quantum Fourier transform, 174
Quantum key distribution (QKD), 53, 
86, 175–176
Quantum parallelism, 141, 168–169
Quantum speedup, 141
Quantum supremacy, 184, 186–188
Quantum teleportation, 132–135, 
176
Quantum tunneling, 186
Qubit. See Quantum bit
Query complexity, 145
Randomness, 10–11
Relative phase, 122
Repetition code, 136–137
Reversible gate, 102–108
Rivest, Ronald, 172
Roots of unity, 174
Rosen, Nathan, 76
RSA encryption, 172–174
RSA Laboratories, 143
Scalar multiplication, 21
Schrödinger, Irwin, 71, 85, 176
Schrödinger equation, 85
Sensitive dependence on initial 
conditions, 11, 77
Shamir, Adi, 172
Shannon, Claude, 89, 98, 105
Sheffer, Henry, 97
Sheffer stroke, 97
Shor, Peter, 144, 170, 172
Simon, Daniel, 169
Spin, 3
Spin state, 39
Spontaneous parametric 
down-conversion, 65
Spooky action at a distance, 69, 76–77
Standard basis, 26, 35
State, 39
Stern, Otto, 1
Stern-Gerlach apparatus, 2
Superdense coding, 129–132
Superluminal communication, 62–64
Teleportation. See Quantum 
teleportation
Tensor product, 57–58
Theory of relativity, 62, 76
194 
Index
Toffoli, Tommaso, 107
Truth tables
and, 90–91
exclusive or, 91
inclusive or, 91
Nand, 96
negation (not), 90
Turing, Alan, 171, 187–188
Turing machine, 188
Universal gate (classical), 101, 108, 110, 
123
Universal quantum gate, 123–124
Vector, 19
addition, 21–22
dot product, 24
length, 20, 30
linear combination, 28
orthogonal, 23
space, 38
state, 39
tensor product, 57–58
unit, 20
Wineland, David, 183